{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TGLs8-tCVnHg",
        "079tQIsukGEt",
        "7Qn60SWsSQD9",
        "vB6PP0enPpMx",
        "gsx_1C1PpsRx",
        "duhBEROv2rcb",
        "t96JCZrR4Ory",
        "mHFseZC0wEXH",
        "w_2sEW1Guy5-",
        "SnCzrLICYbWf",
        "Mptw5z72zfPG",
        "3XRLm8J7Zd27",
        "lCXdVkbcqRO3",
        "V4sbAQwnPNNA",
        "uQhCxXgBKC74",
        "kD8fcfg0p7tn",
        "fNMuahN8qNzy",
        "c07ISyoYwIsS",
        "dyxTr_Y-wB37",
        "0L_sA77WDvLK",
        "f60C_3zKfpNx"
      ],
      "authorship_tag": "ABX9TyNg1i8QO+OR2SPsi22i0E1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/realdope/math/blob/main/linalg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\require{color}\n",
        "\\newcommand{\\arr}[1]{\\overset{\\small #1}{‚Üí}}\n",
        "\\newcommand{\\larr}[1]{\\overset{\\small #1}{‚ü∂}}\n",
        "\\newcommand{\\imply}[1]{\\overset{\\small #1}{‚áí}}\n",
        "\\newcommand{\\red}[1]{\\color{OrangeRed}{#1}}\n",
        "\\newcommand{\\blue}[1]{\\color{RoyalBlue}{#1}}\n",
        "\\newcommand{\\green}[1]{\\color{ForestGreen}{#1}}\n",
        "\\newcommand{\\purple}[1]{\\color{DarkMagenta}{#1}}\n",
        "\\newcommand{\\dg}[1]{\\color{darkgray}{#1}}\n",
        "\\newcommand{\\E}{\\text{ùîº}}\n",
        "\\newcommand{\\V}{\\text{ùïç}}\n",
        "\\newcommand{\\Var}{\\small\\text{Var}\\normalsize}\n",
        "\\newcommand{\\Cov}{\\small\\text{Cov}\\normalsize}\n",
        "\\newcommand{\\Corr}{\\small\\text{Corr}\\normalsize}\n",
        "\\newcommand{\\Bias}{\\small\\text{Bias}\\normalsize}\n",
        "\\newcommand{\\ob}[2]{\\overbrace{#1}^{\\small#2}}\n",
        "\\newcommand{\\ub}[2]{\\underbrace{#1}_{\\small#2}}\n",
        "\\newcommand{\\m}[1]{\\mathcal{#1}}\n",
        "\\newcommand{\\M}[1]{\\mathscr{#1}}\n",
        "\\newcommand{\\e}[1]{\\small{\\exp}\\normalsize\\left\\{#1\\right\\}}\n",
        "\\newcommand{\\t}[1]{\\text{#1}}\n",
        "\\newcommand{\\/}[2]{\\frac{#1}{#2}}\n",
        "\\newcommand{\\v}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\n}[1]{\\left\\lVert#1\\right\\rVert}\n",
        "\\newcommand{\\abs}[1]{\\left|#1\\right|}\n",
        "\\newcommand{\\vn}[1]{\\lVert\\mathbf{#1}\\rVert}\n",
        "\\newcommand{\\Im}{\\small\\text{Im}\\normalsize}\n",
        "\\newcommand{\\dom}{\\small\\text{dom}\\normalsize}\n",
        "\\newcommand{\\epi}{\\small\\text{epi}\\normalsize}\n",
        "\\newcommand{\\null}{\\small\\text{null}\\normalsize}\n",
        "\\newcommand{\\span}{\\small\\text{span}\\normalsize}\n",
        "\\newcommand{\\rk}{\\small\\text{rk}\\normalsize}\n",
        "\\newcommand{\\det}{\\small\\text{det}\\normalsize}\n",
        "\\newcommand{\\ln}{\\small\\text{ln}\\normalsize}\n",
        "\\newcommand{\\dim}{\\small\\text{dim}\\normalsize}\n",
        "\\newcommand{\\tr}{\\small\\text{tr}\\normalsize}\n",
        "\\newcommand{\\diag}{\\small\\text{diag}\\normalsize}\n",
        "\\newcommand{\\BPM}{\\small\\begin{pmatrix}}\n",
        "\\newcommand{\\EPM}{\\end{pmatrix}\\normalsize}\n",
        "\\newcommand{\\BVM}{\\begin{vmatrix}}\n",
        "\\newcommand{\\EVM}{\\end{vmatrix}\\normalsize}\n",
        "\\newcommand{\\BSM}{\\small\\begin{bmatrix}}\n",
        "\\newcommand{\\ESM}{\\end{bmatrix}\\normalsize}\n",
        "\\newcommand{\\BM}{\\small\\left[\\begin{smallmatrix}}\n",
        "\\newcommand{\\EM}{\\end{smallmatrix}\\right]\\normalsize}\n",
        "\\newcommand{\\BC}{\\small\\begin{cases}}\n",
        "\\newcommand{\\EC}{\\end{cases}\\normalsize}\n",
        "\\newcommand{\\Normal}{\\mathcal{N}}\n",
        "\\newcommand{\\liml}{\\lim\\limits}\n",
        "\\newcommand{\\suml}{\\sum\\limits}\n",
        "\\newcommand{\\prodl}{\\prod\\limits}\n",
        "$$\n",
        "# Colley: Vectors"
      ],
      "metadata": {
        "id": "TGLs8-tCVnHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectors**:\n",
        "\n",
        "- $\\v{a}$ and $\\v{b}$ are equal if $a_i=b_i$ for all $i$.\n",
        "\n",
        "- $\\v{c}=\\v{a}+\\v{b}$ is the sum such that $c_i=a_i+b_i$.\n",
        "  - $\\v{a}+\\v{b}=\\v{b}+\\v{a}$ (commutativity)\n",
        "  - $\\v{a}+(\\v{b}+\\v{c})=(\\v{a}+\\v{b})+\\v{c}$ (associativity)\n",
        "  - $\\v{0}$ is zero vector such that $\\v{a}+\\v{0}=\\v{a}$.\n",
        "\n",
        "- $\\v{b}=k\\v{a}$ is scalar multiplication such that $b_i=ka_i$.\n",
        "  - $(k+l)\\v{a}=k\\v{a}+l\\v{a}$ (distributivity)\n",
        "  - $k(\\v{a}+\\v{b})=k\\v{a}+k\\v{b}$ (distributivity)\n",
        "  - $k(l\\v{a})=(kl)\\v{a}=l(k\\v{a})$.\n",
        "\n",
        "- for points $P_1$ and $P_2$, $\\v{c}=\\overrightarrow{P_1P_2}$ is displacement vector such that $c_i=b_i-a_i$.\n",
        "\n",
        "- $\\v{i}=(1,0)$ and $\\v{j}=(0,1)$ are standard basis vectors.\n",
        "\n",
        "**Lines**: $x$ and $y$ are described as functions of parameter $t$. E.g., $\\BC\n",
        "x=2\\cos t\\\\\n",
        "y=2\\sin t\\EC$ where $0‚â§t< 2œÄ$ describes a circle.\n",
        "\n",
        "- Vector parametric equation: A line going through point $\\v{a}$ parallel to direction $\\v{b}$ is described by $\\v{r}(t)=\\v{a}+t\\v{b}$.\n",
        "\n",
        "- Examples:\n",
        "  - Find where line $\\BC\n",
        "  x=t+5\\\\\n",
        "  y=-2t-4\\\\\n",
        "  z=3t+7\n",
        "  \\EC$ intersects plane $3x+2y-7z=2$.\n",
        "  Solve $3(t+5)+2(-2t-4)-7(3t+7)=2$.\n",
        "  - Determine whether $\\BC\n",
        "  x=t+1\\\\\n",
        "  y=5t+6\\\\\n",
        "  z=-2t\\EC$ intersects with $\\BC\n",
        "  x=3t-3\\\\\n",
        "  y=t\\\\\n",
        "  z=t+1\\EC$.\n",
        "  Solve $\\BC\n",
        "  t_1+1=3t_2-3\\\\\n",
        "  5t_1+6=t_2\\\\\n",
        "  -2t_1=t_2+1\\EC$."
      ],
      "metadata": {
        "id": "oPWgarkPFNNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dot product**: $\\v{a}^‚ä§\\v{b}=\\sum_ia_ib_i$ produces a scalar representing the **magnitude of projecting $\\v{a}$ onto $\\v{b}$ multiplied by $\\vn{b}_2$**.\n",
        "\n",
        "- Properties\n",
        "  - $\\v{a}^‚ä§\\v{a}‚â•0$, and $\\v{a}^‚ä§\\v{a}=0$ iff $\\v{a}=\\v{0}$.\n",
        "  - $\\v{a}^‚ä§\\v{b}=\\v{b}^‚ä§\\v{a}$.\n",
        "  - $\\v{a}^‚ä§(\\v{b}+\\v{c})=\\v{a}^‚ä§\\v{b}+\\v{a}^‚ä§\\v{c}$\n",
        "  - $(k\\v{a})^‚ä§\\v{b}=k(\\v{a}^‚ä§\\v{b})=\\v{a}^‚ä§(k\\v{b})$\n",
        "\n",
        "- **Norm** or **magnitude** $\\vn{a}_2=\\sqrt{\\sum_ia_i^2}=\\sqrt{\\v{a}‚ãÖ\\v{a}}$\n",
        "  - $\\v{a}‚ãÖ\\v{a}=\\vn{a}_2^2$\n",
        "  - $\\n{k\\v{a}}_2=|k|\\vn{a}_2$\n",
        "  - $\\/{\\v{a}}{\\vn{a}_2}$ is a unit vector in the direction of non-zero $\\v{a}$.\n",
        "  \n",
        "- $\\red{\\v{a}^‚ä§\\v{b}=\\vn{a}_2\\vn{b}_2\\cos Œ∏}$\n",
        "\n",
        "  - Proof: the law of cosines for triangle with sides $\\v{a}$, $\\v{b}$, and $\\v{c}$ states\n",
        "  $\\vn{c}^2=\\vn{a}^2+\\vn{b}^2-2\\vn{a}\\vn{b}\\cos Œ∏_{a,b}$.\n",
        "  Let $\\v{c}=\\v{b}-\\v{a}$ then\n",
        "  $2\\vn{a}_2\\vn{b}_2\\cos Œ∏=\\vn{a}_2^2+\\vn{b}_2^2-\\n{\\v{b}-\\v{a}}_2^2$\n",
        "  $=\\v{a}‚ãÖ\\v{a}+\\v{b}‚ãÖ\\v{b}-(\\v{b}-\\v{a})‚ãÖ(\\v{b}-\\v{a})$\n",
        "  $=(\\v{a}‚ãÖ\\v{a}+\\v{b}‚ãÖ\\v{b}-\\v{b}‚ãÖ\\v{b}-\\v{a}‚ãÖ\\v{a})+2\\v{a}‚ãÖ\\v{b}$.\n",
        "\n",
        "  - $\\red{\\cosŒ∏=\\/{\\v{a}^‚ä§\\v{b}}{\\vn{a}_2\\vn{b}_2}}$ is the angle between $\\v{a}$ and $\\v{b}$.\n",
        "\n",
        "  - Perpendicular: $0=\\cos(\\/{œÄ}{2})$. Therefore $\\v{a}\\perp\\v{b}‚áí\\v{a}‚ãÖ\\v{b}=0$.\n",
        "\n",
        "  - Let $\\tilde{\\v{a}}=\\/{\\v{a}}{\\vn{a}_2}$ and $\\tilde{\\v{b}}=\\/{\\v{b}}{\\vn{b}_2}$ be unit vectors. Then $\\tilde{\\v{a}}^‚ä§\\tilde{\\v{b}}=\\cos(Œ∏)$. The magnitude of projection of one unit vector onto another is $\\cos(Œ∏)$.\n",
        "\n",
        "- $\\red{\\t{proj}_{\\v{a}}\\v{b}=\\/{\\v{a}^‚ä§\\v{b}}{\\vn{a}_2^2}\\v{a}=\\/{\\v{aa}^‚ä§}{\\vn{a}_2^2}\\v{b}}$ is the projection of $\\v{b}$ onto $\\v{a}$.\n",
        "For unit vector $\\hat{\\v{a}}$, we have $\\t{proj}_\\hat{\\v{a}}\\v{b}=(\\hat{\\v{a}}^‚ä§\\v{b})\\hat{\\v{a}}$\n",
        "\n",
        "  - Proof: $\\t{proj}_{\\v{a}}\\v{b}$\n",
        "  $=(\\blue{\\vn{b}_2\\cos Œ∏})\\/{\\v{a}}{\\vn{a}_2}$\n",
        "  $=\\/{\\vn{a}_2\\vn{b}_2\\cos Œ∏}{\\vn{a}_2}\\/{\\v{a}}{\\vn{a}_2}$\n",
        "  $=\\/{\\v{a}‚ãÖ\\v{b}}{\\vn{a}_2}\\/{\\v{a}}{\\vn{a}_2}$\n",
        "\n",
        "  - $\\t{proj}_\\v{a}\\v{b}=\\v{a}^‚ä§\\v{b}\\/{\\v{a}}{\\vn{a}_2^2}$. Vectors $\\v{a},\\v{b}$ radiate from the origin, then **dot product $\\v{a}^‚ä§\\v{b}$ is the magnitude of the projection of $\\v{b}$ onto $\\v{a}$ multiplied by $\\vn{a}_2$**.\n",
        "\n",
        "- **Cauchy-Schwarz inequality**: $\\red{|\\v{x}^‚ä§\\v{y}|‚â§\\vn{x}_2\\vn{y}_2}‚áí-\\vn{x}_2\\vn{y}_2‚â§\\v{x}^‚ä§\\v{y}‚â§\\vn{x}_2\\vn{y}_2$ is a tight upperbound. I.e., $\\sup\\{\\v{x}^‚ä§\\v{y}\\}=\\vn{x}_2\\vn{y}_2$.\n",
        "\n",
        "  - Proof: $\\v{x}^‚ä§\\v{y}=\\vn{x}_2\\vn{y}_2\\cosŒ∏$.\n",
        "  Then $\\sup\\{\\v{x}^‚ä§\\v{y}\\}=\\vn{x}_2\\vn{y}_2$.\n",
        "\n",
        "  - $\\red{(\\v{x}^‚ä§\\v{y})^‚ä§(\\v{x}^‚ä§\\v{y})‚â§(\\v{x}^‚ä§\\v{x})(\\v{y}^‚ä§\\v{y})}$\n",
        "\n",
        "  - $\\red{(\\sum_ix_iy_i)^2‚â§(\\sum_ix_i^2)(\\sum_iy_i^2)}$\n",
        "\n",
        "- **H√∂lder inequality**: $\\red{|\\v{x}^‚ä§\\v{y}|‚â§\\vn{x}_p\\vn{y}_q}‚áí-\\vn{x}_p\\vn{y}_q‚â§\\v{x}^‚ä§\\v{y}‚â§\\vn{x}_p\\vn{y}_q$ is generalized Cauchy-Schwarz, and is also tight upperbound.\n",
        "\n",
        "  - $p$ and $q$ are H√∂lder conjugates satisfying $\\/{1}{p}+\\/{1}{q}=1$. E.g., $\\vn{x}_2\\vn{y}_2$ or $\\vn{x}_1\\vn{y}_‚àû$.\n",
        "\n",
        "- **Triangle (Minkowski) inequality**: $\\n{\\v{x}+\\v{y}}_2‚â§\\vn{x}_2+\\vn{y}_2$\n",
        "\n",
        "- Algorithm for finding orthogonal vector $\\v{v}$ of $\\v{u}$: Pick a component, say $u_1$, and sum all other components $s=u_2+...u_k$. Then $\\v{v}=(-s,u_1,...,u_1)$.\n",
        "\n",
        "**Cross product**: $\\v{a}√ó\\v{b}$ gives a vector such that $\\n{\\v{a}√ó\\v{b}}_2=\\vn{a}_2\\vn{b}_2\\sin Œ∏$ is the area of the parallelogram formed by $\\v{a}$ and $\\v{b}$. On the right hand where $\\v{a}$ takes the index finger, $\\v{b}$ middle finger, then $\\v{a}√ó\\v{b}$ points in the direction of the thumb.\n",
        "\n",
        "- $\\v{a}√ó\\v{b}=\\BVM\n",
        "\\v{i} & \\v{j} & \\v{k} \\\\\n",
        "a_1 & a_2 & a_3 \\\\\n",
        "b_1 & b_2 & b_3\n",
        "\\EVM$"
      ],
      "metadata": {
        "id": "RPgk8id2oYlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plane** in $‚Ñù^3$ is uniquely defined by (1) a point $P_0=(x_0,y_0,z_0)$ and (2) a normal $\\v{n}=A\\v{i}+B\\v{j}+C\\v{k}$ such that $\\overrightarrow{P_0P}‚ãÖ\\v{n}=0$ for all points $P$ on the plane. This plane is defined by equation $A(x-x_0)+B(y-y_0)+C(z-z_0)=0$ (Colley 1.5)\n",
        "\n",
        "- Examples:\n",
        "  - Given plane $7x+2y-3z=1$, the normal is $\\v{n}=k(7\\v{i}+2\\v{j}-3\\v{k}),\\ k>0$.\n",
        "  - Given 3 points $P_0(1,2,0)$, $P_1(3,1,2)$, and $P_2(0,1,1)$, the normal is $\\v{n}=k(\\overrightarrow{P_0P_1}√ó\\overrightarrow{P_0,P_2}),\\ k>0$.\n",
        "\n",
        "- Vector parametric equation: A plane through point $\\v{a}$ and parallel to nonzero nonparallel vectors $\\v{b}$ and $\\v{c}$ is described by $\\v{x}(s,t)=s\\v{b}+t\\v{c}+\\v{a}$."
      ],
      "metadata": {
        "id": "vHkA6RTA9cdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix multiplication**: $C_{m√óp}=A_{m√ón}B_{n√óp}$, element $c_{ij}$ is dot product of $i$th row of $A$ and $j$th column of $B$: $c_{ij}=\\sum\\limits_{k=1}^na_{ik}b_{kj}$.\n",
        "- By universal convention, $\\v{x}$, $\\v{a}$, $(\\v{x}-\\v{a})$, and all data vectors are $n√ó1$ column matrices. Hence $\\v{x}‚ãÖ\\v{x}=\\v{x}^T\\v{x}$.\n",
        "- Properties:\n",
        "  - $A(BC)=(AB)C$\n",
        "  - $k(AB)=(kA)B=A(kB)$\n",
        "  - $A(B+C)=AB+AC$\n",
        "  - $(A+B)C=AC+BC$\n",
        "  - $AB=AIB=(AC)(C^TB)$ If rows of $C$ are orthogonal\n",
        "  - $AB=AIB=(AC^T)(CB)$ If columns of $C$ are orthogonal."
      ],
      "metadata": {
        "id": "pfs4EVnatVH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colley: Vector Differentiation"
      ],
      "metadata": {
        "id": "079tQIsukGEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7 Indeterminate forms** in limit calculation:\n",
        "- $\\lim\\limits_{x‚Üía}\\/{f(x)}{g(x)}=\\/{0}{0}$ or $\\/{‚àû}{‚àû}$: use L'Hopital's rule: $\\lim\\limits_{x‚Üía}\\/{f(x)}{g(x)}=\\lim\\limits_{x‚Üía}\\/{f'(x)}{g'(x)}$.\n",
        "- $\\lim\\limits_{x‚Üía}f(x)g(x)=0‚ãÖ‚àû$: transform $\\lim\\limits_{x‚Üía}f(x)g(x)=\\lim\\limits_{x‚Üía}\\/{f(x)}{1/g(x)}$\n",
        "- $\\lim\\limits_{x‚Üía}f(x)-g(x)=‚àû-‚àû$: Use first-order Taylor series expansion to reveal the leftover part.\n",
        "- $1^‚àû$, $0^0$, $‚àû^0$: take $\\ln$ then analyze limit of product."
      ],
      "metadata": {
        "id": "e7BnJ5xjBV-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topology**: $\\n{\\v{x}-\\v{a}}=r$ is a sphere centered at $\\v{a}$ with radius $r$. $\\n{\\v{x}-\\v{a}}‚â§r$ is a closed ball. $\\n{\\v{x}-\\v{a}}< r$ is an open ball.\n",
        "  - Set $X‚äÜ‚Ñù^n$ is **open in $‚Ñù^n$** if for every point $\\v{x}‚ààX$, there is some open ball centered at $\\v{x}$ that lies entirely within $X$.\n",
        "  - A **neighborhood** of point $\\v{x}‚ààX$ is an open set in $X$ containing $\\v{x}$.\n",
        "  - Point $\\v{x}‚àà‚Ñù^n$ is **in the boundary of $X‚äÜ‚Ñù^n$** if every open ball at $\\v{x}$ no matter how small contains some points in $X$ and some points not in $X$.\n",
        "  - Set $X‚äÜ‚Ñù^n$ is **closed in $‚Ñù^n$** if it contains all its boundary points.\n",
        "\n",
        "**Function**: For $f:X‚ÜíY$ where $X$ and $Y$ are domain and codomain, $\\t{range} f=\\{y‚ààY|y=f(x)\\t{ for some }x‚ààX\\}$. $f$ is **onto** if $\\t{range} f=Y$: every element in $Y$ has an element in $X$ assigned to it. A function is **one-to-one** if $x_1\\neq x_2‚áî f(x_1)\\neq f(x_2)$: no two elements in $X$ are assigned to a same element in $Y$.\n",
        "\n",
        "- For function $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$, then $\\v{f}(\\v{x})=(f_1(x_1,...,x_n),...,f_m(x_1,...,x_n))$\n",
        "\n",
        "**Limit**: $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{L}$ means we can make $\\n{\\v{f}(\\v{x})-\\v{L}}$ arbitrarily small by keeping $\\n{\\v{x}-\\v{a}}$ sufficiently small. (Colley D2.1)\n",
        "\n",
        "- $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{L}$ means that given any $œµ>0$ you can find a $Œ¥>0$ such that if $\\v{x}‚ààX$ and $0< \\n{\\v{x}-\\v{a}}< Œ¥$, then $\\n{\\v{f}(\\v{x})-\\v{L}}< œµ$. (Colley D2.2) Given any $œµ>0$ you can find a $Œ¥>0$ such that if points $\\v{x}‚ààX$ are inside an open ball centered at $\\v{a}$ with radius $Œ¥$ then points $\\v{f}(\\v{x})$ are inside an openball centered at $\\v{L}$ with radius $œµ$.\n",
        "\n",
        "- If a limit exists, it is unique. That is, If $\\lim_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{L}$ and $\\lim_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{M}$, then $\\v{L}=\\v{M}$. (Colley T2.4)\n",
        "  - Proof:\n",
        "  For any $œµ>0$ we can find $Œ¥_1>0$ such that if $\\n{\\v{x}-\\v{a}}< Œ¥_1$ then\n",
        "  $\\n{\\v{f}(\\v{x})-\\v{L}}< œµ/2$.\n",
        "  Similarly we can find $Œ¥_2>0$ such that if $\\n{\\v{x}-\\v{a}}< Œ¥_2$ then\n",
        "  $\\n{\\v{f}(\\v{x})-\\v{M}}< œµ/2$.\n",
        "  Let $\\n{\\v{x}-\\v{a}}<\\min(Œ¥_1,Œ¥_2)$ then\n",
        "  $\\n{\\v{L}-\\v{M}}$\n",
        "  $=\\n{\\v{L}-\\v{f}(\\v{x})+\\v{f}(\\v{x})-\\v{M}}$\n",
        "  $‚â§\\n{\\v{L}-\\v{f}(\\v{x})}+\\n{\\v{f}(\\v{x})-\\v{M}}$\n",
        "  $< œµ$.\n",
        "  I.e., $\\lim_{\\v{x}‚Üí\\v{a}}\\v{L}=\\v{M}$.\n",
        "\n",
        "- Let $\\v{F},\\v{G}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ be vector functions, $f,g:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be scalar functions, and $k‚àà‚Ñù$. (Colley T2.5)\n",
        "  - If $\\lim_{\\v{x}‚Üí\\v{a}}\\v{F}(\\v{x})=\\v{L}$ and $\\lim_{\\v{x}‚Üí\\v{a}}\\v{G}(\\v{x})=\\v{M}$, then $\\lim_{\\v{x}‚Üí\\v{a}}\\v{F}(\\v{x})+\\v{G}(\\v{x})=\\v{L}+\\v{M}$.\n",
        "  - If $\\lim_{\\v{x}‚Üí\\v{a}}\\v{F}(\\v{x})=\\v{L}$ then $\\lim_{\\v{x}‚Üí\\v{a}}k\\v{F}(\\v{x})=k\\v{L}$.\n",
        "  - If $\\lim_{\\v{x}‚Üí\\v{a}}f(\\v{x})=L$ and $\\lim_{\\v{x}‚Üí\\v{a}}g(\\v{x})=M$ then $\\lim_{\\v{x}‚Üí\\v{a}}f(\\v{x})g(\\v{x})=LM$.\n",
        "  - If $\\lim_{\\v{x}‚Üí\\v{a}}f(\\v{x})=L$, $g(\\v{x})\\neq 0$ for $\\v{x}‚ààX$ and $\\lim_{\\v{x}‚Üí\\v{a}}g(\\v{x})=M$, then $\\lim_{\\v{x}‚Üí\\v{a}}f(\\v{x})/g(\\v{x})=L/M$.\n",
        "\n",
        "- Let $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is a vector-valued function. Then $\\lim_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{L}$ where $\\v{L}=(L_1,...,L_m)$ iff $\\lim_{\\v{x}‚Üí\\v{a}}f_i(\\v{x})=L_i$ for $i=1..m$. (Colley T2.6)\n",
        "\n",
        "**Continuous**: $f:X‚äÜ‚Ñù‚Üí‚Ñù$ is continuous if its graph can be drawn without taking the pen off the paper.\n",
        "\n",
        "- Let $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ and let $\\v{a}‚ààX$. Then $\\v{f}$ is **continuous at $\\v{a}$** if $\\lim_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{f}(\\v{a})$. If $\\v{f}$ is continuous at all points in $X$ then $\\v{f}$ is continuous. (Colley D2.7)\n",
        "\n",
        "- Let $\\v{F},\\v{G}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ be vector functions, $f,g:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be scalar functions, and $k‚àà‚Ñù$.\n",
        "  - If $\\v{F},\\v{G}$ are continuous at $\\v{a}‚ààX$ then $\\v{F}+\\v{G}$ is continuous at $\\v{a}$.\n",
        "  - If $\\v{F}$ is contniuous at $\\v{a}‚ààX$ then $k\\v{F}$ is continuous at $\\v{a}$.\n",
        "  - If $f,g$ are continuous at $\\v{a}‚ààX$ then $f(\\v{x}g(\\v{x})$ and $f(\\v{x})/g(\\v{x})$ are continuous at $\\v{a}$.\n",
        "  - $\\v{F}$ is continuous at $\\v{a}‚ààX$ iff $F_i,\\ i=1..m$ are all continuous at $\\v{a}$.\n",
        "\n",
        "- If $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$, $\\t{range} \\v{f}‚äÜY$, and $g:Y‚äÜ‚Ñù^m‚Üí‚Ñù^p$ are continuous functions, then $g(f):X‚äÜ‚Ñù^n‚Üí‚Ñù^p$ is defined and also continuous. (Colley T2.8)"
      ],
      "metadata": {
        "id": "5uqUaHl7Be89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partial**: Suppose $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ and let $\\v{x}=(x_1,...,x_n)$. Then a **partial function wrt $x_i$** $F(x_i)=f(a_1,...,x_i,...,a_n)$ is obtained by holding all variables constant except $x_i$. (Colley D3.1)\n",
        "\n",
        "- **Partial derivative of $f$ wrt $x_i$** is the derivative of the partial function wrt $x_i$: $\\/{‚àÇf}{‚àÇx_i}=\\lim\\limits_{h‚Üí0}\\/{f(x_1,...,x_i+h,...,x_n)-f(x_1,...,x_n)}{h}$. Notation $\\/{‚àÇf}{‚àÇx_i}$, $D_{x_i}f$, or $f_{x_i}$. (Colley D3.2)\n",
        "\n",
        "- **Gradient** $‚àáf(\\v{x})=(\\/{‚àÇf}{‚àÇx_1},...,\\/{‚àÇf}{‚àÇx_n})$,\n",
        "$‚àáf(\\v{a})=(\\/{‚àÇf}{‚àÇx_1}(\\v{a}),...,\\/{‚àÇf}{‚àÇx_n}(\\v{a}))$.\n",
        "  - $Df(\\v{a})=\\BSM\n",
        "  \\/{‚àÇf}{‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇf}{‚àÇx_n}(\\v{a})\n",
        "  \\ESM$\n",
        "  is row matrix:\n",
        "  $‚àáf(\\v{a})‚ãÖ(\\v{x}-\\v{a})=Df(\\v{a})(\\v{x}-\\v{a})=f(\\v{a})+\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})(x_i-a_i)$.\n",
        "  - Let $X$ be open in $‚Ñù^n$ and let $\\v{f}:X‚Üí‚Ñù^m$, then $D\\v{f}(\\v{x})=\\BSM\n",
        "  \\/{‚àÇf_1}{‚àÇx_1} & ... & \\/{‚àÇf_1}{‚àÇx_n} \\\\\n",
        "  \\vdots & \\ddots & \\vdots \\\\\n",
        "  \\/{‚àÇf_m}{‚àÇx_1} & ... & \\/{‚àÇf_m}{‚àÇx_n}\n",
        "  \\ESM$, and $D\\v{f}(\\v{a})(\\v{x}-\\v{a})$ is $m√ó1$.\n",
        "\n",
        "**Differentiable** ($‚Ñù‚Üí‚Ñù$): the derivative of $f:X‚äÜ‚Ñù‚Üí‚Ñù$ at $a‚ààX$ is $f'(a)=\\lim\\limits_{h‚Üí0}\\/{f(a+h)-f(a)}{h}$ or $f'(a)=\\lim\\limits_{x‚Üía}\\/{f(x)-f(a)}{x-a}$. If $f'(a)$ exists then $f$ is differentiable at $a$.\n",
        "\n",
        "- If $f:X‚äÜ‚Ñù‚Üí‚Ñù$ is differentiable at $a‚ààX$ then tangent line at point $(a,f(a))$ described by $h(x)=f(a)+f'(a)(x-a)$ (i.e., $y=mx+b$ shifted to the right by $a$) is a good linear approximation of $f(x)$ at $a$.\n",
        "\n",
        "- For $f:X‚äÜ‚Ñù^2‚Üí‚Ñù$ the tangent plane at point $(a,b,f(a,b))$ is described by a pair of parametric lines along $x$ and $y$ $\\BC\n",
        "\\v{l}_x(t)=(a,b,f(a,b))+t(1,0,\\/{‚àÇf}{‚àÇx}(a,b)) \\\\\n",
        "\\v{l}_y(t)=(a,b,f(a,b))+t(1,0,\\/{‚àÇf}{‚àÇy}(a,b))\n",
        "\\EC$.\n",
        "The normal of the plane is $\\v{n}=\\/{d\\v{l}_x}{dt}√ó\\/{d\\v{l}_y}{dt}$\n",
        "$=\\BVM\n",
        "\\v{i} & \\v{j} & \\v{k} \\\\\n",
        "1 & 0 & \\/{‚àÇf}{‚àÇx}(a,b) \\\\\n",
        "0 & 1 & \\/{‚àÇf}{‚àÇy}(a,b)\n",
        "\\EVM$\n",
        "$=-\\/{‚àÇf}{‚àÇx}(a,b)\\v{i}-\\/{‚àÇf}{‚àÇy}(a,b)\\v{j}+\\v{k}$.\n",
        "Therefore\n",
        "$-\\/{‚àÇf}{‚àÇx}(a,b)(x-a)-\\/{‚àÇf}{‚àÇy}(a,b)(y-b)+(z-f(a,b))=0$\n",
        "$‚áíz=f(a,b)+\\/{‚àÇf}{‚àÇx}(a,b)(x-a)+\\/{‚àÇf}{‚àÇy}(a,b)(y-b)$\n",
        "describes the tangent plane (Colley T3.3)\n",
        "\n",
        "**Differentiable** ($‚Ñù^2‚Üí‚Ñù$): Let $X$ be open in $‚Ñù^2$. $f:X‚äÜ‚Ñù^2‚Üí‚Ñù$ is differentiable at $(a,b)‚ààX$ if $\\/{‚àÇf}{‚àÇx}(a,b)$ and $\\/{‚àÇf}{‚àÇy}(a,b)$ exist and if tangent plane $h(x,y)=f(a,b)+\\/{‚àÇf}{‚àÇx}(a,b)(x-a)+\\/{‚àÇf}{‚àÇy}(a,b)(y-b)$ is a good linear approximation to $f$ near $(a,b)$ that is $\\lim\\limits_{(x,y)‚Üí(a,b)}\\/{f(x,y)-h(x,y)}{\\n{(x,y)-(a,b)}}=0$. If $f$ is differentiable at all points of its domain, then $f$ is differentiable. (Colley D3.4)\n",
        "\n",
        "- Suppose $X$ is open in $‚Ñù^2$. If $f:X‚Üí‚Ñù$ has continuous partial derivatives in a neighborhood of $(a,b)$ in $X$ then $f$ is differentiable at $(a,b)$. (Colley T3.5)\n",
        "\n",
        "  - Mean Value Theorem: if $F$ is continuous on $[a,b]$ and differentiable on $(a,b)$, then there exists $c‚àà(a,b)$ such that $F(b)-F(a)=F'(c)(b-a)$\n",
        "\n",
        "  - Proof: Show $\\lim\\limits_{(x,y)‚Üí(a,b)}\\/{f(x,y)-h(x,y)}{\\n{(x,y)-(a,b)}}=0$.\n",
        "  $f(x,y)-f(a,b)$\n",
        "  $=[f(x,y)-f(a,y)]+[f(a,y)-f(a,b)]$\n",
        "  $=\\/{‚àÇf}{‚àÇx}(c_1,y)(x-a)$\n",
        "  $+\\/{‚àÇf}{‚àÇy}(a,c_2)(y-b)$.\n",
        "  Numerator\n",
        "  $|f(x,y)-h(x,y)|$\n",
        "  $=|\\/{‚àÇf}{‚àÇx}(c_1,y)(x-a)$\n",
        "  $-\\/{‚àÇf}{‚àÇx}(a,b)(x-a)$\n",
        "  $+\\/{‚àÇf}{‚àÇy}(a,c_2)(y-b)$\n",
        "  $-\\/{‚àÇf}{‚àÇy}(a,b)(y-b)|$\n",
        "  $‚â§|\\/{‚àÇf}{‚àÇx}(c_1,y)(x-a)$\n",
        "  $-\\/{‚àÇf}{‚àÇx}(a,b)(x-a)|$\n",
        "  $+|\\/{‚àÇf}{‚àÇy}(a,c_2)(y-b)$\n",
        "  $-\\/{‚àÇf}{‚àÇy}(a,b)(y-b)|$\n",
        "  $=|\\/{‚àÇf}{‚àÇx}(c_1,y)-\\/{‚àÇf}{‚àÇx}(a,b)||x-a|$\n",
        "  $+|\\/{‚àÇf}{‚àÇy}(a,c_2)-\\/{‚àÇf}{‚àÇy}(a,b)||y-b|$\n",
        "  $‚â§|\\/{‚àÇf}{‚àÇx}(c_1,y)-\\/{‚àÇf}{‚àÇx}(a,b)|\\n{(x,y)-(a,b)}$\n",
        "  $+|\\/{‚àÇf}{‚àÇy}(a,c_2)-\\/{‚àÇf}{‚àÇy}(a,b)|\\n{(x,y)-(a,b)}$.\n",
        "  $\\lim\\limits_{(x,y)‚Üí(a,b)}\\/{f(x,y)-h(x,y)}{\\n{(x,y)-(a,b)}}$\n",
        "  $‚â§\\lim\\limits_{(x,y)‚Üí(a,b)}\\ob{|\\/{‚àÇf}{‚àÇx}(c_1,y)-\\/{‚àÇf}{‚àÇx}(a,b)|}{\\t{by continuity: }(c_1,y)=(a,b)}$\n",
        "  $+\\lim\\limits_{(x,y)‚Üí(a,b)}\\ob{|\\/{‚àÇf}{‚àÇy}(a,c_2)-\\/{‚àÇf}{‚àÇy}(a,b)|}{\\t{by continuity: }c_2=b}=0$.\n",
        "  Therefore continuous partials ‚áí differentiable.\n",
        "\n",
        "- If $f:X‚äÜ‚Ñù^2‚Üí‚Ñù$ is differentiable at $(a,b)$ then it is continuous at $(a,b)$. (Colley T3.6)\n",
        "\n",
        "  - Proof: See Colley T3.9\n",
        "\n",
        "- **Differentiable** ($‚Ñù^n‚Üí‚Ñù$): Let $X$ be open in $‚Ñù^n$, $f:X‚Üí‚Ñù$, and $\\v{a}=(a_1,...,a_n)‚ààX$. Then $f$ is **differentiable at $\\v{a}$** if all partial derivatives $\\/{‚àÇf}{‚àÇx_i}(\\v{a}),\\ i=1..n$ exist and $h:‚Ñù^n‚Üí‚Ñù$ defined by $h(\\v{x})=f(\\v{a})+\\/{‚àÇf}{‚àÇx_1}(\\v{a})(x_1-a_1)+...+\\/{‚àÇf}{‚àÇx_n}(\\v{a})(x_n-a_n)$ is a good linear approximation to $f$ near $\\v{a}$, meaning $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{f(\\v{x})-h(\\v{x})}{\\n{\\v{x}-\\v{a}}}=0$. (Colley D3.7)\n",
        "\n",
        "**Differentiable** ($‚Ñù^n‚Üí‚Ñù^m$): Let $X$ be open in $‚Ñù^n$, let $\\v{f}:X‚Üí‚Ñù^m$, and let $\\v{a}‚ààX$. Then $\\v{f}$ is **differentiable at $\\v{a}$** if $D\\v{f}(\\v{a})$ exists and if the tangent hyperplane $\\v{h}:‚Ñù^n‚Üí‚Ñù^m$ defined by $\\v{h}(\\v{x})=\\v{f}(\\v{a})+D\\v{f}(\\v{a})(\\v{x}-\\v{a})$ is a good linear approximation to $\\v{f}$ near $\\v{a}$. That is, we must have $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{\\n{\\v{f}(\\v{x})-\\v{h}(\\v{x})}}{\\n{\\v{x}-\\v{a}}}=0$. (Colley D3.8)\n",
        "\n",
        "- If $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is differentiable at $\\v{a}$ then it is continuous at $\\v{a}$ (Colley T3.9)\n",
        "\n",
        "  - Proof: Show $\\lim_{\\v{x}‚Üí\\v{a}}\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})}=0$, which defines continuity at $\\v{a}$.\n",
        "  $\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})}$\n",
        "  $=\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})+[D\\v{f}(\\v{a})(\\v{x}-\\v{a})-D\\v{f}(\\v{a})(\\v{x}-\\v{a})]}$\n",
        "  $‚â§\\ob{\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})-D\\v{f}(\\v{a})(\\v{x}-\\v{a})}}{=\\n{\\v{f}(\\v{x})-\\v{h}(\\v{x})}‚â§\\n{\\v{x}-\\v{a}}\\t{ because differentiable}}+\\ob{\\n{D\\v{f}(\\v{a})(\\v{x}-\\v{a})}}{‚â§|D\\v{f}(\\v{a})|‚ãÖ\\n{\\v{x}-\\v{a}}\\t{ triangle}}$\n",
        "  $‚â§(1+|D\\v{f}(\\v{a})|)\\n{\\v{x}-\\v{a}}$\n",
        "  $‚Üí0$. Therefore differentiable ‚áí continuous.\n",
        "  $|D\\v{f}(\\v{a})|=\\left(\\sum_{i,j}b_{i,j}^2\\right)^{1/2}$.\n",
        "\n",
        "- If $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is such that for all $i=1..m$ and $j=1..n$ all $\\/{‚àÇf_i}{‚àÇx_j}$ exist and are continuous in a neighborhood of $\\v{a}‚ààX$ then $\\v{f}$ is differentiable at $\\v{a}$. (Colley T3.10)\n",
        "\n",
        "  - Proof: Extrapolate $f:X‚äÜ‚Ñù^2‚Üí‚Ñù$ (Colley T3.5) to $m$ functions using Colley T3.11.\n",
        "\n",
        "- A function $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is differentiable at $\\v{a}‚ààX$ iff for $i=1..m$ all $f_i:X‚äÜ‚Ñù^n‚Üí‚Ñù$ are differentiable at $\\v{a}$. (Colley T3.11)\n",
        "\n",
        "  - Proof: Show $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})-D\\v{f}(\\v{a})(\\v{x}-\\v{a})}}{\\n{\\v{x}-\\v{a}}}=0$. Let $G_i=f_i(\\v{x})-f_i(\\v{a})-Df_i(\\v{a})(\\v{x}-\\v{a})$.\n",
        "  Then\n",
        "  $\\/{\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})-D\\v{f}(\\v{a})(\\v{x}-\\v{a})}}{\\n{\\v{x}-\\v{a}}}$\n",
        "  $=\\/{\\n{(G_1,...,G_m)}}{\\n{\\v{x}-\\v{a}}}$\n",
        "  $=\\/{(G_1^2+...+G_m^2)^{1/2}}{\\n{\\v{x}-\\v{a}}}$\n",
        "  $‚â§\\/{|G_1|}{\\n{\\v{x}-\\v{a}}}+...+\\/{|G_m|}{\\n{\\v{x}-\\v{a}}}$\n",
        "  $‚Üí0$. Therefore $f_i$ differentiable ‚áí $\\v{f}$ differentiable.\n",
        "  $\\/{(G_1^2+...+G_m^2)^{1/2}}{\\n{\\v{x}-\\v{a}}}$\n",
        "  $‚â•\\/{|G_i|}{\\n{\\v{x}-\\v{a}}}$.\n",
        "  Therefore $\\v{f}$ differentiable ‚áí $f_i$ differentiable.\n"
      ],
      "metadata": {
        "id": "fBuEZmk_C8Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Derivative Summary**:\n",
        "\n",
        "- For differentiable $f:X‚äÜ‚Ñù‚Üí‚Ñù$, the derivative $f'(a)$ is the number such that $h(x)=f(a)+f'(a)(x-a)$ is a good linear approximation to $f(x)$ near $a$.\n",
        "\n",
        "- For differentiable $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$, the derivative $‚àáf(\\v{a})$ is the vector such that $h(\\v{x})=f(\\v{a})+‚àáf(\\v{a})‚ãÖ(\\v{x}-\\v{a})$ is a good linear approximation to $f(\\v{x})$ near $\\v{a}$.\n",
        "\n",
        "- For differentiable $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$, the derivative $D\\v{f}(\\v{a})$ is the matrix such that $\\v{h}(\\v{x})=\\v{f}(\\v{a})+D\\v{f}(\\v{a})(\\v{x}-\\v{a})$ is a good linear approximation to $\\v{f}(\\v{x})$ near $\\v{a}$.\n",
        "\n",
        "- Differentiation is linear: let $\\v{f},\\v{g}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ be differentiable at $\\v{a}‚ààX$ then (Colley 4.1)\n",
        "\n",
        "  - $\\v{h}=\\v{f}+\\v{g}$ is differentiable at $\\v{a}$:\n",
        "  $D\\v{h}(\\v{a})=D(\\v{f}+\\v{g})(\\v{a})=D\\v{f}(\\v{a})+D\\v{g}(\\v{a})$\n",
        "\n",
        "  - $\\v{k}=c\\v{f}$ is differentiable at $\\v{a}$:\n",
        "  $D\\v{k}(\\v{a})=D(c\\v{f})(\\v{a})=cD\\v{f}(\\v{a})$\n",
        "\n",
        "**$k$th order partial derivative** for $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is $\\/{‚àÇ^kf}{‚àÇx_{i_1}...‚àÇx_{i_k}}=\\/{‚àÇ}{‚àÇx_{i_k}}...\\/{‚àÇ}{‚àÇx_{i_1}}f(x_1,...,x_n)$\n",
        "\n",
        "- Suppose $X$ is open in $‚Ñù^n$ and $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ has at least $k$ order continuous partial derivatives. Then $f$ is said to be **of class $C^k$**. $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is of class $C^k$ iff all its component functions are of class $C^k$. (Colley D4.4)\n",
        "\n",
        "- Suppose $X$ is open in $‚Ñù^n$ and $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is of class $C^k$. Then $\\/{‚àÇ^kf}{‚àÇx_{i_1}...‚àÇx_{i_k}}=\\/{‚àÇ^kf}{‚àÇx_{j_1}...‚àÇx_{j_k}}$ where $j_1,...,j_k$ is any permutation of $i_1,...,i_k$. (Colley T4.5)"
      ],
      "metadata": {
        "id": "UmF16mmYCJRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chain rule** ($‚Ñù‚àò‚Ñù‚Üí‚Ñù$): Suppose $X$ and $T$ are open subsets of $‚Ñù$ and $f:X‚äÜ‚Ñù‚Üí‚Ñù$ and $x:T‚äÜ‚Ñù‚Üí‚Ñù$ are defined so that $f‚àòx:T‚Üí‚Ñù$ exists, $x$ is differentiable at $t_0‚ààT$, and $f$ is differentiable at $x_0=x(t_0)$, then $h=f‚àòx$ is differentiable at $t_0$ and $h'(t_0)=f'(x_0)x'(t_0)$ or $\\/{dh}{dt}(t_0)=\\/{df}{dx}(x_0)\\/{dx}{dt}(t_0)$. (Colley T5.1)\n",
        "\n",
        "- **Chain rule** ($‚Ñù‚àò‚Ñù^2‚Üí‚Ñù$): Suppose $\\v{x}:T‚äÜ‚Ñù‚Üí‚Ñù^2$ is differentiable at $t_0‚ààT$ and range $\\v{x}‚äÜX$. Suppose $f:X‚äÜ‚Ñù^2‚Üí‚Ñù$ is $C^1$ and differentiable at $\\v{x}_0=\\v{x}(t_0)=(x_0,y_0)‚ààX$ where $T$ and $X$ are open in $‚Ñù$ and $‚Ñù^2$ respectively. Then $h=f‚àò\\v{x}:T‚Üí‚Ñù$ is differentiable at $t_0$ and $\\/{dh}{dt}(t_0)=\\/{‚àÇf}{‚àÇx}(\\v{x}_0)\\/{dx}{dt}(t_0)+\\/{‚àÇf}{‚àÇy}(\\v{x}_0)\\/{dy}{dt}(t_0)$. (Colley 5.2)\n",
        "\n",
        "  - Proof: Let $h=f‚àò\\v{x}$, $x=x(t)$, $y=y(t)$, $x_0=x(t_0)$ and $y_0=y(t_0)$. Then\n",
        "  $\\/{dh}{dt}(t_0)=\\lim\\limits_{t‚Üít_0}\\/{h(t)-h(t_0)}{t-t_0}$\n",
        "  $=\\lim\\limits_{t‚Üít_0}\\/{f(x,y)-f(x_0,y_0)}{t-t_0}$\n",
        "  $=\\lim\\limits_{t‚Üít_0}\\/{f(x,y)-f(x_0,y)+f(x_0,y)-f(x_0,y_0)}{t-t_0}$\n",
        "  $=\\lim\\limits_{t‚Üít_0}\\/{f(x,y)-f(x_0,y)}{t-t_0}$\n",
        "  $+\\lim\\limits_{t‚Üít_0}\\/{f(x_0,y)-f(x_0,y_0)}{t-t_0}$\n",
        "  $=\\lim\\limits_{t‚Üít_0}\\ob{\\/{‚àÇf}{‚àÇx}(c_1,y)\\/{x-x_0}{t-t_0}}{\\t{mean value theorem}}$\n",
        "  $+\\lim\\limits_{t‚Üít_0}\\ob{\\/{‚àÇf}{‚àÇy}(x_0,c_2)\\/{y-y_0}{t-t_0}}{\\t{mean value theorem}}$\n",
        "  $=\\ob{\\/{‚àÇf}{‚àÇx}(x_0,y_0)}{(c_1,y)‚Üí(x_0,y_0)}\\/{dx}{dt}(t_0)$\n",
        "  $+\\ob{\\/{‚àÇf}{‚àÇy}(x_0,y_0)}{c_2‚Üíy_0}\\/{dy}{dt}(t_0)$\n",
        "\n",
        "  - The requirement that $f$ be of class $C^1$ is redundant because $f$ is differentiable at $\\v{x}_0$.\n",
        "\n",
        "  - Example 2: $f(x,y)=(x+y^2)/(2x^2+1)$, and $\\v{x}(t)=(2t, t+1)$ gives parametric equations for a line.\n",
        "  $f(\\v{x}(t))=\\/{t^2+4t+1}{8t^2+1}$\n",
        "  $‚áí\\/{df}{dt}=\\/{2(2-7t-16t^2)}{(8t^2+1)^2}$.\n",
        "  Alternatively,\n",
        "  $\\/{‚àÇf}{‚àÇx}=\\/{2x^2+1-4x(x+y^2)}{(2x^2+1)^2}$\n",
        "  $=\\/{1-2x^2-4xy^2}{(2x^2+1)^2}$,\n",
        "  $\\/{‚àÇf}{‚àÇy}=\\/{2y}{2x^2+1}$,\n",
        "  $(\\/{dx}{dt},\\/{dy}{dt})=(2,1)$. Therefore\n",
        "  $\\/{df}{dt}=\\/{‚àÇf}{‚àÇx}\\/{dx}{dt}+\\/{‚àÇf}{‚àÇy}\\/{dy}{dt}$\n",
        "  $=\\/{2(2-7t-16t^2)}{(8t^2+1)^2}$.\n",
        "\n",
        "- **Chain rule** ($‚Ñù‚àò‚Ñù^n‚Üí‚Ñù$): Suppose $\\v{x}:T‚äÜ‚Ñù‚Üí‚Ñù^n$ is differentiable at $t_0‚ààT$ and range $\\v{x}‚äÜX$. Suppose $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is differentiable at $\\v{x}_0=\\v{x}(t_0)‚ààX$ where $T$ and $X$ are open in $‚Ñù$ and $‚Ñù^n$ respectively. Let $h=f‚àò\\v{x}:T‚Üí‚Ñù$, then\n",
        "$\\/{dh}{dt}(t_0)=Df(\\v{x}_0)D\\v{x}(t_0)=‚àáf(\\v{x}_0)‚ãÖ\\/{d\\v{x}}{dt}|_{t=t_0}=\n",
        "\\BSM\n",
        "\\/{‚àÇf}{‚àÇx_1}(\\v{x}_0) &\n",
        "... &\n",
        "\\/{‚àÇf}{‚àÇx_n}(\\v{x}_0)\n",
        "\\ESM\\BSM\n",
        "\\/{dx_1}{dt}(t_0) \\\\\n",
        "\\vdots \\\\\n",
        "\\/{dx_n}{dt}(t_0)\n",
        "\\ESM$\n",
        "\n",
        "- **Chain rule** ($‚Ñù^2‚àò‚Ñù^3‚Üí‚Ñù$): Suppose $\\v{x}:T‚äÜ‚Ñù^2‚Üí‚Ñù^3$ is differentiable at $\\v{t}_0=(s_0,t_0)‚ààT$ and range $\\v{x}‚äÜX$. Suppose $f:X‚äÜ‚Ñù^3‚Üí‚Ñù$ is differentiable at $\\v{x}_0=\\v{x}(\\v{t}_0)=(x_0,y_0,z_0)‚ààX$ where $T$ and $X$ are open in $‚Ñù^2$ and $‚Ñù^3$ respectively. Then for $h=f‚àò\\v{x}:T‚Üí‚Ñù$,\n",
        "$\\BC\n",
        "\\/{‚àÇh}{‚àÇs}=\\/{‚àÇf}{‚àÇx}\\/{‚àÇx}{‚àÇs}+\\/{‚àÇf}{‚àÇy}\\/{‚àÇy}{‚àÇs}+\\/{‚àÇf}{‚àÇz}\\/{‚àÇz}{‚àÇs} \\\\\n",
        "\\/{‚àÇh}{‚àÇt}=\\/{‚àÇf}{‚àÇx}\\/{‚àÇx}{‚àÇt}+\\/{‚àÇf}{‚àÇy}\\/{‚àÇy}{‚àÇt}+\\/{‚àÇf}{‚àÇz}\\/{‚àÇz}{‚àÇt}\n",
        "\\EC$\n",
        "\n",
        "- **Chain rule** ($‚Ñù^n‚àò‚Ñù^m‚Üí‚Ñù^p$): Suppose $\\v{x}:T‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is differentiable at $\\v{t}_0‚ààT$ and range $\\v{x}‚äÜX$. Suppose $\\v{f}:X‚äÜ‚Ñù^m‚Üí‚Ñù^p$ is differentiable at $\\v{x}_0=\\v{x}(\\v{t})$ where $T$ and $X$ are open in $‚Ñù^n$ and $‚Ñù^m$ respectively. Then $\\v{h}=\\v{f}‚àò\\v{x}$ is differentiable at $\\v{t}_0$ and $D\\v{h}(\\v{t}_0)=D\\v{f}(\\v{x}_0)D\\v{x}(\\v{t}_0)$\n",
        "where\n",
        "$\\/{‚àÇh_{i=1,...,p}}{‚àÇt_{j=1,...,n}}=\\sum\\limits_{k=1}^m\\/{‚àÇf_i}{‚àÇx_k}\\/{‚àÇx_k}{‚àÇt_j}$. (Colley T5.3)\n",
        "\n",
        "  - Example 4: $\\v{f}(x_1,x_2,x_3)=(x_1-x_2, x_1x_2x_3)$ and $\\v{x}(t_1,t_2)=(t_1t_2,t_1^2,t_2^2)$.\n",
        "  Then\n",
        "  $\\v{f}‚àò\\v{x}=(t_1t_2-t_1^2,t_1^3t_2^3)$ and\n",
        "  $D(\\v{f}‚àò\\v{x})=\\BSM\n",
        "  t_2-2t_1 & t_1 \\\\\n",
        "  3t_1^2t_2^3 & 3t_1^3t_2^2\n",
        "  \\ESM$.\n",
        "  Alternatively,\n",
        "  $D\\v{f}(\\v{x})=\\BSM\n",
        "  1 & -1 & 0 \\\\\n",
        "  x_2x_3 & x_1x_3 & x_1x_2\n",
        "  \\ESM$ and\n",
        "  $D\\v{x}(\\v{t})=\\BSM\n",
        "  t_2 & t_1 \\\\\n",
        "  2t_1 & 0 \\\\\n",
        "  0 & 2t_2\n",
        "  \\ESM$ then\n",
        "  $D\\v{f}(\\v{x})D\\v{x}(\\v{t})=\\BSM\n",
        "  t2-2t_1 & t_1 \\\\\n",
        "  x_2x_3t_2+x_1x_32t_1 & x_2x_3t_1 + x_1x_22t_2\n",
        "  \\ESM$\n",
        "  $=\\BSM\n",
        "  t_2-2t_1 & t_1\\\\\n",
        "  t_1^2t_2^3+2t_1^2t_2^3 & t_1^3t_2^2+2t_1^3t_2^2\n",
        "  \\ESM$\n",
        "\n",
        "  - Example 6: $\\BC x=r\\cosŒ∏ \\\\ y=r\\sinŒ∏\\EC$. For $w=f(x,y)=g(r,Œ∏)$ how are derivatives of $(r,Œ∏)$ related to $(x,y)$?\n",
        "  $Dg(r,Œ∏)=Df(x,y)D\\v{x}(r,Œ∏)$\n",
        "  $‚áí\\BSM\n",
        "  \\/{‚àÇg}{‚àÇr} & \\/{‚àÇg}{‚àÇŒ∏}\n",
        "  \\ESM=\\BSM\n",
        "  \\/{‚àÇf}{‚àÇx} & \\/{‚àÇf}{‚àÇy}\n",
        "  \\ESM\\BSM\n",
        "  \\cosŒ∏ & -r\\sinŒ∏\\\\\n",
        "  \\sinŒ∏ & r\\cosŒ∏\n",
        "  \\ESM$.\n",
        "  $\\BSM\n",
        "  \\/{‚àÇf}{‚àÇx} & \\/{‚àÇf}{‚àÇy}\n",
        "  \\ESM=\\BSM\n",
        "  \\/{‚àÇg}{‚àÇr} & \\/{‚àÇg}{‚àÇŒ∏}\n",
        "  \\ESM\\/{1}{r}\\BSM\n",
        "  r\\cosŒ∏ & -\\sinŒ∏\\\\\n",
        "  r\\sinŒ∏ & \\cosŒ∏\n",
        "  \\ESM$."
      ],
      "metadata": {
        "id": "EWviZ9flILsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Directional derivative**: Let $X$ be open in $‚Ñù^n$, $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ and $\\v{a}‚ààX$. If $\\v{v}‚àà‚Ñù^n$ is a unit direction vector, then the **directional derivative of $f$ at $\\v{a}$ in the direction of $\\v{v}$** is $D_\\v{v}f(\\v{a})=\\lim\\limits_{h‚Üí0}\\/{f(\\v{a}+h\\v{v})-f(\\v{a})}{h}$ (Colley D6.1)\n",
        "\n",
        "- Let $X‚äÜ‚Ñù^n$ be open and $f:X‚Üí‚Ñù$ is differentiable at $\\v{a}‚ààX$. Then the directional derivative $D_\\v{v}f(\\v{a})$ exists for all direction vectors $\\v{v}‚àà‚Ñù^n$ and $D_\\v{v}f(\\v{a})=‚àáf(\\v{a})‚ãÖ\\v{v}$. (Colley T6.2)\n",
        "\n",
        "  - Proof: Let $F(t)=f(\\v{a}+t\\v{v})$ then $D_\\v{v}f(\\v{a})=\\lim\\limits_{h‚Üí0}\\/{F(h)-F(0)}{h}=F'(0)$.\n",
        "  If we let $\\v{x}=\\v{a}+t\\v{v}$ then\n",
        "  $D_\\v{v}f(\\v{a})=\\/{d}{dt}f(\\v{a}+t\\v{v})|_{t=0}$\n",
        "  $=Df(\\v{x})D\\v{x}(t)|_{t=0}$\n",
        "  $=Df(\\v{x})\\v{v}|_{t=0}$\n",
        "  $=Df(\\v{a})\\v{v}$\n",
        "  $=‚àáf(\\v{a})‚ãÖ\\v{v}$.\n",
        "\n",
        "  - Example 2: $f(x,y)=x^2-3xy+2x-5y$ at $(0,0)$ with $\\v{v}=v\\v{i}+w\\v{j}$.\n",
        "  Then $D_\\v{v}f(0,0)=\\BSM\n",
        "  2 & -5\n",
        "  \\ESM\\BSM\n",
        "  v \\\\ w\n",
        "  \\ESM$\n",
        "  $=2v-5w$\n",
        "\n",
        "- $D_\\v{u}f(\\v{a})$ is maximized wrt $\\v{u}$ when $\\v{u}$ points in the same direction as $‚àáf(\\v{a})$ and is minimized when $\\v{u}$ points in the opposite direction. The maximum and minimum values of $D_\\v{u}f(\\v{a})$ are $¬±\\n{‚àáf(\\v{a})}$. (Colley T6.3)\n",
        "\n",
        "  - Proof: Let $Œ∏$ be the angle between $‚àáf(\\v{a})$ and $\\v{u}$. Then $D_\\v{u}f(\\v{a})=\\n{‚àáf(\\v{a})}\\vn{u}\\cosŒ∏$\n",
        "  $=\\n{‚àáf(\\v{a})}\\cosŒ∏$. I.e., $-\\n{‚àáf(\\v{a})}‚â§D_\\v{u}f(\\v{a})‚â§\\n{‚àáf(\\v{a})}$\n",
        "\n",
        "  - If $f(x,y)$ is the height of a hill above the $(x,y)$ plane, then at position $(a,b)$ on the side of the hill $‚àáf(a,b)$ points toward the $(x,y)$ direction with the greatest uphill slope $\\n{‚àáf(a,b)}$.\n"
      ],
      "metadata": {
        "id": "bXxULTmKVYd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient and level set**: Let $X‚äÜ‚Ñù^n$ be open and $f:X‚Üí‚Ñù$ be $C^1$. If $\\v{x}_0$ is on the level set $S=\\{\\v{x}‚ààX|f(\\v{x})=c\\}$, then $‚àáf(\\v{x}_0)$ is perpendicular to $S$. (Colley T6.4)\n",
        "\n",
        "- If $f(x,y)$ is the height of a hill, then the level set is the topographical contour of the mountain at height $c$.\n",
        "\n",
        "- Proof: Let $\\v{x}:T‚äÜ‚Ñù‚ÜíS‚äÇ‚Ñù^n$ be the parametric contour contained in $S$ passing through $\\v{x}_0=\\v{x}(t_0)$ for some $t_0‚ààT$.\n",
        "  - $f(\\v{x}(t))=c$ therefore $\\/{d}{dt}f(\\v{x}(t))|_{t=t_0}=0$.\n",
        "  - Chain rule $\\/{d}{dt}f(\\v{x}(t))=‚àáf(\\v{x}(t))‚ãÖ\\/{d\\v{x}}{dt}$ where $\\/{d\\v{x}}{dt}$ is the slope of the parametric curve in $S$.\n",
        "  - $0=\\/{d}{dt}f(\\v{x}(t))|_{t=t_0}=‚àáf(\\v{x}_0)‚ãÖ\\/{d\\v{x}}{dt}(t_0)$\n",
        "  \n",
        "  As shown, the slope of $S$ at $\\v{x}_0$ is perpendicular to $‚àáf(\\v{x}_0)$.\n",
        "\n",
        "- For spheres $f(x,y)=x^2+y^2=c$ the normal at $\\v{x}_0$ is $‚àáf(\\v{x}_0)$.\n",
        "\n",
        "- For general $z=f(x,y)$ functions, the normal at $(a,b)$ can be determined either using the parametric lines $\\BC\n",
        "\\v{l}_x(t)=(a,b,f(a,b))+t(1,0,\\/{‚àÇf}{‚àÇx}(a,b)) \\\\\n",
        "\\v{l}_y(t)=(a,b,f(a,b))+t(1,0,\\/{‚àÇf}{‚àÇy}(a,b))\n",
        "\\EC$ or by applying the level set method to $F(x,y,z)=f(x,y)-z=0$ to arrive at T3.3 $z=f(a,b)+\\/{‚àÇf}{‚àÇx}(a,b)(x-a)+\\/{‚àÇf}{‚àÇy}(a,b)(y-b)$\n",
        "$=f(\\v{a})+Df(\\v{a})(\\v{x}-\\v{a})$\n",
        "\n",
        "**Implicit function theorem**: Let $F:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be $C^1$ and let $\\v{a}‚ààS=\\{\\v{x}‚àà‚Ñù^n|F(\\v{x})=c\\}$. If $\\/{‚àÇF}{‚àÇx_n}(\\v{a})\\neq0$ then there is a neighborhood $U$ of $(a_1,...,a_{n-1})$ in $‚Ñù^{n-1}$, neighborhood $V$ of $x_n$ in $‚Ñù$, and a function $f:U‚äÜ‚Ñù^{n-1}‚ÜíV$ of $C^1$ such that if $(x_1,...,x_{n-1})‚ààU$ and $x_n‚ààV$ satisfy $F(x_1,...,x_n)=c$ then $x_n=f(x_1,...,x_{n-1})$. (Colley T6.5)\n",
        "\n",
        "- If $\\v{a}$ is a point on $F(\\v{x})=c$ and $‚àáF(\\v{a})\\neq0$, then we can take any $x_i$ such that $\\/{‚àÇF}{‚àÇ_{x_i}}(\\v{a})\\neq0$ and express $x_i$ as a differentiable function of the other components of $\\v{x}$ in the neighborhood of $\\v{a}$.\n",
        "\n",
        "**General implicit function theorem**: Generally, we have system of $m$ equations $\\BC\n",
        "F_1(x_1,...,x_n,y_1,...,y_m)=c_1 \\\\\n",
        "\\vdots \\\\\n",
        "F_m(x_1,...,x_n,y_1,...,y_m)=c_m\n",
        "\\EC$ or $\\v{F}(\\v{x},\\v{y})=\\v{c}$ where we can solve for $\\v{y}=(y_1,...,y_m)$ in terms of $\\v{x}=(x_1,...,x_n)$.\n",
        "  \n",
        "- Suppose $A$ is open in $‚Ñù^{n+m}$, $\\v{F}:A‚Üí‚Ñù^m$ is $C^1$, and $(\\v{a},\\v{b})=(a_1,...,a_n,b_1,...,b_m)‚ààA$ satisfies $\\v{F}(\\v{a},\\v{b})=\\v{c}$. If Jacobian\n",
        "$\\left|\\/{‚àÇ(F_1,...,F_m)}{‚àÇ(y_1,...,y_m)}\\right|_{(\\v{x},\\v{y})=(\\v{a},\\v{b})}$\n",
        "$=\\det\\BSM\n",
        "\\/{‚àÇF_1}{‚àÇy_1}(\\v{a},\\v{b}) & ... & \\/{‚àÇF_1}{‚àÇy_m}(\\v{a},\\v{b}) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\/{‚àÇF_m}{‚àÇy_1}(\\v{a},\\v{b}) & ... & \\/{‚àÇF_m}{‚àÇy_m}(\\v{a},\\v{b})\n",
        "\\ESM \\neq0$, then there is a neighborhood $U$ of $\\v{a}$ in $‚Ñù^n$ and $\\v{f}:U‚Üí‚Ñù^m$ of $C^1$ such that $\\v{f}(\\v{a})=\\v{b}$ and $\\v{F}(\\v{x},\\v{f}(\\v{x}))=\\v{c}$ for all $\\v{x}‚ààU$ (Colley T6.6)\n",
        "\n",
        "  - T6.6 $\\v{f}(\\v{a})=\\v{b}$ corresponds to T6.5 $f:U‚ÜíV$ where $U$ and $V$ are neighborhoods of $(a_1,...,a_{n-1})$ and $a_n$. T6.6 $\\v{F}(\\v{x},\\v{f}(\\v{x}))=\\v{c}$ corresponds to T6.5 $F(x_1,...,x_n)=c$ and $x_n=f(x_1,...,x_{n-1})$.\n",
        "\n",
        "**Inverse function theorem**: Starting with $\\BC\n",
        "y_1=f_1(x_1,...,x_n)\\\\\n",
        "\\vdots\\\\\n",
        "y_n=f_1(x_1,...,x_n)\n",
        "\\EC$ or $\\v{y}=\\v{f}(\\v{x})$ we want to solve for $\\v{x}=\\v{g}(\\v{y})$ by setting $\\v{F}(\\v{x},\\v{y})=\\v{f}(\\v{x})-\\v{y}=\\v{0}$ and then solve for $\\v{x}$ in terms of $\\v{y}$ near $(\\v{a},\\v{b})$.\n",
        "\n",
        "- Suppose $\\v{f}=(f_1,...,f_n)$ is $C^1$ on open set $A‚äÜ‚Ñù^n$. If Jacobian $\\left|\\/{‚àÇ(f_1,...,f_n)}{‚àÇ(x_1,...,x_n)}\\right|_{\\v{x}=\\v{a}}\\neq0$ then there is an open set $U‚äÜ‚Ñù^n$ containing $\\v{a}$ such that $\\v{f}$ is one-to-one on $U$, the set $V=\\v{f}(U)$ is also open, and there is a uniquely determined inverse function $\\v{g}:V‚ÜíU$ of $C^1$. (Colley T6.7)\n",
        "\n",
        "  - If $\\v{F}(\\v{x},\\v{y})=\\v{f}(\\v{x})-\\v{y}$, then $\\/{‚àÇ\\v{F}}{‚àÇ\\v{x}}(\\v{a},\\v{b})=\\/{‚àÇ\\v{f}}{‚àÇ\\v{x}}(\\v{a})$. Therefore T6.7 and T6.6 Jacobian requirements are equivalent: $\\left|\\/{‚àÇ(f_1,...,f_n)}{‚àÇ(x_1,...,x_n)}\\right|_{\\v{x}=\\v{a}}=\\left|\\/{‚àÇ(F_1,...,F_m)}{‚àÇ(x_1,...,x_m)}\\right|_{(\\v{x},\\v{y})=(\\v{a},\\v{b})}$. The phrase \"open set $U‚äÜ‚Ñù^n$ containing $\\v{a}$\" is equivalent to \"neighborhood $U$ of $\\v{a}$ in $‚Ñù^n$\". The phrase \"the set $V=\\v{f}(U)$ is also open\" is equivalent to \"neighborhood $V$ of $\\v{b}$ in $‚Ñù^n$\".\n",
        "\n",
        "  - If the Jacobian requirement is satisfied then $\\v{y}=\\v{f}(\\v{x})$ may be solved uniquely as $\\v{x}=\\v{g}(\\v{y})$ near $(\\v{a},\\v{b})$."
      ],
      "metadata": {
        "id": "y41S1oms2Fmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colley: Maxima and Minima"
      ],
      "metadata": {
        "id": "7Qn60SWsSQD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taylor's theorem** ($‚Ñù‚Üí‚Ñù$): Let $X$ be open in $‚Ñù$ and $f:X‚äÜ‚Ñù‚Üí‚Ñù$ be $C^k$. Given $k$th order Taylor polynomial of $f$ at $a‚ààX$:\n",
        "$p_k(x)=f(a)+f'(a)(x-a)+...+\\/{f^{(k)}(a)}{k!}(x-a)^k$\n",
        "then\n",
        "$R_k(x,a)=f(x)-p_k(x)$\n",
        "satisfies\n",
        "$\\lim\\limits_{x‚Üía}\\/{R_k(x,a)}{(x-a)^k}=0$.\n",
        "(Colley T1.1)\n",
        "\n",
        "- Proof (Taylor polynomial):\n",
        "By fundamental theorem of calculus\n",
        "$f(x)=f(a)+‚à´_a^xf'(t)\\ dt$\n",
        "$=f(a)-‚à´_a^xf'(t)\\ (-dt)$\n",
        "$=f(a)-\\ob{[f'(t)(x-t)]_{t=a}^x+‚à´_a^x(x-t)f''(t)\\ dt}{u=f'(t),\\ v=x-t,\\ dv=-dt,\\ du=f''(t)\\ dt}$\n",
        "$=f(a)+f'(t)(x-t)+‚à´_a^x(x-t)f''(t)\\ dt$.\n",
        "\n",
        "- Proof (remainder convergence):\n",
        "$R_k(x,a)=‚à´_a^x\\/{f^{(k+1)}(t)}{k!}(x-t)^k\\ dt$.\n",
        "Because $a‚â§t‚â§x$, $|x-t|‚â§|x-a|$. Also assume $f$ is $C^{k+1}$ then $f^{(k+1)}(t)$ is bounded.\n",
        "$\\lim\\limits_{x‚Üía}|\\/{R_k(x,a)}{(x-a)^k}|$\n",
        "$=\\lim\\limits_{x‚Üía}|‚à´_a^x\\/{f^{(k+1)}(t)}{k!}\\/{(x-t)^k}{(x-a)^k}\\ dt|$\n",
        "$‚â§\\lim\\limits_{x‚Üía}‚à´_a^x\\/{M}{k!}\\/{|x-a|^k}{(x-a)^k}\\ dt$\n",
        "$=\\lim\\limits_{x‚Üía}\\/{M}{k!}|x-a|$.\n",
        "\n",
        "- Suppose $f:X‚äÜ‚Ñù‚Üí‚Ñù$ differentiable at point $a‚ààX$. The tangent line\n",
        "$p_1(x)=f(a)+f'(a)(x-a)$\n",
        "is the best linear approximation:\n",
        "$p_1(a)=f(a)$ and $p_1'(a)=f'(a)$,\n",
        "while remainder $R_1(x,a)=f(x)-p_1(x)$\n",
        "satisfies $\\lim\\limits_{x‚Üía}\\/{R_1(x,a)}{x-a}=0$.\n",
        "The tangent line is a good approximation over very small neighborhood of $a$. If $f$ is $C^2$ then parabolic approximation may be used\n",
        "$p_2(x)=f(a)+f'(a)(x-a)+\\/{f''(a)}{2}(x-a)^2$\n",
        "so that\n",
        "$p_2(a)=f(a)$,\n",
        "$p_2'(a)=f'(a)$, and\n",
        "$p_2''(a)=f''(a)$,\n",
        "while remainder $R_2(x,a)=f(x)-p_2(x)$\n",
        "satisfies $\\lim\\limits_{x‚Üía}\\/{R_2(x,a)}{(x-a)^2}=0$.\n",
        "\n",
        "- **Lagrange's remainder**: If $f$ is $C^{k+1}$ then there exists $a‚â§z‚â§x$ such that $R_k(x,a)=\\/{f^{(k+1)}(z)}{(k+1)!}(x-a)^{k+1}$. (Colley P1.2)\n",
        "\n",
        "  - Mean value theorem: for continuous $g,h$ where $h$ does not change sign on $[a,b]$ then there is $z‚àà[a,b]$ such that $‚à´_a^bg(t)h(t)\\ dt=g(z)‚à´_a^bh(t)\\ dt$.\n",
        "\n",
        "  - Proof: $R_k(x,a)=‚à´_a^x\\/{f^{(k+1)}(t)}{k!}(x-t)^k\\ dt$\n",
        "  $=\\/{f^{(k+1)}(z)}{k!}‚à´_a^x(x-t)^k\\ dt$\n",
        "  $=\\/{f^{(k+1)}(z)}{(k+1)!}(x-a)^{k+1}$.\n",
        "\n",
        "  - Example 3: $f(x)=\\cos x$ at $x=\\/{œÄ}{2}$. Taylor series $p_5(x)=-(x-\\/{œÄ}{2})+\\/{1}{3!}(x-\\/{œÄ}{2})^3-\\/{1}{5!}(x-\\/{œÄ}{2})^5$ and $R_5(x,\\/{œÄ}{2})=-\\/{\\cos z}{6!}(x-\\/{œÄ}{2})^6$ where $z‚àà[\\/{œÄ}{2},x]$.\n",
        "  Then $|R_5(x,\\/{œÄ}{2})|‚â§|\\/{1}{6!}(x-\\/{œÄ}{2})^6|$.\n",
        "  Therefore\n",
        "  $|R_5(0,\\/{œÄ}{2})|=|R_5(œÄ,\\/{œÄ}{2})|$\n",
        "  $‚â§|\\/{1}{6!}(\\/{œÄ}{2})^6|=0.02$ on $[0,œÄ]$.\n",
        "\n",
        "**First-order Taylor's Series** ($‚Ñù^n‚Üí‚Ñù$): Let $X$ be open in $‚Ñù^n$ and $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is differentiable at $\\v{a}‚ààX$. Let $p_1(\\v{x})=f(\\v{a})+Df(\\v{a})(\\v{x}-\\v{a})$. Then\n",
        "$f(\\v{x})=p_1(\\v{x})+R_1(\\v{x},\\v{a})$ where\n",
        "$\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{R_1(\\v{x},\\v{a})}{\\n{\\v{x}-\\v{a}}}=0$. (Colley T1.3)\n",
        "\n",
        "- $p_1(\\v{x})=f(\\v{a})+\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})(x_i-a_i)$ is also the tangent hyperplane of $f(\\v{x})$ at $\\v{a}$, providing a good linear approximation of $f$ near $\\v{a}$. First order Taylor's series is just linear approximation satisfying\n",
        "$p_1(\\v{a})=f(\\v{a})$ and\n",
        "$\\/{‚àÇp_1}{‚àÇx_i}(\\v{a})=\\/{‚àÇf}{‚àÇx_i}(\\v{a})$.\n",
        "\n",
        "- **Differential**: Let $\\v{h}=\\v{x}-\\v{a}$ then\n",
        "$p_1(\\v{x})=f(\\v{a})+\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})h_i$.\n",
        "The incremental change is $Œîf=f(\\v{a}+\\v{h})-f(\\v{a})$.\n",
        "The total differential is $df(\\v{a},\\v{h})=\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})h_i$.\n",
        "For $\\v{h}‚âà\\v{0}$, $Œîf=df$. (Colley D1.4)\n",
        "\n",
        "  - $Œîx_i$ and $dx_i$ are often used instead of $h_i$: $df(\\v{a},\\v{dx})=\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})dx_i$.\n",
        "\n",
        "**Second-order Taylor's Series** ($‚Ñù^n‚Üí‚Ñù$): Let $X$ be open in $‚Ñù^n$ and $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is $C^2$. Let\n",
        "$\\v{h}=\\v{x}-\\v{a}$ and\n",
        "$p_2(\\v{x})=f(\\v{a})+Df(\\v{a})\\v{h}+\\/{1}{2}\\v{h}^THf(\\v{a})\\v{h}$ then\n",
        "$f(\\v{x})=p_2(\\v{x})+R_2(\\v{x},\\v{a})$ where\n",
        "$\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{|R_2(\\v{x},\\v{a})|}{\\n{\\v{x}-\\v{a}}^2}=0$. (Colley T1.5)\n",
        "\n",
        "- $p_2(\\v{x})=f(\\v{a})+\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})(x_i-a_i)+\\/{1}{2}\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n\\/{‚àÇf}{‚àÇx_i‚àÇx_j}(\\v{a})(x_i-a_i)(x_j-a_j)$\n",
        "satisfies\n",
        "$p_2(\\v{a})=f(\\v{a})$,\n",
        "$\\/{‚àÇp_2}{‚àÇx_i}(\\v{a})=\\/{‚àÇf}{‚àÇx_i}(\\v{a})$, and\n",
        "$\\/{‚àÇ^2p_2}{‚àÇx_i‚àÇx_j}(\\v{a})=\\/{‚àÇ^2f}{‚àÇx_i‚àÇx_j}(\\v{a})$.\n",
        "\n",
        "- **Hessian** of $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is the symmetric matrix\n",
        "$Hf(\\v{a})=‚àá_\\v{x}^2f(\\v{a})=\\BSM\n",
        "\\/{‚àÇ^2f}{‚àÇx_1‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇ^2f}{‚àÇx_1‚àÇx_n}(\\v{a})\\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\/{‚àÇ^2f}{‚àÇx_n‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇ^2f}{‚àÇx_n‚àÇx_n}(\\v{a})\n",
        "\\ESM$\n",
        "\n",
        "**Remainders**: In $‚Ñù‚Üí‚Ñù$, let $h=x-a$ then\n",
        "$R_k(x,a)=‚à´_a^x\\/{f^{(k+1)}(s)}{k!}(x-s)^k\\ ds$\n",
        "$=‚à´_0^1\\/{f^{(k+1)}(a+th)}{k!}h^{k+1}(1-t)^k\\ dt$.\n",
        "$R_1(x,a)=‚à´_a^xf''(t)(x-t)\\ dt$\n",
        "$=‚à´_0^1(1-t)f''(a+th)h^2\\ dt$\n",
        "\n",
        "- Proof: Let $s=a+(x-a)t$ then $t=\\/{s-a}{x-a}$ and $ds=(x-a)\\ dt$.\n",
        "Integrand\n",
        "$(x-s)^k\\ ds$\n",
        "$=[x-a-(x-a)t]^k(x-a)\\ dt$\n",
        "$=[(x-a)(1-t)]^k(x-a)\\ dt$\n",
        "$=(x-a)^{k+1}(1-t)^k\\ dt$.\n",
        "Therefore\n",
        "$R_k(x,a)=‚à´_0^1\\/{f^{(k+1)}(a+t(x-a))}{k!}(x-a)^{k+1}(1-t)^k\\ dt$\n",
        "$=‚à´_0^1\\/{f^{(k+1)}(a+th)}{k!}h^{k+1}(1-t)^k\\ dt$.\n",
        "\n",
        "- Let $\\v{h}=\\v{x}-\\v{a}$, then $R_1(\\v{x},\\v{a})=‚à´_0^1(1-t)\\v{h}^THf(\\v{a}+t\\v{h})\\v{h}\\ dt$\n",
        "$=\\sum\\limits_{i,j=1}^n‚à´_0^1(1-t)\\/{‚àÇ^2f}{‚àÇx_i‚àÇx_j}(\\v{a}+t\\v{h})h_ih_j\\ dt$.\n",
        "\n",
        "- $R_2(\\v{x},\\v{a})=\\sum\\limits_{i,j,k=1}^n‚à´_0^1\\/{(1-t)^2}{2}\\/{‚àÇ^3f}{‚àÇx_i‚àÇx_j‚àÇx_k}(\\v{a}+t\\v{h})h_ih_jh_k\\ dt$\n",
        "\n",
        "- **Lagrange's remainder**: $R_k(x,a)=\\/{f^{(k+1)}(z)}{(k+1)!}(x-a)^{k+1}$\n",
        "\n",
        "  - $R_1(\\v{x},\\v{a})=\\/{1}{2}\\sum\\limits_{i,j=1}^n\\/{‚àÇ^2f}{‚àÇx_i‚àÇx_j}(\\v{z})h_ih_j$ for suitable $\\v{z}$ on the line segment joining $\\v{a}$ and $\\v{x}=\\v{a}+\\v{h}$.\n",
        "\n",
        "  - $R_2(\\v{x},\\v{a})=\\/{1}{3!}\\sum\\limits_{i,j,k=1}^n\\/{‚àÇ^3f}{‚àÇx_i‚àÇx_j‚àÇx_k}(\\v{z})h_ih_jh_k$ for suitable $\\v{z}$ on the line segment joining $\\v{a}$ and $\\v{x}=\\v{a}+\\v{h}$."
      ],
      "metadata": {
        "id": "Kp7XyEue8B1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f$ has **local minimum** at $\\v{a}‚ààX$ if there some neighborhood $U$ of $\\v{a}$ such that $f(\\v{x})‚â•f(\\v{a})$ for all $\\v{x}‚ààU$. $f$ has **local maximum** at $\\v{a}‚ààX$ if $f(\\v{x})‚â§f(\\v{a})$ for all $\\v{x}‚ààU$. (Colley D2.1) **Critical point** is a point $\\v{a}‚ààX$ where $Df(\\v{a})$ is either 0 or undefined.\n",
        "\n",
        "- Let $X$ be open in $‚Ñù^n$ and $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be $C^1$. If $f$ has local extremum at $\\v{a}‚ààX$ then $Df(\\v{a})=\\v{0}$. (Colley T2.2)\n",
        "\n",
        "**Second derivative test (Hessian test)**: Let $X$ be open in $‚Ñù^n$, $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be $C^2$, and $\\v{a}‚ààX$ be a critical point. Find $Hf(\\v{a})$,\n",
        "$d_1=\\BVM \\/{‚àÇ^2f}{‚àÇx_1‚àÇx_1}(\\v{a}) \\EVM$,\n",
        "$d_2=\\BVM\n",
        "\\/{‚àÇ^2f}{‚àÇx_1‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_1‚àÇx_2}(\\v{a})\\\\\n",
        "\\/{‚àÇ^2f}{‚àÇx_2‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_2‚àÇx_2}(\\v{a})\n",
        "\\EVM$,\n",
        "$d_3=\\BVM\n",
        "\\/{‚àÇ^2f}{‚àÇx_1‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_1‚àÇx_2}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_1‚àÇx_3}(\\v{a})\\\\\n",
        "\\/{‚àÇ^2f}{‚àÇx_2‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_2‚àÇx_2}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_2‚àÇx_3}(\\v{a})\\\\\n",
        "\\/{‚àÇ^2f}{‚àÇx_3‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_3‚àÇx_2}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_3‚àÇx_3}(\\v{a})\\\\\n",
        "\\EVM$, ... (sequence of principal minors)\n",
        "1. If $d_k>0$ for all $k$ then $Hf(\\v{a})$ is positive definite and $\\v{a}$ is a local minimum.\n",
        "2. If $\\BC\n",
        "d_k>0 &k\\t{ even}\\\\\n",
        "d_k< 0&k\\t{ odd}\n",
        "\\EC$ then $Hf(\\v{a})$ is negative definite and $\\v{a}$ is a local maximum.\n",
        "3. If $\\det Hf(\\v{a})\\neq0$ but $Hf(\\v{a})$ is indefinite (neither positive nor negative definite) then $\\v{a}$ is a saddle point.\n",
        "4. If $\\det Hf(\\v{a})=0$ then $\\v{a}$ is degenerate. (Colley T2.3)\n",
        "\n",
        "- Example 3: $f(x,y)=x^2+xy+y^2+2x-2y+5$ has $\\BC\n",
        "\\/{‚àÇf}{‚àÇx}=2x+y+2=0\\\\\n",
        "\\/{‚àÇf}{‚àÇy}=x+2y-2=0\n",
        "\\EC$\n",
        "so $(-2,2)$ is a critical point.\n",
        "$Œîf(\\v{a})=f(\\v{a}+\\v{h})-f(\\v{a})$\n",
        "$=f(-2+h,2+k)-f(-2,2)$\n",
        "$=h^2+hk+k^2$.\n",
        "If $Œîf‚â§0$ for all small $\\v{h}$ then $(-2,2)$ is a local maximum. If $Œîf‚â•0$ then $(-2,2)$ is a local minimum.\n",
        "$Œîf=h^2+hk+k^2$\n",
        "$=(h+\\/{1}{2}k)^2+\\/{3}{4}k^2$\n",
        "$‚â•0$ for all $\\v{h}$ therefore $(-2,2)$ is a local minimum.\n",
        "- Example 4: $f(x,y)=x^2+xy+y^2+2x-2y+5$ has $Hf(\\v{a})=\\BSM\n",
        "2 & 1 \\\\\n",
        "1 & 2\n",
        "\\ESM$.\n",
        "Then $d_1=\\BVM 2 \\EVM$ and\n",
        "$d_2=\\BVM\n",
        "2 & 1 \\\\\n",
        "1 & 2\n",
        "\\EVM=3$.\n",
        "The Hessian is positive definite therefore $(-2,2)$ is a local minimum.\n",
        "\n",
        "- Suppose $f(\\v{x})$ is $C^2$ and $\\v{a}$ is critical point of $f$. Then by second-order Taylor series\n",
        "$Œîf=f(\\v{a}+\\v{h})-f(\\v{a})$\n",
        "$‚âà\\ob{p_2(\\v{a}+\\v{h})}{p_2(\\v{x})}-f(\\v{a})$\n",
        "$=\\ob{Df(\\v{a})\\v{h}}{Df(\\v{a})=\\v{0}}+\\/{1}{2}\\v{h}^THf(\\v{a})\\v{h}$.\n",
        "Therefore\n",
        "$Œîf‚âà\\/{1}{2}\\v{h}^THf(\\v{a})\\v{h}$.\n",
        "\n",
        "  - **Quadratic form** $Q(\\v{h})=\\v{h}^TB\\v{h}=\\sum\\limits_{i,j=1}^nb_{ij}h_ih_j$ where $B$ is a symmetric matrix $B^T=B$ or $b_{ij}=b_{ji}$ is said to be **positive definite** if $Q(\\v{h})>0$ for all $\\v{h}\\neq\\v{0}$ and **negative definite** if $Q(\\v{h})< 0$ for all $\\v{h}\\neq\\v{0}$.\n",
        "\n",
        "  - Let $B_k,\\ k=1,...,n$ be the upper-left-most $k√ók$ submatrix of symmetric matrix $B$ such that\n",
        "  $B_1=\\BSM b_{11} \\ESM$,\n",
        "  $B_2=\\BSM\n",
        "  b_{11} & b_{12}\\\\\n",
        "  b_{21} & b_{22}\n",
        "  \\ESM$,\n",
        "  $B_3=\\BSM\n",
        "  b_{11} & b_{12} & b_{13}\\\\\n",
        "  b_{21} & b_{22} & b_{23}\\\\\n",
        "  b_{31} & b_{32} & b_{33}\\\\\n",
        "  \\ESM$, ...\n",
        "  If $(\\det B_1, \\det B_2, ..., \\det B_n)$ consists entirely of positive numbers then $B$ and $Q$ are positive definite. If $\\BC\n",
        "  \\det B_k< 0 &k\\t{ odd} \\\\\n",
        "  \\det B_k> 0 &k\\t{ even}\n",
        "  \\EC$ then $B$ and $Q$ are negative definite.\n",
        "\n",
        "- **Extreme value theorem**: Subset $X‚äÜ‚Ñù^n$ is **compact** if it is both closed and bounded (Colley D2.4) If $X$ is compact and $f|_X:‚Ñù^n‚Üí‚Ñù$ is continuous, then there must exist $\\v{a}_\\max$ and $\\v{a}_\\min$ in $X$ such that for all $\\v{x}‚ààX$, $f(\\v{a}_\\min)‚â§f(\\v{x})‚â§f(\\v{a}_\\max)$. (Colley T2.5)\n",
        "\n",
        "  - $f|_X$ is the restriction of $f$ to compact subset $X$.\n",
        "\n",
        "- Proof: If $B$ is symmetric matrix associated with a positive definite $Q$, then there exists $M>0$ such that $Q(\\v{h})‚â•M\\vn{h}^2$ for all $\\v{h}‚àà‚Ñù^n$.\n",
        "  1. If $\\v{h}=\\v{0}$ then $Q(\\v{h})=0$. Therefore $M>0$ exists.\n",
        "  2. If $\\vn{h}=1$ then $S=\\{\\v{h}‚àà‚Ñù^n:\\vn{h}=1\\}$ is a compact set and $Q|_S:S‚äÜ‚Ñù^n‚Üí‚Ñù$ must have a global minimum $M$ such that $Q(\\v{h})‚â•M$ for all $\\v{h}‚ààS$. Because $Q$ is positive definite, $Q(\\v{h})>0$ for all $\\v{h}\\neq\\v{0}$ and therefore $M>0$ exists.\n",
        "  3. for nonzero $\\v{h}‚àà‚Ñù^n$, then $\\/{\\v{h}}{\\vn{h}}$ is a unit vector such that $Q(\\vn{h}\\/{\\v{h}}{\\vn{h}})‚â•\\vn{h}^2M$.\n",
        "\n",
        "  Now,\n",
        "  $Œîf=\\/{1}{2}\\v{h}^THf(\\v{a})\\v{h}+R_2(\\v{x},\\v{a})$\n",
        "  where\n",
        "  $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{|R_2(\\v{x},\\v{a})|}{\\n{\\v{x}-\\v{a}}^2}=0$.\n",
        "\n",
        "  1. **Positive definite** Suppose $Hf(\\v{a})$ is positive definite. There must exist $M>0$ such that\n",
        "  $\\/{1}{2}\\v{h}^THf(\\v{a})\\v{h}‚â•M\\vn{h}^2$.\n",
        "  $\\lim\\limits_{\\v{h}‚Üí\\v{0}}\\/{|R_2(\\v{x},\\v{a})|}{\\vn{h}^2}=0$\n",
        "  therefore there must exist $\\v{h}$ small enough such that\n",
        "  $\\/{|R_2(\\v{x},\\v{a})|}{\\vn{h}^2}< M‚áí|R_2(\\v{x},\\v{a})|< M\\vn{h}^2$.\n",
        "  Under such conditions $0< Œîf< ‚àû$ and $\\v{a}$ is a local minimum.\n",
        "\n",
        "  2. **Negative definite** Suppose $Hf(\\v{a})$ is negative definite. Let $g=-f$ then $Hg(\\v{a})$ is positive definite. Therefore $\\v{a}$ is a local minimum of $g$ and a local maximum of $f$.\n",
        "\n"
      ],
      "metadata": {
        "id": "aKF7qZ6edtwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constrained optimization**: Let $X$ be open in $‚Ñù^n$ and $f,g:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be $C^1$. Let $S=\\{\\v{x}‚ààX|g(\\v{x})=c\\}$ denote the level set of $g$ at height $c$. Then if $f|_S$ has an extremum at $\\v{x}_0‚ààS$ such that $‚àág(\\v{x}_0)\\neq\\v{0}$, there must some **Lagrange multiplier** $Œª‚àà‚Ñù$ such that $‚àáf(\\v{x}_0)=Œª‚àág(\\v{x}_0)$. (Colley T3.1)\n",
        "\n",
        "- Solve $\\BC\\t{extremum}&f(\\v{x})\\\\\\t{subject to}&g(\\v{x})=c\\EC$ with $\\red{\\BC ‚àáf(\\v{x})=Œª‚àág(\\v{x})\\\\g(\\v{x})=c\\EC}$ equivalent to Lagrangian $\\m{L}(\\v{x},Œª)=f(\\v{x})-Œª(g(\\v{x})-c)$\n",
        "\n",
        "- Intuition: level set $S=\\{\\v{x}‚ààX|g(\\v{x})=c\\}$ creates a restricted domain for $\\v{x}$ satisfying $g$ constraint. This restricted domain looks like a contour on the domain, and acts as a path. We're interested in extremum for $f|_S$, which is surface $f$ limited to a path/contour in the domain. We walk along the domain path until we touch the extrema of $f|_S$, which have flat slope and are tangential to the level set of $f$.\n",
        "\n",
        "- Proof: $‚àág(\\v{x}_0)$ and $‚àáf(\\v{x}_0)$ are perpendicular to level sets of $g$ and $f$ (Ch2T6.4). $‚àáf(\\v{x}_0)=Œª‚àág(\\v{x}_0)$ says for $\\v{x}_0$ to be a constrained extremum, the level sets of $f$ and $g$ are tangent at $\\v{x}_0$. Suppose $\\v{x}_0$ is an extremum of $f$ restricted to a parametric curve $\\v{x}:T‚äÜ‚Ñù‚ÜíS‚äÇ‚Ñù^n$ on hyperplane $S=\\{\\v{x}|g(\\v{x})=c\\}$ with $\\v{x}(t_0)=\\v{x}_0$ for some $t_0‚ààT$.\n",
        "\n",
        "  - $\\v{x}_0$ is an extremum of $f$ therefore $\\/{d}{dt}f(\\v{x}(t))|_{t=t_0}=0$.\n",
        "\n",
        "  - Chain rule: $\\/{d}{dt}f(\\v{x}(t))=‚àáf(\\v{x}(t))‚ãÖ\\/{d\\v{x}}{dt}$ where $\\/{d\\v{x}}{dt}(t)$ is the slope of the parametric curve on $S$.\n",
        "\n",
        "  - $0=\\/{d}{dt}f(\\v{x}(t))|_{t=t_0}=‚àáf(\\v{x}_0)‚ãÖ\\/{d\\v{x}}{dt}(t_0)$.\n",
        "\n",
        "  As shown, $‚àáf(\\v{x}_0)$ and $‚àág(\\v{x}_0)$ are both perpendicular to $S$. Therefore  $‚àáf(\\v{x}_0)=Œª‚àág(\\v{x}_0)$.\n",
        "\n",
        "**Multiple constraint Lagrange**: Let $X$ be open in $‚Ñù^n$ and $f,g_1,...,g_k:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be $C^1$ where $k< n$. Let $S=\\{\\v{x}‚ààX|g_1(\\v{x})=c_1,...,g_2(\\v{x})=c_k\\}$. If $f|_S$ has extremum at $\\v{x}_0‚ààX$ where $‚àág_1(\\v{x}_0),...,‚àág_k(\\v{x}_0)$ are linearly independent vectors, then there must exist $Œª_1,...,Œª_k‚àà‚Ñù$ such that $‚àáf(\\v{x}_0)=Œª_1‚àág_1(\\v{x}_0)+...+Œª_k‚àág_k(\\v{x}_0)$. (Colley T3.2)\n",
        "\n",
        "- $S$ is the intersection of $S_1,...,S_k$ where $S_j=\\{\\v{x}‚àà‚Ñù^n|g_j(\\v{x})=c_j\\}$. Any vector tangent to $S$ at $\\v{x}_0$ must be perpendicular to each $‚àág_j(\\v{x}_0)$.\n",
        "\n",
        "**Second derivative test (bordered Hessian test)**: We seek extrema of $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ subject to constraints $g_1(\\v{x})=c_1,...,g_k(\\v{x})=c_k,\\ k< n$ which are all $C^2$. Then $(Œª;\\v{a})=(Œª_1,...,Œª_k;a_1,...,a_n)$ is an unconstrained critical point to Lagrangian function $L(l_1,...,l_k;x_1,...,x_n)=f(\\v{x})-\\sum\\limits_{i=1}^kl_i(g_i(\\v{x})-c_i)$.\n",
        "The implicit function theorem requires that $D\\v{g}(\\v{a})=\\BSM\n",
        "\\/{‚àÇg_1}{‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇg_1}{‚àÇ\\blue{x_n}}(\\v{a}) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\/{‚àÇg_k}{‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇg_k}{‚àÇ\\blue{x_n}}(\\v{a})\n",
        "\\ESM$ satisfies $\\det\\BVM\n",
        "\\/{‚àÇg_1}{‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇg_1}{‚àÇ\\blue{x_k}}(\\v{a}) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\/{‚àÇg_k}{‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇg_k}{‚àÇ\\blue{x_k}}(\\v{a})\n",
        "\\EVM\\neq0$ for any $k$ of the $n$ columns, which is true if $‚àág_{1,...,k}(\\v{a})$ are linearly independent.\n",
        "$HL(Œª;\\v{a})=\\BSM\n",
        "0 & -(‚àág)^T \\\\\n",
        "‚àág & ‚àá_\\v{x}^2L\n",
        "\\ESM=\\BSM\n",
        "0 & ... & 0 & -\\/{‚àÇg_1}{‚àÇx_1}(\\v{a}) & ... & -\\/{‚àÇg_1}{‚àÇx_n}(\\v{a}) \\\\\n",
        "\\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & ... & 0 & -\\/{‚àÇg_k}{‚àÇx_1}(\\v{a}) & ... & -\\/{‚àÇg_k}{‚àÇx_n}(\\v{a}) \\\\\n",
        "-\\/{‚àÇg_1}{‚àÇx_1}(\\v{a}) & ... & -\\/{‚àÇg_k}{‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2L}{‚àÇx_1‚àÇx_1}(Œª;\\v{a}) & ... & \\/{‚àÇ^2L}{‚àÇx_1‚àÇx_n}(Œª;\\v{a}) \\\\\n",
        "\\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "-\\/{‚àÇg_1}{‚àÇx_n}(\\v{a}) & ... & -\\/{‚àÇg_k}{‚àÇx_n}(\\v{a}) & \\/{‚àÇ^2L}{‚àÇx_n‚àÇx_1}(Œª;\\v{a}) & ... & \\/{‚àÇ^2L}{‚àÇx_n‚àÇx_n}(Œª;\\v{a})\n",
        "\\ESM$ is a $(n+k)√ó(n+k)$ symmetric matrix, from which we find the sequence of minors $s_1,...,s_{n-k}=(-1)^kd_{2k+1},\\ ...,\\ (-1)^kd_{k+n}$ and conclude positive definite or negative definite: $s_j>0$ for all $j$ then positive definite, $s_1< 0$ followed by alternating signs then negative definite.\n",
        "\n",
        "- Implicit function theorem requirement needs any combination of $k$ out of $n$ columns to produce a non-zero determinant. If it fails, the \"border\" rows and columns lose rank and $\\det HL(Œª;\\v{a})=0$. The critical point will be degenerate.\n",
        "\n",
        "- Let $f,g:U‚äÜ‚Ñù^2‚Üí‚Ñù$ be $C^2$. Let $\\v{a}$ be critical point of $f|_S$ constrained by $S=\\{\\v{x}‚ààU|g(\\v{x})=c\\}$. Then $L(\\v{x})=f(\\v{x})-Œªg(\\v{x})$. Then the **bordered Hessian** $HL(\\v{a})=\\BSM\n",
        "0 & -\\/{‚àÇg}{‚àÇx}(\\v{a}) & -\\/{‚àÇg}{‚àÇy}(\\v{a}) \\\\\n",
        "-\\/{‚àÇg}{‚àÇx}(\\v{a}) & \\/{‚àÇ^2g}{‚àÇx^2}(\\v{a}) & \\/{‚àÇ^2g}{‚àÇx‚àÇy}(\\v{a}) \\\\\n",
        "-\\/{‚àÇg}{‚àÇy}(\\v{a}) & \\/{‚àÇ^2g}{‚àÇy‚àÇx}(\\v{a}) & \\/{‚àÇ^2g}{‚àÇy^2}(\\v{a})\n",
        "\\ESM$, then use $s_1=(-1)^kd_{2k+1}=\\det HL(\\v{a})$ to determine its positive/negative definiteness. (Marsden 3.4 T10)\n",
        "\n",
        "- Example 1: An open box is to have a fixed volume of 4. What dimensions should the box have to minimize the material used?\n",
        "$A(x,y,z)=2xy+2yz+xz$\n",
        "with constraint\n",
        "$V(x,y,z)=xyz=4$.\n",
        "Create new area function with two variables\n",
        "$a(x,y)=A(x,y,\\/{4}{xy})$\n",
        "$=2xy+\\/{8}{x}+\\/{4}{y}$.\n",
        "$\\BC\n",
        "\\/{‚àÇa}{‚àÇx}=2y-\\/{8}{x^2}=0 \\\\\n",
        "\\/{‚àÇa}{‚àÇy}=2x-\\/{4}{y^2}=0\n",
        "\\EC$\n",
        "$‚áíx(1-\\/{1}{8}x^3)=0$\n",
        "$‚áíx=0,2$.\n",
        "Constrained critical point of $A$ is $(2,1,2)$.\n",
        "Hessian $Ha=\\BSM\n",
        "\\/{16}{x^3} & 2 \\\\ 2 & \\/{8}{y^3}\n",
        "\\ESM$\n",
        "so $Ha(2,1)=\\BSM\n",
        "2 & 2 \\\\ 2 & 8\n",
        "\\ESM$ is positive definite so $(2,1)$ is a local minimum of $a$.\n",
        "  - The creation of $a(x,y)$ applies implicit function theorem.\n",
        "- Example 2: $A(x,y,z)=2xy+2yz+xz$ constrained by $V(x,y,z)=xyz=4$. Then $\\BC\n",
        "2y+z=Œªyz\\\\\n",
        "2x+z=Œªxz\\\\\n",
        "2y+x=Œªxy\\\\\n",
        "xyz=4\n",
        "\\EC$\n",
        "using Lagrangian setup has one solution $(2,1,2)$.\n",
        "\n",
        "- Example 6: $A(x,y,z)=2xy+2yz+xz$ constrained by $V(x,y,z)=xyz=4$. $f|_g$ has critical point $\\v{a}=(2,1,2)$.\n",
        "$L(x,y,z)=2xy-2yz+xz-Œªxyz$.\n",
        "$HL(\\v{a})=\\BSM\n",
        "0 & -yz & -xz & -xy \\\\\n",
        "-yz & 0 & 2-Œªz & 1-Œªy \\\\\n",
        "-xz & 2-Œªz & 0 & 2-Œªx \\\\\n",
        "-xy & 1-Œªy & 2-Œªx & 0\n",
        "\\ESM$ gives minors\n",
        "$s_1=-d_{3}=32$ and $s_2=-d_{4}=48$ therefore $\\v{a}$ is a local minimum."
      ],
      "metadata": {
        "id": "hEjiFlHfeuv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Linear Independence, Basis, and Transformations"
      ],
      "metadata": {
        "id": "vB6PP0enPpMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector** is a $‚Ñù^{m√ó1}$ single column matrix. **Matrix** $\\v{A}‚àà‚Ñù^{m√ón}$ has $m$ rows and $n$ columns and $\\v{B}‚àà‚Ñù^{n√ók}$, then matrix multiplication is a matrix of dot products\n",
        "$\\v{AB}‚àà‚Ñù^{m√ók}=\\BSM\n",
        "\\sum_{i=1}^na_{1i}b_{i1} & \\sum_{i=1}^na_{1i}b_{i2} & ... & \\sum_{i=1}^na_{1i}b_{ik}\\\\\n",
        "\\sum_{i=1}^na_{2i}b_{i1} & \\sum_{i=1}^na_{2i}b_{i2} & ... & \\sum_{i=1}^na_{2i}b_{ik}\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\sum_{i=1}^na_{mi}b_{i1} & \\sum_{i=1}^na_{mi}b_{i2} & ... & \\sum_{i=1}^na_{mi}b_{ik}\n",
        "\\ESM$\n",
        "\n",
        "- Properties\n",
        "\n",
        "  - $(\\v{AB})\\v{C}=\\v{A}(\\v{BC})$\n",
        "  - $(\\v{A}+\\v{B})\\v{C}=\\v{AC}+\\v{BC}$\n",
        "  and\n",
        "  $A(\\v{C}+\\v{D})=\\v{AC}+\\v{AD}$\n",
        "  - $\\v{I}_m\\v{A}=\\v{A}\\v{I}_n=\\v{A}$\n",
        "\n",
        "- Transpose and inverse: If $\\v{A}‚àà‚Ñù^{2√ó2}$, then $\\v{A}^{-1}=\\/{1}{\\det\\v{A}}\\BM a_{22} & -a_{12} \\\\ -a_{21} & a_{11} \\EM$.\n",
        "\n",
        "  - If $\\v{A}‚àà‚Ñù^{m√óm}$ then: $\\v{A}\\v{A}^{-1}=\\v{I}_m=\\v{A}^{-1}\\v{A}$\n",
        "  - $(\\v{A}^{-1})^{-1}=\\v{A}$\n",
        "  - $(\\v{AB})^{-1}=\\v{B}^{-1}\\v{A}^{-1}$\n",
        "  - **Symmetric matrix**: $\\v{A}^‚ä§=\\v{A}$: sums of symmetric matrices are also symmetric. Inverse of symmetric matrices are also symmetric.\n",
        "  - $(\\v{A}^‚ä§)^‚ä§=\\v{A}$\n",
        "  - $(\\v{AB})^‚ä§=\\v{B}^‚ä§\\v{A}^‚ä§$\n",
        "  - $(\\v{A}+\\v{B})^‚ä§=\\v{A}^‚ä§+\\v{B}^‚ä§$\n",
        "\n",
        "- **Invertibility**: $\\v{A}$ is **nonsingular** iff $\\v{A}$ is square, full rank, and $\\det(\\v{A})\\neq0$. If $\\v{A}$ is symmetric then $\\v{A}$ is nonsingular iff $\\v{A}$ is positive-definite and have positive eigenvalues.\n",
        "\n",
        "**System of equations**\n",
        "$\\BC\n",
        "2x_1+3x_2+5x_3=1\\\\\n",
        "4x_1-2x_2-7x_3=8\\\\\n",
        "9x_1+5x_2-3x_3=2\n",
        "\\EC‚üπ\\BSM 2 & 3 & 5 \\\\ 4 & -2 & -7 \\\\ 9 & 5 & -3\\ESM\n",
        "\\BSM x_1 \\\\ x_2 \\\\ x_3\\ESM=\n",
        "x_1\\BSM 2\\\\4\\\\9\\ESM+x_2\\BSM 3\\\\-2\\\\5\\ESM+x_3\\BSM 5\\\\-7\\\\-3\\ESM=\n",
        "\\BSM 1 \\\\ 8 \\\\ 2\\ESM$\n",
        "\n",
        "- **Gaussian elimination**: row echelon form by elementary transformations (1) exchange 2 rows, (2) multiplication of a row with constant, (3) addition of two rows. The computations required is $O(n^3)$.\n",
        "\n",
        "  - **Row echelon form**: staircase structure where each nonzero row has its **pivot** is to the right of the pivot on the row above it. The pivots are basic variables, the others are free variables.\n",
        "\n",
        "  - **Reduced row echelon form** is when every pivot is 1, and it is the only nonzero entry in its column.\n",
        "\n",
        "  - **Minus-1 trick** for determining null space: Augument $\\t{rref}(\\v{A})$ by inserting a new row with a $-1$ pivot for each free variable. We end up with a square matrix, and the $-1$ columns are the bases of the null space.\n",
        "\n",
        "- **Square Inverse**: $[A|I_n]‚Üí[I_n|A^{-1}]$ using RREF.\n",
        "\n",
        "- Inverse solution: $\\v{x}=\\v{A}^{-1}\\v{b}$ requires $\\v{A}$ to be invertible and square. If $\\v{A}$ is tall rectangular we can use **Moore-Penrose pseudoinverse** $\\v{x}=(\\v{A}^‚ä§\\v{A})^{-1}\\v{A}^‚ä§\\v{b}$, which forces $\\v{A}$ into a square shape. However $(\\v{A}^‚ä§\\v{A})^{-1}$ is very computationally expensive and prone to compounded rounding errors. This finds the least square solution $\\v{x}$.\n",
        "\n",
        "- Iterative solution: Let $\\v{x}^*$ be a solution, and let $\\v{x}^{(k+1)}=\\v{C}\\v{x}^{(k)}+\\v{d}$, and iteratively reduce $\\n{\\v{x}^{(k+1)}-\\v{x}^*}$.\n",
        "\n",
        "- 2.3.1: $\\BM1&0&8&-4\\\\0&1&2&12\\EM\\v{x}=\\BM 42\\\\8\\EM$\n",
        "$\\BC x_1+8x_3-4x_4=42 \\\\ x_2+2x_3+12x_4=8\\EC$.\n",
        "  \n",
        "  - $\\v{x}_0=\\BM42\\\\8\\\\0\\\\0\\EM$ is one solution by setting non-pivot free variables to 0. The full solution set is $\\v{x}_0+\\null(\\v{A})$, where\n",
        "  $\\null(\\v{A})=\\{\\v{x}:\\v{Ax}=\\v{0}\\}$.\n",
        "  We obtain the kernel by expressing non-pivot columns as a combination of pivot columns. $\\v{a}_1=\\BM1\\\\0\\EM$ and $\\v{a}_2=\\BM0\\\\1\\EM$ are pivot columns. $\\v{a}_3=\\BM8\\\\2\\EM$ and $\\v{a}_4=\\BM-4\\\\12\\EM$ are non-pivot.\n",
        "  \n",
        "  - $8\\v{a}_1+2\\v{a}_2-\\v{a}_3+0\\v{a}_4=\\v{0}$, or\n",
        "  $\\BM1&0&8&-4\\\\0&1&2&12\\EM\\BM8\\\\2\\\\-1\\\\0\\EM=\\BM0\\\\0\\EM$ and\n",
        "  $\\BM8\\\\2\\\\-1\\\\0\\EM‚àà\\null(\\v{A})$.\n",
        "  \n",
        "  - Similarly,\n",
        "  $-4\\v{a}_1+12\\v{a}_2+0\\v{a}_3-\\v{a}_4=\\v{0}$, and\n",
        "  $\\BM-4\\\\12\\\\0\\\\-1\\EM‚àà\\null(\\v{A})$.\n",
        "  \n",
        "  - Therefore\n",
        "  $\\v{x}=\\BM 42\\\\8\\\\0\\\\0 \\EM+\n",
        "  Œª_1\\BM 8\\\\2\\\\-1\\\\0 \\EM+\n",
        "  Œª_2\\BM -4\\\\12\\\\0\\\\-1 \\EM$\n",
        "  for all $Œª_1,Œª_2‚àà‚Ñù$.\n",
        "\n",
        "  - (minus-1 trick): $\\BM 1 & 0 & \\green{8} & \\blue{-4}\\\\0 & 1 & \\green{2} & \\blue{12}\\\\\\dg{0} & \\dg{0} & \\green{-1} & \\blue{0}\\\\\\dg{0} & \\dg{0} & \\green{0} & \\blue{-1}\\EM$\n",
        "\n",
        "- 2.6: $\\BM 1&-2&1&-1&1\\\\0&0&1&-1&3\\\\0&0&0&1&-2 \\EM \\v{x}=\\BM 0\\\\-2\\\\1 \\EM$\n",
        "$‚áí\\BM \\green{1}&-2&0&0&-2&\\green{2}\\\\0&0&\\green{1}&0&1&\\green{-1}\\\\0&0&0&\\green{1}&-2&\\green{1} \\EM$.\n",
        "One solution is $\\v{x}_0=\\BM2\\\\0\\\\-1\\\\1\\\\0\\EM$.\n",
        "Using minus-1 trick $\\BM 1&\\green{-2}&0&0&\\blue{-2}\\\\\\dg{0}&\\green{-1}&\\dg{0}&\\dg{0}&\\blue{0}\\\\0&\\green{0}&1&0&\\blue{1}\\\\0&\\green{0}&0&1&\\blue{-2}\\\\\\dg{0}&\\green{0}&\\dg{0}&\\dg{0}&\\blue{-1} \\EM$,\n",
        "$\\null(\\v{A})=\\span(\\BM-2\\\\-1\\\\0\\\\0\\\\0\\EM,\\BM-2\\\\0\\\\1\\\\-2\\\\-1\\EM)$.\n",
        "The full solution is $\\BM2\\\\0\\\\-1\\\\1\\\\0\\EM+Œª_1\\BM-2\\\\-1\\\\0\\\\0\\\\0\\EM+Œª_2\\BM-2\\\\0\\\\1\\\\-2\\\\-1\\EM$."
      ],
      "metadata": {
        "id": "EMCRVv2VPnJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group**: $(\\m{G},‚äó)$ is called a **group** for set $\\m{G}$ and operation $‚äó:\\m{G}√ó\\m{G}‚Üí\\m{G}$ if the following 4 properties hold:\n",
        "$\\BC\n",
        "1.& \\t{Closure} & ‚àÄx,y‚àà\\m{G}:x‚äó\\v{y}‚àà\\m{G}\\\\\n",
        "2.& \\t{Associativity} & ‚àÄx,y,z‚àà\\m{G}:(x‚äóy)‚äóz=x‚äó(y‚äóz)\\\\\n",
        "3.& \\t{Neutral element} & ‚àÉe‚àà\\m{G}‚àÄx‚àà\\m{G}:x‚äóe=e‚äóx=x\\\\\n",
        "4.& \\t{Inverse element} & ‚àÄx‚àà\\m{G}‚àÉy‚àà\\m{G}:x‚äóy=y‚äóx=e\\\\\n",
        "*& \\t{Abelian group} & ‚àÄx,y‚àà\\m{G}:x‚äóy=y‚äóx\n",
        "\\EC$\n",
        "\n",
        "- The neutral element rule says there is only 1 element $e‚àà\\m{G}$ that all choices of $x$ work with. The inverse rule says every element $x$ has a corresponding $y$ (which may be different for every choice of $x$).\n",
        "\n",
        "**Vector space** $V=(\\m{V},+,‚ãÖ)$ is a set $\\m{V}$ with $\\BC\n",
        "\\t{Inner operation} &+:\\m{V}√ó\\m{V}‚Üí\\m{V}\\\\\n",
        "\\t{Outer operation} &‚ãÖ:‚Ñù√ó\\m{V}‚Üí\\m{V}\n",
        "\\EC$ satisfying $\\BC\n",
        "1.& (\\m{V},+)\\t{ is Abelian} & ‚àÄ\\v{x},\\v{y}‚àà\\m{V}:\\v{x}+\\v{y}=\\v{y}+\\v{x}\\\\\n",
        "2.& \\t{Distributivity} & ‚àÄŒª‚àà‚Ñù,\\v{x},\\v{y}‚àà\\m{V}:Œª(\\v{x}+\\v{y})=Œª\\v{x}+Œª\\v{y}\\\\\n",
        "& & ‚àÄŒª,œà‚àà‚Ñù,\\v{x}‚àà\\m{V}:(Œª+œà)\\v{x}=Œª\\v{x}+œà\\v{x}\\\\\n",
        "3.& \\t{Associativity} & ‚àÄŒª,œà‚àà‚Ñù,\\v{x}‚àà\\m{V}:Œª(œà\\v{x})=œà(Œª\\v{x})\\\\\n",
        "4.& \\t{Neutral element}&‚àÄ\\v{x}‚àà\\m{V}:1\\v{x}=\\v{x}\n",
        "\\EC$\n",
        "\n",
        "- Vector space/subspace is defined by closure under linear operations (vector addition and scalar multiple).\n",
        "\n",
        "- **Subspace**: Let $V=(\\m{V},+,‚ãÖ)$ be a vector space and let $\\m{U}‚äÜ\\m{V},\\m{U}\\neq‚àÖ$. Then $U=(\\m{U},+,‚ãÖ)‚äÜV$ is called a vector subspace of $V$ if $U$ is a vector space with $+$ and $‚ãÖ$ restricted to $\\m{U}√ó\\m{U}$ and $‚Ñù√ó\\m{U}$.\n",
        "\n",
        "  - To prove $(\\m{U},+,‚ãÖ)‚äÜV$ is a subspace, need to show closure $\\BC\n",
        "  1.& \\v{0}‚àà\\m{U}\\\\\n",
        "  2.& ‚àÄŒª‚àà‚Ñù‚àÄ\\v{x}‚àà\\m{U}:Œª\\v{x}‚àà\\m{U}\\\\\n",
        "  3.& ‚àÄ\\v{x},\\v{y}‚àà\\m{U}:\\v{x}+\\v{y}‚àà\\m{U}\n",
        "  \\EC$.\n",
        "\n",
        "**Affine subspace**: Let $V$ be a vector space, $\\v{x}_0‚ààV$ be **support point** and $U‚äÜV$ be **direction space**, then $L=\\v{x}_0+U:=\\{\\v{x}_0+\\v{u}:\\v{u}‚ààU\\}$\n",
        "$=\\{\\v{v}‚ààV|‚àÉ\\v{u}‚ààU:\\v{v}=\\v{x}_0+\\v{u}\\}‚äÜV$ is called **affine subspace / hyperplane** of $V$.\n",
        "\n",
        "- Affine subspaces are not required to pass through $\\v{0}$.\n",
        "\n",
        "- Consider two affine spaces $L=\\v{x}_0+U$ and $\\tilde{L}=\\tilde{\\v{x}}_0+\\tilde{U}$. Then $L‚äÜ\\tilde{L}$ iff $U‚äÜ\\tilde{U}$ and $\\v{x}_0-\\tilde{\\v{x}}_0‚àà\\tilde{U}$.\n",
        "\n",
        "- If $B=(\\v{b}_1,...,\\v{b}_k)$ is an ordered basis of $U$, then every $\\v{x}‚ààL$ is parametrically described by $\\v{x}=\\v{x}_0+Œª_1\\v{b}_1+...+Œª_k\\v{b}_k$.\n",
        "\n",
        "- For vector spaces $V,W$ and linear mapping $Œ¶:V‚ÜíW$, and $\\v{a}‚ààW$, then $œï:V‚ÜíW,œï(\\v{x})=\\v{a}+Œ¶(\\v{x})$ is called an **affine mapping** from $V$ to $W$ and $\\v{a}$ is called the **translation vector** of $œï$.\n",
        "\n",
        "  - $œï:V‚ÜíW,œï=œÑ‚àòŒ¶$ is the composition of a linear mapping $Œ¶:V‚ÜíW$ and a translation $œÑ:W‚ÜíW$."
      ],
      "metadata": {
        "id": "z7GOqQ7CL6V_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear independence**: Consider vector space $V$ and $\\v{x}_{1..k}‚ààV$ where $k‚àà‚Ñï$ and $Œª_{1..k}‚àà‚Ñù$. Every $\\v{v}‚ààV$ of the form $\\v{v}=Œª_1\\v{x}_1+...+Œª_k\\v{x}_k=\\sum_{i=1}^kŒª_i\\v{x}_i‚ààV$ is called a **linear combination** of $\\v{x}_1,...,\\v{x}_k$. Furthermore $\\v{x}_1,...,\\v{x}_k$ are called **linearly independent** if $\\sum_{i=1}^kŒª_i\\v{x}_i=\\v{0}‚áíŒª_1=...=Œª_k=0$.\n",
        "\n",
        "- Let $\\v{B}‚àà‚Ñù^{m√ón}$ and $\\v{Œª}‚àà‚Ñù^n$, then $\\v{BŒª}=Œª_1\\BM B_{11}\\\\\\vdots\\\\B_{m1}\\EM+Œª_2\\BM B_{12}\\\\\\vdots\\\\B_{m2}\\EM...+Œª_n\\BM B_{1n}\\\\\\vdots\\\\B_{mn}\\EM$ is a linear combination of $\\v{B}$ columns. Columns of $\\v{B}=[\\v{x}_1,...,\\v{x}_k]$ are called linearly independent iff $\\sum_{i=1}^kŒª_i\\v{x}_i=\\v{B}\\v{Œª}=\\v{0}$ implies $\\v{Œª}=\\v{0}$.\n",
        "\n",
        "- **Linear dependence**: If $\\v{0}$ is one of the vectors then they're dependent. If $\\v{x}_i$ is a linear combination of the others, then they're dependent. For $m>k$, $m$ linear combinations of $k$ vectors are guaranteed to be dependent.\n",
        "\n",
        "- **Row echelon form**: Vectors are independent if $\\t{ref}([\\v{x}_1,...,\\v{x}_k])$ has a pivot in every column. A non-pivot column is a linear combination of the pivot columns to the left.\n",
        "  - E.g., in $\\BM 1&-1&3 \\\\ 0&1&-2 \\EM$, which can be solved using RREF $\\BM 1&0&1 \\\\ 0&1&-2 \\EM$ resulting in $\\v{c}_3=\\v{c}_1-2\\v{c}_2$.\n",
        "\n",
        "- Suppose $\\v{B}=[\\v{b}_1,...,\\v{b}_k]$ is a matrix whose columns are linearly independent vectors. Define $\\v{x}_1,...,\\v{x}_m$ as linear combinations of $\\v{B}$ columns:\n",
        "$\\v{x}_j=Œª_{1j}\\v{b}_1+...+Œª_{kj}\\v{b}_k$\n",
        "$=\\v{B}\\v{Œª}_j$.\n",
        "Then $\\v{x}_1,...,\\v{x}_m$ are linearly independent iff\n",
        "$\\v{Œª}_1,...,\\v{Œª}_m$ are linearly independent.\n",
        "\n",
        "  - Convention: $\\v{Œª}_{j=1..m}=\\BM Œª_{1j}\\\\\\vdots\\\\Œª_{kj}\\EM$ refers to the $j$-th column vector of matrix $\\v{Œª}‚àà‚Ñù^{k√óm}$.\n",
        "\n",
        "  - Proof: $\\sum_{j=1}^mœà_j\\v{x}_j=\\v{0}$\n",
        "  $‚áí\\sum_{j=1}^mœà_j\\v{B}\\v{Œª}_j=\\v{0}$\n",
        "  $‚áí\\v{B}\\sum_{j=1}^mœà_j\\v{Œª}_j=\\v{0}$\n",
        "  $‚áí\\sum_{j=1}^mœà_j\\v{Œª}_j=\\v{0}$\n",
        "  because $\\v{B}$ has linearly independent columns.\n",
        "  Therefore\n",
        "  $\\v{x}_1,...,\\v{x}_m$ are linearly independent iff\n",
        "  $\\sum_{j=1}^mœà_j\\v{Œª}_j=\\v{0}$\n",
        "  implies $œà_1=...=œà_m=0$ i.e.,\n",
        "  $\\v{Œª}_1,...,\\v{Œª}_m$ are linearly independent.\n",
        "\n",
        "**Generating set and basis**: Consider vector space $V=(\\m{V},+,‚ãÖ)$ and vectors $\\m{A}=\\{\\v{b}_1,...,\\v{b}_k\\}‚äÜ\\m{V}$. If every vector in $\\m{V}$ can be expressed as a linear combination of $\\v{b}_1,...,\\v{b}_k$ then $\\m{A}$ is called the **generating set** of $V$. The set of all linear combinations of vectors in $\\m{A}$ is called the **span** of $\\m{A}$. If vectors in $\\m{A}$ span $V$ and are linearly independent and there exists no smaller set that spans $V$ then the minimal generating set $\\m{A}$ is called the **basis** of $V$.\n",
        "\n",
        "- For $V=(\\m{V},+,‚ãÖ)$ and $\\m{B}‚äÜ\\m{V},\\m{B}\\neq‚àÖ$, the following statements are equivalent:\n",
        "  \n",
        "  - $\\m{B}$ is the minimal generating set and basis of $V$.\n",
        "  \n",
        "  - $\\m{B}$ is a maximal linearly independent set of vectors in $V$; adding another vector will make it dependent.\n",
        "\n",
        "  - Every vector $\\v{x}‚ààV$ is a unique linear combination of vectors in $\\m{B}$. If $\\v{x}=\\sum_iŒª_i\\v{b}_i=\\sum_iœà_i\\v{b}_i$ then $Œª_i=œà_i$.\n",
        "\n",
        "- **Basis** of $U=\\span(\\v{x}_1,...,\\v{x}_m)‚äÜ‚Ñù^n$ are the vectors corresponding to the pivot columns of $\\t{ref}([\\v{x}_1,...,\\v{x}_k])$.\n",
        "\n",
        "  - Ordered basis $B=(\\v{b}_1,...,\\v{b}_n)$, unordered basis $\\m{B}=\\{\\v{b}_1,...,\\v{b}_n\\}$, and basis matrix $\\v{B}=[\\v{b}_1,...,\\v{b}_n]$.\n",
        "\n",
        "- **Dimension** of $V$ is the number of basis vectors that all bases of $V$ possess. Let $U‚äÜV$ be a subspace, then $\\dim(U)‚â§\\dim(V)$. Equality $\\dim(U)=\\dim(V)$ iff $U=V$.\n",
        "\n",
        "  - $\\dim(V)$ is not the number of elements of each basis vector. For example, $V=\\span(\\BM 0\\\\1 \\EM)$ is one-dimensional although the basis vector has 2 elements.\n",
        "\n",
        "- **Rank** of a matrix $\\v{A}‚àà‚Ñù^{m√ón}$, $\\rk(\\v{A})$, is the number of linearly independent columns and rows of $\\v{A}$.\n",
        "\n",
        "  - $\\v{A}‚àà‚Ñù^{m√ón}$ has **full rank** if $\\rk(A)=\\min(m,n)$ is the largest possible for a matrix of the same dimensions.\n",
        "\n",
        "  - $\\rk(\\v{A})=\\rk(\\v{A}^‚ä§)$: the number of linearly independent columns and rows are equal.\n",
        "\n",
        "  - Let $U=\\span(\\v{A})$. Then $\\dim(U)=\\rk(A)$.\n",
        "  \n",
        "  - Matrix $\\v{A}‚àà‚Ñù^{n√ón}$ is invertible iff $\\rk(\\v{A})=n$.\n",
        "\n",
        "  - The system $\\v{Ax}=\\v{b}$ can be solved iff $\\rk(\\v{A})=\\rk(\\v{A}|\\v{b})$, where $\\v{A}|\\v{b}$ denotes augmented system.\n",
        "\n",
        "  - Proof (row rank = column rank): $\\t{rref}(\\v{A})$ and $\\t{rref}(\\v{A}^‚ä§)$ are kind of the same and have same number of pivots.\n",
        "\n",
        "- $\\rk(\\v{uv}^‚ä§)=1$ outer product produces rank-1 matrix.\n",
        "\n",
        "  - Proof: $\\v{uv}^‚ä§=\\v{u}\\BM v_1&...&v_n\\EM=\\BM v_1\\v{u}&...&v_n\\v{u}\\EM$. $\\span(\\BM v_1\\v{u}&...&v_n\\v{u}\\EM)=\\span(\\v{u})$.\n",
        "\n",
        "- 2.14: $x_1=\\BM 1\\\\2\\\\-3\\\\4\\EM$,\n",
        "$x_2=\\BM 1\\\\1\\\\0\\\\2\\EM$,\n",
        "$x_3=\\BM -1\\\\-2\\\\1\\\\1\\EM$.\n",
        "Then\n",
        "$\\BM 1&1&-1\\\\2&1&-2\\\\-3&0&1\\\\4&2&2\\EM\n",
        "=\\BM 1&1&-1\\\\0&1&0\\\\0&3&-2\\\\0&2&-6\\EM\n",
        "=\\BM \\red{1}&1&-1\\\\0&\\red{1}&0\\\\0&0&\\red{2}\\\\0&0&0\\EM$.\n",
        "Therefore independent.\n",
        "\n",
        "- 2.15: Consider linearly independent $\\v{b}_{1..4}‚àà‚Ñù^n$.\n",
        "$\\BC\n",
        "x_1=\\v{b}_1-2\\v{b}_2+\\v{b}_3-\\v{b}_4\\\\\n",
        "x_2=-4\\v{b}_1-2\\v{b}_2+4\\v{b}_4\\\\\n",
        "x_3=2\\v{b}_1+3\\v{b}_2-\\v{b}_3-3\\v{b}_4\\\\\n",
        "x_4=17\\v{b}_1-10\\v{b}_2+11\\v{b}_3+\\v{b}_4\n",
        "\\EC$\n",
        "$‚áí\\BM 1&-4&2&17 \\\\ -2&-2&3&-10 \\\\ 1&0&-1&11 \\\\ -1&4&-3&1 \\EM$\n",
        "$‚áí\\BM 1&-4&2&17\\\\0&-10&7&24\\\\0&-4&3&6\\\\0&0&-1&18 \\EM$\n",
        "$‚áí\\BM 1&-4&2&17\\\\0&-10&7&24\\\\0&0&1&-18\\\\0&0&0&0 \\EM$\n",
        "Therefore $x_1,x_2,x_3,x_4$ are not independent.\n",
        "\n",
        "- 2.17: Find the basis of $U‚äÜ‚Ñù^5$ spanned by $x_1=\\BM 1\\\\2\\\\-1\\\\-1\\\\-1 \\EM$, $x_2=\\BM 2\\\\-1\\\\1\\\\2\\\\-2 \\EM$, $x_3=\\BM 3\\\\-4\\\\3\\\\5\\\\-3 \\EM$, and $x_4=\\BM -1\\\\8\\\\-5\\\\-6\\\\1 \\EM$.\n",
        "$\\BM 1&2&3&-1\\\\2&-1&-4&8\\\\-1&1&3&-5\\\\-1&2&5&-6\\\\-1&-2&-3&1\\EM$\n",
        "$‚Üí\\BM 1&2&3&-1\\\\0&1&2&-2\\\\0&1&2&-2\\\\0&4&8&-7\\\\0&0&0&0\\EM$\n",
        "$‚Üí\\BM \\red{1}&2&3&-1\\\\0&\\red{1}&2&-2\\\\0&0&0&\\red{1}\\\\0&0&0&0\\\\0&0&0&0\\EM$. Therefore $x_1,x_2,x_4$ are a basis of $U$."
      ],
      "metadata": {
        "id": "egNCOD3arpYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Mappings**: For vector spaces $V,W$, mapping $Œ¶:V‚ÜíW$ is called a **linear mapping** (linear transformation or homomorphism) if $‚àÄ\\v{x},\\v{y}‚ààV\\ ‚àÄŒª,œà‚àà‚Ñù:Œ¶(Œª\\v{x}+œà\\v{y})=ŒªŒ¶(\\v{x})+œàŒ¶(\\v{y})$. Furthermore $Œ¶:\\m{V}‚Üí\\m{W}$ is called\n",
        "$\\BC\n",
        "\\t{Injective (one-to-one) if}& ‚àÄ\\v{x},\\v{y}‚àà\\m{V}:Œ¶(\\v{x})=Œ¶(\\v{y})‚áí\\v{x}=\\v{y}\\\\\n",
        "\\t{Surjective (onto) if}& Œ¶(\\m{V})=\\m{W}\\\\\n",
        "\\t{Bijective (invertible) if}& \\t{both injective and surjective}\n",
        "\\EC$\n",
        "\n",
        "- If $Œ¶$ is bijective then $‚àÉŒ®:\\m{W}‚Üí\\m{V}$ such that $Œ®‚àòŒ¶(\\v{x})=\\v{x}$ and denoted $Œ®=Œ¶^{-1}$. Linear bijective $V‚ÜíW$ mappings are called **isomorphisms**, which exist between vector spaces $V$ and $W$ iff $\\dim(V)=\\dim(W)$.\n",
        "\n",
        "  - Vector spaces of the same dimension are called **isomorphic**, and can be transformed into each other.\n",
        "\n",
        "  - Linear $Œ¶:V‚ÜíV$ mappings are called **endomorphic**, or **automorphic** if also bijective.\n",
        "\n",
        "  - Consider $‚Ñù^n$ canonical basis vectors $\\v{e}_1,...,\\v{e}_n$ and $n$-dimensional vector space $V$ with ordered basis $B$, then the mapping $Œ¶:‚Ñù^n‚ÜíV,Œ¶(\\v{e}_i)=\\v{b}_i$ is an isomorphism.\n",
        "\n",
        "- Consider ordered basis $B=(\\v{b}_1,...,\\v{b}_n)$ of vector space $V$ and vector $\\red{\\v{x}=\\v{B}\\v{x}_B}=x_{B1}\\v{b}_1+...+x_{Bn}\\v{b}_n‚ààV$, then $\\v{x}_B=\\BM x_{B1}\\\\\\vdots\\\\x_{Bn}\\EM$ are the **coordinates** of $\\v{x}$ wrt $B$.\n",
        "\n",
        "  - $\\v{x}=\\v{B}\\v{x}_B$: the basis $\\v{B}$ is the \"unit of measurement\" for the coordinate values.\n",
        "\n",
        "**Transformation matrix**: Consider vector spaces $V,W$ with ordered bases $B=(\\v{b}_1,...,\\v{b}_n)$ and $C=(\\v{c}_1,...,\\v{c}_m)$. Consider linear mapping\n",
        "$Œ¶:V‚ÜíW,Œ¶(\\v{b}_{j=1..n})=Œ±_{1j}\\v{c}_1+...+Œ±_{mj}\\v{c}_m=\\sum_{i=1}^mŒ±_{ij}\\v{c}_i$.\n",
        "Then $\\v{A}_{BC}=[Œ±_{ij}]‚àà‚Ñù^{m√ón}$\n",
        "is the transformation matrix of $Œ¶$ wrt $B$ and $C$ such that $\\red{Œ¶(\\v{B})=\\v{C}\\v{A}_{BC}}$.\n",
        "\n",
        "- $\\v{A}_{BC}$ takes input coordinate vector in $B$-units, converts into standard Cartesian units, transforms the vector, converts into $C$-units, and then outputs coordinate vector in $C$-units.\n",
        "\n",
        "- Let $\\v{x}_B$ be the coordinate vector of $\\v{x}‚ààV$ wrt $B$ and let $\\v{y}_C$ be the coordinate vector of $\\v{y}=Œ¶(\\v{x})‚ààW$ wrt $C$, then $\\red{\\v{C}\\v{y}_C=Œ¶(\\v{B})\\v{x}_B}$ and $\\red{\\v{y}_C=\\v{A}_{BC}\\v{x}_B}$.\n",
        "\n",
        "  - Proof: $Œ¶(\\v{B}\\v{x}_B)$\n",
        "  $=Œ¶(x_{B1}\\v{b}_1+...+x_{Bn}\\v{b}_n)$\n",
        "  $=x_{B1}Œ¶(\\v{b}_1)+...+x_{Bn}Œ¶(\\v{b}_n)$\n",
        "  $=Œ¶(\\v{B})\\v{x}_B$\n",
        "  by property of linear mapping.\n",
        "  $\\v{y}=Œ¶(\\v{x})$\n",
        "  $‚áí\\v{C}\\v{y}_C=Œ¶(\\v{B}\\v{x}_B)$\n",
        "  $=Œ¶(\\v{B})\\v{x}_B$\n",
        "  $=\\v{C}\\v{A}_{BC}\\v{x}_B$\n",
        "  $‚áí\\v{C}^{-1}\\v{C}\\v{y}_C=\\v{C}^{-1}\\v{C}\\v{A}_{BC}\\v{x}_B$\n",
        "  $‚áí\\v{y}_C=\\v{A}_{BC}\\v{x}_B$.\n",
        "\n",
        "- **Standard matrix** $\\v{A}_{std}$ takes coordinate vector $\\v{x}$ of standard basis $\\v{I}_n$ as input and outputs coordinate vector $\\v{y}$ of standard basis $\\v{I}_m$. It corresponds to transformation $Œ¶:‚Ñù^n‚Üí‚Ñù^m$. Then $\\v{y}=\\v{A}\\v{x}$ and $\\red{\\v{A}_{BC}=\\v{C}^{-1}\\v{A}_{std}\\v{B}}$.\n",
        "  \n",
        "  - Intuition: Suppose $Œ¶(\\v{x})=\\BM a_{11}x_1+...+a_{1n}x_n\\\\a_{21}x_1+...+a_{2n}x_n\\\\\\vdots\\\\a_{m1}x_1+...+a_{mn}x_n\\EM$,\n",
        "  $\\v{A}_{std}=\\BM a_{11}&...&a_{1n}\\\\\\vdots&\\ddots&\\vdots\\\\a_{m1}&...&a_{mn}\\EM$,\n",
        "  and $Œ¶(\\v{x})=\\v{A}_{std}\\v{x}$. Then\n",
        "  $Œ¶(\\v{B})=\\v{A}_{std}\\v{B}=\\v{B}'$ transforms the gridlines so that $\\v{x}=\\v{B}\\v{x}_B$ becomes $\\v{x}'=\\v{B}'\\v{x}_B$, which keeps the same coordinates.\n",
        "  $Œ¶(\\v{B})=\\v{B}'=\\v{CA}_{BC}$ expresses the new gridlines in $C$-measurement units.\n",
        "  $\\v{CA}_{BC}=\\v{A}_{std}\\v{B}$\n",
        "  $‚áí\\v{A}_{BC}=\\v{C}^{-1}\\v{A}_{std}\\v{B}$.\n",
        "\n",
        "- Consider $V,W$ with bases $B,C$ and transformation $Œ¶:V‚ÜíW,Œ¶(\\v{b}_j)=Œ±_{1j}\\v{c}_1+...+Œ±_{mj}\\v{c}_m$. Let $\\v{x}=\\BM \\v{b}_1&...&\\v{b}_n\\EM\\v{x}_B$\n",
        "$=\\v{B}\\v{x}_B$,\n",
        "then\n",
        "$\\v{y}=Œ¶(\\v{x})=Œ¶(\\v{B}\\v{x}_B)$\n",
        "$=\\BM Œ¶(\\v{b}_1)&...&Œ¶(\\v{b}_n)\\EM\\v{x}_B$\n",
        "$=\\BM Œ±_{11}\\v{c}_1+...+Œ±_{n1}\\v{c}_n&...&Œ±_{1n}\\v{c}_1+...+Œ±_{nn}\\v{c}_n\\EM\\v{x}_B$\n",
        "$=\\BM\\v{c}_1&...&\\v{c}_n\\EM\\BM Œ±_{11}&...&Œ±_{1n}\\\\\\vdots&\\ddots&\\vdots\\\\Œ±_{n1}&...&Œ±_{nn}\\EM\\v{x}_B$\n",
        "$=\\v{C}\\v{A}_{BC}\\v{x}_B=\\v{C}\\v{y}_C$.\n",
        "Therefore\n",
        "$\\v{C}\\v{y}_C=Œ¶(\\v{B})\\v{x}_B$ and\n",
        "$\\v{y}_C=\\v{A}_{BC}\\v{x}_B$.\n",
        "\n",
        "**Basis Change**: Consider linear mapping $Œ¶:V‚ÜíW$, ordered bases $B$ and $\\tilde{B}$ of $V$ and ordered bases $C$ and $\\tilde{C}$ of $W$ such that $\\tilde{\\v{B}}=\\v{B}\\v{S}$ and $\\tilde{\\v{C}}=\\v{C}\\v{T}$. Let $\\v{A}_{BC}‚àà‚Ñù^{m√ón}$ be transformation matrix wrt $B$ and $C$, then $\\tilde{\\v{A}}_{BC}$ corresponding to $\\tilde{B}$ and $\\tilde{C}$ is given by $\\red{\\tilde{\\v{A}}_{BC}=\\v{T}^{-1}\\v{A}_{BC}\\v{S}}$, where $\\v{S}=\\v{B}^{-1}\\tilde{\\v{B}}‚àà‚Ñù^{n√ón}$ and $\\v{T}=\\v{C}^{-1}\\tilde{\\v{C}}‚àà‚Ñù^{m√óm}$. **Basis change is affine pre-composition**.\n",
        "\n",
        "- Proof: Let $\\tilde{\\v{B}}=\\v{B}\\v{S}$ and $\\tilde{\\v{C}}=\\v{C}\\v{T}$.\n",
        "Deriving through the new bases:\n",
        "$Œ¶(\\tilde{\\v{B}})=\\tilde{\\v{C}}\\tilde{\\v{A}}_{BC}$\n",
        "$=\\v{C}\\v{T}\\tilde{\\v{A}}_{BC}$.\n",
        "Deriving through the old bases:\n",
        "$Œ¶(\\tilde{\\v{B}})=Œ¶(\\v{BS})$\n",
        "$=Œ¶(\\BM \\v{b}_1 & ... & \\v{b}_n\\EM\\BM s_{11} &...& s_{1n}\\\\\\vdots&\\ddots&\\vdots\\\\ s_{n1} &...& s_{nn}\\EM)$\n",
        "$=\\BM Œ¶(\\v{b}_1) & ... & Œ¶(\\v{b}_n)\\EM\\BM s_{11} &...& s_{1n}\\\\\\vdots&\\ddots&\\vdots\\\\ s_{n1} &...& s_{nn}\\EM$\n",
        "$=Œ¶(\\v{B})\\v{S}=\\v{CA}_{BC}\\v{S}$.\n",
        "Therefore\n",
        "$\\green{\\v{T}\\tilde{\\v{A}}_{BC}=\\v{A}_{BC}\\v{S}}$\n",
        "$‚áí\\tilde{\\v{A}}_{BC}=\\v{T}^{-1}\\v{A}_{BC}\\v{S}$.\n",
        "\n",
        "- Consider vector space $V$ with basis $B$ and linear transformation $Œ¶(\\v{B}):V‚ÜíV,Œ¶(\\v{B})=\\v{BA}_{BB}$. The transformation matrix $\\v{A}_{BB}$ converts input $\\v{x}=\\v{B}\\v{x}_B$ to output $\\v{y}=\\v{B}\\v{y}_B$ by $\\v{y}_B=\\v{A}_{BB}\\v{x}_B$.\n",
        "  \n",
        "  - Consider vector space $W$ with basis $C$. If we want output $\\v{y}=\\v{C}\\v{y}_C$, then $Œ¶(\\v{B}):V‚ÜíW,Œ¶(\\v{B})=\\v{CA}_{BC}$ describes the same transformation but measured with a different output basis. $\\green{\\v{BA}_{BB}=\\v{CA}_{BC}}$\n",
        "  $‚áí\\blue{\\v{A}_{BC}=(\\v{C}^{-1}\\v{B})\\v{A}_{BB}}$, where $\\v{y}_C=(\\v{C}^{-1}\\v{B})\\v{y}_B$.\n",
        "  Therefore $\\v{y}_C=\\v{A}_{BC}\\v{x}_B$.\n",
        "\n",
        "  - If we want input $\\v{x}=\\v{C}\\v{x}_C$, then $Œ¶(\\v{C}):W‚ÜíW,Œ¶(\\v{C})=\\v{CA}_{CC}$ describes the same transformation but measured with both different input and output bases.\n",
        "  $Œ¶(\\v{B})=Œ¶(\\v{CC}^{-1}\\v{B})=Œ¶(\\v{C})(\\v{C}^{-1}\\v{B})$\n",
        "  $=\\green{\\v{CA}_{CC}(\\v{C}^{-1}\\v{B})=\\v{BA}_{BB}}$\n",
        "  $‚áí\\v{A}_{CC}=(\\v{C}^{-1}\\v{B})\\v{A}_{BB}(\\v{C}^{-1}\\v{B})^{-1}$\n",
        "  $‚áí\\blue{\\v{A}_{CC}=(\\v{C}^{-1}\\v{B})\\v{A}_{BB}(\\v{B}^{-1}\\v{C})}$, where\n",
        "  $\\v{x}_B=(\\v{B}^{-1}\\v{C})\\v{x}_C$. Therefore\n",
        "  $\\v{y}_C=\\v{A}_{CC}\\v{x}_C$.\n",
        "\n",
        "- $\\tilde{\\v{B}}\\larr{\\v{S}}\\v{B}\\larr{\\v{A}_{BC}}\\v{C}\\larr{\\v{T}^{-1}}\\tilde{\\v{C}}$.\n",
        "\n",
        "- $\\v{A},\\tilde{\\v{A}}‚àà‚Ñù^{m√ón}$ are **equivalent** if there exist invertible $\\v{S}‚àà‚Ñù^{n√ón},\\v{T}‚àà‚Ñù^{m√óm}$ such that $\\tilde{\\v{A}}=\\v{T}^{-1}\\v{A}\\v{S}$. Two matrices are equivalent if you can apply different basis conversion to input & output of one matrix to get the other.\n",
        "\n",
        "  - $\\rk(\\v{A})=\\rk(\\tilde{\\v{A}})$\n",
        "\n",
        "- $\\v{A},\\tilde{\\v{A}}‚àà‚Ñù^{n√ón}$ are **similar** if there exist invertible $\\v{S}‚àà‚Ñù^{n√ón}$ such that $\\tilde{\\v{A}}=\\v{S}^{-1}\\v{A}\\v{S}$. Two matrices are similar if you can apply the same basis conversion and its inverse to input & output of one to get the other.\n",
        "\n",
        "  - $\\rk(\\v{A})=\\rk(\\tilde{\\v{A}})$, $\\det(\\v{A})=\\det(\\tilde{\\v{A}})$, $\\tr(\\v{A})=\\tr(\\tilde{\\v{A}})$, same eigenvalues, and same characteristic polynomials.\n",
        "\n",
        "- 2.21: Let $\\BC\n",
        "Œ¶(\\v{b}_1)=\\v{c}_1-\\v{c}_2+3\\v{c}_3-\\v{c}_4\\\\\n",
        "Œ¶(\\v{b}_2)=2\\v{c}_1+\\v{c}_2+7\\v{c}_3+2\\v{c}_4\\\\\n",
        "Œ¶(\\v{b}_3)=3\\v{c}_2+\\v{c}_3+4\\v{c}_4\n",
        "\\EC$, then $\\v{A}_{BC}=[\\v{Œ±}_1,\\v{Œ±}_2,\\v{Œ±}_3]=\\BM\n",
        "1&2&0\\\\-1&1&3\\\\3&7&1\\\\-1&2&4\\EM$.\n",
        "\n",
        "- 2.22: Let $\\v{A}_{BB}=\\BM \\cos(œÄ/4)&-\\sin(œÄ/4) \\\\\n",
        "\\sin(œÄ/4)&\\cos(œÄ/4) \\EM$\n",
        "$=\\BM 1/\\sqrt{2}&-1/\\sqrt{2}\\\\1/\\sqrt{2}&1/\\sqrt{2} \\EM$  be an endomorphic transformation in basis $B$, i.e., $Œ¶(\\v{B})=\\v{B}\\v{A}_{BB}$.\n",
        "Let $\\v{B}=\\BM 1&0\\\\0&1\\EM$ and $\\v{x}=\\v{B}\\BM 1\\\\1\\EM$.\n",
        "If we transform $\\v{x}$ in canonical basis, then $\\v{y}_B=\\v{A}_{BB}\\BM 1\\\\1\\EM$\n",
        "$=\\BM 0\\\\\\sqrt{2}\\EM$ wrt basis $B$.\n",
        "If we transform $\\v{x}$ and express it wrt basis $\\v{C}=\\BM 1/\\sqrt{2}&-1/\\sqrt{2}\\\\1/\\sqrt{2}&1/\\sqrt{2} \\EM$, then\n",
        "$Œ¶(\\v{B})=\\v{BA}_{BB}=\\v{C}\\v{A}_{BC}$\n",
        "$‚áí\\v{A}_{BC}=\\v{C}^{-1}\\v{B}\\v{A}_{BB}$\n",
        "$=\\v{I}_n$.\n",
        "Then\n",
        "$\\v{y}_C=\\v{A}_{BC}\\v{x}_B$\n",
        "$=\\BM 1\\\\1\\EM$.\n",
        "\n",
        "- 2.23: $\\v{A}_{II}=\\BM 2&1\\\\1&2\\EM$ corresponds to canonical basis in $‚Ñù^2$. Define new basis $\\v{B}=\\BM 1&1\\\\1&-1\\EM$.\n",
        "$\\v{B}^{-1}=\\/{1}{-2}\\BM -1&-1\\\\-1&1\\EM$\n",
        "$=\\BM 1/2&1/2\\\\1/2&-1/2\\EM$.\n",
        "Then $\\v{I}\\v{A}_{II}=\\v{B}\\v{A}_{IB}$\n",
        "$‚áí\\v{A}_{IB}=\\v{B}^{-1}\\v{A}_{II}$.\n",
        "Then $\\v{A}_{BB}=\\v{B}^{-1}\\v{A}_{II}\\v{B}$\n",
        "$=\\BM 3/2&3/2\\\\1/2&-1/2\\EM\\v{B}$\n",
        "$=\\BM 3&0\\\\0&1\\EM$.\n"
      ],
      "metadata": {
        "id": "H9eRDx-zy46D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image and Kernel**: For $Œ¶:V‚äÜ‚Ñù^n‚ÜíW‚äÜ‚Ñù^m,Œ¶(\\v{x})=\\v{Ax}$ where $\\v{A}‚àà‚Ñù^{m√ón}$. The **kernel / null space** and **image / column space** are $\\BC\n",
        "\\blue{\\null(\\v{A}):=\\{\\v{v}‚ààV|\\v{Av}=\\v{0}_W\\}} \\\\\n",
        "\\blue{\\Im(\\v{A}):=\\{\\v{w}‚ààW|\\v{Av}=\\v{w}\\t{ for some }\\v{v}\\}}\n",
        "\\EC$.\n",
        "\n",
        "- $\\null(Œ¶)\\neq‚àÖ$ because $Œ¶(\\v{0}_V)=\\v{0}_W$.\n",
        "\n",
        "- $\\red{\\null(\\v{A})=\\span(\\t{minus-1 columns of rref}(\\v{A}))}‚äÜV$ is called the **null space** of $\\v{A}$, and is the general solution to $\\v{Ax}=\\v{0}$, which captures all possible linear combinations of $V$ that produce $\\v{0}_W$.\n",
        "\n",
        "  - $Œ¶$ is injective (one-to-one) iff $\\null(\\v{A})=\\{\\v{0}\\}$.\n",
        "\n",
        "- $\\red{\\span(\\v{A})=\\span(\\t{pivot columns of }\\v{A})}‚äÜW$ is called **column space** of $\\v{A}$.\n",
        "\n",
        "  - If $\\dim(V)=\\dim(W)$ then $Œ¶$ is injective ‚áî $Œ¶$ is surjective ‚áî $Œ¶$ is bijective.\n",
        "\n",
        "- $\\null(\\v{A}^‚ä§)‚äÜW$ is the **left null space** of $\\v{A}$. $\\red{\\span(\\v{A})^\\perp=\\null(\\v{A}^‚ä§)}$: every vector in that subspace is orthogonal to every vector in $\\span(\\v{A})$, or the subspace that the linear combinations of columns of $\\v{A}$ cannot reach. $\\null(\\v{A}^‚ä§)‚à©\\span(\\v{A})=\\{\\v{0}_W\\}$.\n",
        "\n",
        "  - $\\dim(\\span(\\v{A}))+\\dim(\\null(\\v{A}^‚ä§))=m$\n",
        "\n",
        "- $\\span(\\v{A}^‚ä§)$ is the **row space** of $\\v{A}$.\n",
        "\n",
        "  - $\\red{\\null(\\v{A})^\\perp=\\span(\\v{A}^‚ä§)}$\n",
        "\n",
        "- **Rank-Nullity Theorem**: $\\dim(\\null(\\v{A}))+\\dim(\\span(\\v{A}))=n$.\n",
        "\n",
        "  - $\\rk(\\v{A})=\\dim(\\span(\\v{A}))$\n",
        "\n",
        "  - If $\\rk(\\v{A})=n$ then $\\null(Œ¶)=\\{\\v{0}\\}$.\n",
        "\n",
        "  - The subspace of solutions for $\\v{Ax}=\\v{0}$ has dimension $n-\\rk(\\v{A})$.\n",
        "\n",
        "- Let $\\v{A}‚àà‚Ñù^{m√ón},\\v{F}‚àà‚Ñù^{n√ók}$, and let $\\span(\\v{F})=\\null(\\v{A})$, then\n",
        "\n",
        "  - $\\v{AF}=\\v{0}$\n",
        "\n",
        "  - $\\rk(\\v{A})=n-k$.\n",
        "\n",
        "  - If $\\v{b}‚àà\\span(\\v{A})$, then $\\v{Ax}=\\v{b}$ has solution set $\\{\\v{x}_0+\\null(A)\\}=\\{\\v{x}_0+\\v{Fz}\\ ‚àÄ\\v{z}‚àà‚Ñù^k\\}$. Here $\\v{x}_0‚àà‚Ñù^n$ is one solution e.g., $\\v{x}_0=\\v{A}^‚Ä†\\v{b}$.\n",
        "\n",
        "- 2.25: $\\v{A}=\\BM 1&2&-1&0\\\\1&0&0&1 \\EM$\n",
        "$‚Üí\\BM 1&0&0&1 \\\\ 0&1&-1/2&-1/2 \\\\0&0&\\gray{-1}&0 \\\\ 0&0&0&\\gray{-1}\\EM$.\n",
        "$\\span(\\v{A})=\\span(\\BM 1\\\\1 \\EM,\\BM 2\\\\0 \\EM)$.\n",
        "$\\null(\\v{A})=\\span(\\BM 0\\\\-1/2\\\\-1\\\\0\\EM, \\BM 1\\\\-1/2\\\\0\\\\-1\\EM)$"
      ],
      "metadata": {
        "id": "X6b70vpY81n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2.5: Solve systems of equations\n",
        "\n",
        "  - $\\v{A}=\\BM 1&1&-1&-1\\\\2&5&-7&-5\\\\2&-1&1&3\\\\5&2&-4&2\\EM$ and $\\v{b}=\\BM 1\\\\-2\\\\4\\\\6\\EM$.\n",
        "  $‚áí\\BM 1&1&-1&-1&1\\\\0&-3&5&3&4\\\\0&3&-3&-5&-2\\\\0&3&-1&-7&-1\\EM$\n",
        "  $‚Üí\\BM 1&1&-1&-1&1\\\\0&-3&5&3&4\\\\0&0&2&-2&2\\\\0&0&4&-4&3\\EM$\n",
        "  $‚Üí\\BM 1&1&-1&-1&1\\\\0&-3&5&3&4\\\\0&0&1&-1&1\\\\0&0&0&0&1\\EM$. No solution.\n",
        "\n",
        "  - $\\v{A}=\\BM 1&-1&0&0&1\\\\1&1&0&-3&0\\\\2&-1&0&1&-1\\\\-1&2&0&-2&-1\\EM$, and $\\v{b}=\\BM 3\\\\6\\\\5\\\\-1\\EM$.\n",
        "  $‚áí\\BM 1&0&0&0&-1&3\\\\0&1&0&0&-2&0\\\\0&0&0&1&-1&-1\\\\0&0&0&0&0&0\\EM$.\n",
        "  Then the solution affine subspace after applying minus-1 rule is $\\v{x}=\\BM 3\\\\0\\\\0\\\\-1\\\\0\\EM+Œª_1\\BM 0\\\\0\\\\1\\\\0\\\\0\\EM+Œª_2\\BM 1\\\\2\\\\0\\\\1\\\\1\\EM$\n",
        "\n",
        "- 2.6: $\\v{A}=\\BM0&1&0&0&1&0\\\\0&0&0&1&1&0\\\\0&1&0&0&0&1\\EM$ and $\\v{b}=\\BM2\\\\-1\\\\1\\EM$.\n",
        "$‚áí\\BM0&1&0&0&0&1&1\\\\0&0&0&1&0&1&-2\\\\0&0&0&0&1&-1&1\\EM$.\n",
        "Therefore the affine subspace is $\\v{x}=\\BM0\\\\1\\\\0\\\\-2\\\\1\\\\0\\EM+Œª_1\\BM1\\\\0\\\\0\\\\0\\\\0\\\\0\\EM+Œª_2\\BM0\\\\0\\\\1\\\\0\\\\0\\\\0\\EM+Œª_3\\BM0\\\\1\\\\0\\\\1\\\\-1\\\\-1\\EM$\n",
        "\n",
        "- 2.7 (eigen): Let $\\v{A}=\\BM6&4&3\\\\6&0&9\\\\0&8&0\\EM$. Solve $\\v{Ax}=12\\v{x}$ subject to constraint $\\sum_ix_i=1$.\n",
        "Rewrite problem as $(\\v{A}-12\\v{I})\\v{x}=\\v{0}$.\n",
        "Then\n",
        "$\\BM-6&4&3&0\\\\6&-12&9&0\\\\0&8&-12&0\\EM$\n",
        "$‚Üí\\BM1&0&-3/2&0\\\\0&1&-3/2&0\\\\0&0&0&0\\EM$.\n",
        "Then $\\v{x}=Œª\\BM3/2\\\\3/2\\\\1\\EM$\n",
        "$=\\BM3/8\\\\3/8\\\\1/4\\EM$\n",
        "\n",
        "- 2.8: Solve inverse\n",
        "\n",
        "  - $\\v{A}=\\BM2&3&4\\\\3&4&5\\\\4&5&6\\EM$.\n",
        "  $\\BM2&3&4&1&0&0\\\\3&4&5&0&1&0\\\\4&5&6&0&0&1\\EM‚Üí\n",
        "  \\BM1&0&-1&0&-5&4\\\\0&1&2&0&4&-3\\\\0&0&0&1&-2&1\\EM$.\n",
        "  No inverse because $\\rk(\\v{A})< 3$.\n",
        "\n",
        "  - $\\v{A}=\\BM1&0&1&0\\\\0&1&1&0\\\\1&1&0&1\\\\1&1&1&0\\EM$.\n",
        "  $\\BM1&0&1&0&1&0&0&0\\\\0&1&1&0&0&1&0&0\\\\1&1&0&1&0&0&1&0\\\\1&1&1&0&0&0&0&1\\EM$\n",
        "  $‚Üí‚Äã\\BM1&0&0&0&0&-1&0&1\\\\0&1&0&0&-1&0&0&1\\\\0&0&1&0&1&1&0&-1\\\\0&0&0&1&1&1&1&-2\\EM$\n",
        "\n",
        "- 2.9: Subspace of $‚Ñù^3$?\n",
        "\n",
        "  - $A=\\{(Œª,Œª+Œº^3,Œª-Œº^3)|Œª,Œº‚àà‚Ñù\\}$\n",
        "  $=Œª\\BM1\\\\1\\\\1\\EM+Œº^3\\BM0\\\\1\\\\-1\\EM$\n",
        "  is a linear combination passing through $\\v{0}$.\n",
        "\n",
        "  - $B=\\{(Œª^2,-Œª^2,0)|Œª‚àà‚Ñù\\}$\n",
        "  $=Œª^2\\BM1\\\\-1\\\\0\\EM$\n",
        "  is a line through $\\v{0}$, but $Œª^2>0$ and $-Œª^2\\BM1\\\\-1\\\\0\\EM‚àâB$.\n",
        "  Not subspace.\n",
        "\n",
        "- 2.10:Linearly independent?\n",
        "\n",
        "  - $\\v{x}_1=\\BM2\\\\-1\\\\3\\EM$, $\\v{x}_2=\\BM1\\\\1\\\\-2\\EM$, $\\v{x}_3=\\BM3\\\\-3\\\\8\\EM$.\n",
        "  $\\BM2&1&3\\\\-1&1&-3\\\\3&-2&8\\EM$\n",
        "  $‚Üí\\BM1&0&2\\\\0&1&-1\\\\0&0&0\\EM$.\n",
        "  $\\v{x}_3=2\\v{x}_1-\\v{x}_2$.\n",
        "\n",
        "- 2.11: write $\\v{y}=\\BM1\\\\-2\\\\4\\EM$ as a linear combination of $\\v{x}_1=\\BM1\\\\1\\\\1\\EM$, $\\v{x}_2=\\BM1\\\\2\\\\3\\EM$, and $\\v{x}_3=\\BM2\\\\-1\\\\1\\EM$.\n",
        "$\\BM1&1&2&1\\\\1&2&-1&-2\\\\1&3&1&4\\EM$\n",
        "$‚Üí\\BM1&0&0&-5\\\\0&1&0&12/5\\\\0&0&1&9/5\\EM$.\n",
        "Then $\\v{y}=-5\\v{x}_1+\\/{12}{5}\\v{x}_2+\\/{9}{5}\\v{x}_3$.\n",
        "\n",
        "- 2.12 (intersection of spans): $U_1=\\span(\\BM1\\\\1\\\\-3\\\\1\\EM,\\BM2\\\\-1\\\\0\\\\-1\\EM,\\BM-1\\\\1\\\\-1\\\\1\\EM)$ and $U_2=\\span(\\BM-1\\\\-2\\\\2\\\\1\\EM,\\BM2\\\\-2\\\\0\\\\0\\EM,\\BM-3\\\\6\\\\-2\\\\-1\\EM)$.\n",
        "Determine the basis for $U_1‚à©U_2$.\n",
        "First we simplify them.\n",
        "$U_1$ basis:\n",
        "$\\BM1&2&-1\\\\1&-1&1\\\\-3&0&-1\\\\1&-1&1\\EM$\n",
        "$‚Üí\\BM1&0&1/3\\\\0&1&-2/3\\\\0&0&0\\\\0&0&0\\EM$ and\n",
        "$U_1=\\span(\\BM1\\\\1\\\\-3\\\\1\\EM,\\BM2\\\\-1\\\\0\\\\-1\\EM)$.\n",
        "$U_2$ basis:\n",
        "$\\BM-1&2&-3\\\\-2&-2&6\\\\2&0&-2\\\\1&0&-1\\EM$\n",
        "$‚Üí\\BM1&0&-1\\\\0&1&-2\\\\0&0&0\\\\0&0&0\\EM$ and\n",
        "$U_2=\\span(\\BM-1\\\\-2\\\\2\\\\1\\EM,\\BM2\\\\-2\\\\0\\\\0\\EM)$.\n",
        "Then the intersection $U_1‚à©U_2$ is at\n",
        "$a\\BM1\\\\1\\\\-3\\\\1\\EM+b\\BM2\\\\-1\\\\0\\\\-1\\EM=c\\BM-1\\\\-2\\\\2\\\\1\\EM+d\\BM2\\\\-2\\\\0\\\\0\\EM$.\n",
        "Solving\n",
        "$\\BM1&2&1&-2&0\\\\1&-1&2&2&0\\\\-3&0&-2&0&0\\\\1&-1&-1&0&0\\EM$\n",
        "$‚Üí\\BM1&0&0&-4/9&0\\\\0&1&0&-10/9&0\\\\0&0&1&2/3&0\\\\0&0&0&0&0\\EM$\n",
        "we have\n",
        "$\\BC\n",
        "a-4/9d=0\\\\\n",
        "b-10/9d=0\\\\\n",
        "c+2/3d=0\n",
        "\\EC$.\n",
        "Let $d=9$ then $a=4$, $b=10$, and $c=-6$.\n",
        "Then $U_1‚à©U_2=\\span(a\\BM1\\\\1\\\\-3\\\\1\\EM+b\\BM2\\\\-1\\\\0\\\\-1\\EM)$\n",
        "$=\\span(\\BM24\\\\-6\\\\-12\\\\-6\\EM)$.\n",
        "\n",
        "- 2.13: Let $\\v{A}_1=\\BM1&0&1\\\\1&-2&-1\\\\2&1&3\\\\1&0&1\\EM$\n",
        "$‚Üí\\BM1&0&1\\\\0&1&1\\\\0&0&0\\\\0&0&0\\EM$ and $\\v{A}_2=\\BM3&-3&0\\\\1&2&3\\\\7&-5&2\\\\3&-1&2\\EM$\n",
        "$‚Üí\\BM1&0&1\\\\0&1&1\\\\0&0&0\\\\0&0&0\\EM$. Let $U_1,U_2$ be the solution space of $\\v{A}_1\\v{x}=\\v{0}$ and $\\v{A}_2\\v{x}=\\v{0}$.\n",
        "\n",
        "  - Determine dimension of $U_1$ and $U_2$.\n",
        "  $\\max\\rk(\\v{A}_1)=\\max\\rk(\\v{A}_2)=\\min(m,n)=3$.\n",
        "  $\\rk(\\v{A}_1)=\\rk(\\v{A}_2)=2$. Therefore $\\dim(U_1)=\\dim(U_2)=1$.\n",
        "\n",
        "  - Determine the bases of $U_1$ and $U_2$.\n",
        "  $U_1=U_2=\\span(\\BM1\\\\1\\\\-1\\EM)$.\n",
        "\n",
        "- 2.14: Let $\\v{A}_1=\\BM1&0&1\\\\1&-2&-1\\\\2&1&3\\\\1&0&1\\EM$ $‚Üí\\BM1&0&1\\\\0&1&1\\\\0&0&0\\\\0&0&0\\EM$ and $\\v{A}_2=\\BM3&-3&0\\\\1&2&3\\\\7&-5&2\\\\3&-1&2\\EM$\n",
        "$‚Üí\\BM1&0&1\\\\0&1&1\\\\0&0&0\\\\0&0&0\\EM$. Let $U_1,U_2$ be spanned by columns of $\\v{A}_1,\\v{A}_2$.\n",
        "\n",
        "  - Determine the dimension of $U_1,U_2$.\n",
        "  $\\dim(U_1)=\\dim(U_2)=\\rk(\\v{A}_1)=\\rk(\\v{A}_2)=2$\n",
        "\n",
        "  - Determine the bases of $U_1,U_2$.\n",
        "  $U_1=\\span(\\BM1\\\\1\\\\2\\\\1\\EM,\\BM0\\\\-2\\\\1\\\\0\\EM)$ and\n",
        "  $U_2=\\span(\\BM3\\\\1\\\\7\\\\3\\EM,\\BM-3\\\\2\\\\-5\\\\-1\\EM)$.\n",
        "\n",
        "  - Determine a basis of $U_1‚à©U_2$.\n",
        "  $a\\BM1\\\\1\\\\2\\\\1\\EM+b\\BM0\\\\-2\\\\1\\\\0\\EM=c\\BM3\\\\1\\\\7\\\\3\\EM+d\\BM-3\\\\2\\\\-5\\\\-1\\EM$.\n",
        "  Then $\\BM1&0&3&-3\\\\1&-2&1&2\\\\2&1&7&-5\\\\1&0&3&-1\\EM$\n",
        "  $‚Üí\\BM1&0&3&0\\\\0&1&1&0\\\\0&0&0&1\\\\0&0&0&0\\EM$.\n",
        "  Therefore\n",
        "  $d=0$. Let $c=1$ then $a=3$, and $b=1$,\n",
        "  then $U_1‚à©U_2=\\span(\\BM3\\\\1\\\\7\\\\3\\EM)$.\n",
        "\n",
        "- 2.17: $Œ¶:‚Ñù^3‚Üí‚Ñù^4$. $Œ¶(\\BM x_1\\\\x_2\\\\x_3\\EM)=\\BM\n",
        "3x_1+2x_2+x_3\\\\x_1+x_2+x_3\\\\x_1-3x_2\\\\2x_1+3x_2+x_3\\EM$.\n",
        "\n",
        "  - Find $\\v{A}_Œ¶$.\n",
        "  $\\v{A}_{std}=\\BM3&2&1\\\\1&1&1\\\\1&-3&0\\\\2&3&1\\EM$.\n",
        "  Let $\\v{B}=\\v{I}_3,\\v{C}=\\v{I}_4$ be the basis of $‚Ñù^3,‚Ñù^4$, then $\\v{A}_Œ¶=\\v{A}_{std}$.\n",
        "\n",
        "  - Determine $\\rk(\\v{A}_Œ¶)$.\n",
        "  $\\v{A}_Œ¶‚Üí\\BM1&0&0\\\\0&1&0\\\\0&0&1\\\\0&0&0\\EM$. Therefore $\\rk(\\v{A}_Œ¶)=3$.\n",
        "\n",
        "  - Determine Image and kernel of $Œ¶$.\n",
        "  $\\Im(Œ¶)=\\span(\\BM3\\\\1\\\\1\\\\2\\EM,\\BM2\\\\1\\\\-3\\\\3\\EM,\\BM1\\\\1\\\\0\\\\1\\EM)$.\n",
        "  Then $\\null(Œ¶)=\\{\\v{0}\\}$."
      ],
      "metadata": {
        "id": "qJlgv1OFQxc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Inner Products, Norms, Orthogonal, and Projections"
      ],
      "metadata": {
        "id": "gsx_1C1PpsRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inner product**: Let $V$ be a vector space. Then inner product $‚ü®‚ãÖ,‚ãÖ‚ü©:V√óV‚Üí‚Ñù$ is a positive-definite, symmetric, bilinear mapping on $V$. Furthermore $(V,‚ü®‚ãÖ,‚ãÖ‚ü©)$ is called an inner product space.\n",
        "\n",
        "- **Bilinear mapping**: Recall $Œ¶:V‚ÜíW$ is linear if $‚àÄ\\v{x},\\v{y}‚ààV\\ ‚àÄŒª,œà‚àà‚Ñù:Œ¶(Œª\\v{x}+œà\\v{y})=ŒªŒ¶(\\v{x})+œàŒ¶(\\v{y})$. $Œ©:V√óV‚Üí‚Ñù$ is **bilinear** if $‚àÄ\\v{x},\\v{y},\\v{z}‚ààV\\ ‚àÄŒª,œà‚àà‚Ñù:\\BC\n",
        "Œ©(Œª\\v{x}+œà\\v{y},\\v{z})=ŒªŒ©(\\v{x},\\v{z})+œàŒ©(\\v{y},\\v{z})\\\\\n",
        "Œ©(\\v{x},Œª\\v{y}+œà\\v{z})=ŒªŒ©(\\v{x},\\v{y})+œàŒ©(\\v{x},\\v{z})\n",
        "\\EC$ (covariance rules), and is\n",
        "$\\BC\n",
        "\\t{symmetric}& \\t{if }Œ©(\\v{x},\\v{y})=Œ©(\\v{y},\\v{x})‚àÄ\\v{x},\\v{y}‚ààV\\\\\n",
        "\\t{positive-definite}& \\t{if }‚àÄ\\v{x}‚ààV\\backslash\\{\\v{0}\\}:Œ©(\\v{x},\\v{x})>0\\t{, and }Œ©(\\v{0},\\v{0})=0\n",
        "\\EC$\n",
        "\n",
        "- Matrix $\\v{A}‚àà‚Ñù^{n√ón}$ is **positive-definite** if $\\green{‚àÄ\\v{x}‚ààV\\backslash\\{\\v{0}\\}:\\v{x}^‚ä§\\v{Ax}>0}$ (quadratic form). Matrix $\\v{A}$ is positive-**semi**definite if $‚àÄ\\v{x}‚ààV\\backslash\\{\\v{0}\\}:\\v{x}^‚ä§\\v{Ax}‚â•0$.\n",
        "\n",
        "  - If positive definite then $\\null(\\v{A})=\\{\\v{0}\\}$ because $\\v{x}^‚ä§\\v{Ax}>0‚áî\\v{Ax}>0‚áî\\v{A}$ invertible.\n",
        "\n",
        "  - If positive definite then diagonals are positive $a_{ii}=\\v{e}_i^‚ä§\\v{A}\\v{e}_i>0$, where $\\v{e}_i$ is the $i$-th column of $\\v{I}_n$\n",
        "\n",
        "  - Positive-definite matrix can be non-symmetric, but in application and by convention positive-definite is always symmetric and inherit all properties of symmetry.\n",
        "\n",
        "- For vector space $V$ with basis $B$, it holds that $‚ü®‚ãÖ,‚ãÖ‚ü©:V√óV‚Üí‚Ñù$ is an inner product iff there exists positive-definite matrix $\\v{A}‚àà‚Ñù^{n√ón}$ with $\\red{‚ü®\\v{x},\\v{y}‚ü©=\\v{x}_B^‚ä§\\v{A}\\v{y}_B}$, where $\\v{x}_B,\\v{y}_B$ are coordinates of $\\v{x},\\v{y}‚ààV$ wrt $B$.\n",
        "\n",
        "  - $\\green{‚ü®\\v{x},\\v{y}‚ü©=‚ü®\\sum_{i=1}^nx_{Bi}\\v{b}_i,\\sum_{j=1}^ny_{Bj}\\v{b}_j‚ü©=\\sum_{i=1}^n\\sum_{j=1}^nx_{Bi}‚ü®\\v{b}_i,\\v{b}_j‚ü©y_{Bj}=\\v{x}_B^‚ä§\\v{A}\\v{y}_B}$\n",
        "\n",
        "  - Proof: $\\blue{A_{ij}=‚ü®\\v{b}_i,\\v{b}_j‚ü©}=‚ü®\\v{b}_j,\\v{b}_i‚ü©=A_{ji}$. Also $\\v{x}_B^‚ä§\\v{A}\\v{y}_B=\\v{y}_B^‚ä§\\v{A}\\v{x}_B$ iff $\\v{A}$ is symmetric.\n",
        "  Furthermore, inner product positive-definite $‚ü®\\v{x},\\v{x}‚ü©>0‚áî\\v{x}_B^‚ä§\\v{A}\\v{x}_B>0‚áî\\v{A}$ is positive-definite.\n",
        "\n",
        "- **Dot product** is inner product for the standard basis $\\v{I}_n$: $\\red{‚ü®\\v{x},\\v{y}‚ü©=\\v{x}^‚ä§\\v{y}}$. I.e., $A_{ij}=‚ü®\\v{b}_i,\\v{b}_j‚ü©=\\v{b}_i^‚ä§\\v{b}_j$.\n",
        "\n",
        "  - $\\green{‚ü®\\v{x},\\v{y}‚ü©=‚ü®\\v{Bx}_B,\\v{By}_B‚ü©=(\\v{Bx}_B)^‚ä§(\\v{By}_B)=\\v{x}_B^‚ä§\\v{B}^‚ä§\\v{B}\\v{y}_B=\\v{x}_B^‚ä§\\v{A}\\v{y}_B}‚áí\\red{\\v{A}=\\v{B}^‚ä§\\v{B}}$.\n",
        "\n",
        "  - Inner product can still be redefined $‚ü®\\v{x},\\v{y}‚ü©_\\v{A}=\\v{x}^‚ä§\\v{A}\\v{y}$ in standard basis instead of using dot product.\n",
        "\n",
        "- 3.4: $\\v{A}_1=\\BM9&6\\\\6&5\\EM$, $\\v{A}_2=\\BM9&6\\\\6&3\\EM$. Test positive-definite.\n",
        "\n",
        "  - $\\v{x}^‚ä§\\v{A}_1\\v{x}=\\BM 9x_1+6x_2&6x_1+5x_2\\EM\\v{x}$\n",
        "  $=9x_1^2+12x_1x_2+5x_2^2$\n",
        "  $=(3x_1+2x_2)^2+x_2^2>0$\n",
        "  by completing square therefore positive-definite.\n",
        "  By principal minors, $d_1=9>0$, $d_2=\\det(\\v{A}_1)=9>0$.\n",
        "\n",
        "  - $\\v{x}^‚ä§\\v{A}_2\\v{x}=9x_1^2+12x_1x_2+3x_2^2$\n",
        "  $=(3x_1+2x_2)^2-x_2^2$ by completing square, therefore not positive-definite.\n",
        "  B principal minors, $d_1=9>0$, $d_2=\\det(\\v{A}_2)=-9< 0$, and therefore not positive-definite."
      ],
      "metadata": {
        "id": "KhjWgRLEqNL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Norm** on a vector space $V$ is function $\\n{‚ãÖ}:\\v{x}‚ààV‚Üí\\vn{x}‚àà‚Ñù$ equal to its length, with properties\n",
        "$\\BC\n",
        "\\t{absolutely homogeneous} &\\n{Œª\\v{x}}=|Œª|\\vn{x}\\\\\n",
        "\\t{triangle inequality} &\\n{\\v{x}+\\v{y}}‚â§\\vn{x}+\\vn{y}\\\\\n",
        "\\t{positive-definite} &\\vn{x}‚â•0\\t{, and }\\vn{x}=0‚áî\\v{x}=0\\\\\n",
        "\\t{Cauchy-Schwarz ($‚Ñì_2$)} &|‚ü®\\v{x},\\v{y}‚ü©|‚â§\\vn{x}\\vn{y}\n",
        "\\EC$\n",
        "\n",
        "- $‚Ñì_1$ **norm** (Manhattan): $\\vn{x}_1=\\sum_{i=1}^n|x_i|$. Not induced by an inner product and has no inner product space.\n",
        "\n",
        "  - $\\vn{x}_1=c$ is a diamond-shape.\n",
        "\n",
        "- $‚Ñì_2$ **norm** (Euclidean): $\\vn{x}_2=\\sqrt{\\sum_{i=1}^nx_i^2}=\\sqrt{\\v{x}^‚ä§\\v{x}}=\\sqrt{‚ü®\\v{x},\\v{x}‚ü©}$. Has inner product space $(V,‚ü®‚ãÖ,‚ãÖ‚ü©)$. We use this.\n",
        "\n",
        "  - $\\vn{x}_2=c$ is a sphere.\n",
        "\n",
        "- $\\v{P}$-quadratic norm: $\\vn{x}_\\v{P}=\\sqrt{\\v{x}^‚ä§\\v{Px}}$.\n",
        "\n",
        "  - If $\\v{P}$ is positive-definite then $\\v{P}=\\v{P}^{1/2}\\v{P}^{1/2}$ by Cholesky. Then $\\sqrt{\\v{x}^‚ä§\\v{Px}}=\\sqrt{\\v{xP}^{1/2}\\v{P}^{1/2}\\v{x}}=\\n{\\v{P}^{1/2}\\v{x}}_2$\n",
        "\n",
        "- **Distance**: On inner product space $(V,‚ü®‚ãÖ,‚ãÖ‚ü©)$, the **metric** $d:V√óV‚Üí‚Ñù,\\blue{d(\\v{x},\\v{y})=\\n{\\v{x}-\\v{y}}}=\\sqrt{‚ü®\\v{x}-\\v{y},\\v{x}-\\v{y}‚ü©}$ is the distance between $\\v{x},\\v{y}‚ààV$, and satisfies $\\BC\n",
        "1.&\\t{positive-definite}&d(\\v{x},\\v{y})‚â•0\\ ‚àÄ\\v{x},\\v{y}‚ààV\\t{ and }d(\\v{x},\\v{y})=0‚áî\\v{x}=\\v{y}\\\\\n",
        "2.&\\t{symmetry}&d(\\v{x},\\v{y})=d(\\v{y},\\v{x})\\ ‚àÄ\\v{x},\\v{y}‚ààV\\\\\n",
        "3.&\\t{triangle inequality}&d(\\v{x},\\v{z})‚â§d(\\v{x},\\v{y})+d(\\v{y},\\v{z})\\ ‚àÄ\\v{x},\\v{y},\\v{c}‚ààV\n",
        "\\EC$\n",
        "\n",
        "  - $d(\\v{x},\\v{y})=\\n{\\v{x}-\\v{y}}=\\sqrt{\\sum_{i=1}^n(x_i-y_i)^2}$ represents the root squares error.\n",
        "\n",
        "- **Angle** between $\\v{x},\\v{y}‚ààV$ is $\\red{\\cosœâ=\\/{‚ü®\\v{x},\\v{y}‚ü©}{\\vn{x}\\vn{y}}=\\/{‚ü®\\v{x},\\v{y}‚ü©}{\\sqrt{‚ü®\\v{x},\\v{x}‚ü©‚ü®\\v{y},\\v{y}‚ü©}}}$, which by Cauchy-Schwarz satisfies $\\blue{-1‚â§\\/{‚ü®\\v{x},\\v{y}‚ü©}{\\vn{x}\\vn{y}}‚â§1}$.\n"
      ],
      "metadata": {
        "id": "WS2OseUG6Sfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Orthogonal**: $\\v{x}\\perp\\v{y}‚áî‚ü®\\v{x},\\v{y}‚ü©=0$. Additionally if $\\vn{x}=\\vn{y}=1$ then $\\v{x},\\v{y}$ are called **orthonormal**.\n",
        "\n",
        "- $\\v{0}$ is orthogonal to every vector.\n",
        "\n",
        "- **Orthogonal/orthonormal matrix** $\\v{A}‚àà‚Ñù^{n√ón}$ satisfies $\\red{\\v{A}^‚ä§\\v{A}=\\v{I}}$ or $\\v{A}^{-1}=\\v{A}^‚ä§$. This implies all columns are orthogonal to each other, and have unit length. **Linear transformation using orthonormal matrix does not change vector norm or relative angle between vectors**. They're rotations ($\\det(\\v{A})=1$) and reflections ($\\det(\\v{A})=-1$).\n",
        "\n",
        "  - Proof: $\\vn{Ax}=\\sqrt{(\\v{Ax})^‚ä§(\\v{Ax})}=\\vn{x}$.\n",
        "  Angle\n",
        "  $\\cosœâ=\\/{‚ü®\\v{Ax},\\v{Ay}‚ü©}{\\vn{Ax}\\vn{Ay}}$\n",
        "  $=\\/{(\\v{Ax})^‚ä§(\\v{Ay})}{\\vn{Ax}\\vn{Ay}}$\n",
        "  $=\\/{‚ü®\\v{x},\\v{y}‚ü©}{\\vn{x}\\vn{y}}$.\n",
        "\n",
        "- **Orthonormal basis**: Consider $n$-dimensional vector space $V$ with basis $B$. If the basis satisfies $\\BC ‚ü®\\v{b}_i,\\v{b}_j‚ü©=0&i\\neq j\\\\‚ü®\\v{b}_i,\\v{b}_i‚ü©=1\\EC$ then it is called orthonormal basis (ONB). ONB matrix is an orthogonal matrix.\n",
        "\n",
        "  - All ONB matrices are rotation or reflection transformations.\n",
        "\n",
        "  - **Gram-Schmidt process**: Given basis $\\tilde{\\v{B}}$. Start with $\\v{u}_1=\\tilde{\\v{b}}_1$, form $\\v{u}_2=\\tilde{\\v{b}}_2-œÄ_{\\v{u_1}}(\\tilde{\\v{b}}_2)$, then $\\v{u}_3=\\tilde{\\v{b}}_3-œÄ_{\\v{u_1}}(\\tilde{\\v{b}}_3)-œÄ_{\\v{u_2}}(\\tilde{\\v{b}}_3)$, etc.\n",
        "  Finally normalize. Streamlined algorithm: construct augmented matrix and find row echelon form $\\t{ref}([\\tilde{\\v{B}}^‚ä§\\tilde{\\v{B}}|\\tilde{\\v{B}}^‚ä§])$.\n",
        "  At the end point LHS is upper-triangular, and RHS rows are the orthogonal basis. Transpose and normalize.\n",
        "\n",
        "- **Orthogonal complement**: Consider vector space $V$ and $U‚äÜV$ where $\\dim(V)=D$ and $\\dim(U)=M$. Then orthogonal complement $U^\\perp‚äÜV$ where $U‚à©U^\\perp=\\{\\v{0}\\}$ and $\\dim(U^\\perp)=D-M$ contains all vectors in $V$ that are orthogonal to every vector in $U$. Any vector $\\v{x}‚ààV$ can be decomposed as $\\v{x}=\\v{B}\\v{x}_B+\\v{B}^\\perp\\v{x}_{B^\\perp}$ for basis $\\v{B}$ of $U$ and $\\v{B}^\\perp$ of $U^\\perp$ where $\\v{B}^\\perp=\\null(\\v{B}^‚ä§)$\n"
      ],
      "metadata": {
        "id": "cb1LwgH3eNtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Orthogonal Projection onto Line**: Let $U‚äÜV$ be a 1-dimensional subspace with basis $\\v{b}$. A linear mapping $œÄ:V‚ÜíU$ is a projection if $\\red{œÄ^2=œÄ‚àòœÄ=œÄ}$. Let $\\v{x}‚ààV$, then $\\BC\n",
        "\\t{projection component}&œÄ_U(\\v{x})‚ààU\\t{ or }œÄ_U(\\v{x})=Œª\\v{b}, Œª‚àà‚Ñù\\\\\n",
        "\\t{orthogonal complement}&(\\v{x}-\\v{a})‚ààU^\\perp\\t{ or }‚ü®\\v{x}-\\v{a},\\v{b}‚ü©=0\\\\\n",
        "\\t{least squares error}&œÄ_U(\\v{x})=\\arg\\min_\\v{a}\\n{\\v{x}-\\v{a}}\n",
        "\\EC$\n",
        "\n",
        "- The orthogonal projection of $\\v{x}$ onto $\\v{b}$ is $\\red{\\/{\\v{b}^‚ä§\\v{x}}{\\vn{b}^2}\\v{b}=\\/{\\v{bb}^‚ä§}{\\v{b}^‚ä§\\v{b}}\\v{x}}$.\n",
        "\n",
        "- $\\blue{œÄ_U(\\v{x})=\\/{\\v{b}\\v{b}^‚ä§}{\\vn{b}^2}\\v{x}}=\\v{P}_œÄ\\v{x}$ where $\\red{\\v{P}_œÄ=\\/{\\v{b}\\v{b}^‚ä§}{\\vn{b}^2}}‚àà‚Ñù^{n√ón}$ is the **projection matrix**.\n",
        "\n",
        "  - Proof: $‚ü®\\v{b},\\v{x}-œÄ_U(\\v{x})‚ü©=‚ü®\\v{b},\\v{x}-Œª\\v{b}‚ü©$\n",
        "  $=‚ü®\\v{b},\\v{x}‚ü©-Œª‚ü®\\v{b},\\v{b}‚ü©=0$\n",
        "  $‚áí\\green{Œª=\\/{‚ü®\\v{b},\\v{x}‚ü©}{\\vn{b}^2}}$\n",
        "  $‚áíœÄ_U(\\v{x})=Œª\\v{b}=\\v{b}Œª=\\/{\\v{b}\\v{b}^‚ä§\\v{x}}{\\vn{b}^2}$\n",
        "\n",
        "  - $\\green{\\n{œÄ_U(\\v{x})}}=\\vn{b}\\/{|\\v{b}^‚ä§\\v{x}|}{\\vn{b}^2}\\/{\\vn{x}}{\\vn{x}}$\n",
        "  $=\\/{\\vn{b}}{\\vn{b}}\\/{|\\v{b}^‚ä§\\v{x}|}{\\vn{b}\\vn{x}}\\vn{x}$\n",
        "  $=\\green{|\\cosœâ|\\vn{x}}$.\n",
        "\n",
        "**Orthogonal Projection onto Subspace**: Let $U‚äÜV$ with $\\dim(U)=m‚â§n$ with basis $\\v{B}‚àà‚Ñù^{n√óm}$. Then $œÄ_U(\\v{x})=\\v{BŒª}$, where $\\v{Œª}‚àà‚Ñù^m$ are the coordinates of the projection.\n",
        "\n",
        "- $\\blue{œÄ_U(\\v{x})=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§\\v{x}}=\\v{P}_œÄ\\v{x}$ where $\\red{\\v{P}_œÄ=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§}‚àà‚Ñù^{n√ón}$ is the projection matrix (hat matrix).\n",
        "\n",
        "  - Proof: $‚ü®\\v{b}_i,\\v{x}-œÄ_U(\\v{x})‚ü©=\\v{b}_i^‚ä§(\\v{x}-\\v{BŒª})=0$\n",
        "  $‚áí\\green{\\v{B}^‚ä§(\\v{x}-\\v{BŒª})=\\v{0}}$\n",
        "  $‚áí\\v{B}^‚ä§\\v{x}-\\v{B}^‚ä§\\v{BŒª}=\\v{0}$\n",
        "  $‚áí\\green{\\v{Œª}=(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§\\v{x}}$.\n",
        "  Then\n",
        "  $œÄ_U(\\v{x})=\\v{BŒª}=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§\\v{x}$\n",
        "\n",
        "  - If $\\v{B}$ is orthonormal basis, then $\\v{P}_œÄ=\\v{BB}^‚ä§$.\n",
        "\n",
        "  - If using $‚ü®\\v{x},\\v{y}‚ü©_\\v{A}=\\v{x}^‚ä§\\v{Ay}$, then\n",
        "  $‚ü®\\v{b}_i,\\v{x}-œÄ_U(\\v{x})‚ü©_\\v{A}=\\v{b}_i^‚ä§\\v{A}(\\v{x}-\\v{BŒª})$\n",
        "  $‚áí\\v{B}^‚ä§\\v{A}(\\v{x}-\\v{BŒª})=\\v{0}$\n",
        "  $‚áí\\v{Œª}=(\\v{B}^‚ä§\\v{AB})^{-1}\\v{B}^‚ä§\\v{A}$\n",
        "  $‚áí\\v{P}_œÄ=\\v{B}(\\v{B}^‚ä§\\v{AB})^{-1}\\v{B}^‚ä§\\v{A}$\n",
        "\n",
        "- **Normal equation** $\\green{\\v{Œª}=(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§\\v{x}}=\\arg\\min_\\v{œà}\\n{\\v{x}-\\v{Bœà}}^2$ is the least squares estimator to the $\\v{BŒª}=\\v{x}$ regression. $\\v{B}^‚ä§\\v{B}‚àà‚Ñù^{m√óm}$ is invertible and positive-definite. $(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§$ is the **pseudoinverse** of $\\v{B}$.\n",
        "\n",
        "  - Proof: We know $\\null(\\v{B})=\\{\\v{0}\\}$. By equivalence,\n",
        "  $\\v{B}^‚ä§\\v{Bx}=\\v{0}$\n",
        "  $‚áî\\v{x}^‚ä§(\\v{B}^‚ä§\\v{Bx})=(\\v{Bx})^‚ä§(\\v{Bx})=\\n{\\v{Bx}}^2=0$\n",
        "  $‚áî\\v{Bx}=\\v{0}$.\n",
        "  Therefore $\\null(\\v{B}^‚ä§\\v{B})=\\null(\\v{B})=\\{\\v{0}\\}$ and $\\v{B}^‚ä§\\v{B}$ is invertible.\n",
        "  $\\BC\n",
        "  \\v{x}^‚ä§(\\v{B}^‚ä§\\v{B})\\v{x}=\\n{\\v{Bx}}^2>0‚áî\\v{Bx}\\neq\\v{0}‚áî\\v{x}\\neq\\v{0}\\\\\n",
        "  \\v{x}^‚ä§(\\v{B}^‚ä§\\v{B})\\v{x}=\\n{\\v{Bx}}^2=0‚áî\\v{Bx}=\\v{0}‚áî\\v{x}=\\v{0}\n",
        "  \\EC$\n",
        "  therefore $\\v{B}^‚ä§\\v{B}$ is positive-definite.\n",
        "\n",
        "  - System of equations: $\\v{Ax}=\\v{b}$ where $\\v{b}$ does not lie in the subspace $\\span(\\v{A})$.\n",
        "\n",
        "- **Projection onto affine subspace** $L=\\v{x}_0+U$ is $œÄ_L(\\v{x})=\\v{x}_0+œÄ_U(\\v{x}-\\v{x}_0)$.\n",
        "\n",
        "- **Idempotent**: $œÄ_U‚àòœÄ_U(\\v{x})=œÄ_U(œÄ_U(\\v{x}))=œÄ_U(\\v{x})$. Projection matrix $\\v{P}_œÄ\\v{P}_œÄ=\\v{P}_œÄ$ is symmetric.\n",
        "\n",
        "  - Proof (idempotent): $\\v{P}_œÄ\\v{P}_œÄ=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}(\\v{B}^‚ä§\\v{B})(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§$\n",
        "  $=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§=\\v{P}_œÄ$.\n",
        "\n",
        "  - Proof (symmetric): $\\v{P}_œÄ^‚ä§=(\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§)^‚ä§$\n",
        "  $=(\\v{B}^‚ä§)^‚ä§((\\v{B}^‚ä§\\v{B})^{-1})^‚ä§(\\v{B})^‚ä§$\n",
        "  $=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§=\\v{P}_œÄ$ because inverse of a symmetric matrix is also symmetric.\n",
        "\n",
        "- **Orthogonal complement**: Let $œÄ:V‚ÜíV$ be a projection and $\\t{id}_V:V‚ÜíV,\\t{id}_V(\\v{x})=\\v{x}$ be identity. Then $\\t{id}_V-œÄ$ is a projection, $\\Im(\\t{id}_V-œÄ)=\\null(œÄ)$, and $\\null(\\t{id}_V-œÄ)=\\Im(œÄ)$.\n",
        "Similarly $\\v{P}_œÄ$ and $\\v{I}-\\v{P}_œÄ$ are projection matrices that project to orthogonal subspaces. Together they decompose $\\v{x}$ into orthogonal components $\\v{x}=\\v{P}_œÄ\\v{x}+(\\v{I}-\\v{P}_œÄ)\\v{x}$. Furthermore $\\blue{\\v{P}_œÄ(\\v{I}-\\v{P}_œÄ)=\\v{0}}$.\n",
        "\n",
        "  - Proof: $(\\t{id}_V-œÄ)‚àò(\\t{id}_V-œÄ)=\\t{id}_V‚àò\\t{id}_V-2œÄ‚àò\\t{id}_V+œÄ‚àòœÄ$\n",
        "  $=\\t{id}_V-2œÄ+œÄ$\n",
        "  $=(\\t{id}_V-œÄ)$ idempotent.\n",
        "  $(\\t{id}_V-œÄ)(\\v{x})=\\green{\\v{x}-œÄ(\\v{x})}$\n",
        "  $‚áí\\BC\n",
        "  œÄ(\\green{\\v{x}-œÄ(\\v{x})})=œÄ(\\v{x})-œÄ(\\v{x})=\\v{0}&\\Im(\\t{id}_V-œÄ)=\\null(œÄ)\\\\\n",
        "  \\green{\\v{x}-œÄ(\\v{x})}=\\v{0}‚áí\\v{x}=œÄ(\\v{x})&\\null(\\t{id}_V-œÄ)=\\Im(œÄ)\n",
        "  \\EC$\n",
        "\n",
        "  - Proof: $(\\v{I}-\\v{P}_œÄ)(\\v{I}-\\v{P}_œÄ)=\\v{I}-2\\v{P}_œÄ+\\v{P}_œÄ\\v{P}_œÄ=\\v{I}-\\v{P}_œÄ$ idempotent.\n",
        "  $\\v{P}_œÄ(\\v{I}-\\v{P}_œÄ)=\\v{P}_œÄ-\\v{P}_œÄ\\v{P}_œÄ$\n",
        "  $=\\v{P}_œÄ-\\v{P}_œÄ=\\v{0}$. Furthermore\n",
        "  $‚ü®\\v{P}_œÄ\\v{x},(\\v{I}-\\v{P}_œÄ)\\v{x}‚ü©$\n",
        "  $=(\\v{P}_œÄ\\v{x})^‚ä§((\\v{I}-\\v{P}_œÄ)\\v{x})$\n",
        "  $=\\v{x}^‚ä§\\v{P}_œÄ^‚ä§(\\v{I}-\\v{P}_œÄ)\\v{x}$\n",
        "  $=\\v{x}^‚ä§\\v{P}_œÄ(\\v{I}-\\v{P}_œÄ)\\v{x}$\n",
        "  $=0$.\n",
        "\n",
        "- $œÄ_U(\\v{x})$ is an eigenvector of $\\v{P}_œÄ$.\n",
        "\n",
        "- 3.10: $\\v{b}=\\BM1\\\\2\\\\2\\EM$. Then $P_œÄ=\\/{\\v{b}\\v{b}^‚ä§}{\\vn{b}^2}$\n",
        "$=\\/{1}{9}\\BM1&2&2\\\\2&4&4\\\\2&4&4\\EM$.\n",
        "$P_œÄP_œÄ=\\/{1}{9^2}\\BM9&18&18\\\\18&36&36\\\\18&36&36\\EM$\n",
        "$=\\/{1}{9}\\BM1&2&2\\\\2&4&4\\\\2&4&4\\EM$.\n",
        "\n",
        "- 3.11: $U=\\span(\\BM1\\\\1\\\\1\\EM,\\BM0\\\\1\\\\2\\EM)$ and $\\v{x}=\\BM6\\\\0\\\\0\\EM$.\n",
        "Then $\\v{P}_œÄ=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§$\n",
        "$=\\/{1}{6}\\BM5&2&-1\\\\2&2&2\\\\-1&2&5\\EM$.\n",
        "Then $\\v{P}_œÄ\\v{x}=\\BM5\\\\2\\\\-1\\EM$"
      ],
      "metadata": {
        "id": "Sft1aiKn0t4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3.3: compute the distance between $\\v{x}=\\BM1\\\\2\\\\3\\EM$ and $\\v{y}=\\BM-1\\\\-1\\\\0\\EM$.\n",
        "$\\v{x}-\\v{y}=\\BM-2\\\\-3\\\\3\\EM$\n",
        "and $d=\\sqrt{‚ü®\\v{x}-\\v{y},\\v{x}-\\v{y}‚ü©}$.\n",
        "\n",
        "  - $‚ü®\\v{x},\\v{y}‚ü©=\\v{x}^‚ä§\\v{y}$. Then\n",
        "  $d(\\v{x},\\v{y})=\\sqrt{22}$.\n",
        "\n",
        "  - $‚ü®\\v{x},\\v{y}‚ü©=\\v{x}^‚ä§\\BM2&1&0\\\\1&3&-1\\\\0&-1&2\\EM\\v{y}$. Then $d(\\v{x},\\v{y})=\\sqrt{83}$.\n",
        "\n",
        "- 3.4: compute the angle between $\\v{x}=\\BM1\\\\2\\EM$ and $\\v{y}=\\BM-1\\\\-1\\EM$.\n",
        "The angle $\\cosœâ=\\/{‚ü®\\v{x},\\v{y}‚ü©}{\\sqrt{‚ü®\\v{x},\\v{x}‚ü©‚ü®\\v{y},\\v{y}‚ü©}}$.\n",
        "\n",
        "  - $‚ü®\\v{x},\\v{y}‚ü©=\\v{x}^‚ä§\\v{y}$. Then $\\cosœâ=\\/{-3}{\\sqrt{10}}$\n",
        "\n",
        "- 3.5: $U=\\span(\\BM0\\\\-1\\\\2\\\\0\\\\2\\EM,\\BM1\\\\-3\\\\1\\\\-1\\\\2\\EM,\\BM-3\\\\4\\\\1\\\\2\\\\1\\EM,\\BM-1\\\\-3\\\\5\\\\0\\\\7\\EM)$. $\\v{x}=\\BM-1\\\\-9\\\\-1\\\\4\\\\1\\EM$.\n",
        "\n",
        "  - Determine $œÄ_U(\\v{x})$.\n",
        "  $\\BM0&1&-3&-1\\\\-1&-3&4&-3\\\\2&1&1&5\\\\0&-1&2&0\\\\2&2&1&7\\EM‚Üí\\BM1&0&0&1\\\\0&1&0&2\\\\0&0&1&1\\\\0&0&0&0\\\\0&0&0&0\\EM$.\n",
        "  $\\v{B}=\\BM0&1&-3\\\\-1&-3&4\\\\2&1&1\\\\0&-1&2\\\\2&2&1\\EM$.\n",
        "  $œÄ_U(\\v{x})=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§\\v{x}=\\BM1\\\\-5\\\\-1\\\\-2\\\\3\\EM$.\n",
        "\n",
        "  - Determine the distance $d(\\v{x},U)$.\n",
        "  $\\v{x}-œÄ_U(\\v{x})=\\BM2\\\\4\\\\0\\\\-6\\\\2\\EM$ and $d(\\v{x},U)=\\sqrt{‚ü®\\v{x}-œÄ_U(\\v{x}),\\v{x}-œÄ_U(\\v{x})‚ü©}$\n",
        "\n",
        "- 3.6: Consider $‚Ñù^3$ with $‚ü®\\v{x},\\v{y}‚ü©_\\v{A}:=\\v{x}^‚ä§\\BM2&1&0\\\\1&2&-1\\\\0&-1&2\\EM\\v{y}$. Define $(\\v{e}_1,\\v{e}_2,\\v{e}_3)$ as standard basis.\n",
        "\n",
        "  - Let $U=\\span(\\v{e}_1,\\v{e}_3)$. Find $œÄ_U(\\v{e}_2)$.\n",
        "  $\\v{B}=\\BM1&0\\\\0&0\\\\0&1\\EM$.\n",
        "  $œÄ_U(\\v{e}_2)=\\v{B}(\\v{B}^‚ä§\\v{AB})^{-1}\\v{B}^‚ä§\\v{Ax}$\n",
        "  $=\\BM1/2\\\\0\\\\-1/2\\EM$."
      ],
      "metadata": {
        "id": "oENJE81ZVW1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schur Complement**: Given symmetric block matrix $\\v{M}=\\BM \\v{A}&\\v{B}\\\\\\v{B}^‚ä§&\\v{C}\\EM$. If $\\v{C}$ is positive-definite (invertible), then Schur Complement of $\\v{C}$ is $\\v{A}-\\v{BC}^{-1}\\v{B}^‚ä§$. If $\\v{A}$ is invertible, then Schur Complement of $\\v{A}$ is $\\v{C}-\\v{B}^‚ä§\\v{A}^{-1}\\v{B}$.\n",
        "\n",
        "- **Schur Complement Lemma**: $\\v{M}$ is positive-definite iff $\\v{C}$ is positive-definite and $\\v{A}-\\v{BC}^{-1}\\v{B}^‚ä§$ is positive-definite.\n",
        "\n",
        "  - Or $\\v{M}$ is positive-definite iff $\\v{A}$ is positive-definite and $\\v{C}-\\v{B}^‚ä§\\v{A}^{-1}\\v{B}$  is positive-definite.\n",
        "\n",
        "  - $\\v{M}$ is positive-definite iff $\\v{A}$ and $\\v{C}$ are positive-definite. Intuitive check: $\\v{A}$ and $\\v{C}$ are both principal minors of $\\v{M}$.\n",
        "\n",
        "  - If $\\v{C}$ is strictly positive-definite, then $\\v{M}$ is positive-semidefinite iff $\\v{A}-\\v{BC}^{-1}\\v{B}^‚ä§$ is positive-semidefinite.\n",
        "\n",
        "- Used to check if large matrix $\\v{M}$ is positive-definite without calculating REF or eigenvalues.\n",
        "\n",
        "- Proof (block inverse): Define inverse\n",
        "$\\BM\\v{Œ£}_\\v{xx}&\\v{Œ£}_\\v{xy}\\\\\\v{Œ£}_\\v{yx}&\\v{Œ£}_\\v{yy}\\EM\\BM\\v{Œõ}_\\v{xx}&\\v{Œõ}_\\v{xy}\\\\\\v{Œõ}_\\v{yx}&\\v{Œõ}_\\v{yy}\\EM=\\BM\\v{I}&\\v{0}\\\\\\v{0}&\\v{I}\\EM$. Then, $\\BC\n",
        "\\t{bottom-left:}&\n",
        "\\v{Œ£}_\\v{yx}\\v{Œõ}_\\v{xx}+\\v{Œ£}_\\v{yy}\\v{Œõ}_\\v{yx}=\\v{0}&\n",
        "\\v{Œõ}_\\v{yx}=-\\v{Œ£}_\\v{yy}^{-1}\\v{Œ£}_\\v{yx}\\v{Œõ}_\\v{xx}&\n",
        "\\v{Œõ}_\\v{yx}=-\\v{Œõ}_\\v{yy}\\v{Œ£}_\\v{yx}\\v{Œ£}_\\v{xx}^{-1}\\\\\n",
        "\\t{top-right:}&\n",
        "\\v{Œ£}_\\v{xx}\\v{Œõ}_\\v{xy}+\\v{Œ£}_\\v{xy}\\v{Œõ}_\\v{yy}=\\v{0}&\n",
        "\\v{Œõ}_\\v{xy}=-\\v{Œ£}_\\v{xx}^{-1}\\v{Œ£}_\\v{xy}\\v{Œõ}_\\v{yy}&\n",
        "\\v{Œõ}_\\v{xy}=-\\v{Œõ}_\\v{xx}\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}\\\\\n",
        "\\t{top-left:}&\n",
        "\\v{Œ£}_\\v{xx}\\v{Œõ}_\\v{xx}+\\v{Œ£}_\\v{xy}\\v{Œõ}_\\v{yx}=\\v{I}\\\\\n",
        "&(\\v{Œ£}_\\v{xx}-\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}\\v{Œ£}_\\v{yx})\\v{Œõ}_\\v{xx}=\\v{I}&\n",
        "\\v{Œõ}_\\v{xx}=(\\v{Œ£}_\\v{xx}-\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}\\v{Œ£}_\\v{yx})^{-1}\\\\\n",
        "\\t{bottom-right:}&\n",
        "\\v{Œ£}_\\v{yx}\\v{Œõ}_\\v{xy}+\\v{Œ£}_\\v{yy}\\v{Œõ}_\\v{yy}=\\v{I}\\\\\n",
        "&(-\\v{Œ£}_\\v{yx}\\v{Œ£}_\\v{xx}^{-1}\\v{Œ£}_\\v{xy}+\\v{Œ£}_\\v{yy})\\v{Œõ}_\\v{yy}=\\v{I}&\n",
        "\\v{Œõ}_\\v{yy}=(\\v{Œ£}_\\v{yy}-\\v{Œ£}_\\v{yx}\\v{Œ£}_\\v{xx}^{-1}\\v{Œ£}_\\v{xy})^{-1}\n",
        "\\EC$\n",
        "\n",
        "- Proof (minimizing quadratic): B&V 3.15"
      ],
      "metadata": {
        "id": "YpA3UiwR1pDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Principal minors for positive-definite/positive-semidefinite**:\n",
        "\n",
        "- **Principal minor** of $\\v{A}‚àà‚Ñù^{n√ón}$: determinant of the submatrix constructed using rows and columns chosen from subset of $k$ indices from $\\{1,...,n\\}$.\n",
        "\n",
        "  - **Leading principal minor**: determinant of submatrix that includes the first $k$ rows/columns of $\\v{A}$ as a contiguous top-left block.\n",
        "\n",
        "- $\\v{A}$ is positive-definite iff all leading principal minors are positive.\n",
        "\n",
        "- $\\v{A}$ is positive-semidefinite iff all principal minors are positive.\n",
        "\n",
        "**Quadratic forms**:\n",
        "\n",
        "- Quadratic form $\\v{x}^‚ä§\\v{Qx}$ always has a symmetric matrix $\\v{Q}$.\n",
        "  - Proof: A square matrix $\\v{Q}‚àà‚Ñù^{n√ón}=\\v{P}+\\v{S}=\\/{\\v{Q}+\\v{Q}^‚ä§}{2}+\\/{\\v{Q}-\\v{Q}^‚ä§}{2}$ can be split into symmetric $\\v{P}$ and skew-symmetric $\\v{S}$ who has special property $\\v{S}^‚ä§=-\\v{S}$. Quadratic form $\\v{x}^‚ä§\\v{Qx}=\\v{x}^‚ä§(\\v{P}+\\v{S})\\v{x}=\\v{x}^‚ä§\\v{Px}+\\v{x}^‚ä§\\v{Sx}$. But $(\\v{x}^‚ä§\\v{Sx})^‚ä§=-\\v{x}^‚ä§\\v{Sx}‚àà‚Ñù$. Therefore $\\v{x}^‚ä§\\v{Sx}=0$ and $\\v{x}^‚ä§\\v{Qx}=\\v{x}^‚ä§\\v{Px}$.\n",
        "\n",
        "- Quadratic form $\\v{x}^‚ä§\\v{Qx}$ is convex iff $\\v{Q}$ is positive-semidefinite.\n",
        "\n",
        "  - Proof: $‚àá_\\v{x}(\\v{x}^‚ä§\\v{Qx})=\\v{x}^‚ä§(\\v{Q}+\\v{Q}^‚ä§)=2\\v{x}^‚ä§\\v{Q}‚áí‚àá_\\v{x}^2(\\v{x}^‚ä§\\v{Qx})=2\\v{Q}$. I.e., $2\\v{Q}$ is the Hessian.\n",
        "\n",
        "  - Though the existence of a single optimal $\\v{x}^*$ requires positive-definite $\\v{Q}$."
      ],
      "metadata": {
        "id": "BGvDwUGS0dA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Determinants, Eigendecomposition, and SVD"
      ],
      "metadata": {
        "id": "duhBEROv2rcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Determinant**: If columns of $\\v{A}‚àà‚Ñù^{\\red{n√ón}}$ are edges of a parallelepiped extending from origin, then $|\\det(\\v{A})|$ is its volume.\n",
        "\n",
        "- **Laplace expansion**: recursive determinant calculation. For $\\v{A}‚àà‚Ñù^{n√ón}$, pick any $j=1..n$ and go along the $j$-th column $\\det(\\v{A})=\\suml_{k=1}^n(-1)^{k+j}a_{kj}\\det(\\v{A}_{kj})$ or go along the $j$-th row $\\det(\\v{A})=\\suml_{k=1}^n(-1)^{k+j}a_{jk}\\det(\\v{A}_{jk})$, where $\\v{A}_{kj}‚àà‚Ñù^{(n-1)√ó(n-1)}$ is the submatrix obtained by deleting row $k$ and column $j$.\n",
        "\n",
        "  - If $\\v{T}‚àà‚Ñù^{n√ón}$ is a triangular matrix, $\\red{\\det(\\v{T})=\\prod_iT_{ii}}$. Therefore find $\\t{ref}(\\v{A})$ while keeping tally of row swaps $R_i‚ÜîR_j$ ($√ó-1$) and scaling a row $R_i‚ÜêkR_i$ ($√ók$).\n",
        "\n",
        "- Let $\\v{A}‚àà‚Ñù^{n√ón}$ be invertible. Then $\\t{rref}(\\v{A})=\\v{E}_k...\\v{E}_1\\v{A}=\\v{I}_n‚áí\\green{\\v{A}=\\v{E}_1^{-1}...\\v{E}_k^{-1}\\v{I}_n}$ where $\\v{E}_i‚àà‚Ñù^{n√ón}$ is  **elementary matrix** representing a row operation $\\BC\n",
        "R_{i}‚ÜîR_{j}&\\v{E}_\\t{swap}=-1&\\det(\\v{E}_\\t{swap}\\v{B})=-\\det(\\v{B})\\\\\n",
        "R_{i}‚ÜêkR_{i}&\\v{E}_\\t{scale}=k&\\det(\\v{E}_\\t{scale}\\v{B})=k\\det(\\v{B})\\\\\n",
        "R_{i}‚ÜêR_{i}+kR_{j}&\\v{E}_\\t{add}=1&\\det(\\v{E}_\\t{add}\\v{B})=\\det(\\v{B})\n",
        "\\EC$\n",
        "\n",
        "  - Matrix $\\v{A}‚àà‚Ñù^{n√ón}$ is invertible $‚áî\\det(\\v{A})\\neq0‚áî\\rk(\\v{A})=n$. Proof: $\\det(\\v{I}_n)=1$ and $\\v{E}_i\\neq\\v{0}$.\n",
        "\n",
        "  - $\\det(\\v{AB})=\\det(\\v{A})\\det(\\v{B})$. Proof: both $\\v{A}$ and $\\v{B}$ are products of elementary matrices.\n",
        "\n",
        "  - $\\det(Œª\\v{A})=Œª^n\\det(\\v{A})$. Proof: application of $\\v{E}_\\t{scale}$ on $n$ rows.\n",
        "\n",
        "  - $\\det(\\v{A}^‚ä§)=\\det(\\v{A})$. Proof: from Laplace expansion\n",
        "\n",
        "  - $\\det(\\v{A}^{-1})=\\/{1}{\\det(\\v{A})}$. Proof: $\\det(\\v{I}_n)=1$.\n",
        "\n",
        "  - For $Œ¶:V‚ÜíV$, similar matrices $\\v{A}_{BB}=\\v{BA}_{std}\\v{B}^{-1}$ have equal determinant $\\det(\\v{A}_{std})=\\det(\\v{A}_{BB})$.\n",
        "\n",
        "**Trace** of a square matrix $\\v{A}‚àà‚Ñù^{n√ón}$ is $\\red{\\tr(\\v{A})=\\suml_{i=1}^na_{ii}}$ satisfying $\\BC\n",
        "\\tr(\\v{A}+\\v{B})=\\tr(\\v{A})+\\tr(\\v{B}),\\ \\v{A},\\v{B}‚àà‚Ñù^{n√ón}\\\\\n",
        "\\tr(Œ±\\v{A})=Œ±\\tr(\\v{A}),\\ Œ±‚àà‚Ñù,\\v{A}‚àà‚Ñù^{n√ón}\\\\\n",
        "\\tr(\\v{I}_n)=n\\\\\n",
        "\\tr(\\v{AB})=\\tr(\\v{BA}),\\ \\v{A}‚àà‚Ñù^{n√ók},\\v{B}‚àà‚Ñù^{k√ón}\n",
        "\\EC$\n",
        "\n",
        "- Cyclic invariance: $\\tr(\\v{AKL})=\\tr(\\v{KLA})$ for $\\v{A}‚àà‚Ñù^{a√ók}$, $\\v{K}‚àà‚Ñù^{k√ól}$, $\\v{L}‚àà‚Ñù^{l√óa}$.\n",
        "\n",
        "  - If $\\v{x},\\v{y}‚àà‚Ñù^n$ are two vectors, then $\\tr(\\v{x}\\v{y}^‚ä§)=\\tr(\\v{y}^‚ä§\\v{x})=\\v{y}^‚ä§\\v{x}‚àà‚Ñù$.\n",
        "\n",
        "- Trace is a property of the transformation $Œ¶:V‚ÜíV$  and is independent of the basis. Similar matrices $\\v{A}_{BB}=\\v{BA}_{std}\\v{B}^{-1}$ have equal trace.\n",
        "\n",
        "  - Proof: By cyclic property $\\tr(\\v{A}_{BB})=\\tr(\\v{BA}_{std}\\v{B}^{-1})=\\tr(\\v{B}^{-1}\\v{BA}_{std})=\\tr(\\v{A}_{std})$.\n",
        "\n",
        "- **Frobenius inner product** $\\red{‚ü®\\v{A},\\v{B}‚ü©=\\tr(\\v{A}^‚ä§\\v{B})=\\suml_{i=1}^m\\suml_{j=1}^nA_{ij}B_{ij}}‚àà‚Ñù$ where $\\v{A},\\v{B}‚àà‚Ñù^{m√ón}$ is matrix dot product.\n",
        "\n",
        "  - Proof: $(\\v{A}^‚ä§\\v{B})_{uv}=\\suml_{i=1}^mA_{iu}B_{iv}$\n",
        "  $‚áí\\tr(\\v{A}^‚ä§\\v{B})=\\suml_{j=1}^n(\\v{A}^‚ä§\\v{B})_{jj}$\n",
        "  $=\\suml_{j=1}^n\\left(\\suml_{i=1}^mA_{ij}B_{ij}\\right)$\n",
        "\n",
        "  - **Frobenius norm** $\\vn{A}_F=\\sqrt{\\tr(\\v{A}^‚ä§\\v{A})}$\n",
        "\n",
        "  - **Quadratic form**: Let $\\v{M}‚àà‚Ñù^{n√ón}$, then $\\tr(\\v{M}\\v{xx}^‚ä§)=\\tr(\\v{x}^‚ä§\\v{Mx})=\\v{x}^‚ä§\\v{Mx}‚àà‚Ñù$\n",
        "  by cyclic property.\n",
        "\n",
        "- $\\tr(\\v{xx}^‚ä§)=\\vn{x}_2^2$."
      ],
      "metadata": {
        "id": "SdT_KPCn4EtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eigenvalues and eigenvectors**: $\\red{\\v{Ax}=Œª\\v{x}}$ where $\\v{A}‚àà‚Ñù^{\\red{n√ón}}$, $Œª‚àà‚Ñù$, and $\\v{x}‚àà‚Ñù^n\\backslash\\{\\v{0}\\}$. Then $Œª$ is an **eigenvalue** of $\\v{A}$ and $\\v{x}$ is a corresponding **eigenvector**. Imagine a set of arrows pointing outwards from a circle. When transformation $\\v{A}$ is applied, most are knocked off course except eigenvectors that stay in the same directions but change their lengths by factors equal to their eigenvalues. Eigenvectors are the structural axes of the transformation.\n",
        "\n",
        "- If $Œª$ is an eigenvalue, then $\\green{(\\v{A}-Œª\\v{I}_n)\\v{x}=\\v{0}}$ for nontrivial $\\v{x}‚àà\\red{E_Œª=\\null(\\v{A}-Œª\\v{I}_n)\\backslash\\{\\v{0}\\}}$ called the **eigenspace** wrt $Œª$. It also holds that $\\rk(\\v{A}-Œª\\v{I}_n)< n$, and $\\det(\\v{A}-Œª\\v{I}_n)=0$.\n",
        "\n",
        "  - If $\\v{x}‚ààE_Œª$, then all collinear vectors $c\\v{x}‚ààE_Œª,\\ c‚àà‚Ñù\\backslash\\{0\\}$.\n",
        "\n",
        "  - Two vectors are **collinear** if they're parallel, and **codirected** if they also share the same direction.\n",
        "\n",
        "  - $\\dim(E_Œª)‚â•1$ is called the **geometric multiplicity** of $Œª$.\n",
        "\n",
        "- **Characteristic polynomial** for $\\v{A}‚àà‚Ñù^{n√ón}$ and $Œª‚àà‚Ñù$ is $\\red{p_\\v{A}:=\\det(\\v{A}-Œª\\v{I})}=\\suml_{i=1}^{n-1}c_iŒª^i+(-1)^nŒª^n$, where $c_0=\\det(\\v{A})$ and $c_{n-1}=(-1)^{n-1}\\tr(\\v{A})$.\n",
        "\n",
        "  - $Œª$ is an eigenvalue of $\\v{A}$ iff $Œª$ is a root of $p_\\v{A}$. The number of times $Œª$ appears as a root in $p_\\v{A}$ is called its **algebraic multiplicity**. The set of all eigenvalues is called the **eigenspectrum**. $\\v{A}$ is guaranteed to have $n$ eigenvalues, including repeats and complex numbers.\n",
        "\n",
        "- $\\v{A}$ and $\\v{A}^‚ä§$ share the same eigenvalues.\n",
        "\n",
        "  - Proof: $\\det(\\v{A}^‚ä§-Œª\\v{I})=\\det((\\v{A}-Œª\\v{I})^‚ä§)=\\det(\\v{A}-Œª\\v{I})$.\n",
        "\n",
        "- Similar matrices $\\v{A}_{BB}=\\v{BA}_{std}\\v{B}^{-1}$ share the same eigenvalues.\n",
        "\n",
        "  - Proof: $\\det(\\v{A}_{BB}-Œª\\v{I})=\\det(\\v{BA}_{std}\\v{B}^{-1}-Œª\\v{BIB}^{-1})=\\det(\\v{B}(\\v{A}_{std}-Œª\\v{I})\\v{B}^{-1})=\\det(\\v{A}_{std}-Œª\\v{I})$.\n",
        "\n",
        "- Let $\\v{T}=\\v{A}+c\\v{I}$. Then eigenvalues of $\\v{T}$ are $Œª_i+c$ for each eigenvalue $Œª_i$ of $\\v{A}$.\n",
        "\n",
        "  - Proof: Let $Œª_i$ be an eigenvalue of $\\v{A}$ with eigenvector $\\v{x}_i$. Then\n",
        "  $\\v{T}\\v{x}_i=(\\v{A}+c\\v{I})\\v{x}_i=Œª_i\\v{x}_i+c\\v{x}_i$\n",
        "  $=(Œª_i+c)\\v{x}_i$.\n",
        "  Therefore $Œª_i+c$ is an eigenvalue of $\\v{T}$.\n",
        "\n",
        "**Examples of transformations**:\n",
        "\n",
        "- $\\v{A}_1=\\BM1/2&0\\\\0&2\\EM$ stretch. $\\det(\\v{A}_1)=1$ area is preserved.\n",
        "$\\det(\\v{A}-Œª\\v{I})=(1/2-Œª)(2-Œª)$. $E_{1/2}=\\span(\\BM1\\\\0\\EM)$.\n",
        "$E_2=\\span(\\BM0\\\\1\\EM)$. The horizontal axis is compressed by half, and vertical axis is stretched by 2.\n",
        "\n",
        "- $\\v{A}_2=\\BM1&1/2\\\\0&1\\EM$ horizontal shear. $\\det(\\v{A}_2)=1$ area is preserved.\n",
        "$\\det(\\v{A}-Œª\\v{I})=(1-Œª)^2$. $E_1=\\span(\\BM1\\\\0\\EM)$.\n",
        "Defective. Only vectors along the horizontal axis preserve direction.\n",
        "$\\BM1&1/2\\\\0&1\\EM\\BM0\\\\a\\EM=\\BM a/2\\\\a\\EM$ all vertical components are horizontally displaced by a 1/2 factor.\n",
        "\n",
        "  - $\\v{A}_2=\\BM1&1\\\\0&2\\EM$ shear. $\\det(\\v{A}_2)=2$. $E_1=\\span(\\BM-1\\\\0\\EM)$. $E_2=\\span(\\BM-1\\\\-1\\EM)$.\n",
        "  Non-defective. Then\n",
        "  $\\v{D}=\\BM2&0\\\\0&1\\EM$, $\\v{P}=\\BM-1&-1\\\\-1&0\\EM$. $\\v{P}^{-1}=\\BM0&-1\\\\-1&1\\EM$.\n",
        "  $\\v{PDP}^{-1}=\\BM-2&-1\\\\-2&0\\EM\\BM0&-1\\\\-1&1\\EM$\n",
        "  $=\\BM1&1\\\\0&2\\EM$. Diagonlized, but $\\v{P}$ is not orthogonal.\n",
        "\n",
        "- $\\v{A}_3=\\BM\\cos(œÄ/6)&-sin(œÄ/6)\\\\sin(œÄ/6)&\\cos(œÄ/6)\\EM=\\/{1}{2}\\BM\\sqrt{3}&-1\\\\1&\\sqrt{3}\\EM$. Rotation matrices are orthonormal with $\\det(\\v{A}_3)=1$ preserving area. Reflections are orthonormal with $\\det(\\v{A})=-1$.\n",
        "$\\det(\\v{A}-Œª\\v{I})=(\\sqrt{3}-Œª)^2+1$\n",
        "$‚áíŒª=\\/{\\sqrt{3}}{2}¬±\\/{j}{2}$.\n",
        "\n",
        "- $\\v{A}_4=\\BM1&-1\\\\-1&1\\EM$. $\\det(\\v{A}_4)=0$ area is collapsed.\n",
        "$\\det(\\v{A}-Œª\\v{I})=\\det\\BM1-Œª&-1\\\\-1&1-Œª\\EM$\n",
        "$=\\det\\BM-Œª&-Œª\\\\-1&1-Œª\\EM=\\det\\BM-Œª&0\\\\-1&2-Œª\\EM$\n",
        "$=Œª(2-Œª)$.\n",
        "$E_0=\\span(\\BM1\\\\1\\EM)$. $E_2=\\span(\\BM1\\\\-1\\EM)$.\n",
        "2-dimensional domain is collapsed onto 1-dimensional.\n",
        "\n",
        "- $\\v{A}_5=\\BM1&1/2\\\\1/2&1\\EM$ shear/stretch. $\\det(\\v{A}_5)=3/4$.\n",
        "$\\det(\\v{A}-Œª\\v{I})=\\det\\BM1-Œª&1/2\\\\1/2&1-Œª\\EM$\n",
        "$=(1-Œª)^2-1/4=3/4-2Œª+Œª^2$. $Œª=\\/{2¬±\\sqrt{4-3}}{2}$.\n",
        "$E_{1/2}=\\span(\\BM1\\\\-1\\EM)$.\n",
        "$E_{3/2}=\\span(\\BM1\\\\1\\EM)$.\n",
        "\n",
        "- 4.5: $\\v{A}=\\BM4&2\\\\1&3\\EM$. Calculate eigenvalues and eigenspaces.\n",
        "$\\v{A}-Œª\\v{I}=\\BM4-Œª&2\\\\1&3-Œª\\EM$.\n",
        "$\\det(\\v{A}-Œª\\v{I})=(4-Œª)(3-Œª)-2=10-7Œª+Œª^2$.\n",
        "$Œª=\\/{7¬±\\sqrt{49-40}}{2}=(5,2)$.\n",
        "$\\v{A}-5\\v{I}=\\BM-1&2\\\\1&-2\\EM‚Üí\\BM1&-2\\\\0&0\\EM$.\n",
        "$E_5=\\span(\\BM-2\\\\-1\\EM)$.\n",
        "$\\v{A}-2\\v{I}=\\BM2&2\\\\1&1\\EM‚Üí\\BM1&1\\\\0&0\\EM$.\n",
        "$E_2=\\span(\\BM1\\\\-1\\EM)$.\n",
        "\n",
        "- 4.8: $\\v{A}=\\BM3&2&2\\\\2&3&2\\\\2&2&3\\EM$. $\\det(\\v{A}-Œª\\v{I})=\\det\\BM3-Œª&2&2\\\\2&3-Œª&2\\\\2&2&3-Œª\\EM$\n",
        "$=\\det\\BM7-Œª&7-Œª&7-Œª\\\\2&3-Œª&2\\\\2&2&3-Œª\\EM$\n",
        "$=(7-Œª)\\det\\BM1&1&1\\\\2&3-Œª&2\\\\2&2&3-Œª\\EM$\n",
        "$=(7-Œª)\\det\\BM1&0&0\\\\2&1-Œª&0\\\\2&0&1-Œª\\EM$\n",
        "$=(7-Œª)(1-Œª)^2$.\n",
        "$\\v{A}-7\\v{I}=\\BM-4&2&2\\\\2&-4&2\\\\2&2&-4\\EM‚Üí\\BM1&0&-1\\\\0&1&-1\\\\0&0&0\\EM$.\n",
        "$E_7=\\span(\\BM1\\\\1\\\\1\\EM)$.\n",
        "$\\v{A}-\\v{I}=\\BM2&2&2\\\\2&2&2\\\\2&2&2\\EM‚Üí\\BM1&1&1\\\\0&0&0\\\\0&0&0\\EM$.\n",
        "$E_1=\\span(\\BM1\\\\-1\\\\0\\EM,\\BM1\\\\0\\\\-1\\EM)$.\n",
        "By Gram-Schmidt,\n",
        "$\\v{u}_1=\\BM1\\\\1\\\\1\\EM$, $\\v{u}_2=\\BM1\\\\-1\\\\0\\EM$.\n",
        "$œÄ_{\\v{u}_2}(\\BM1\\\\0\\\\-1\\EM)=\\/{\\v{uu}^‚ä§}{\\vn{u}^2}\\BM1\\\\0\\\\-1\\EM=\\/{1}{2}\\BM1&-1&0\\\\-1&1&0\\\\0&0&0\\EM\\BM1\\\\0\\\\-1\\EM=\\BM1/2\\\\-1/2\\\\0\\EM$.\n",
        "$\\v{u}_3=\\BM1\\\\0\\\\-1\\EM-œÄ_{\\v{u}_2}(\\BM1\\\\0\\\\-1\\EM)$\n",
        "$=\\BM1/2\\\\1/2\\\\-1\\EM$."
      ],
      "metadata": {
        "id": "ZubxYH7_f1Ox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Non-defective matrix**: $\\v{A}‚àà‚Ñù^{n√ón}$ is called **defective** if it has $< n$ linearly independent eigenvectors. A non-defective matrix does not require $n$ distinct eigenvalues if geometric multiplicity is equal to algebraic multiplicity.\n",
        "\n",
        "- Eigenvectors $\\v{x}_1,...,\\v{x}_n$ of $\\v{A}‚àà‚Ñù^{n√ón}$ with distinct $Œª_1,...,Œª_n$ are linearly independent.\n",
        "\n",
        "  - Proof: Given distinct $Œª_1,Œª_2$, then the definition of linearly independent says $c_1\\v{x}_1+c_2\\v{x}_2=\\v{0}‚áíc_1=c_2=0$.\n",
        "\n",
        "  - **Base case**: $c_1\\v{x}_1=\\v{0}‚áíc_1=0$ because $\\v{x}_i\\neq\\v{0}$.\n",
        "\n",
        "  - **Hypothesis $k$**: assume when given $Œª_1,...,Œª_k$ all distinct, $c_1\\v{x}_1+...+c_k\\v{x}_k=\\v{0}‚áíc_1=...=c_k=0$.\n",
        "\n",
        "  - **Prove $k+1$**: $\\BC\n",
        "  \\t{1. need to show}&c_1\\v{x}_1+...+c_{k+1}\\v{x}_{k+1}=\\v{0}‚áíc_1=...=c_{k}=c_{k+1}=0\\\\\n",
        "  \\t{2. $\\v{A}‚ãÖ(1)$}&c_1\\v{Ax}_1...+c_{k+1}\\v{Ax}_{k+1}=c_1Œª_1\\v{x}_1...+\\green{c_{k+1}Œª_{k+1}\\v{x}_{k+1}}=\\v{0}\\\\\n",
        "  \\t{3. $Œª_{k+1}‚ãÖ(1)$}&c_1Œª_{k+1}\\v{x}_1...+\\green{c_{k+1}Œª_{k+1}\\v{x}_{k+1}}=\\v{0}\\\\\n",
        "  \\t{4. $(3)-(2)$}&c_1(Œª_{k+1}-Œª1)\\v{x}_1...+c_k(Œª_{k+1}-Œª_k)\\v{x}_k=\\v{0}‚áíc_1=...=c_{k}=0\\\\\n",
        "  \\t{5. back to $(1)$}&0\\v{x}_1+...+0\\v{x}_k+c_{k+1}\\v{x}_{k+1}=\\v{0}‚áíc_{k+1}=0\n",
        "  \\EC$\n",
        "\n",
        "- Given $\\v{A}‚àà‚Ñù^{m√ón}$ **Gram matrix** $\\v{S}:=\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$. Then $\\v{S}$ is symmetric and $\\blue{\\rk(\\v{A})=\\rk(\\v{S})}$.\n",
        "\n",
        "  - Proof (symmetric): $\\v{S}^‚ä§=(\\v{A}^‚ä§\\v{A})^‚ä§=(\\v{A})^‚ä§(\\v{A}^‚ä§)^‚ä§=\\v{A}^‚ä§\\v{A}=\\v{S}$\n",
        "\n",
        "  - Proof: (1) $\\v{Ax}=\\v{0}‚áí\\v{A}^‚ä§\\v{Ax}=\\v{0}$.\n",
        "  (2) Assume $\\v{A}^‚ä§\\v{Ax}=\\v{0}$.\n",
        "  Then $\\v{x}^‚ä§\\v{A}^‚ä§\\v{Ax}=0$\n",
        "  $‚áí(\\v{Ax})^‚ä§\\v{Ax}=0$\n",
        "  $‚áí\\vn{Ax}_2^2=0$\n",
        "  $‚áí\\v{Ax}=\\v{0}$.\n",
        "  Therefore $\\v{Ax}=\\v{0}‚áî\\v{Sx}=\\v{0}$ and $\\null(\\v{A})=\\null(\\v{S})$.\n",
        "\n",
        "- Gram matrix $\\v{S}=\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$ is positive-semidefinite. **$\\rk(\\v{S})=n‚áî\\v{S}$ is positive-definite $‚áî\\v{S}$ invertible**.\n",
        "\n",
        "  - Proof (positive-semidefinite): $\\v{x}^‚ä§\\v{Sx}=\\v{x}^‚ä§\\v{A}^‚ä§\\v{A}\\v{x}=(\\v{Ax})^‚ä§\\v{Ax}=\\vn{Ax}^2‚â•0$.\n",
        "\n",
        "  - Proof (positive-definite): $\\v{x}^‚ä§\\v{Sx}=\\vn{Ax}^2=0‚áî\\v{Ax}=\\v{0}$. If $\\rk(\\v{A})=n$ then $\\v{Ax}=\\v{0}‚áî\\v{x}=\\v{0}$.\n",
        "\n",
        "- Symmetric matrix **positive-definite matrix ‚áî positive, real eigenvalues**.\n",
        "\n",
        "  - Proof (symmetric ‚áî real eigenvalues): If $z=a+bi$ then conjugate $\\bar{z}=a-bi$ and $|z|^2=z‚ãÖ\\bar{z}$.\n",
        "  Then $‚ü®\\v{x},\\v{x}‚ü©=\\bar{\\v{x}}^‚ä§\\v{x}=:\\v{x}^*\\v{x}$ where $\\v{x}^*$ is called conjugate transpose.\n",
        "  We compare $Œª$ vs $\\bar{Œª}$ conjugate pair:\n",
        "  $\\v{x}^*\\v{Sx}=\\v{x}^*Œª\\v{x}=Œª\\vn{x}^2$.\n",
        "  $(\\v{x}^*\\v{Sx})^*=\\green{\\bar{Œª}}\\vn{x}^2=(\\v{x})^*(\\v{S})^*(\\v{x}^*)^*=\\v{x}^*\\green{\\v{S}^‚ä§}\\v{x}=\\v{x}^*\\green{\\v{S}}\\v{x}=\\green{Œª}\\vn{x}^2$\n",
        "  $‚áíŒª=\\bar{Œª}‚àà‚Ñù$\n",
        "  \n",
        "  - Proof (positive-definite ‚áî positive eigenvalues): $\\v{Sx}=Œª\\v{x}‚áí\\green{\\v{x}^‚ä§\\v{Sx}=Œª\\vn{x}^2}‚áíŒª=\\/{\\v{x}^‚ä§\\v{Sx}}{\\vn{x}^2}$\n",
        "  $‚áí\\blue{\\BC Œª>0&\\t{if $\\v{S}$ is positive-definite}\\\\\n",
        "  Œª‚â•0&\\t{if $\\v{S}$ is positive-semidefinite}\n",
        "  \\EC}$\n",
        "\n",
        "- Gram matrix $\\v{S}=\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$ has number of nonzero eigenvalues of $\\v{S}$ equal to $\\rk(\\v{S})$.\n",
        "\n",
        "  - Proof: $\\rk(\\v{S})=\\rk(\\v{D})$ where $\\v{S}=\\v{PDP}^{-1}$. Proof below.\n",
        "\n",
        "- Given $\\v{S}=\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$. Then $\\v{T}:=\\v{S}+c\\v{I}_n$ where $c>0$ is guaranteed to be positive-definite and full rank.\n",
        "\n",
        "  - Proof: Eigenvalues of $\\v{T}$ are $Œª_i+c>0$ for each eigenvalue $Œª_i$ of $\\v{S}$.\n",
        "\n",
        "- **Spectral theorem**: If $\\v{A}‚àà‚Ñù^{n√ón}$ is symmetric, then it is non-defective and there exists an orthonormal basis consisting of eigenvectors of $\\v{A}$, and each eigenvalue is real.\n",
        "\n",
        "  - Proof (symmetric ‚áí orthogonal eigenvectors): Let $Œª_1\\neqŒª_2$. Then $\\v{Ax}_1=Œª_1\\v{x}_1$ and $\\v{Ax}_2=Œª_2\\v{x}_2$.\n",
        "  $\\BC\n",
        "  (\\v{Ax}_1)^‚ä§\\v{x}_2=\\v{x}_1^‚ä§\\v{A}^‚ä§\\v{x}_2=\\v{x}_1^‚ä§\\v{Ax}_2=\\v{x}_1^‚ä§Œª_2\\v{x}_2=Œª_2\\v{x}_1^‚ä§\\v{x}_2\\\\\n",
        "  (\\v{Ax}_1)^‚ä§\\v{x}_2=Œª_1\\v{x}_1^‚ä§\\v{x}_2\n",
        "  \\EC$\n",
        "  $‚áíŒª_1\\v{x}_1^‚ä§\\v{x}_2=Œª_2\\v{x}_1^‚ä§\\v{x}_2‚áí\\v{x}_1^‚ä§\\v{x}_2=0$.\n",
        "\n",
        "  - **Schur's proof** (symmetric ‚áí non-defective): **Base case**: $\\v{A}‚àà‚Ñù^{1√ó1}$ is non-defective.\n",
        "\n",
        "  - **Hypothesis $n-1$**: Assume symmetric $\\v{A}‚àà‚Ñù^{(n-1)√ó(n-1)}$ has $(n-1)$ orthogonal eigenvectors.\n",
        "\n",
        "  - **Prove $n$**: $\\BC\n",
        "  \\t{Pick $\\v{Ax}_1=Œª_1\\v{x}_1$:}& W=\\span(\\v{x}_1)\\\\\n",
        "  \\t{Pick $\\v{w}‚ààW^\\perp$:}&(\\v{Aw})^‚ä§\\v{x}_1=\\v{w}^‚ä§\\v{A}^‚ä§\\v{x}_1=\\v{w}^‚ä§\\v{A}\\v{x}_1=Œª_1\\v{w}^‚ä§\\v{x}_1=0‚áí(\\v{Aw})‚ààW^\\perp \\\\\n",
        "  \\t{Find $\\tilde{\\v{A}}=\\v{A}_{W^\\perp}$:}& \\t{ONB }\\v{B}=\\BM\\v{x}_1&\\v{u}_2&...&\\v{u}_n\\EM\\small‚áí\\v{B}^{-1}(\\v{AB})=\\BM\\v{x}_1^‚ä§\\\\\\v{u}_2^‚ä§\\\\\\vdots\\\\\\v{u}_n^‚ä§\\EM\\BMŒª_1\\v{x}_1&\\v{Au}_2&...&\\v{Au}_n\\EM=\\BMŒª_1&\\v{0}^‚ä§\\\\\\v{0}&\\tilde{\\v{A}}\\EM\n",
        "  \\EC$.\n",
        "  $\\tilde{\\v{A}}\\v{w}=\\v{Aw}\\ ‚àÄ\\v{w}‚ààW^\\perp$ is separated from $\\v{x}_1$ by the $\\v{B}^{-1}\\v{AB}$ basis change.\n",
        "  Because $\\v{B}^{-1}\\v{AB}$ is symmetric then $\\tilde{\\v{A}}‚àà‚Ñù^{(n-1)√ó(n-1)}$ is also symmetric and by hypothesis has $n-1$ orthogonal eigenvectors."
      ],
      "metadata": {
        "id": "zqq5Gh74nKdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eigendecomposition**: Matrix $\\v{A}‚àà‚Ñù^{n√ón}$ is diagonalizable if it is similar to a diagonal matrix $\\v{D}$: there exists $\\v{P}‚àà‚Ñù^{n√ón}$ such that $\\red{\\v{A}=\\v{PDP}^{-1}}$. Non-defective matrix $\\v{A}‚àà‚Ñù^{n√ón}$ can be diagonlized into $\\v{A}=\\v{PDP}^‚ä§$ where $\\v{P}‚àà‚Ñù^{n√ón}$ is the orthonormal basis consisting of eigenvectors of $\\v{A}$ and diagonals of $\\v{D}$ are eigenvalues of $\\v{A}$.\n",
        "\n",
        "- $\\v{AP}=\\v{A}\\BM\\v{p}_1&...&\\v{p}_n\\EM=\\BM\\v{Ap}_1&...&\\v{Ap}_n\\EM$,\n",
        "$\\v{PD}=\\BM\\v{p}_1&...&\\v{p}_n\\EM\\BMŒª_1&...&0\\\\\\vdots&\\ddots&\\vdots\\\\0&...&Œª_n\\EM=\\BMŒª_1\\v{p}_1&...&Œª_n\\v{p}_n\\EM$. Then\n",
        "$\\v{AP}=\\v{PD}‚áí\\v{Ap}_i=Œª_i\\v{p}_i$.\n",
        "\n",
        "- Spectral theorem says symmetric matrix $\\v{S}‚àà‚Ñù^{n√ón}$ is similar to a diagonal matrix of real eigenvalues.\n",
        "\n",
        "- Geometric pipeline: $\\v{A}=\\v{PDP}^{-1}$ stretches and rotates space in the standard grid. A standard coordinate is taken in, rebased along eigenbasis to another coordinate (by rotation or reflection), stretched along eigenvectors by eigenvalues, and rebased back to standard coordinate as output.\n",
        "\n",
        "- $\\red{\\v{A}^k=\\v{PD}^k\\v{P}^‚ä§}$.\n",
        "\n",
        "- $\\red{\\det(\\v{A})=\\det(\\v{D})}$.\n",
        "\n",
        "- $\\red{\\rk(\\v{A})=\\rk(\\v{D})}$. Diagonalizable rank-$r$ matrices have $r$ non-zero eigenvalues.\n",
        "\n",
        "  - Proof: $\\rk(\\v{XY})‚â§\\min(\\rk(\\v{X}),\\rk(\\v{Y}))$. Matrix rank is invariant to multiplication with invertible matrix $\\v{P}$ and $\\v{P}^{-1}$\n",
        "  $\\BC\\small\n",
        "  \\rk(\\v{PD})‚â§\\min(\\rk(\\v{P}),\\rk(\\v{D})) & \\rk(\\v{D})‚â§\\min(\\rk(\\v{P}^{-1}),\\rk(\\v{PD}))&\\rk(\\v{PD})=\\rk(\\v{D}) \\\\\n",
        "  \\rk(\\v{DP}^{-1})‚â§\\min(\\rk(\\v{D}),\\rk(\\v{P}^{-1})) & \\rk(\\v{D})‚â§\\min(\\rk(\\v{DP}^{-1}),\\rk(\\v{P}))&\\rk(\\v{DP}^{-1})=\\rk(\\v{D})\n",
        "  \\EC$.  \n",
        "  $\\v{S}=\\v{PDP}^{-1}‚áí\\rk(\\v{D})=\\rk(\\v{S})$. The rank of a diagonal matrix is the number of non-zero diagonal entries.\n",
        "\n",
        "  - Symmetric positive-semidefinite rank-$r$ matrices have $r$ positive eigenvalues. The rest are zero.\n",
        "\n",
        "- 4.11: $\\v{A}=\\/{1}{2}\\BM5&-2\\\\-2&5\\EM$.\n",
        "$\\det(\\v{A}-Œª\\v{I})=\\/{1}{4}\\det(\\BM5-2Œª&-2\\\\-2&5-2Œª\\EM)$\n",
        "$=\\/{1}{4}\\det\\BM3-2Œª&3-2Œª\\\\-2&5-2Œª\\EM$\n",
        "$=\\/{1}{4}\\det\\BM3-2Œª&0\\\\-2&7-2Œª\\EM$\n",
        "$=(3/2-Œª)(7/2-Œª)$.\n",
        "$\\v{A}-\\/{3}{2}\\v{I}=\\/{1}{2}\\BM2&-2\\\\-2&2\\EM‚Üí\\BM1&-1\\\\0&0\\EM$\n",
        "$‚áíE_{3/2}=\\span(\\BM1\\\\1\\EM)$.\n",
        "$\\v{A}-\\/{7}{2}\\v{I}=\\/{1}{2}\\BM-2&-2\\\\-2&-2\\EM‚Üí\\BM1&1\\\\0&0\\EM$\n",
        "$‚áíE_{7/2}=\\span(\\BM1\\\\-1\\EM)$.\n",
        "Therefore $\\v{P}=\\/{1}{\\sqrt{2}}\\BM1&1\\\\1&-1\\EM$, $\\v{D}=\\/{1}{2}\\BM3&0\\\\0&7\\EM$, and\n",
        "$\\v{PDP}^‚ä§=\\/{1}{4}\\BM3&7\\\\3&-7\\EM\\BM1&1\\\\1&-1\\EM$\n",
        "$=\\/{1}{4}\\BM10&-4\\\\-4&10\\EM=\\/{1}{2}\\BM5&-2\\\\-2&5\\EM=\\v{A}$"
      ],
      "metadata": {
        "id": "0nvT89S1sKvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More on determinants and traces**: Let $\\v{A}$ be symmetrical.\n",
        "\n",
        "- $\\red{\\det(\\v{A})=\\prod\\limits_{i=1}^nŒª_i}$. Determinant is a product of eigenvalues.\n",
        "\n",
        "  - Proof: $\\det(\\v{A}-Œª\\v{I})=\\prod_i(Œª_i-Œª)$\n",
        "  $‚áí\\det(\\v{A})=\\prod_iŒª_i$.\n",
        "\n",
        "- $\\red{\\tr(\\v{A})=\\suml_{i=1}^nŒª_i}$. Trace is a sum of eigenvalues.\n",
        "\n",
        "  - Proof: $\\det(\\v{A}-Œª\\v{I})=(-1)^n\\prod_i(Œª-Œª_i)$. Then $\\BC\n",
        "  \\t{summing polynomial coefficients}& c_{n-1}=(-1)^n\\sum_i(-Œª_i)\n",
        "  =(-1)^{n+1}\\sum_iŒª_i\\\\\n",
        "  \\t{multiplying diagonal of $\\v{A}-Œª\\v{I}$}& c_{n-1}=(-1)^{n-1}\\sum_ia_{ii}\n",
        "  \\EC$\n",
        "\n",
        "- $\\ln(\\det(\\v{A}))=\\tr(\\ln(\\v{A}))$\n",
        "\n",
        "  - Proof: $\\ln(\\det(\\v{A}))=\\ln(\\prod_iŒª_i)=\\sum_i\\ln(Œª_i)=\\tr(\\ln(\\v{A}))$\n",
        "\n",
        "- \"Spectral\" and \"spectrum\" in the context of symmetrical matrices always refer to eigenvalues.\n",
        "\n",
        "- Quick 2√ó2 eigenvalues: Let $m=\\tr(\\v{A})$ and $p=\\det(\\v{A})$. Then $Œª_{1,2}=m¬±\\sqrt{m^2-p}$.\n",
        "\n",
        "- **Matrix factoring**: $\\BC\n",
        "\\v{X}+t\\v{V}=\\v{X}(I+t\\v{X}^{-1}\\v{V})&\\v{X}^{-1}\\v{V}\\t{ is usually not symmetric}\\\\\n",
        "\\v{X}+t\\v{V}=\\v{X}^{1/2}(I+t\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2})\\v{X}^{1/2}&\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2}\\t{ symmetry guaranteed}\\\\\n",
        "\\EC$\n",
        "\n",
        "  - $\\v{X}^{-1}\\v{V}=\\v{X}^{-1/2}(\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2})\\v{X}^{1/2}$. Therefore $\\v{X}^{-1}\\v{V}$ and $\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2}$ are similar matrices and have the same eigenvalues.\n",
        "\n",
        "  - $\\det(\\v{X}^{1/2}(I+t\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2})\\v{X}^{1/2})=\\det(\\v{X})\\det(I+t\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2})=\\det(\\v{X})\\prod_{i=1}^n(1+tŒª_i)$ where $Œª_i‚àà‚Ñù$ are eigenvalues of symmetric $\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2}$.\n",
        "\n",
        "  - $\\v{X}^{1/2}$ is in practice calculated as Cholesky factors $\\v{LL}^‚ä§=\\v{X}$.\n",
        "\n",
        "- $\\v{u}$ is an eigenvector of outer product $\\v{uv}^‚ä§$ with eigenvalue $\\v{v}^‚ä§\\v{u}$. The other $(n-1)$ eigenvalues are 0.\n",
        "\n",
        "  - Proof: $\\v{u}(\\v{v}^‚ä§\\v{u})=(\\v{v}^‚ä§\\v{u})\\v{u}$ as $\\v{v}^‚ä§\\v{u}$ is scalar.\n",
        "  Because $\\rk(\\v{uv}^‚ä§)=1$ as outer product, $\\null(\\v{uv}^‚ä§)=n-1$, which means there are $(n-1)$ 0 eigenvalues corresponding to eigenvectors $\\v{x}$ such that $\\v{v}^‚ä§\\v{x}=0$.\n",
        "\n",
        "- **Matrix determinant lemma**: $\\det(\\v{A}+\\v{uv}^‚ä§)=\\det(\\v{A})(1+\\v{v}^‚ä§\\v{A}^{-1}\\v{u})$\n",
        "\n",
        "  - Proof: $\\v{A}+\\v{uv}^‚ä§=\\v{A}(\\v{I}+\\v{A}^{-1}\\v{uv}^‚ä§)$\n",
        "  $=\\v{A}(\\v{I}+\\v{u}'\\v{v}^‚ä§)$ where $\\v{u}'=\\v{A}^{-1}\\v{u}$.\n",
        "  Eigenvalues of $\\v{I}+\\v{u}'\\v{v}^‚ä§$ are $1$ and $1+\\v{v}^‚ä§\\v{u}'$.\n",
        "  Therefore $\\det(\\v{I}+\\v{u}'\\v{v}^‚ä§)=1+\\v{v}^‚ä§\\v{A}^{-1}\\v{u}$.\n",
        "\n",
        "  - $\\det(\\v{I}+t\\v{uu}^‚ä§)=1+t\\vn{u}_2^2$.\n",
        "\n",
        "- Any positive-semidefinite $\\v{Z}‚àà‚Ñù^{n√ón}$ can be decomposed into $\\v{Z}=\\sum_{i=1}^n\\v{v}_i\\v{v}_i^‚ä§$ where $\\v{v}_i‚àà‚Ñù^n$.\n",
        "\n",
        "  - Proof: $\\v{Z}$ is positive-semidefinite if diagonalizable with eigenvalues $Œª_i‚â•0\\ ‚àÄi=1,...,n$.\n",
        "  $\\v{Z}=\\v{PDP}^‚ä§=\\sum_{i=1}^nŒª_i\\v{p}_i\\v{p}_i^‚ä§$\n",
        "  $=\\sum_{i=1}^n\\v{v}_i\\v{v}_i^‚ä§$\n",
        "  where $\\v{v}_i=\\sqrt{Œª_i}\\v{p}_i$\n",
        "\n",
        "- **Rayleigh quotient**: Given $\\v{A}‚àà\\v{S}^n$ and nonzero $\\v{x}$, Rayleigh quotient $\\red{R(\\v{A},\\v{x})=\\/{\\v{x}^‚ä§\\v{Ax}}{\\v{x}^‚ä§\\v{x}}}$ is the normalized quadratic form satisfying **Rayleigh-Ritz theorem** $\\blue{Œª_\\min(\\v{A})‚â§\\/{\\v{x}^‚ä§\\v{Ax}}{\\v{x}^‚ä§\\v{x}}‚â§Œª_\\max(\\v{A})}$.\n",
        "I.e., $\\BC\\t{minimize}&\\v{x}^‚ä§\\v{Ax}\\\\\n",
        "\\t{subject to}&\\vn{x}_2^2=1\n",
        "\\EC$ $‚áíp^*=Œª_\\min(\\v{A})$"
      ],
      "metadata": {
        "id": "X7sLUKmpq-Iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single Value Decomposition**: Matrix $\\v{A}‚àà‚Ñù^{m√ón}$ of rank $r‚àà[0,\\min(m,n)]$. Then $\\red{\\v{A}=\\v{UŒ£V}^‚ä§}$ where $\\v{U}‚àà‚Ñù^{m√óm},\\v{V}‚àà‚Ñù^{n√ón}$ are orthogonal matrices, and $\\v{Œ£}‚àà‚Ñù^{m√ón}$ is a truncated diagonal matrix such that $Œ£_{ii}=œÉ_i‚â•0$ and $Œ£_{ij}=0$ off diagonal. $\\blue{œÉ_1‚â•...‚â•œÉ_r>0}$ are called singular values, and $\\v{u}_i,\\v{v}_j$ are called left/right singular vectors.\n",
        "\n",
        "- $\\v{A}$ represents $Œ¶:‚Ñù^n‚Üí‚Ñù^m$. A $n$-dimensional coordinate is taken in standard basis, rebased along $\\v{V}$ through rotation or reflection, transformed by $œÉ_{i}$ rescaling or dimension augmentation/reduction via $\\v{Œ£}$, and then released as a $m$-dimensional coordinate in $\\v{U}$ basis.\n",
        "\n",
        "- Let $(\\v{v}_1,...,\\v{v}_n)$ be the eigenbasis of $\\v{A}^‚ä§\\v{A}$, then $(\\v{Av}_1,...,\\v{Av}_n)$ preserves orthogonality.\n",
        "\n",
        "  - Proof: $\\v{S}=\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$ is symmetric positive-semidefinite and non-defective.\n",
        "  $‚ü®\\v{Av}_i,\\v{Av}_j‚ü©=(\\v{Av}_i)^‚ä§(\\v{Av}_j)$\n",
        "  $=\\v{v}_i^‚ä§(\\v{A}^‚ä§\\v{A}\\v{v}_j)$\n",
        "  $=\\v{v}_i^‚ä§(Œª_j\\v{v}_j)$\n",
        "  $=0$\n",
        "\n",
        "- $\\v{A}=\\v{UŒ£V}^‚ä§$ requires $\\red{\\v{Av}_i=œÉ_i\\v{u}_i}$ to hold true for $i=1..r$ where $r=\\rk(\\v{A})‚àà[0,\\min(m,n)]$.\n",
        "\n",
        "  - Example: $\\v{A}‚àà‚Ñù^{3√ó2}$. Then $\\v{AV}=\\v{UŒ£}$\n",
        "  $‚áí\\BM a_{11}&a_{12}\\\\a_{21}&a_{22}\\\\a_{31}&a_{32}\\EM\\BM v_{11}&v_{12}\\\\v_{21}&v_{22}\\EM=\\BM u_{11}&u_{12}&u_{13}\\\\u_{21}&u_{22}&u_{23}\\\\u_{31}&u_{32}&u_{33}\\EM\\BMœÉ_1&0\\\\0&œÉ_2\\\\0&0\\EM$\n",
        "  $‚áí\\BM a_{11}v_{11}+a_{12}v_{21}&a_{11}v_{12}+a_{12}v_{22}\\\\a_{21}v_{11}+a_{22}v_{21}&a_{21}v_{12}+a_{22}v_{22}\\\\a_{31}v_{11}+a_{32}v_{21}&a_{31}v_{12}+a_{32}v_{22}\\EM=\\BM œÉ_1u_{11}&œÉ_2u_{12}\\\\œÉ_1u_{21}&œÉ_2u_{22}\\\\œÉ_1u_{31}&œÉ_2u_{32}\\EM$\n",
        "  $‚áí\\BM \\v{Av}_1&\\v{Av}_2\\EM=\\BM œÉ_1\\v{u}_1&œÉ_2\\v{u}_2\\EM$.\n",
        "\n",
        "- $\\v{A}^‚ä§\\v{A}$ and $\\v{AA}^‚ä§$ have the same non-zero eigenvalues.\n",
        "\n",
        "  - Proof: $(\\v{A}^‚ä§\\v{A})\\v{x}=Œª\\v{x}‚áí\\v{A}(\\v{A}^‚ä§\\v{A}\\v{x})=\\v{A}(Œª\\v{x})‚áí(\\v{AA}^‚ä§)(\\v{Ax})=Œª(\\v{Ax})‚áí(\\v{AA}^‚ä§)\\v{y}=Œª\\v{y}$\n",
        "\n",
        "- The eigenvectors associated with 0-eigenvalues of $\\v{AA}^‚ä§$ are the basis vectors for the left null space of $\\v{A}$.\n",
        "\n",
        "  - Proof: Let $\\v{x}$ be an eigenvector for 0-eigenvalue of $\\v{AA}^‚ä§$, then $\\v{AA}^‚ä§\\v{x}=0\\v{x}=\\v{0}$\n",
        "  $‚áí\\v{x}^‚ä§\\v{AA}^‚ä§\\v{x}=(\\v{A}^‚ä§\\v{x})^‚ä§(\\v{A}^‚ä§\\v{x})$\n",
        "  $=\\n{\\v{A}^‚ä§\\v{x}}^2=0$\n",
        "  $‚áí\\v{A}^‚ä§\\v{x}=\\v{0}$\n",
        "  $‚áí\\v{x}‚àà\\null(\\v{A}^‚ä§)$.\n",
        "\n",
        "- **Full SVD**: Right singular vectors $\\v{V}$ are the orthonormal eigenbasis of $\\v{A}^‚ä§\\v{A}$. The singular values $œÉ_i=\\sqrt{Œª_i}$ are square roots of eigenvalues of $\\v{A}^‚ä§\\v{A}$. Left singular vector $\\v{u}_1,...,\\v{u}_r$ are obtained by $\\red{\\v{u}_i=\\/{1}{œÉ}\\v{Av}_i}$. The leftovers are filled with orthonormal basis of $\\null(\\v{A}^‚ä§)$.\n",
        "\n",
        "  - Proof ($\\v{V},\\v{Œ£}$): Symmetric non-defective $\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$. Then $\\v{A}^‚ä§\\v{A}=\\green{\\v{P}\\BMŒª_1&...&0\\\\\\vdots&\\ddots&\\vdots\\\\0&...&Œª_n\\EM\\v{P}^‚ä§}$ by eigendecomposition.\n",
        "  $\\v{A}^‚ä§\\v{A}=(\\v{UŒ£V}^‚ä§)^‚ä§(\\v{UŒ£V}^‚ä§)$\n",
        "  $=(\\v{V}^‚ä§)^‚ä§\\v{Œ£}^‚ä§(\\v{U}^‚ä§\\v{U})\\v{Œ£V}^‚ä§$\n",
        "  $=\\v{VŒ£}^‚ä§\\v{Œ£V}^‚ä§$\n",
        "  $=\\green{\\v{V}\\BMœÉ_1^2&...&0\\\\\\vdots&\\ddots&\\vdots\\\\0&...&œÉ_n^2\\EM\\v{V}^‚ä§}$. Because $\\v{A}^‚ä§\\v{A}$ is positive-semidefinite, only $Œª_1,...,Œª_r$ (therefore $œÉ_1,...,œÉ_r$) are non-zero.\n",
        "\n",
        "  - Proof ($\\v{U}$):\n",
        "  Symmetric non-defective $\\v{AA}^‚ä§‚àà‚Ñù^{m√óm}$.\n",
        "  $\\v{AA}^‚ä§=(\\v{UŒ£V}^‚ä§)(\\v{UŒ£V}^‚ä§)^‚ä§$\n",
        "  $=\\v{UŒ£V}^‚ä§\\v{VŒ£}^‚ä§\\v{U}^‚ä§$\n",
        "  $=\\v{UŒ£Œ£}^‚ä§\\v{U}^‚ä§$\n",
        "  $=\\green{\\v{U}\\BMœÉ_1^2&...&0\\\\\\vdots&\\ddots&\\vdots\\\\0&...&œÉ_m^2\\EM\\v{U}^‚ä§}$, again only $œÉ_1,...,œÉ_r$ are non-zero.\n",
        "  While $\\v{U}$ is the orthonormal eigenbasis of $\\v{AA}^‚ä§$, it cannot be computed that way independently from $\\v{V}$ because the $¬±$ signs of $\\v{u}_i$ and $\\v{v}_i$ are constrained by the requirement $\\v{Av}_i=œÉ_i\\v{u}_i$ for $i=1,...,r$.\n",
        "  Therefore the first $r$ columns are computed by $\\v{u}_i=\\/{1}{œÉ_i}\\v{Av}_i$ for $œÉ_1,...,œÉ_r$. The remaining $m-r$ columns correspond to zero eigenvalues of $\\v{AA}^‚ä§$, and are filled with those orthonormal eigenvectors, or equivalently with orthonormal basis of $\\null(\\v{A}^‚ä§)$.\n",
        "\n",
        "- **Reduced SVD**: $\\v{A}‚àà‚Ñù^{m√ón}$, then $\\v{U}‚àà‚Ñù^{m√ón}$, $\\v{Œ£}‚àà‚Ñù^{n√ón}$ is a diagonal matrix, and $\\v{V}‚àà‚Ñù^{n√ón}$. There is no need to fill $\\v{U}$ leftover columns with $\\null(\\v{A}^‚ä§)$.\n",
        "\n",
        "- **Reduced rank SVD**: $\\v{A}‚àà‚Ñù^{m√ón}$ is rank-$r$. Then $\\v{U}_r‚àà‚Ñù^{m√ór}$, $\\v{Œ£}_r‚àà‚Ñù^{r√ór}$ is non-zero diagonal, $\\v{V}_r‚àà‚Ñù^{n√ór}$ only takes eigenvectors of non-zero eigenvalues. $\\red{\\v{A}=\\v{U}_r\\v{Œ£}_r\\v{V}_r^‚ä§}=\\suml_{i=1}^rœÉ_i\\v{u}_i\\v{v}_i^‚ä§=\\suml_{i=1}^rœÉ_i\\v{A}_i$ where $\\v{A}_i=\\v{u}_i\\v{v}_i^‚ä§$.\n",
        "\n",
        "**$\\v{A}=\\v{PDP}^{-1}$ vs $\\v{A}=\\v{UŒ£V}^‚ä§$**:\n",
        "  \n",
        "- For symmetric matrices, SVD and eigendecomposition are the same.\n",
        "\n",
        "- SVD always exists for any matrix $‚Ñù^{m√ón}$. Eigendecomposition only exists for non-defective square matrices.\n",
        "\n",
        "- $\\v{P}$ is not necessarily orthogonal, but $\\v{U},\\v{V}$ are always orthonormal.\n",
        "\n",
        "- Both are a pipeline of 3 operations: change coordinates to new basis in domain, independent scaling along new basis, and change basis again. Key difference is SVD domain/codomain can be vector spaces of different dimensions.\n",
        "\n",
        "- $\\v{V}$ are eigenvectors of $\\v{A}^‚ä§\\v{A}$. $\\v{U}$ are eigenvectors of $\\v{AA}^‚ä§$. $\\v{Œ£}$ non-zero diagonals are square roots of non-zero eigenvalues of both $\\v{AA}^‚ä§$ and $\\v{A}^‚ä§\\v{A}$.\n",
        "\n",
        "  - $\\v{Œ£}$ diagonal are all real and non-negative. That is not generally true for $\\v{D}$.\n",
        "\n",
        "- 4.12: $\\v{A}=\\BM1&-0.8\\\\0&1\\\\1&0\\EM$.\n",
        "Then $\\v{S}=\\v{A}^‚ä§\\v{A}=\\BM1&0&1\\\\-0.8&1&0\\EM\\BM1&-0.8\\\\0&1\\\\1&0\\EM=\\BM2&-0.8\\\\-0.8&1.64\\EM$.\n",
        "$\\v{S}-Œª\\v{I}=\\BM2-Œª&-0.8\\\\-0.8&1.64-Œª\\EM$\n",
        "$=2.64-3.64Œª+Œª^2$\n",
        "$‚áíŒª=\\/{3.64¬±\\sqrt{3.64^2-4(2.64)}}{2}=1,2.64$.\n",
        "$\\BM2-1&-0.8\\\\-0.8&1.64-1\\EM‚Üí\\BM1&-0.8\\\\0&0\\EM$.\n",
        "$E_1=\\span(\\BM4\\\\5\\EM)$.\n",
        "$\\BM2-2.64&-0.8\\\\-0.8&1.64-2.64\\EM‚Üí\\BM1&5/4\\\\0&0\\EM$. $E_{2.64}=\\span(\\BM5\\\\-4\\EM)$.\n",
        "Therefore $\\v{Œ£}=\\BM\\sqrt{2.64}&0\\\\0&1\\\\0&0\\EM$,\n",
        "$\\v{V}=\\BM0.78&0.63\\\\-0.63&0.78\\EM$.\n",
        "$\\v{u}_1=\\/{1}{1.62}\\v{Av}_1=\\BM0.79\\\\-0.39\\\\0.48\\EM$.\n",
        "$\\v{u}_2=\\v{Av}_2=\\BM0\\\\0.78\\\\0.63\\EM$.\n",
        "$\\v{A}^‚ä§=\\BM1&0&1\\\\-0.8&1&0\\EM‚Üí\\BM1&0&1\\\\0&1&4/5\\EM$.\n",
        "$\\null(\\v{A}^‚ä§)=\\span(\\BM5\\\\4\\\\-5\\EM)$.\n",
        "$\\v{u}_3=\\BM5\\\\4\\\\-5\\EM-\\v{u}_1\\v{u}_1^‚ä§\\BM5\\\\4\\\\-5\\EM-\\v{u}_2\\v{u}_2^‚ä§\\BM5\\\\4\\\\-5\\EM=\\BM0.62\\\\0.49\\\\-0.61\\EM$.\n",
        "Therefore\n",
        "$\\v{A}=\\BM0.79&0&0.62\\\\-0.39&0.78&0.49\\\\0.48&0.63&-0.61\\EM\\BM\\sqrt{2.64}&0\\\\0&1\\\\0&0\\EM\\BM0.78&-0.63\\\\0.63&0.78\\EM$\n",
        "\n",
        "- 4.13: $\\v{A}=\\BM1&0&1\\\\-2&1&0\\EM$.\n",
        "$\\v{A}^‚ä§\\v{A}=\\BM5&-2&1\\\\-2&1&0\\\\1&0&1\\EM$ with eigenbasis $E_6=\\span(\\BM5\\\\-2\\\\1\\EM)$, $E_1=\\span(\\BM0\\\\1\\\\2\\EM)$, and $E_0=\\span(\\BM-1\\\\-2\\\\1\\EM)$.\n",
        "Therefore $\\v{Œ£}=\\BM\\sqrt{6}&0&0\\\\0&1&0\\EM$,\n",
        "$\\v{V}=\\BM5/\\sqrt{30}&0&-1\\sqrt{6}\\\\-2/\\sqrt{30}&1/\\sqrt{5}&-2/\\sqrt{6}\\\\1/\\sqrt{30}&2/\\sqrt{5}&1/\\sqrt{6}\\EM$.\n",
        "$\\v{u}_1=\\/{1}{\\sqrt{6}}\\v{Av}_1=\\BM1/\\sqrt{5}\\\\-2/\\sqrt{5}\\EM$.\n",
        "$\\v{u}_2=\\v{Av}_2=\\BM2/\\sqrt{5}\\\\1/\\sqrt{5}\\EM$.\n",
        "$\\v{U}=\\/{1}{\\sqrt{5}}\\BM1&2\\\\-2&1\\EM$."
      ],
      "metadata": {
        "id": "f92zhXBq8gLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix approximations**: Let matrix $\\v{A}‚àà‚Ñù^{m√ón}$ have rank-$r$. The **rank-$k$ approximation** is $\\red{\\hat{\\v{A}}(k):=\\suml_{i=1}^kœÉ_i\\v{A}_i}$ such that $\\rk(\\hat{\\v{A}})=k$, and rank-1 matrices $\\v{A}_i:=\\v{u}_i\\v{v}_i^‚ä§$ are outer-products.\n",
        "\n",
        "- **Spectral norm** $\\vn{A}_2:=\\max_\\v{x}\\/{\\vn{Ax}_2}{\\vn{x}_2}$ measures the maximum norm amplification multiple of a vector by $\\v{A}$.\n",
        "\n",
        "  - $\\v{A}=\\sum_{i=1}^rœÉ_i\\v{A}_i$, then $\\vn{A}_2=œÉ_1$ its largest singular value is the greatest scale factor.\n",
        "  \n",
        "  - $\\n{\\hat{\\v{A}}(1)}_2=\\vn{A}_2$\n",
        "\n",
        "- **Eckart-Young theorem**: Let $\\v{A}‚àà‚Ñù^{m√ón}$ have rank-$r$, and let $\\v{B}‚àà‚Ñù^{m√ón}$ have rank-$k$. For $k‚â§r$ with $\\hat{\\v{A}}=\\sum_{i=1}^kœÉ_i\\v{u}_i\\v{v}_i^‚ä§$, it holds that $\\blue{\\hat{\\v{A}}(k)=\\arg\\min_{\\rk(\\v{B})=k}\\n{\\v{A}-\\v{B}}_2}$: $\\hat{\\v{A}}(k)$ is the least spectral norm error rank-$k$ approximation. Furthermore, $\\blue{\\n{\\v{A}-\\hat{\\v{A}}(k)}_2=œÉ_{k+1}}$.\n",
        "\n",
        "  - Proof ($\\n{\\v{A}-\\hat{\\v{A}}(k)}_2=œÉ_{k+1}$): The error matrix $\\v{A}-\\hat{\\v{A}}(k)=\\sum_{i=k+1}^rœÉ_i\\v{u}_i\\v{v}_i^‚ä§$ is also a sum of lower rank matrices. Then spectral norm of the error $\\n{\\v{A}-\\hat{\\v{A}}}_2=œÉ_{k+1}$ is its largest singular value.\n",
        "\n",
        "  - Grassmann formula: Let $V,W‚äÜ‚Ñù^n$. Then $\\dim(V+W)=\\dim(V)+\\dim(W)-\\dim(V‚à©W)$. If $\\dim(V)+\\dim(W)>n$ then $\\dim(V‚à©W)>0$.\n",
        "\n",
        "  - Proof ($\\n{\\v{A}-\\v{B}}_2‚â•œÉ_{k+1}$): To prove this, we only need to find one unit vector $\\v{z}$ that meets the worst-case scenario $\\n{(\\v{A}-\\v{B})\\v{z}}‚â•œÉ_{k+1}$. Then $\\v{Bz}=\\v{0}$ and $\\vn{Az}‚â•œÉ_{k+1}$, and $\\v{z}‚àà\\null(\\v{B})‚à©\\span(\\v{v}_1,...,\\v{v}_{k+1})$. Because $\\dim(\\null(\\v{B}))+\\dim(\\span(\\v{v}_1,...,\\v{v}_{k+1}))=(n-k)+(k+1)=n+1>n$, their intersection is non-empty $\\null(\\v{B})‚à©\\span(\\v{v}_1,...,\\v{v}_{k+1})\\neq‚àÖ$. Therefore unit vector $\\v{z}=c_1\\v{v}_1+...+c_{k+1}\\v{v}_{k+1}$ exists and\n",
        "  $\\n{(\\v{A}-\\v{B})\\v{z}}=\\vn{Az}=\\sqrt{\\sum_{i=1}^{k+1}œÉ_i^2c_i^2\\n{\\v{v}_i}^2}‚â•œÉ_{k+1}^2\\sqrt{\\sum_{i=1}^{k+1}c_i^2\\n{\\v{v}_i}^2}=œÉ_{k+1}$"
      ],
      "metadata": {
        "id": "R-5e1_xuSDMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 4.1: $\\v{A}=\\BM1&3&5\\\\2&4&6\\\\0&2&4\\EM$.\n",
        "$\\det(\\v{A})=2-24+20=-2$.\n",
        "\n",
        "- 4.2: $\\BM2&0&1&2&0\\\\2&-1&0&1&1\\\\0&1&2&1&2\\\\-2&0&2&-1&2\\\\2&0&0&1&1\\EM\\arr{1}$$\\BM2&0&1&2&0\\\\0&1&1&1&-1\\\\0&1&2&1&2\\\\0&0&3&1&2\\\\0&0&1&1&-1\\EM$$‚Üí\\BM2&0&1&2&0\\\\0&1&1&1&-1\\\\0&0&1&0&3\\\\0&0&3&1&2\\\\0&0&1&1&-1\\EM$$\\arr{1}\\BM2&0&1&2&0\\\\0&1&1&1&-1\\\\0&0&1&0&3\\\\0&0&0&-1&7\\\\0&0&0&-1&4\\EM‚Üí$$\\BM2&0&1&2&0\\\\0&1&1&1&-1\\\\0&0&1&0&3\\\\0&0&0&-1&7\\\\0&0&0&0&-3\\EM$. $\\det(\\v{A})=6$.\n",
        "\n",
        "- 4.3: $\\v{A}=\\BM1&0\\\\1&1\\EM‚Üí\\BM1-Œª&0\\\\1&1-Œª\\EM$. $Œª=1‚Üí\\BM0&0\\\\1&0\\EM$. $E_1=\\span(\\BM0\\\\-1\\EM)$.\n",
        "$\\v{B}=\\BM-2&2\\\\2&1\\EM‚Üí\\BM-2-Œª&2\\\\2&1-Œª\\EM$.\n",
        "$(-2-Œª)(1-Œª)-4=-6+Œª+Œª^2‚ÜíŒª=\\/{-1¬±\\sqrt{1+24}}{2}$.\n",
        "$Œª=-3‚Üí\\BM1&2\\\\2&4\\EM$$‚Üí\\BM1&2\\\\0&0\\EM$$‚ÜíE_{-3}=\\span(\\BM2\\\\-1\\EM)$.\n",
        "$Œª=2‚Üí\\BM-4&2\\\\2&-1\\EM$$‚Üí\\BM1&-1/2\\\\0&0\\EM$$‚ÜíE_{2}=\\span(\\BM-1/2\\\\-1\\EM)$\n",
        "\n",
        "- 4.6: $\\v{A}=\\BM2&3&0\\\\1&4&3\\\\0&0&1\\EM$$‚Üí\\BM2-Œª&3&0\\\\1&4-Œª&3\\\\0&0&1-Œª\\EM$.\n",
        "$\\det(\\v{A})=(2-Œª)(4-Œª)(1-Œª)-3(1-Œª)=(1-Œª)(Œª^2-6Œª+5)$\n",
        "$Œª=\\/{6¬±\\sqrt{36-20}}{2}=1,5$.\n",
        "$Œª=1‚Üí\\BM1&3&0\\\\1&3&3\\\\0&0&0\\EM$$‚Üí\\BM1&3&0\\\\0&0&0\\\\0&0&1\\EM$$‚ÜíE_1=\\span(\\BM3\\\\-1\\\\0\\EM)$.\n",
        "$Œª=5‚Üí\\BM-3&3&0\\\\1&-1&3\\\\0&0&-4\\EM$$‚Üí\\BM1&-1&0\\\\0&0&0\\\\0&0&1\\EM$$‚ÜíE_5=\\span(\\BM-1\\\\-1\\\\0\\EM)$.\n",
        "\n",
        "- 4.8: $\\v{A}=\\BM3&2&2\\\\2&3&-2\\EM$. Rank 2.\n",
        "$\\v{A}^‚ä§\\v{A}=\\BM13&12&2\\\\12&13&-2\\\\2&-2&8\\EM$.\n",
        "$E_9=\\span(\\BM1\\\\-1\\\\4\\EM)$.\n",
        "$E_{25}=\\span(\\BM1\\\\1\\\\0\\EM)$.\n",
        "$\\v{V}=\\BM1/\\sqrt{2}&1/\\sqrt{18}\\\\1/\\sqrt{2}&-1/\\sqrt{18}\\\\0&4/\\sqrt{18}\\EM$.\n",
        "$\\v{Œ£}=\\BM5&0\\\\0&3\\EM$.\n",
        "$\\v{u}_1=\\/{1}{5\\sqrt{2}}\\v{A}\\BM1\\\\1\\\\0\\EM=\\BM1/\\sqrt{2}\\\\1/\\sqrt{2}\\EM$.\n",
        "$\\v{u}_2=\\/{1}{3\\sqrt{18}}\\v{A}\\BM1\\\\-1\\\\4\\EM=\\BM1/\\sqrt{2}\\\\-1/\\sqrt{2}\\EM$.\n",
        "$\\v{A}=\\BM1/\\sqrt{2}&1/\\sqrt{2}\\\\1/\\sqrt{2}&-1/\\sqrt{2}\\EM\\BM5&0\\\\0&3\\EM\\BM1/\\sqrt{2}&1/\\sqrt{2}&0\\\\1/\\sqrt{18}&-1/\\sqrt{18}&4/\\sqrt{18}\\EM$"
      ],
      "metadata": {
        "id": "-WPnEzsdiV3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Vector Calculus\n",
        "\n",
        "- Numerator layout $‚àáf(x)(y-x)$"
      ],
      "metadata": {
        "id": "t96JCZrR4Ory"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jacobian gradients**: Differentiating a scalar wrt a vector $f:‚Ñù^n‚Üí‚Ñù,\\v{x}‚Ü¶f(\\v{x}),\\v{x}‚àà‚Ñù^n$ gives the Jacobian vector $\\red{‚àá_\\v{x}f‚â°\\/{df}{d\\v{x}}‚â°\\/{‚àÇf}{‚àÇ\\v{x}}=\\BSM \\/{‚àÇf(\\v{x})}{‚àÇx_1}&...&\\/{‚àÇf(\\v{x})}{‚àÇx_n}\\ESM}‚àà‚Ñù^{1√ón}$ using **numerator layout**: If the vector in the derivative numerator $\\/{‚àÇ\\blue{\\v{x}}}{‚àÇt}$ then the Jacobian is a column matrix.\n",
        "If the vector is in the denominator $\\/{‚àÇf}{‚àÇ\\blue{\\v{x}}}$ then the Jacobian is a row matrix.\n",
        "In the Jacobian matrix, each column corresponds to a denominator element and each row corresponds to a numerator element.\n",
        "\n",
        "- Suppose $f:‚Ñù^n‚Üí‚Ñù$, then gradient $‚àáf(\\v{x})$ lives in $‚Ñù^n$. Imagine $‚Ñù^n$ is the ground, and $f$ is a surface floating in the air. Then $‚àáf(\\v{x}_0)$ is a vector parallel to the ground radiating from $\\v{x}_0$ toward the direction that results in the steepest ascent on the floating surface. The magnitude of $‚àáf(\\v{x}_0)$ is the rate of change in the steepest ascent at $\\v{x}_0$.\n",
        "\n",
        "  - Level set $f(\\v{x})=c$ is a contour parallel to the ground and also floating in the air through the surface. $‚àáf$ too is parallel to the ground with the level set, but it is perpendicular to the path of the contour.\n",
        "\n",
        "  - The tangent plane at $f(\\v{x}_0)$ has normal $(‚àáf(\\v{x}_0),-1)$.\n",
        "\n",
        "- $\\BC\n",
        "\\t{sum rule:}&\\/{‚àÇ}{‚àÇ\\v{x}}(f(\\v{x})+g(\\v{x}))=\\/{‚àÇf}{‚àÇ\\v{x}}(\\v{x})+\\/{‚àÇg}{‚àÇ\\v{x}}(\\v{x})\\\\\n",
        "\\t{product rule:}&\\/{‚àÇ}{‚àÇ\\v{x}}(f(\\v{x})g(\\v{x}))=(\\/{‚àÇf}{‚àÇ\\v{x}}g)(\\v{x})+(\\/{‚àÇg}{‚àÇ\\v{x}}f)(\\v{x})\\\\\n",
        "\\t{chain rule:}&\\/{‚àÇ}{‚àÇ\\v{x}}(g‚àòf)(\\v{x})=\\/{‚àÇ}{‚àÇ\\v{x}}g(f(\\v{x}))=\\/{‚àÇg}{‚àÇf}\\/{‚àÇf}{‚àÇ\\v{x}}\\\\\n",
        "\\EC$\n",
        "\n",
        "  - $f:‚Ñù^2‚Üí‚Ñù,(x_1(t),x_2(t))‚Ü¶f(\\v{x})$. Then $\\/{df}{dt}=\\/{‚àÇf}{‚àÇ\\v{x}}\\/{‚àÇ\\v{x}}{‚àÇt}=\\BSM\\/{‚àÇf}{‚àÇx_1}&\\/{‚àÇf}{‚àÇx_2}\\ESM\\BSM\\/{dx_1}{dt}\\\\\\/{dx_2}{dt}\\ESM$\n",
        "  $=\\/{‚àÇf}{‚àÇx_1}\\/{dx_1}{dt}+\\/{‚àÇf}{‚àÇx_2}\\/{dx_2}{dt}$\n",
        "\n",
        "  - $f:‚Ñù^2‚Üí‚Ñù,(x_1(s,t),x_2(s,t))‚Ü¶f(\\v{x})$. Then $\\/{df}{d(s,t)}=\\/{‚àÇf}{‚àÇ\\v{x}}\\/{‚àÇ\\v{x}}{‚àÇ(s,t)}=\\BSM\\/{‚àÇf}{‚àÇx_1}&\\/{‚àÇf}{‚àÇx_2}\\ESM\\BSM\\/{dx_1}{ds}&\\/{dx_1}{dt}\\\\\\/{dx_2}{ds}&\\/{dx_2}{dt}\\ESM$\n",
        "\n",
        "- **Vector-vector gradients**: $\\v{f}:‚Ñù^n‚Üí‚Ñù^m,\\v{x}‚Ü¶\\v{f}(\\v{x}),\\v{x}‚àà‚Ñù^n$, then $\\v{f}(\\v{x})=\\BSM f_1(\\v{x})\\\\\\vdots\\\\f_m(\\v{x})\\ESM$, and the **Jacobian** gradient matrix is $\\red{\\v{J}‚â°‚àá_\\v{x}\\v{f}‚â°\\/{d\\v{f}}{d\\v{x}}‚â°\\/{‚àÇ\\v{f}}{‚àÇ\\v{x}}=\\BSM\\/{‚àÇf_1}{‚àÇx_1}&...&\\/{‚àÇf_1}{‚àÇx_n}\\\\\\vdots&\\ddots&\\vdots\\\\\\/{‚àÇf_n}{‚àÇx_1}&...&\\/{‚àÇf_n}{‚àÇx_n}\\ESM}‚àà‚Ñù^{m√ón}$.\n",
        "\n",
        "- **Vector-matrix and matrix-matrix gradients**: Differentiating a vector wrt a vector $\\v{f}:‚Ñù^n‚Üí‚Ñù^m$, or differentiating a scalar wrt a matrix $\\v{f}:‚Ñù^{m√ón}‚Üí‚Ñù$ gives a 2-dimensional Jacobian matrix. Differentiating vector wrt matrix $\\v{f}:‚Ñù^{p√óq}‚Üí‚Ñù^m$ gives 3-dimensional Jacobian tensor. Differentiating matrix wrt matrix $\\v{f}:‚Ñù^{p√óq}‚Üí‚Ñù^{m√ón}$ gives a 4-dimensional tensor. Bad. Instead we vectorize the matrices (isomorphism) to stay in 2-dimensional.\n",
        "\n",
        "**Hessian**: second derivatives matrix requires one more dimension than Jacobian gradients. For $f:‚Ñù^n‚Üí‚Ñù$, the Hessian matrix is $\\red{\\v{H}‚â°‚àá_\\v{x}^2f‚â°\\/{‚àÇ^2f}{‚àÇ\\v{x}^2}=\\BSM\\/{‚àÇf}{‚àÇx_1^2}&...&\\/{‚àÇf}{‚àÇx_1‚àÇx_n}\\\\\\vdots&\\ddots&\\vdots\\\\\\/{‚àÇf}{‚àÇx_nx_1}&...&\\/{‚àÇf}{‚àÇx_n^2}\\ESM}‚àà‚Ñù^{n√ón}$\n",
        "\n",
        "- 5.6: $f(x,y)=(x+2y^3)^2$. Then $\\/{‚àÇf}{‚àÇx}=2(x+2y^3)$, and $\\/{‚àÇf}{‚àÇy}=2(x+2y^3)6y^2$.\n",
        "\n",
        "- 5.10: $h(t)=(f‚àòg)(t)$ where $f:‚Ñù^2‚Üí‚Ñù,\\v{x}‚Ü¶e^{x_1x_2^2}$ and $g:‚Ñù‚Üí‚Ñù^2,t‚Ü¶\\BM t\\cos t\\\\t\\sin t\\EM$.\n",
        "Then $\\/{dh}{dt}=\\/{‚àÇf}{‚àÇ\\v{x}}\\/{‚àÇ\\v{x}}{‚àÇt}$\n",
        "$=\\BM x_2^2e^{x_1x_2^2}&2x_1x_2e^{x_1x_2^2}\\EM\\BM \\cos t-t\\sin t\\\\\\sin t+t\\cos t\\EM$\n",
        "\n",
        "- 5.11: $\\v{y}=\\v{Œ¶Œ∏}$, where $\\v{Œ∏}‚àà‚Ñù^{D√ó1}$, $\\v{Œ¶}‚àà‚Ñù^{N√óD}$, and $\\v{y}‚àà‚Ñù^{N√ó1}$. Let $\\v{e}=\\v{y}-\\v{Œ¶Œ∏}$. Find $\\arg\\min_\\v{Œ∏}\\vn{e}^2$.\n",
        "$\\/{‚àÇ\\vn{e}^2}{‚àÇ\\v{Œ∏}}|_{1√ó2}=\\/{‚àÇ\\vn{e}^2}{‚àÇ\\v{e}}|_{1√óN}\\/{‚àÇ\\v{e}}{‚àÇ\\v{Œ∏}}|_{N√ó2}$\n",
        "$=-2\\v{e}^‚ä§\\v{Œ¶}=-2(\\v{y}^‚ä§-\\v{Œ∏}^‚ä§\\v{Œ¶}^‚ä§)\\v{Œ¶}=\\v{0}$\n",
        "$‚áí\\v{Œ∏}^‚ä§\\v{Œ¶}^‚ä§\\v{Œ¶}=\\v{y}^‚ä§\\v{Œ¶}$\n",
        "$‚áí\\v{Œ¶}^‚ä§\\v{Œ¶}\\v{Œ∏}=\\v{Œ¶}^‚ä§\\v{y}$\n",
        "$‚áí\\v{Œ∏}=(\\v{Œ¶}^‚ä§\\v{Œ¶})^{-1}\\v{Œ¶}^‚ä§\\v{y}$\n",
        "\n",
        "- 5.12: $\\v{f}=\\v{Ax}$, $\\v{f}‚àà‚Ñù^{M}$, $\\v{A}‚àà‚Ñù^{M√óN}$, $\\v{x}‚àà‚Ñù^{N}$. Then $\\/{d\\v{f}}{d\\v{A}}=\\BM‚àÇ_\\v{A}f_1\\\\\\vdots\\\\‚àÇ_\\v{A}f_n\\EM‚àà‚Ñù^{M√ó(M√óN)}$.\n",
        "Always first write out $f_i=\\sum_{j=1}^NA_{ij}x_j$\n",
        "$‚áí\\/{‚àÇf_i}{‚àÇA_{ij}}=x_j$\n",
        "$‚áí\\/{‚àÇf_i}{‚àÇA_{k:}}=\\BC \\v{x}^‚ä§&k=i\\\\\\v{0}^‚ä§&k\\neq i\\EC$\n",
        "$‚áí\\/{‚àÇf_i}{‚àÇ\\v{A}}=\\BM\\vdots\\\\\\v{0}^‚ä§\\\\\\v{x}^‚ä§\\\\\\v{0}^‚ä§\\\\\\vdots\\EM‚àà‚Ñù^{1√óM√óN}$.\n"
      ],
      "metadata": {
        "id": "fPhQ38f04abG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector/Matrix derivatives**:\n",
        "\n",
        "- $\\BC\n",
        "\\t{identity}&\\/{d}{d\\v{x}}\\v{x}=\\/{d}{d\\v{x}^‚ä§}\\v{x}=\\/{d}{d\\v{x}}\\v{x}^‚ä§=\\v{I}_n \\\\\n",
        "\\t{transpose}& d(\\v{x}^‚ä§)=(d\\v{x})^‚ä§\\\\\n",
        "&\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})^‚ä§=\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})\\right)^‚ä§ \\\\\n",
        "\\t{trace}& \\/{‚àÇ}{‚àÇ\\v{X}}\\tr(\\v{f}(\\v{X}))=\\tr(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X}))\\\\\n",
        "\\t{inner product}& \\/{‚àÇ}{‚àÇ\\v{X}}\\tr(\\v{Y}^‚ä§\\v{X})=\\v{Y}^‚ä§\\\\\n",
        "\\t{determinant}& \\/{‚àÇ}{‚àÇ\\v{X}}\\det(\\v{X})=\\det(\\v{X})\\v{X}^{-1} \\\\\n",
        "\\t{inverse}& \\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})^{-1}=-\\v{f}(\\v{X})^{-1}\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})\\right)\\v{f}(\\v{X})^{-1} \\\\\n",
        "& \\/{‚àÇ}{‚àÇ\\v{X}}(\\v{a}^‚ä§\\v{X}^{-1}\\v{b})=-(\\v{X}^{-1})^‚ä§\\v{ab}^‚ä§(\\v{X}^{-1})^‚ä§ \\\\\n",
        "\\t{dot product}& \\/{‚àÇ}{‚àÇ\\v{x}}(\\v{x}^‚ä§\\v{a})=\\/{‚àÇ}{‚àÇ\\v{x}}(\\v{a}^‚ä§\\v{x})=\\v{a}^‚ä§ \\\\\n",
        "\\t{norm}& \\/{‚àÇ}{‚àÇ\\v{x}}\\vn{x}^2=\\/{‚àÇ}{‚àÇ\\v{x}}(\\v{x}^‚ä§\\v{x})=2\\v{x}^‚ä§ \\\\\n",
        "\\t{inner product}& \\/{‚àÇ}{‚àÇ\\v{X}}(\\v{a}^‚ä§\\v{Xb})=\\v{ab}^‚ä§ \\\\\n",
        "\\t{quadratic form}& \\/{‚àÇ}{‚àÇ\\v{x}}(\\v{x}^‚ä§\\v{Bx})=\\v{x}^‚ä§(\\v{B}+\\v{B}^‚ä§) \\\\\n",
        "\\t{symmetric $\\v{W}$}& \\/{‚àÇ}{‚àÇ\\v{s}}(\\v{x}-\\v{As})^‚ä§\\v{W}(\\v{x}-\\v{As})=-2(\\v{x}-\\v{As})^‚ä§\\v{WA}\n",
        "\\EC$\n",
        "\n",
        "  - Proof (identity): $\\v{f}(\\v{x})=\\BM x_1\\\\\\vdots\\\\x_n\\EM$. Then $\\/{d\\v{f}}{d\\v{x}}=\\BM‚àÇ_{x_1}x_1&...&‚àÇ_{x_n}x_1\\\\\\vdots&\\ddots&\\vdots\\\\‚àÇ_{x_1}x_n&...&‚àÇ_{x_n}x_n\\EM$\n",
        "  $=\\BM1&...&0\\\\\\vdots&\\ddots&\\vdots\\\\0&...&1\\EM=\\v{I}_n$\n",
        "\n",
        "  - Proof (trace): Trace is a linear operator. $\\/{‚àÇ}{‚àÇ\\v{X}}\\tr(\\v{f}(\\v{X}))=\\/{‚àÇ}{‚àÇ\\v{X}}\\sum_{k=1}^n\\v{f}(\\v{X})_{kk}$\n",
        "  $=\\sum_{k=1}^n\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})_{kk}$\n",
        "\n",
        "  - Proof (matrix inner product): $\\/{‚àÇ}{‚àÇX_{ij}}\\tr(\\v{Y}^‚ä§\\v{X})=Y_{ij}$. Numerator layout results in vector transpose for gradients so $\\/{‚àÇ}{‚àÇX}\\tr(\\v{Y}^‚ä§\\v{X})=\\v{Y}^‚ä§$.\n",
        "\n",
        "  - Proof (determinant): Let $\\v{X}^{-1}=\\/{1}{\\det(\\v{X})}\\v{C}^‚ä§$, then $\\v{C}=\\det(\\v{X})(\\v{X}^{-1})^‚ä§$ is the cofactor of the Laplace expansion $\\det(\\v{X})=\\sum_{k=1}^nX_{ik}C_{ik}$ and $\\/{‚àÇ}{‚àÇX_{ij}}\\det(\\v{X})=C_{ij}$.\n",
        "\n",
        "  - Proof (inverse): $\\v{0}=\\/{‚àÇ}{‚àÇ\\v{X}}\\v{I}=\\/{‚àÇ}{‚àÇ\\v{X}}(\\v{f}(\\v{X})\\v{f}(\\v{X})^{-1})=\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})\\right)\\v{f}(\\v{X})^{-1}+\\v{f}(\\v{X})\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})^{-1}\\right)$\n",
        "  $‚áí\\v{f}(\\v{X})\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})^{-1}\\right)=-\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})\\right)\\v{f}(\\v{X})^{-1}$\n",
        "  $‚áí\\green{\\v{f}(\\v{X})^{-1}}\\v{f}(\\v{X})\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})^{-1}\\right)=-\\green{\\v{f}(\\v{X})^{-1}}\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})\\right)\\v{f}(\\v{X})^{-1}$\n",
        "\n",
        "  - Proof (dot product): $\\/{‚àÇ}{‚àÇ\\v{x}}(\\v{x}^‚ä§\\v{a})=\\/{‚àÇ}{‚àÇ\\v{x}}(\\v{a}^‚ä§\\v{x})=\\/{‚àÇ}{‚àÇ\\v{x}}\\sum_ia_ix_i=\\BM a_1&...&a_n\\EM=\\v{a}^‚ä§$\n",
        "  \n",
        "  - Proof (norm): $f(\\v{x})=\\v{x}^‚ä§\\v{x}=\\sum_ix_i^2$. Then $‚àá_\\v{x}f=\\/{df}{d\\v{x}}=\\/{d}{d\\v{x}}\\sum_ix_i^2=\\BM2x_1&...&2x_n\\EM=2\\v{x}^‚ä§$\n",
        "\n",
        "  - Proof (inner product): $\\v{a}^‚ä§\\v{Xb}=\\sum_i\\sum_ja_iX_{ij}b_j‚áí\\/{‚àÇ(\\v{a}^‚ä§\\v{Xb})}{‚àÇX_{ij}}=a_ib_j$\n",
        "  $‚áí\\/{‚àÇ}{‚àÇ\\v{X}}(\\v{a}^‚ä§\\v{Xb})=\\v{ab}^‚ä§$ outer product.\n",
        "\n",
        "  - Proof (quadratic form): $\\v{x}^‚ä§\\v{Bx}=\\sum_i\\sum_jx_iB_{ij}x_j‚áí\\/{‚àÇ(\\v{x}^‚ä§\\v{Bx})}{‚àÇx_k}=\\/{‚àÇ}{‚àÇx_k}(\\sum_jx_kB_{kj}x_j+\\sum_ix_iB_{ik}x_k)$\n",
        "  $=\\sum_jB_{kj}x_j+\\sum_ix_iB_{ik}$\n",
        "  $=\\v{x}^‚ä§(\\v{b}^‚ä§)_{k}+\\v{x}^‚ä§\\v{b}_k$\n",
        "  where $(\\v{b}^‚ä§)_k,\\v{b}_k$ are the $k$-th column of $\\v{B}^‚ä§,\\v{B}$.\n",
        "\n",
        "  - Proof (symmetric $\\v{W}$): $\\/{‚àÇ}{‚àÇ\\v{s}}(\\v{x}-\\v{As})^‚ä§\\v{W}(\\v{x}-\\v{As})=\\/{‚àÇ}{‚àÇ\\v{u}}(\\v{u}^‚ä§\\v{W}\\v{u})\\/{‚àÇ\\v{u}}{‚àÇ\\v{s}}$\n",
        "  $=-\\v{u}^‚ä§(\\v{W}+\\v{W}^‚ä§)\\v{A}$\n",
        "\n",
        "- $‚àá_\\v{X}(\\ln\\det(\\v{X}))=\\/{1}{\\det(\\v{X})}\\det(\\v{X})\\v{X}^{-1}=\\v{X}^{-1}$"
      ],
      "metadata": {
        "id": "FM0IMQX7APnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taylor polynomial** of $n$-degree $\\red{T_n(x)=\\suml_{k=0}^n\\/{f^{(k)}(x_0)}{k!}(x-x_0)^k}$ approximates $f:‚Ñù‚Üí‚Ñù,f‚ààC^n$ in a neighborhood around $x_0$. If Taylor series $T_‚àû(x)=f(x)$ then $f$ is called analytic.\n",
        "\n",
        "- Consider $f:‚Ñù^D‚Üí‚Ñù,\\v{x}‚Ü¶f(\\v{x}),\\v{x}‚àà‚Ñù^D$ that is smooth at $\\v{x}_0$. Let $\\v{Œ¥}=\\v{x}-\\v{x}_0$, then the $n$-degree Taylor polynomial in the neighborhood of $\\v{x}_0$ is $\\red{T_n(\\v{x})=\\sum_{k=0}^n\\/{‚ü®‚àá_\\v{x}^kf(\\v{x}_0),\\v{Œ¥}^k‚ü©}{k!}}$, where $‚àá_\\v{x}^kf(\\v{x}_0)$ is the $k$-th derivative $k$-dimensional tensor of $f$ wrt $\\v{x}$ evaluated at $\\v{x}_0$, and $\\v{Œ¥}^k‚àà‚Ñù^{D√ó...√óD}$ is the $k$-dimensional outer product tensor.\n",
        "\n",
        "  - $\\v{Œ¥}^2=\\v{Œ¥}‚äó\\v{Œ¥}‚àà‚Ñù^{D√óD}$ where $(\\v{Œ¥}^2)_{i,j}=Œ¥_iŒ¥_j$\n",
        "\n",
        "  - $\\blue{‚ü®‚àá_\\v{x}^kf(\\v{x}_0),\\v{Œ¥}^k‚ü©}=\\sum_{i_1=1}^D...\\sum_{i_k=1}^D‚àá_\\v{x}^kf(\\v{x}_0)_{i_1,...,i_k}Œ¥_{i_1}...Œ¥_{i_k}‚àà‚Ñù$ the Taylor polynomial ultimately sums up to a single scalar, so these tensor multiplications must be inner products.\n",
        "\n",
        "  - $\\BC\n",
        "  k=0& ‚ü®‚àá_\\v{x}^0f(\\v{x}_0)\\v{Œ¥}^0‚ü©&=f(\\v{x}_0) \\\\\n",
        "  k=1& ‚ü®‚àá_\\v{x}^1f(\\v{x}_0)\\v{Œ¥}^1‚ü©&=\\ub{‚àá_\\v{x}f(\\v{x}_0)}{1√óD}\\ub{(\\v{Œ¥})}{D√ó1}&=\\sum_{i=1}^D(‚àá_\\v{x}f(\\v{x}_0))_iŒ¥_i‚àà‚Ñù\\\\\n",
        "  k=2& ‚ü®‚àá_\\v{x}^2f(\\v{x}_0)\\v{Œ¥}^2‚ü©&=\\tr(\\ub{‚àá_\\v{x}^2f(\\v{x}_0)}{D√óD}\\ub{(\\v{Œ¥}‚äó\\v{Œ¥})}{D√óD})=\\v{Œ¥}^‚ä§‚àá_\\v{x}^2f(\\v{x}_0)\\v{Œ¥}&=\\sum_{i=1}^D\\sum_{j=1}^D(‚àá_\\v{x}^2f(\\v{x}_0))_{ij}Œ¥_iŒ¥_j‚àà‚Ñù\\\\\n",
        "  \\EC$\n",
        "\n",
        "- 5.15: $f(x,y)=x^2+2xy+y^3$ at $\\v{x}_0=(1,2)$. Then $\\v{Œ¥}=\\BM x-1\\\\y-2\\EM$ and $\\BC\n",
        "k=1& ‚àá_\\v{x}f=\\BM 2x+2y&2x+3y^2\\EM=\\BM6&14\\EM\\\\\n",
        "k=2& ‚àá_\\v{x}^2f=\\BM 2&2\\\\2&6y\\EM=\\BM2&2\\\\2&12\\EM\\\\\n",
        "\\EC$"
      ],
      "metadata": {
        "id": "1vRFO-VbeEgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Vector Probability"
      ],
      "metadata": {
        "id": "mHFseZC0wEXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given probability space $(Œ©,\\m{A},P)$ with sample space, event space, and probability measure, random variable is function $X:Œ©‚Üí\\m{T}$ where $\\m{T}$ is the \"target space\" (support) of $X$ whose elements are \"states\".\n",
        "\n",
        "- Function $p:‚Ñù^D‚Üí‚Ñù$ is a multivariate PDF if $p(\\v{x})‚â•0$ for all $\\v{x}‚àà‚Ñù^D$ and $‚à´_{‚Ñù^D}p(\\v{x})\\ d\\v{x}=1$.\n",
        "\n",
        "  - $\\v{x}‚àà‚Ñù^D$ represents one data point, whereas $\\v{x}_1,...,\\v{x}_N$ is a size-$N$ random sample.\n",
        "\n",
        "- Function $F_X(\\v{x})=P(X_1‚â§x_1,...,X_D‚â§x_D)=‚à´_{-‚àû}^{x_1}...‚à´_{-‚àû}^{x_D}p(z_1,...,z_D)\\ dz_1...dz_D$ is CDF for multivariate random variable $X=\\BM X_1\\\\\\vdots\\\\X_D\\EM$ with states $\\v{x}=\\BM x_1\\\\\\vdots\\\\x_D\\EM$\n",
        "\n",
        "- **Sum rule**: marginal distribution $p(\\v{x})=‚à´_Yp(\\v{x},\\v{y})\\ d\\v{y}=‚à´_Yp(\\v{x}|\\v{y})p(\\v{y})\\ d\\v{y}=\\E_Y[p(\\v{x}|\\v{y})]$\n",
        "\n",
        "  - LOTP $P(X)=\\sum_yP(X|Y=y)P(Y=y)=\\E[P(X|Y)]$\n",
        "\n",
        "- **Product rule**: conditional distribution $p(\\v{x},\\v{y})=p(\\v{y}|\\v{x})p(\\v{x})$\n",
        "\n",
        "**Negative log-likelihood**: In supervised learning, we have $(\\v{x}_1,y_1),...,(\\v{x}_N,y_N)$ iid with $\\v{x}_n‚àà‚Ñù^D$ and labels $y_n‚àà‚Ñù$. Given likelihood $p(y_n|\\v{x}_n,\\v{Œ∏})$, negative log-likelihood $l(\\v{Œ∏})=-\\ln p(y_n|\\v{x}_n,\\v{Œ∏})$ is function of $\\v{Œ∏}$ with $\\v{x}_n$ and $y_n$ fixed.\n",
        "\n",
        "- Split the dataset $\\v{y}=[y_1,...,y_N]^‚ä§$ and $\\v{X}=[\\v{x}_1,...,\\v{x}_N]^‚ä§$. The likelihood is $p(\\v{y}|\\v{X},\\v{Œ∏})=\\prodl_{n=1}^Np(y_n|\\v{x}_n,\\v{Œ∏})$ and negative log-likelihood $l(\\v{Œ∏})=-\\suml_{n=1}^N\\ln p(y_n|\\v{x}_n,\\v{Œ∏})$.\n",
        "\n",
        "- **Likelihood function** $p(y_n|\\v{x}_n,\\v{Œ∏})$ is a normalized probability distribution wrt $y_n$, but it does not integrate to 1 wrt $\\v{Œ∏}$, and may not even be integrable wrt $\\v{Œ∏}$.\n",
        "\n",
        "**Bayes theorem**: $\\red{p(\\v{Œ∏}|\\v{x})=\\/{p(\\v{x}|\\v{Œ∏})p(\\v{Œ∏})}{p(\\v{x})}}$ making inference about unobserved latent random variables $\\v{Œ∏}$ based observations of other random variables $\\v{x}$, prior knowledge $p(\\v{Œ∏})$, and relationship between them $p(\\v{x}|\\v{Œ∏})$\n",
        "\n",
        "- Prior $p(\\v{Œ∏})$ encapsulates our subjective prior knowledge of the latent $\\v{Œ∏}$\n",
        "\n",
        "- Likelihood $p(\\v{x}|\\v{Œ∏})$ how likely it is to observe value $\\v{x}$, assuming our latent variable value is $\\v{Œ∏}$. This is analogous as classical likelihood function of $\\v{Œ∏}$.\n",
        "\n",
        "- Posterior $p(\\v{Œ∏}|\\v{x})$ what we believe is the distribution of $\\v{Œ∏}$ after observing value $\\v{x}$\n",
        "\n",
        "- Evidence $p(\\v{x})=‚à´_Œò(\\v{x}|\\v{Œ∏})p(\\v{Œ∏})\\ d\\v{Œ∏}=\\E_\\v{Œ∏}[p(\\v{x}|\\v{Œ∏})]$ is the LOTP probability of $\\v{x}$ weighted by prior $p(\\v{Œ∏})$. It also acts as normalizing constant to ensure posterior sums to 1.\n",
        "\n",
        "- **Maximum A Posteriori** (MAP) $\\v{Œ∏}^*=\\arg\\sup_\\v{Œ∏}p(\\v{Œ∏}|\\v{x})=\\arg\\sup_\\v{Œ∏}\\/{p(\\v{x}|\\v{Œ∏})p(\\v{Œ∏})}{p(\\v{x})}=\\arg\\sup_\\v{Œ∏}p(\\v{x}|\\v{Œ∏})p(\\v{Œ∏})$, which drops out the difficult integral $p(\\v{x})$.\n",
        "\n",
        "  - MLE assumes parameters $\\v{Œ∏}$ are fixed, and it's classically determined from likelihood function $p(\\v{Œ∏}|\\v{x})$. MAP is the Bayesian analog determined from $p(\\v{Œ∏}|\\v{x})‚àùp(\\v{x}|\\v{Œ∏})p(\\v{Œ∏})$.\n",
        "\n",
        "  - From $\\v{Œ∏}^*$, we make predictions $p(\\v{x}|\\v{Œ∏}^*)$. This again turns the problem into a fixed parameter inference.\n",
        "\n",
        "- **Bayesian inference**: Let $\\v{X}=[\\v{x}_1,...,\\v{x}_N]^‚ä§$ be data and $\\v{x}$ be a data point. The goal is to compute posterior $p(\\v{Œ∏}|\\v{X})=\\/{p(\\v{X}|\\v{Œ∏})p(\\v{Œ∏})}{p(\\v{X})}$ and use that to inference $p(\\v{x})=\\E_\\v{Œ∏}[p(\\v{x}|\\v{Œ∏})]$ averages all plausible parameters.\n",
        "\n",
        "  - Computation is difficult. Unless using conjugate priors, we need to use approximations: MCMC, Laplace approximation, variational inference, and expectation propagation.\n",
        "\n",
        "- **Latent variables**: we assume $p(\\v{x}|z,\\v{Œ∏})$ for unobserved $z$, which needs to be marginalized out in likelihood $p(\\v{x}|\\v{Œ∏})=\\E_z[p(\\v{x}|z,\\v{Œ∏})]$. Expectation maximization (EM) is used to handle latent variables."
      ],
      "metadata": {
        "id": "MrUUmbsBbELC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean and covariance**: $\\E_X$ does not mean \"expectation parameterized by value $X$\", as per statistics texts. In Bayesian texts $\\E_X$ means expectation taken wrt the PDF of $X$.\n",
        "\n",
        "- $\\E_X[g(x)]=‚à´_\\m{X}g(x)p(x)\\ dx$.\n",
        "\n",
        "  - $\\E_X[g(\\v{x})]=\\BSM E_{X_1}[g(x_1)]\\\\\\vdots\\\\E_{X_D}[g(x_D)]\\ESM$. $\\E_X[\\v{x}]=\\BSM \\E_{X_1}[x_1]\\\\\\vdots\\\\\\E_{X_D}[x_D]\\ESM$ where\n",
        "  $\\E_{X_i}[x_i]=‚à´_\\m{X}x_ip(x_i)\\ dx_i$\n",
        "\n",
        "- $\\Cov_{X,Y}[x,y]=\\E_{X,Y}[(x-\\E_X[x])(y-\\E_Y[y])]=\\E_{X,Y}[xy]-\\E_X[x]\\E_Y[y]$\n",
        "\n",
        "  - $\\red{\\v{Œ£}_\\v{xy}:=\\Cov[\\v{x},\\v{y}]=\\E[(\\v{x}-\\E[\\v{x}])(\\v{y}-\\E[\\v{y}])^‚ä§]=\\E[\\v{xy}^‚ä§]-\\E[\\v{x}]\\E[\\v{y}]^‚ä§=\\Cov[\\v{y},\\v{x}]^‚ä§}‚àà‚Ñù^{D√óE}$\n",
        "  where $\\v{x}‚àà‚Ñù^D$ and $\\v{y}‚àà‚Ñù^E$\n",
        "\n",
        "  - Covariance matrix $\\red{\\v{Œ£}_\\v{xx}:=\\V(\\v{x})=\\Cov[\\v{x},\\v{x}]}=\\E[\\v{xx}^‚ä§]-\\E[\\v{x}]\\E[\\v{x}]^‚ä§‚àà‚Ñù^{D√óD}$ is symmetric and positive-semidefinite, but assumed positive-definite (for $N$ data points of $\\v{x}$).\n",
        "\n",
        "  - $\\Corr[x,y]=\\/{\\Cov[x,y]}{\\sqrt{\\V[x]\\V[y]}}$. Correlation matrix is covariance matrix applied to standardized $(x-\\E[x])/œÉ_x$.\n",
        "\n",
        "  - $\\BC\n",
        "  \\blue{\\V[\\v{x}+\\v{y}]=\\V[\\v{x}]+\\V[\\v{y}]+\\Cov[\\v{x},\\v{y}]+\\Cov[\\v{y},\\v{x}]}\\\\\n",
        "  \\V[\\v{x}-\\v{y}]=\\V[\\v{x}]+\\V[\\v{y}]-\\Cov[\\v{x},\\v{y}]-\\Cov[\\v{y},\\v{x}]\\EC$\n",
        "\n",
        "- **Information geometry**: $\\V[x+y]=\\V[x]+\\V[y]+2\\Cov[x,y]$. If $\\Cov[x,y]=0$ then $\\V[x+y]=\\V[x]+\\V[y]$ thereby establishing Pythagorean setup where $\\red{‚ü®X,Y‚ü©:=\\Cov[x,y]}$ (Eaton 2007).\n",
        "\n",
        "  - $\\blue{\\n{X}=\\sqrt{\\V[x]}=œÉ_X}$\n",
        "\n",
        "  - $\\blue{\\cos œâ=\\/{‚ü®X,Y‚ü©}{\\n{X}\\n{Y}}=\\/{\\Cov[x,y]}{\\sqrt{\\V[x]\\V[y]}}=\\Corr[x,y]}$\n",
        "\n",
        "  - $\\blue{X\\perp Y ‚áî ‚ü®X,Y‚ü©=0 ‚áî \\Cov[x,y]=0}$\n",
        "\n",
        "- **Affine transformation**: Consider $X$ with $\\v{Œº}=\\E[\\v{x}]$ and $\\v{Œ£}=\\Cov[\\v{x},\\v{x}]$. Let $\\v{y}=\\v{Ax}+\\v{b}$ where $\\v{A}$ and $\\v{b}$ are known.\n",
        "Then $\\BC\n",
        "\\E_Y[\\v{y}]=\\v{AŒº}+\\v{b}&\\E_Y[\\v{y}]=\\E_X[\\v{Ax}+\\v{b}]=\\v{A}\\E_X[\\v{x}]+\\v{b}=\\v{AŒº}+\\v{b} \\\\\n",
        "\\V_Y[\\v{y}]=\\v{AŒ£A}^‚ä§&\\V_Y[\\v{y}]=\\V_X[\\v{Ax}+\\v{b}]=\\v{A}\\V_X[\\v{x}]\\v{A}^‚ä§=\\v{AŒ£A}^‚ä§ \\\\\n",
        "\\Cov\\small[\\v{x},\\v{y}]=\\v{Œ£A}^‚ä§&\\Cov\\small[\\v{x},\\v{y}]=\\E[\\v{x}(\\v{Ax}+\\v{b})^‚ä§]-\\E[\\v{x}]\\E[\\v{Ax}+\\v{b}]‚ä§\\\\\n",
        "&=\\E[\\v{x}\\v{x}^‚ä§]\\v{A}^‚ä§+\\E[\\v{x}]\\v{b}^‚ä§-\\E[\\v{x}]\\E[\\v{x}]^‚ä§\\v{A}^‚ä§-\\E[\\v{x}]\\v{b}^‚ä§ \\\\\n",
        "&=\\E[\\v{x}\\v{x}^‚ä§]\\v{A}^‚ä§-\\E[\\v{x}]\\E[\\v{x}]^‚ä§\\v{A}^‚ä§ \\\\\n",
        "&=\\v{Œ£A}^‚ä§\n",
        "\\EC$\n",
        "\n",
        "**Empirical data**: Consider random sample $n=1,...,N$ where $\\v{x}_n‚àà‚Ñù^D$ is a data point\n",
        "\n",
        "- $\\bar{\\v{x}}=\\/{1}{N}\\sum_{n=1}^N\\v{x}_n$ and the empirical covariance matrix is $\\red{\\v{Œ£}=\\/{1}{N}\\sum_{n=1}^N(\\v{x}_n-\\bar{\\v{x}})(\\v{x}_n-\\bar{\\v{x}})^‚ä§}‚àà‚Ñù^{D√óD}$\n",
        "\n",
        "- $\\V[x]=\\E[(x-\\E[x])^2]$ requires two passes. $\\V[x]=\\E[x^2]-(\\E[x])^2$ requires one pass, but numerically unstable. Empirically use pairwise difference $\\red{\\/{1}{N^2}\\sum_{i,j=1}^N(x_i-x_j)^2}=2[\\/{\\sum_ix_i^2}{N}-(\\/{\\sum_ix_i}{N})^2]$.\n",
        "\n",
        "  - Proof: $\\/{1}{N^2}\\sum_{i,j=1}^N(x_i-x_j)^2$\n",
        "  $=\\/{1}{N^2}\\sum_{i,j=1}^N(x_i^2+x_j^2-2x_ix_j)$\n",
        "  $=\\/{1}{N^2}\\sum_{i,j=1}^Nx_i^2+\\/{1}{N^2}\\sum_{i,j=1}^Nx_j^2$\n",
        "  $-\\/{2}{N^2}\\sum_{i,j=1}^Nx_ix_j$\n",
        "  $=\\/{2}{N}\\sum_{i=1}^Nx_i^2-\\/{2}{N^2}(\\sum_{i=1}^Nx_i)(\\sum_{j=1}^Nx_j)$\n",
        "  $=2[\\/{\\sum_ix_i^2}{N}-(\\/{\\sum_ix_i}{N})^2]$\n"
      ],
      "metadata": {
        "id": "TiUJCI9ejoEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaussian distribution** $X‚àº\\Normal(\\v{Œº},\\v{Œ£})$ then $\\red{p(\\v{x}|\\v{Œº},\\v{Œ£})=\\/{1}{\\sqrt{(2œÄ)^D\\det|\\v{Œ£}|}}\\e{-\\/{(\\v{x}-\\v{Œº})^‚ä§\\v{Œ£}^{-1}(\\v{x}-\\v{Œº})}{2}}}$\n",
        "\n",
        "- Consider $\\Normal(\\v{x}|\\v{Œº}_\\v{x},\\v{Œ£}_\\v{xx})$ and $\\Normal(\\v{y}|\\v{Œº}_\\v{y},\\v{Œ£}_\\v{yy})$ with dimensions $\\v{x}‚àà‚Ñù^D$ and $\\v{y}‚àà‚Ñù^E$. Then we concatenate them $\\BSM\\v{x}\\\\\\v{y}\\ESM‚àà‚Ñù^{D+E}$ and $p(\\v{x},\\v{y})=\\Normal(\\BSM\\v{Œº}_\\v{x}\\\\\\v{Œº}_\\v{y}\\ESM,\\BSM\\v{Œ£}_\\v{xx}&\\v{Œ£}_\\v{xy}\\\\\\v{Œ£}_\\v{yx}&\\v{Œ£}_\\v{yy}\\ESM)$ where $\\v{Œ£}_\\v{xy}=\\Cov[\\v{x},\\v{y}]‚àà‚Ñù^{D√óE}$.\n",
        "Then ignoring normalizing constants $p(\\v{x},\\v{y})‚àù\\e{-\\/{1}{2}\\BSM\\v{x}-\\v{Œº}_\\v{x}\\\\\\v{y}-\\v{Œº}_\\v{y}\\ESM^‚ä§\\BSM\\v{Œ£}_\\v{xx}&\\v{Œ£}_\\v{xy}\\\\\\v{Œ£}_\\v{yx}&\\v{Œ£}_\\v{yy}\\ESM^{-1}\\BSM\\v{x}-\\v{Œº}_\\v{x}\\\\\\v{y}-\\v{Œº}_\\v{y}\\ESM}$.\n",
        "\n",
        "  - Marginal $p(\\v{x})=‚à´_\\m{Y}p(\\v{x},\\v{y})\\ d\\v{y}=\\Normal(\\v{x}|\\v{Œº}_\\v{x},\\v{Œ£}_\\v{xx})$, where notation $\\Normal(\\v{x}|\\v{Œº},\\v{Œ£}):=p(\\v{x}|\\v{Œº},\\v{Œ£})$\n",
        "\n",
        "- Conditional $p(\\v{x}|\\v{y})=\\Normal(\\v{Œº}_{\\v{x}|\\v{y}},\\v{Œ£}_{\\v{x}|\\v{y}})$ where $\\blue{\\BC\n",
        "\\v{Œº}_{\\v{x}|\\v{y}}=\\v{Œº}_\\v{x}+\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}(\\v{y}-\\v{Œº}_\\v{y}) \\\\\n",
        "\\v{Œ£}_{\\v{x}|\\v{y}}=\\v{Œ£}_\\v{xx}-\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}\\v{Œ£}_\\v{yx}\n",
        "\\EC}$\n",
        "\n",
        "  - Proof: Let $\\BM \\v{d}_\\v{x}\\\\\\v{d}_\\v{y}\\EM=\\BM\\v{x}-\\v{Œº}_\\v{x}\\\\\\v{y}-\\v{Œº}_\\v{y}\\EM$.\n",
        "  Define inverse\n",
        "  $\\BM\\v{Œ£}_\\v{xx}&\\v{Œ£}_\\v{xy}\\\\\\v{Œ£}_\\v{yx}&\\v{Œ£}_\\v{yy}\\EM\\BM\\v{Œõ}_\\v{xx}&\\v{Œõ}_\\v{xy}\\\\\\v{Œõ}_\\v{yx}&\\v{Œõ}_\\v{yy}\\EM=\\BM\\v{I}&\\v{0}\\\\\\v{0}&\\v{I}\\EM$.\n",
        "  Then $p(\\v{x},\\v{y})‚àù\\e{-\\/{1}{2}\\BM \\v{d}_\\v{x}^‚ä§&\\v{d}_\\v{y}^‚ä§\\EM\\BM\\v{Œõ}_\\v{xx}&\\v{Œõ}_\\v{xy}\\\\\\v{Œõ}_\\v{yx}&\\v{Œõ}_\\v{yy}\\EM\\BM \\v{d}_\\v{x}\\\\\\v{d}_\\v{y}\\EM}$\n",
        "  $=\\e{-\\/{1}{2}\\small(\\v{d}_\\v{x}^‚ä§\\v{Œõ}_\\v{xx}\\v{d}_\\v{x}+2\\v{d}_\\v{x}^‚ä§\\v{Œõ}_\\v{xy}\\v{d}_\\v{y}+\\v{d}_\\v{y}^‚ä§\\v{Œõ}_\\v{yy}\\v{d}_\\v{y})}$\n",
        "  $=p(\\v{x}|\\v{y})p(\\v{y})$.\n",
        "  $\\BC\n",
        "  p(\\v{x}|\\v{y})\\t{ quadratic form}&=-\\/{1}{2}(\\v{x}-\\v{Œº}_{\\v{x}|\\v{y}})^‚ä§\\v{Œ£}_{\\v{x}|\\v{y}}^{-1}(\\v{x}-\\v{Œº}_{\\v{x}|\\v{y}})\\\\\n",
        "  &=-\\/{1}{2}(\\green{\\v{d}_\\v{x}-\\v{m}})^‚ä§\\green{\\v{P}}(\\v{d}_\\v{x}-\\v{m})\\\\\n",
        "  &=-\\/{1}{2}(\\v{d}_\\v{x}^‚ä§\\v{P}\\v{d}_\\v{x}-2\\v{d}_\\v{x}^‚ä§\\v{Pm}+\\t{const})\\\\\n",
        "  p(\\v{x},\\v{y})\\t{ quadratic form}&=-\\/{1}{2}(\\v{d}_\\v{x}^‚ä§\\v{Œõ}_\\v{xx}\\v{d}_\\v{x}+2\\v{d}_\\v{x}^‚ä§\\v{Œõ}_\\v{xy}\\v{d}_\\v{y}+\\t{const}) \\\\\n",
        "  \\EC$\n",
        "  $‚áí\\BC \\v{P}=\\v{Œõ}_\\v{xx}\\\\\\v{m}=-\\v{Œõ}_\\v{xx}^{-1}\\v{Œõ}_\\v{xy}\\v{d}_\\v{y}\\EC$.\n",
        "  Therefore $\\v{Œ£}_{\\v{x}|\\v{y}}=\\v{P}^{-1}=\\v{Œ£}_\\v{xx}-\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}\\v{Œ£}_\\v{yx}$ and\n",
        "  $\\v{Œº}_{\\v{x}|\\v{y}}=\\v{Œº}_\\v{x}+\\v{m}=\\v{Œº}_\\v{x}-\\v{Œõ}_\\v{xx}^{-1}\\v{Œõ}_\\v{xy}(\\v{y}-\\v{Œº}_\\v{y})$\n",
        "  $=\\v{Œº}_\\v{x}+\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}(\\v{y}-\\v{Œº}_\\v{y})$.\n",
        "\n",
        "- **Sum of independent Gaussians**: $a\\Normal(\\v{Œº}_\\v{x},\\v{Œ£}_\\v{xx})+b\\Normal(\\v{Œº}_\\v{y},\\v{Œ£}_\\v{yy})‚àº\\Normal(a\\v{Œº}_\\v{x}+b\\v{Œº}_\\v{y},a^2\\v{Œ£}_\\v{xx}+b^2\\v{Œ£}_\\v{yy})$.\n",
        "\n",
        "  - Linear transformation: $\\v{A}\\Normal(\\v{x}|\\v{Œº},\\v{Œ£})‚àº\\Normal(\\v{AŒº},\\v{AŒ£A}^‚ä§)$\n",
        "\n",
        "  - Affine transformation: $\\v{A}\\Normal(\\v{x}|\\v{Œº},\\v{Œ£})+\\v{b}‚àº\\Normal(\\v{AŒº}+\\v{b},\\v{AŒ£A}^‚ä§)$\n",
        "\n",
        "- **Prior/likelihood convolution**: $\\blue{‚à´\\Normal(\\v{x}|\\v{Œº},\\v{A})\\Normal(\\v{y}|\\v{x},\\v{B})\\ d\\v{x}=\\Normal(\\v{y}|\\v{Œº},\\v{A}+\\v{B})}$\n",
        "\n",
        "  - Proof: $p(\\v{y})=‚à´p(\\v{x},\\v{y})\\ d\\v{x}=‚à´p(\\v{y}|\\v{x})p(\\v{x})\\ d\\v{x}=\\Normal(\\v{y}|\\v{x},\\v{B})‚à´\\Normal(\\v{x}|\\v{Œº},\\v{A})\\ d\\v{x}$ The marginal is the convolution of prior and likelihood. This is equivalent to $\\BC\n",
        "  \\v{y}=\\v{x}+\\v{b},&\\v{b}‚àº\\Normal(\\v{0},\\v{B})\\\\\n",
        "  \\v{x}=\\v{Œº}+\\v{a},&\\v{a}‚àº\\Normal(\\v{0},\\v{A})\n",
        "  \\EC‚áí\\v{y}=\\v{Œº}+(\\v{a}+\\v{b})$.\n",
        "  Therefore $p(\\v{y})=\\Normal(\\v{y}|\\v{Œº},\\v{A}+\\v{B})$.\n",
        "\n",
        "  - $\\v{y}=\\v{Ax}+\\v{b}+\\v{w}$ where $\\v{w}‚àº\\Normal(\\v{0},\\v{Q})$ and $\\v{x}‚àº\\Normal(\\v{Œº},\\v{Œ£})$. Then $\\BC\n",
        "  \\t{by integral}&p(\\v{y})=‚à´p(\\v{y}|\\v{x})p(\\v{x})\\ d\\v{x}=‚à´\\Normal(\\v{y}|\\v{Ax+b},\\v{Q})\\Normal(\\v{x}|\\v{Œº},\\v{Œ£})\\ d\\v{x}\\\\\n",
        "  \\t{break up $\\v{x}$}&\\v{x}=\\v{Œº}+\\v{u},\\t{ where }\\v{u}‚àº\\Normal(\\v{0},\\v{Œ£})\\\\\n",
        "  &\\v{y}=\\v{AŒº}+\\v{Au}+\\v{b}+\\v{w}\\\\\n",
        "  \\t{finally}&p(\\v{y})=\\Normal(\\v{y}|\\v{AŒº}+\\v{b},\\v{AŒ£A}^‚ä§+\\v{Q})\n",
        "  \\EC$\n",
        "\n",
        "- **Product of Gaussians**: $\\Normal(\\v{x}|\\v{a},\\v{A})\\Normal(\\v{x}|\\v{b},\\v{B})=c\\Normal(\\v{x}|\\v{c},\\v{C})$ where $\\v{C}=(\\v{A}^{-1}+\\v{B}^{-1})^{-1}$, $\\v{c}=\\v{C}(\\v{A}^{-1}\\v{a}+\\v{B}^{-1}\\v{b})$, and $c=\\Normal(\\v{b}|\\v{a},\\v{A}+\\v{B})=\\/{1}{\\sqrt{(2œÄ)^D\\det|\\v{A}+\\v{B}|}}\\e{-\\/{(\\v{a}-\\v{b})^‚ä§(\\v{A}+\\v{B})^{-1}(\\v{a}-\\v{b})}{2}}$.\n",
        "\n",
        "  - Proof: $\\Normal(\\v{x}|\\v{a},\\v{A})\\Normal(\\v{x}|\\v{b},\\v{B})=c\\Normal(\\v{x}|\\v{c},\\v{C})$. Then $\\BC\n",
        "  \\t{left quadratic}&\n",
        "  =-\\/{(\\v{x}-\\v{a})^‚ä§\\v{A}^{-1}(\\v{x}-\\v{a})+(\\v{x}-\\v{b})^‚ä§\\v{B}^{-1}(\\v{x}-\\v{b})}{2}\n",
        "  =-\\/{\\v{x}^‚ä§(\\v{A}^{-1}+\\v{B}^{-1})\\v{x}-2\\v{x}^‚ä§(\\v{A}^{-1}\\v{a}+\\v{B}^{-1}\\v{b})+\\t{const}}{2}\\\\\n",
        "  \\t{right quadratic}&\n",
        "  =-\\/{(\\v{x}-\\v{c})^‚ä§\\v{C}^{-1}(\\v{x}-\\v{c})}{2}\n",
        "  =-\\/{\\v{x}^‚ä§\\v{C}^{-1}\\v{x}-2\\v{x}^‚ä§\\v{C}^{-1}\\v{c}+\\t{const}}{2}\\\\\n",
        "  \\t{mean and variance}&\\v{C}=\\v{A}^{-1}+\\v{B}^{-1}\\t{, }\\v{c}=\\v{C}(\\v{A}^{-1}\\v{a}+\\v{B}^{-1}\\v{b})\n",
        "  \\EC$\n",
        "  $c=‚à´c\\Normal(\\v{x}|\\v{c},\\v{C})\\ d\\v{x}=‚à´\\Normal(\\v{x}|\\v{a},\\v{A})\\Normal(\\v{x}|\\v{b},\\v{B})\\ d\\v{x}$\n",
        "  $=‚à´\\Normal(\\v{x}|\\v{a},\\v{A})\\Normal(\\v{b}|\\v{x},\\v{B})\\ d\\v{x}$\n",
        "  $=\\Normal(\\v{b}|\\v{a},\\v{A}+\\v{B})$\n",
        "\n",
        "- **Weighted sum of Gaussians**: $p(x)=Œ±\\Normal(x|Œº_1,œÉ_1^2)+(1-Œ±)\\Normal(x|Œº_2,œÉ_2^2)$. Then\n",
        "$\\blue{\\BC \\E[x]=Œ±Œº_1+(1-Œ±)Œº_2\\\\\n",
        "\\V[x]=[Œ±œÉ_1^2+(1-Œ±)œÉ_2^2]+[Œ±Œº_1^2+(1-Œ±)Œº_2^2-(Œ±Œº_1+(1-Œ±)Œº_2)^2]\n",
        "\\EC}$\n",
        "\n",
        "  - Proof: $\\E[x^2]=Œ±\\E_{p_1}[x^2]+(1-Œ±)\\E_{p_2}[x^2]$\n",
        "  $=Œ±(Œº_1^2+œÉ_1^2)+(1-Œ±)(Œº_2^2+œÉ_2^2)$.\n",
        "  $\\V[x]=E[x^2]-E[x]^2=Œ±(Œº_1^2+œÉ_1^2)+(1-Œ±)(Œº_2^2+œÉ_2^2)-(Œ±Œº_1+(1-Œ±)Œº_2)^2$\n",
        "  $=[Œ±œÉ_1^2+(1-Œ±)œÉ_2^2]+[Œ±Œº_1^2+(1-Œ±)Œº_2^2-(Œ±Œº_1+(1-Œ±)Œº_2)^2]$\n",
        "\n",
        "  - Traditional setup: Let $E[I]=Œ±$, $(X|I=1)‚àº\\Normal(Œº_1,œÉ_1^2)$, and $(X|I=0)‚àº\\Normal(Œº_2,œÉ_2^2)$.\n",
        "  By Adam's law,\n",
        "  $\\E[X]=\\E[\\E[X|I]]=Œ±Œº_1+(1-Œ±)Œº_2$.\n",
        "  By Eve's law ,\n",
        "  $\\E[\\Var(X|I)]=Œ±œÉ_1^2+(1-Œ±)œÉ_2^2$, and\n",
        "  $\\Var(E[X|I])=\\E[(E[X|I]-\\E[X])^2]=Œ±(Œº_1-\\E[X])^2+(1-Œ±)(Œº_2-\\E[X])^2$\n",
        "\n",
        "- Reverse transformation: Let $\\v{A}‚àà‚Ñù^{M√óN}$ and $p(\\v{y})=\\Normal(\\v{y}|\\v{Ax},\\v{Œ£})$. Then $\\v{x}=(\\v{A}^‚ä§\\v{A})^{-1}\\v{A}^‚ä§\\v{y}$ and $p(\\v{x})=\\Normal(\\v{x}|(\\v{A}^‚ä§\\v{A})^{-1}\\v{A}^‚ä§\\v{y},(\\v{A}^‚ä§\\v{A})^{-1}\\v{A}^‚ä§\\v{Œ£}\\v{A}(\\v{A}^‚ä§\\v{A})^{-1})$"
      ],
      "metadata": {
        "id": "WdoFidyp_lxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exponential family** $\\red{p(\\v{x}|\\v{Œ∏})=h(\\v{x})\\e{‚ü®\\v{œï}(\\v{x}),\\v{Œ∏}‚ü©-A(\\v{Œ∏})}}$ is the multivariate likelihood function for one data point where $\\v{Œ∏}‚àà‚Ñù^k$ are natural parameters, $\\v{œï}(\\v{x})‚àà‚Ñù^k$ are sufficient statistics, and $A(\\v{Œ∏})$ is the log-partition function.\n",
        "\n",
        "- In Casella, $f(x|\\v{Œ∏})=h(x)c(\\v{Œ∏})\\e{\\sum_{i=1}^kw_i(\\v{Œ∏})t_i(x)}$ or $f(x|\\v{Œ∑})=h(x)c(\\v{Œ∑})\\e{\\sum_{i=1}^kŒ∑_it_i(x)}$ where $\\v{Œ∑}$ are natural parameters that correspond with $\\v{Œ∏}$ in MML.\n",
        "\n",
        "- Log-partition function $\\green{A(\\v{Œ∏})=\\ln ‚à´_\\m{X}h(\\v{x})\\e{\\v{œï}(\\v{x})^‚ä§\\v{Œ∏}}\\ d\\v{x}}$ is the normalizing constant, but with MGF properties. $\\blue{‚àá_\\v{Œ∏}A(\\v{Œ∏})=\\E[\\v{œï}(\\v{x})]}$ and $\\blue{‚àá_\\v{Œ∏}^2A(\\v{Œ∏})=\\V[\\v{œï}(\\v{x})]}$ is the positive-semidefinite Fisher Information matrix.\n",
        "\n",
        "  - Proof: $l(\\v{Œ∏})=\\ln h(\\v{x})+\\v{œï}(\\v{x})^‚ä§\\v{Œ∏}-A(\\v{Œ∏})$.\n",
        "  Score $\\green{‚àá_\\v{Œ∏}l(\\v{Œ∏})=\\v{œï}(\\v{x})^‚ä§-‚àá_\\v{Œ∏}A(\\v{Œ∏})}$. Hession $\\green{‚àá_\\v{Œ∏}^2l(\\v{Œ∏})=-‚àá_\\v{Œ∏}^2A(\\v{Œ∏})}$.\n",
        "  $\\E_X[‚àá_\\v{Œ∏}l(\\v{Œ∏})]=\\v{0}=\\E_X[\\v{œï}(\\v{x})^‚ä§]-\\E_X[‚àá_\\v{Œ∏}A(\\v{Œ∏})]‚áí‚àá_\\v{Œ∏}A(\\v{Œ∏})=\\E_X[\\v{œï}(\\v{x})^‚ä§]$.\n",
        "  Fisher information $‚àá_\\v{Œ∏}^2A(\\v{Œ∏})=-\\E_X[‚àá_\\v{Œ∏}^2l(\\v{Œ∏})]=I_1(\\v{Œ∏})$ is variance of the score:\n",
        "  $\\V_X[‚àá_\\v{Œ∏}l(\\v{Œ∏})]=\\V_X[\\v{œï}(\\v{x})^‚ä§]$.\n",
        "\n",
        "- Every member of the exponential family has a conjugate prior $\\red{p(\\v{Œ∏}|\\v{Œ≥})=h_c(\\v{Œ∏})\\e{‚ü®\\v{Œ≥},\\BSM \\v{Œ∏}\\\\-A(\\v{Œ∏})\\ESM‚ü©-A_c(\\v{Œ≥})}}$.\n",
        "\n",
        "  - $\\v{Œ≥}=\\BSM\\v{Œ≥}_1\\\\Œ≥_2\\ESM$ where $\\v{Œ≥}_1‚àà‚Ñù^k$ and $Œ≥_2‚àà‚Ñù$. The inner product is $\\green{‚ü®\\v{Œ≥},\\BSM \\v{Œ∏}\\\\-A(\\v{Œ∏})\\ESM‚ü©=\\v{Œ≥}_1^‚ä§\\v{Œ∏}-Œ≥_2A(\\v{Œ∏})}$. Together the two elements of $\\v{Œ≥}$ represent a hypothetical dataset observed before real experiment began.\n",
        "\n",
        "  - $\\v{Œ≥}_1$ interacts with $\\v{Œ∏}$ in the prior $p(\\v{Œ∏})$ the same way sufficient statistics $\\v{œï}(\\v{x})$ does in the likelihood $p(\\v{x}|\\v{Œ∏})$. Therefore, $\\v{Œ≥}_1$ represents $\\BSM\\sum_{i=1}^Nœï_1(\\v{x}_i)\\\\\\vdots\\\\\\sum_{i=1}^Nœï_k(\\v{x}_i)\\ESM$ in the prior. $Œ≥_2$ represents $N$ in the prior.\n",
        "\n",
        "  - Bayesian update with one data point of likelihood: $p(\\v{Œ∏}|\\v{x})‚àù\\e{\\v{Œ≥}_1^‚ä§\\v{Œ∏}-Œ≥_2A(\\v{Œ∏})}\\e{\\v{œï}(\\v{x})^‚ä§\\v{Œ∏}-A(\\v{Œ∏})}$\n",
        "  $=\\e{\\green{(\\v{Œ≥}_1+\\v{œï}(\\v{x}))}^‚ä§\\v{Œ∏}-\\green{(Œ≥_2+1)}A(\\v{Œ∏})}$\n",
        "\n",
        "- 6.13: $\\Normal(x|Œº,œÉ^2)=\\/{1}{\\sqrt{2œÄ}}\\e{-\\/{(x-Œº)^2}{2œÉ^2}-\\/{\\ln œÉ^2}{2}}$\n",
        "$‚àù\\e{-\\/{x^2}{2œÉ^2}+\\/{Œºx}{œÉ^2}-\\/{Œº^2}{2œÉ^2}-\\/{\\ln œÉ^2}{2}}$.\n",
        "Therefore $\\v{œï}(x)=\\BM x^2\\\\x\\EM$ and $\\v{Œ∏}=\\BM-1/2œÉ^2\\\\Œº/œÉ^2\\EM$\n",
        "\n",
        "- 6.14/6.15: $p(x|Œº)=Œº^x(1-Œº)^{1-x}=\\e{x\\ln(\\/{Œº}{1-Œº})+\\ln(1-Œº)}$. Then $œï(x)=x$, $Œ∏=\\ln(\\/{Œº}{1-Œº})$, and $Œº=\\/{e^Œ∏}{1+e^Œ∏}=\\/{1}{1+e^{-Œ∏}}$.\n",
        "Conjugate prior $p(Œº|Œ≥_1,Œ≥_2)‚àù\\e{Œ≥_1\\ln(\\/{Œº}{1-Œº})+Œ≥_2\\ln(1-Œº)}$\n",
        "$=(\\/{Œº}{1-Œº})^{Œ≥_1}(1-Œº)^{Œ≥_2}$\n",
        "$=Œº^{Œ≥_1}(1-Œº)^{Œ≥_2-Œ≥_1}$"
      ],
      "metadata": {
        "id": "ZM5Mx0WdwHTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 6.2: $0.4\\Normal(\\BM10\\\\2\\EM,\\v{I})+0.6\\Normal(\\v{0},\\BM8.4&2.0\\\\2.0&1.7\\EM)$\n",
        "\n",
        "  - Compute marginal distributions.\n",
        "  $0.4\\Normal(x_1|10,1)+0.6\\Normal(x_1|0,8.4)$.\n",
        "  $\\E[x_1]=4$.\n",
        "  $\\V[x_1]=(0.4)(1)+(0.6)(8.4)+(0.4)(10-4)^2+(0.6)(0-4)^2=29.44$.\n",
        "  $0.4\\Normal(x_2|2,1)+0.6\\Normal(x_2|0,1.7)$.\n",
        "  $\\E[x_2]=0.8$.\n",
        "  $\\V[x_2]=(0.4)(1)+(0.6)(1.7)+(0.4)(2-0.8)^2+(0.6)(0-0.8)^2=2.38$.\n",
        "\n",
        "  - Compute mean, mode, median for each marginal distribution.\n",
        "  Mean $\\E[x_1]=4$ and $\\E[x_2]=0.8$.\n",
        "  Each distribution is bimodal.\n",
        "  The median and mode calculations must be done numerically.\n",
        "  $x_1$ highest peak is at 10 because much smaller variance $(0.6)(1)< (0.4)(8.4)$.\n",
        "\n",
        "- 6.12: Consider $\\v{x}‚àº\\Normal(\\v{Œº}_x,\\v{Œ£}_x)‚àà‚Ñù^D$ and $\\v{y}=\\v{Ax}+\\v{b}+\\v{w}$ where $\\v{A}‚àà‚Ñù^{E√óD}$, and $\\v{w}‚àº\\Normal(\\v{0},\\v{Q})$ is independent with diagonal $\\v{Q}$.\n",
        "\n",
        "  - Find $p(\\v{y}|\\v{x})$.\n",
        "  Likelihood $p(\\v{y}|\\v{x})=\\Normal(\\v{y}|\\v{Ax}+\\v{b},\\v{Q})$\n",
        "\n",
        "  - Find $p(\\v{y})$.\n",
        "  Marginal $p(\\v{y})=‚à´p(\\v{y}|\\v{x})p(\\v{x})\\ d\\v{x}$\n",
        "  $=‚à´\\Normal(\\v{y}|\\v{Ax}+\\v{b},\\v{Q})\\Normal(\\v{x}|\\v{Œº}_x,\\v{Œ£}_x)\\ d\\v{x}$.\n",
        "  Let $\\v{x}=\\v{Œº}_x+\\v{u}$ where $\\v{u}‚àº\\Normal(\\v{0},\\v{Œ£})$, then $\\v{y}=\\v{AŒº}_x+\\v{b}+\\v{Au}+\\v{w}$ and\n",
        "  $p(\\v{y})=\\Normal(\\v{y}|\\v{AŒº}_x+\\v{b},\\v{AŒ£}_x\\v{A}^‚ä§+\\v{Q})$.\n",
        "\n",
        "  - Let $\\v{z}=\\v{Cy}+\\v{v}$ where $\\v{C}‚àà‚Ñù^{F√óE}$ and $\\v{v}‚àº\\Normal(\\v{v}|\\v{0},\\v{R})$ is independent.\n",
        "  Then $\\v{z}=\\v{CAŒº}_x+\\v{Cb}+\\v{CAu}+\\v{Cw}+\\v{v}$.\n",
        "  $p(\\v{z})=\\Normal(\\v{z}|\\v{CAŒº}_x+\\v{Cb},\\v{CAŒ£A}^‚ä§\\v{C}^‚ä§+\\v{CQC}^‚ä§+\\v{R})$"
      ],
      "metadata": {
        "id": "TsCue6ANSpoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Convex Optimization"
      ],
      "metadata": {
        "id": "w_2sEW1Guy5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent**: for objective function $f:‚Ñù^D‚Üí‚Ñù$, gradient $‚àá_\\v{x}f(\\v{x}_i)^‚ä§$ points towards the direction of steepest ascent at point $(\\v{x}_i,f(\\v{x}_i))$, and is orthogonal to the contour line. Gradient descent finds $\\arg\\min_\\v{x}f(\\v{x})$ iteratively at step size $Œ≥_i$ where each step moves to a new position $\\red{\\v{x}_{i+1}=\\v{x}_i-Œ≥_i‚àá_\\v{x}f(\\v{x}_i)^‚ä§}$.\n",
        "\n",
        "- Gradient descent with adaptive step-size: $\\BC \\t{Undo and $Œ≥_i‚áì$}&\\t{if }f(\\v{x}_{i+1})>f(\\v{x}_i)\\\\Œ≥_{i+1}‚áë&\\t{if }f(\\v{x}_{i+1})< f(\\v{x}_i)\\EC$.\n",
        "\n",
        "- For $\\v{Ax}=\\v{b}$, we can run gradient descent on $\\red{\\BC f(\\v{x})=\\n{\\v{Ax}-\\v{b}}^2=(\\v{Ax}-\\v{b})^‚ä§(\\v{Ax}-\\v{b}) \\\\ ‚àá_\\v{x}=2(\\v{Ax}-\\v{b})^‚ä§\\v{A} \\EC}$ as the objective function. Setting $‚àá_\\v{x}=0$ and solving for $\\v{x}$ gives the normal equation $\\v{x}=(\\v{A}^‚ä§\\v{A})^{-1}\\v{A}^‚ä§\\v{b}$.\n",
        "\n",
        "  - Gradient descent speed depends on **condition number** $Œ∫=\\/{œÉ(\\v{A})_\\max}{œÉ(\\v{A})_\\min}$, which is a ratio of single values of $\\v{A}$ that represent the ratio of the most curved slope vs least curved slope. Because gradient direction is orthogonal to contour line, gradient descent runs zig-zag along thin valleys, which has very high condition number.\n",
        "\n",
        "  - Pre-conditioning uses $\\v{P}$ to reduce condition number down near 1, and solve $\\v{P}^{-1}(\\v{Ax}-\\v{b})=\\v{0}$.\n",
        "\n",
        "- **Gradient descent with momentum**: let $Œ±‚àà[0,1]$, then $\\BC \\v{x}_{i+1}=\\v{x}_i-Œ≥_i‚àá_\\v{x}f(\\v{x}_i)^‚ä§+Œ±Œî\\v{x}_i \\\\ Œî\\v{x}_i=\\v{x}_i-\\v{x}_{i-1}\\EC$\n",
        "\n",
        "- **Stochastic gradient descent**: finding $\\arg\\min_\\v{Œ∏}\\sum_{n=1}^NL_n(\\v{Œ∏})$ requires $\\v{Œ∏}_{i+1}=\\v{Œ∏}_i-Œ≥_i\\sum_{n=1}^N‚àá_\\v{Œ∏}L(\\v{Œ∏}_i)^‚ä§$ by **batch** for large $N$ at each step. **SGD** mini-batch computes a subset $\\red{\\v{Œ∏}_{i+1}=\\v{Œ∏}_i-Œ≥_i\\sum_{n‚àà\\t{subset}}‚àá_\\v{Œ∏}L(\\v{Œ∏}_i)^‚ä§}$ at each step based on unbiased estimator $\\green{\\E[‚àá_\\v{Œ∏}L_n(\\v{Œ∏}_i)]=\\/{1}{N}\\sum_{n=1}^N‚àá_\\v{Œ∏}L_n(\\v{Œ∏}_i)}$ (LLN: $\\bar{X}\\larr{a.s.}Œº=\\E[X_1]$).\n",
        "\n",
        "- 7.1: $f(x_1,x_2)=\\/{1}{2}\\BM x_1\\\\x_2\\EM^‚ä§\\BM2&1\\\\1&20\\EM\\BM x_1\\\\x_2\\EM-\\BM5\\\\3\\EM^‚ä§\\BM x_1\\\\x_2\\EM$.\n",
        "Then $f(\\v{x})=\\/{1}{2}\\v{x}^‚ä§\\v{Ax}-\\v{b}^‚ä§\\v{x}$\n",
        "$‚áí‚àá_\\v{x}f=\\/{1}{2}\\v{x}^‚ä§(\\v{A}+\\v{A}^‚ä§)-\\v{b}^‚ä§$\n",
        "$=\\v{x}^‚ä§\\BM2&1\\\\1&20\\EM-\\BM5&3\\EM$.\n",
        "Starting at $\\v{x}_0=(-3,-1)$ with $Œ≥=0.085$ gives $\\v{x}_1=\\BM-3\\\\-1\\EM-0.085\\BM-12\\\\-26\\EM=\\BM-1.98\\\\1.21\\EM$"
      ],
      "metadata": {
        "id": "-gABm00Iu31Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convex Optimization**: $\\BC\\min_\\v{x}&f(\\v{x})\\\\\\t{subject to}&g_i(\\v{x})‚â§0,\\ i=1,...,m&h_j(\\v{x})=0,\\ j=1,...,n\\EC$ is called convex optimization problem if $f,g_i$ are convex functions, and $h_j(\\v{x})=0$ are convex sets (i.e., affine).\n",
        "\n",
        "- Set $\\m{C}$ is a **convex set** if for any $x,y‚àà\\m{C}$ and scalar $Œ∏‚àà[0,1]$ we have $\\red{Œ∏x+(1-Œ∏)y‚àà\\m{C}}$.\n",
        "\n",
        "  - Convex sets are sets such that a straight line connecting any two elements of the set lie inside the set.\n",
        "\n",
        "  - An affine function is such that for any $\\v{x},\\v{y}‚àà\\m{C}$, a line drawn between them also lies in $\\m{C}$.\n",
        "\n",
        "- Let $f:‚Ñù^D‚Üí‚Ñù$ be a function whose domain $\\t{dom} f$ is a convex set. $f$ is a **convex function** if for all $\\v{x},\\v{y}‚àà\\t{dom} f$ and scalar $Œ∏‚àà[0,1]$ we have $\\red{f(Œ∏\\v{x}+(1-Œ∏)\\v{y})‚â§Œ∏f(\\v{x})+(1-Œ∏)f(\\v{y})}$\n",
        "\n",
        "  - Jensen's inequality $g(\\E[X])‚â§\\E[g(X)]$ if $g$ is convex.\n",
        "\n",
        "  - If $g$ is a concave function then $-g$ is convex.\n",
        "\n",
        "  - $f:‚Ñù^D‚Üí‚Ñù$ is $C^1$, then $f$ is convex iff for any $\\v{x},\\v{y}‚àà\\t{dom}f$ it holds that $\\blue{f(\\v{y})‚â•f(\\v{x})+‚àá_\\v{x}f(\\v{x})(\\v{y}-\\v{x})}$ (second point higher than first order Taylor approximation).\n",
        "\n",
        "  - $f:‚Ñù^D‚Üí‚Ñù$ is $C^2$, then $f$ is convex iff $‚àá_\\v{x}^2f(\\v{x})$ is positive-semidefinite.\n",
        "\n",
        "  - Linear closure: A weighted sum of convex functions is convex.\n",
        "\n",
        "- 7.3: $f(x)=x\\log_2x$. Check convexity using $x=2,4$. Midway method: $f(3)=4.75$ vs $\\/{f(2)+f(4)}{2}=5$.\n",
        "\n"
      ],
      "metadata": {
        "id": "JFFr5r-_gCmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lagrangian duality**: for $f:‚Ñù^D‚Üí‚Ñù$ and $g_i:‚Ñù^D‚Üí‚Ñù$, **primal problem** $\\BC\\min_\\v{x}&f(\\v{x})\\\\\\t{subject to}&g_i(\\v{x})‚â§0,\\ i=1,...,m\\EC$ using Lagrangian $\\red{\\m{L}(\\v{x},\\v{Œª})=f(\\v{x})+\\v{Œª}^‚ä§\\v{g}(\\v{x})}$ becomes **dual problem** $\\BC\\max_\\v{Œª}&\\m{D}(\\v{Œª})=\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})\\\\\\t{subject to}&\\v{Œª}‚â•\\v{0}\\EC$\n",
        "\n",
        "- **Primal problem**: the objective function is a cost to be minimized. The Lagrangian $\\m{L}(\\v{x},\\v{Œª})$ adds cost penalties for constraint violations $\\sum_{i=1}^mŒª_ig_i(\\v{x})$ with dual variables $\\v{Œª}$ (Lagrange multipliers). To set infinite penalty against constraint violation, the objective function becomes $\\green{\\max_{\\v{Œª}‚â•0}\\m{L}(\\v{x},\\v{Œª})=f(\\v{x})+\\sum_{i=1}^m\\max_{Œª_i‚â•0}Œª_ig_i(\\v{x})}$, where the costs are\n",
        "$\\BC\n",
        "g_i(\\v{x})>0&\n",
        "Œª_i=‚àû&\n",
        "\\max_{Œª_i‚â•0}Œª_ig_i(\\v{x})=‚àû&\n",
        "\\max_{\\v{Œª}‚â•0}\\m{L}(\\v{x},\\v{Œª})=‚àû\\t{ if any }g_i(\\v{x})>0\\\\\n",
        "g_i(\\v{x})‚â§0&\n",
        "Œª_i=0&\n",
        "\\max_{Œª_i‚â•0}Œª_ig_i(\\v{x})=0&\n",
        "\\max_{\\v{Œª}‚â•0}\\m{L}(\\v{x},\\v{Œª})=f(\\v{x})\\t{ if all }g_i(\\v{x})‚â§0\\\\\n",
        "\\EC$. The original problem $\\min_\\v{x}f(\\v{x})$ has primal solution $\\red{p^*=\\min_\\v{x}\\max_{\\v{Œª}‚â•\\v{0}}\\m{L}(\\v{x},\\v{Œª})}$.\n",
        "\n",
        "- **Dual problem**: dual function $\\green{\\BC\\m{D}(\\v{Œª})=\\m{L}(\\v{x}_\\min(\\v{Œª}),\\v{Œª})\\\\\\v{x}_\\min(\\v{Œª})=\\arg\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})\\EC}$ maps any $\\v{Œª}$ to the absolute lowest cost (which may violate constraints), and is therefore lower bound to the optimal solution $\\m{D}(\\v{Œª})‚â§p^*$ (which upholds constraints). Dual ascent searches for the tighest lower bound by climbing the $\\m{D}(\\v{Œª})$ hill, whose slope is $\\blue{‚àá_\\v{Œª}\\m{D}(\\v{Œª})=\\v{g}(\\v{x}_\\min)}$ (Danskin's theorem). $\\BC\\t{left of peak}&g_i(\\v{x}_\\min)>0&\\t{raise $Œª_i$ until $g_i(\\v{x}_\\min)=0$}\\\\\\t{right of peak}&g_i(\\v{x}_\\min)< 0&\\t{lower $Œª_i$ toward $g_i(\\v{x}_\\min)=0$}\\EC$. The absolute peak of $\\m{D}(\\v{Œª})$ has flat slope $\\v{g}(\\v{x}_\\min(\\v{Œª}_\\t{peak}))=\\v{0}$, where some $Œª_{\\t{peak},i}< 0$ are negative because $g_i(\\v{x}_\\min(\\v{Œª}))|_{Œª_i=0}< 0$. The dual solution $\\red{d^*=\\max_{\\v{Œª}‚â•\\v{0}}\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})}$ is the tightest lower bound while respecting $\\v{Œª}‚â•\\v{0}$, so it's often placed at the right of the absolute peak where the slope $‚àá_\\v{Œª}\\m{D}(\\v{Œª})\\neq\\v{0}$.\n",
        "\n",
        "  - $\\m{D}(\\v{Œª})$ **is guaranteed to be concave (hill)** regardless of what $f,g_i$ look like. Proof (lower envelope principle): $\\m{L}(\\v{x},Œª)=f(\\v{x})+\\v{Œª}^‚ä§\\v{g}(\\v{x})$ is an affine function ($\\m{L}=mŒª+b$) of $\\v{Œª}$. Every $\\v{x}‚àà‚Ñù^D$ produces a linear function of $\\v{Œª}$: some up-sloping, some down-sloping, and some flat. Because $\\m{D}(\\v{Œª})=\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})$, tracing the bottom outline of all these lines forms a concave \"hill\" shape.\n",
        "\n",
        "- **Strong duality** is achieved when $\\green{\\min_\\v{x}\\max_{\\v{Œª}‚â•\\v{0}}\\m{L}(\\v{x},\\v{Œª})=\\max_{\\v{Œª}‚â•\\v{0}}\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})}$. When duality breaks, dual solution < primal solution (duality gap). The dual solution has constraint violations and the peak of $\\m{D}(\\v{Œª})$ is not smooth (i.e., $‚àá_\\v{Œª}\\m{D}(\\v{Œª})$ doesn't exist at the peak).\n",
        "  \n",
        "  - **Weak duality**: Minimax inequality says $\\red{\\max_y\\min_xœï(x,y)‚â§\\min_x\\max_yœï(x,y)}$. Proof: $\\min_xœï(x,y)‚â§\\max_yœï(x,y)‚áí\\BC \\max_y\\min_xœï(x,y)‚â§\\max_yœï(x,y) \\\\ \\min_xœï(x,y)‚â§\\min_x\\max_yœï(x,y)\\EC$. If LHS set is still less than the RHS set, then maximum of the LHS set is stll less than the minimum of the RHS set.\n",
        "\n",
        "  - **Slater's condition**: strong duality holds for convex $f,g_i$ if there exists $\\v{x}‚àà\\bigcap\\limits_{i=1}^m(g_i(\\v{x})< 0)$.\n",
        "\n",
        "- $\\v{Œª}$ **as shadow price**: If we relax the constraint $\\BC p^*(\\green{0})&\\min_xf(x)\\\\\\t{subject to}&g(x)‚â§\\green{0}\\EC‚áí\\BC p^*(\\green{u})&\\min_xf(x)\\\\\\t{subject to}&g(x)‚â§\\green{u}\\EC$, where the primal optimal value is $p^*(u)=\\min_x\\max_Œª\\m{L}(x,Œª)=f(x)+Œª(g(x)-u)$, then $\\/{‚àÇp^*}{‚àÇu}=-Œª=:\\blue{-Œª^*}$ is the rate of change in the optimal value wrt a unit relaxation in constraint.\n",
        "\n",
        "  - The shadow price $Œª^*$ is the marginal value to be gained from relaxing the resource constraint by 1 unit.\n",
        "  \n",
        "- **Equality constraint**: $\\BC\n",
        "\\min_\\v{x}&f(\\v{x})\\\\\n",
        "\\t{subject to}&g_i(\\v{x})‚â§0,\\ i=1,...,m\\\\\n",
        "&h_j(\\v{x})=0,\\ j=1,...,n\\EC‚áî\\BC\n",
        "\\min_\\v{x}&f(\\v{x})\\\\\n",
        "\\t{subject to}&g_i(\\v{x})‚â§0,\\ i=1,...,m\\\\\n",
        "&h_j(\\v{x})‚â§0,-h_j(\\v{x})‚â§0,\\ j=1,...,n\\\\\n",
        "\\EC$\n",
        "\n",
        "  - Slater's condition updated: convex $f,g_i$, affine $h_j$, and $\\v{x}‚àà(\\bigcap\\limits_{i=1}^m(g_i(\\v{x})< 0))‚à©(\\bigcap\\limits_{j=1}^n(h_j(\\v{x})=0))$.\n",
        "\n",
        "- **Calculation flow**: $\\BC\n",
        "\\t{Minimize}&\\t{Set }‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})=\\v{0}&\\t{get }\\v{x}_\\min(\\v{Œª})\\\\\n",
        "\\t{Substitute}&\\m{D}(\\v{Œª})=\\m{L}(\\v{x}_\\min(\\v{Œª}),\\v{Œª})\\\\\n",
        "\\t{Dual ascend}&\\max_{\\v{Œª}‚â•\\v{0}}\\m{D}(\\v{Œª})&\\t{get }\\v{Œª}^*\\\\\n",
        "\\t{Recovery}&\\t{Set }‚àá_\\v{x}\\m{L}(\\v{x}^*,\\v{Œª}^*)=\\v{0}&\\t{get $\\v{x}^*$ if strong duality}\n",
        "\\EC$\n",
        "\n",
        "  - **Recovery**: With $\\v{Œª}^*‚àà‚Ñù^m$, we recover $\\v{x}^*‚àà‚Ñù^d$ using a system of $d$ equations from KKT stationarity $\\blue{‚àá_\\v{x}\\m{L}(\\v{x}^*,\\v{Œª}^*)=‚àá_\\v{x}f(\\v{x}^*)+\\v{Œª}^{*‚ä§}‚àá_\\v{x}\\v{g}(\\v{x}^*)=\\v{0}}$.\n",
        "\n",
        "  - Special case: $\\BC\\min_\\v{x}&f(\\v{x})\\\\\\t{subject to}&\\v{h}(\\v{x})=\\v{0}\\EC‚áí\\BC‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})=‚àá_\\v{x}f(\\v{x})+\\v{Œª}^‚ä§‚àá_\\v{x}\\v{h}(\\v{x})=\\v{0}\\\\‚àá_\\v{Œª}\\m{L}(\\v{x},\\v{Œª})=\\v{h}(\\v{x})=\\v{0}\\EC$ is a system of $(m+d)$ equations and $(m+d)$ variables where $\\m{L}(\\v{x},\\v{Œª})=f(\\v{x})+\\v{Œª}^‚ä§\\v{h}(\\v{x})$. Since equality constraints are always active, finding active/inactive elements of $\\v{Œª}$ through dual ascent is no longer necessary.\n",
        "\n",
        "- **KKT Conditions** (Karush-Kuhn-Tucker): for convex problem with strong duality, solution $(\\v{x}^*,\\v{Œª}^*)$ is optimal **IFF**\n",
        "  \n",
        "  1. stationarity: $\\red{‚àá_\\v{x}\\m{L}(\\v{x}^*,\\v{Œª}^*)=‚àá_\\v{x}f(\\v{x}^*)+\\v{Œª}^{*‚ä§}‚àá_\\v{x}\\v{g}(\\v{x}^*)=\\v{0}}$. The optimal solution $(\\v{x}^*,\\v{Œª}^*)$ is the saddle point of the Lagrangian where $‚àá_\\v{x}\\m{L}(\\v{x}^*,\\v{Œª}^*)=\\v{0}$ but often $‚àá_\\v{Œª}\\m{L}(\\v{x}^*,\\v{Œª}^*)\\neq\\v{0}$ (dual ascent).\n",
        "  $\\BC\n",
        "  g(\\v{x}^*)=0&\\t{active constraint}&\\v{Œª}^*>\\v{0}\n",
        "  &\\m{L}(\\v{x}^*,\\v{Œª}^*)=f(\\v{x}^*)\n",
        "  &\\m{L}(\\v{x},\\v{Œª}^*)\\neq f(\\v{x})\n",
        "  &‚àá_\\v{x}\\m{L}=0\\neq‚àá_\\v{x}f\\\\\n",
        "  g(\\v{x}^*)< 0&\\t{inactive constraint}&\\v{Œª}^*=\\v{0}\n",
        "  &\\m{L}(\\v{x}^*,\\v{Œª}^*)=f(\\v{x}^*)\n",
        "  &\\m{L}(\\v{x},\\v{Œª}^*)=f(\\v{x})\n",
        "  &‚àá_\\v{x}\\m{L}=0=‚àá_\\v{x}f\\\\\n",
        "  \\EC$.\n",
        "  $\\blue{‚àá_\\v{x}f(\\v{x}^*)=-\\v{Œª}^{*‚ä§}‚àá_\\v{x}\\v{g}(\\v{x}^*)}$ \"Force balance\": the rate of further improvement in $f$ is equal to the shadow price $-\\v{Œª}^*$. Further improvement is limited by active constraints.\n",
        "\n",
        "  2. primal feasibility: $g_i(\\v{x}^*)‚â§0$ for $i=1,...,m$\n",
        "\n",
        "  3. dual feasibility: $Œª_i^*‚â•0$ for $i=1,...,m$\n",
        "\n",
        "  4. complementary slackness: $\\red{\\diag(\\v{Œª}^*)\\v{g}(\\v{x}^*)=\\v{0}}$ or $\\red{Œª_i^*g_i(\\v{x}^*)=0}$ for $i=1,...,m$.\n",
        "  $\\BC\n",
        "  g_i(\\v{x}^*)< 0&\\t{inactive constraint}&Œª_i^*=0&\\t{no improvement on $p^*$ by relaxing this constraint}\\\\\n",
        "  g_i(\\v{x}^*)=0&\\t{active constraint}&Œª_i^*>0&\\t{relaxing this constraint lowers $p^*$ by $Œª_i^*$}\n",
        "  \\EC$\n",
        "\n",
        "  - Meeting the 4 KKT conditions is sufficient to prove $(\\v{x}^*,\\v{Œª}^*)$ is optimal.\n",
        "\n",
        "- Example: Minimize $f(x)=x^2$ subject to $x‚â•2$. Lagrangian $\\m{L}(x,Œª)=x^2+Œª(2-x)$. Find the dual function $\\/{‚àÇ\\m{L}}{‚àÇx}=2x-Œª=0‚áíx=\\/{Œª}{2}$ and $\\m{D}=(\\/{Œª}{2})^2+Œª(2-\\/{Œª}{2})=2Œª-\\/{Œª^2}{4}$. Maximize $\\/{‚àÇ\\m{D}}{‚àÇŒª}=2-\\/{Œª}{2}=0‚áíŒª=4$. At $\\max\\m{D}$, $Œª=4$ and $x=2$. Duality achieved with active constraint ($x-2=0$).\n",
        "\n",
        "  - $\\m{L}(x,Œª^*)=x^2-4x+8$. $\\/{‚àÇ\\m{L}}{‚àÇx}=2x-4$. $\\/{‚àÇ\\m{L}}{‚àÇx}(2)=0$. Stationarity achieved."
      ],
      "metadata": {
        "id": "hIsXkFstjIaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear programming**: $f,g_i$ are all linear: $\\BC\\min_{\\v{x}‚àà‚Ñù^d}&\\v{c}^‚ä§\\v{x}\\\\\\t{subject to}&\\v{Ax}‚â§\\v{b}\\EC$ where $\\v{A}‚àà‚Ñù^{m√ód}$ is a linear program with $d$ variables and $m$ linear constraints.\n",
        "\n",
        "- Lagrangian $\\m{L}(\\v{x},\\v{Œª})=\\v{c}^‚ä§\\v{x}+\\v{Œª}^‚ä§(\\v{Ax}-\\v{b})$\n",
        "$=(\\green{\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A}})\\v{x}\\red{-\\v{Œª}^‚ä§\\v{b}}$ affine.\n",
        "Dual function is only defined if the slope is 0.\n",
        "$\\m{D}(\\v{Œª})=\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})=\\BC-\\v{Œª}^‚ä§\\v{b}&\\t{if }‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})=\\v{0}\\\\-‚àû&\\t{if }‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})\\neq\\v{0}\\EC$.\n",
        "\n",
        "  - Dual program\n",
        "  $\\BC\\max_{\\v{Œª}‚àà‚Ñù^m}&-\\v{b}^‚ä§\\v{Œª}\\\\\\t{subject to}&\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A}=\\v{0}\\\\&\\v{Œª}‚â•0\\EC$\n",
        "\n",
        "- Either the primal program ($\\v{x}^*‚àà‚Ñù^d$) or the dual program ($\\v{Œª}^*‚àà‚Ñù^m$) with $\\v{x}^*$ recovery can be solved using linear program polygonal algorithms.\n",
        "\n",
        "- 7.5: $\\min_\\v{x} -\\BM5&3\\EM\\v{x}$ subject to $\\BM2&2\\\\2&-4\\\\-2&1\\\\0&-1\\\\0&1\\EM\\v{x}‚â§\\BM33\\\\8\\\\5\\\\-1\\\\8\\EM$.\n",
        "Stationarity\n",
        "$‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})=\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A}=\\v{0}$\n",
        "$‚áí\\v{A}^‚ä§\\v{Œª}=-\\v{c}$.\n",
        "$\\BM2&2&-2&0&0\\\\2&-4&1&-1&1\\EM\\BMŒª_1\\\\Œª_2\\\\Œª_3\\\\Œª_4\\\\Œª_5\\EM=\\BM5\\\\3\\EM$ subject to $Œª_i‚â•0$.\n",
        "Find a $\\v{Œª}$ that meets dual feasibility.\n",
        "RREF $\\BM1&0&-1/2&-1/6&1/6&13/6\\\\0&1&-1/2&1/6&-1/6&1/3\\EM$\n",
        "$‚áí\\v{Œª}^*=\\BM13/6\\\\1/3\\\\0\\\\0\\\\0\\EM$.\n",
        "Complementary slackness\n",
        "$\\BC\\/{13}{6}(2x_1+2x_2-33)=0\\\\\\/{1}{3}(2x_1-4x_2-8)=0\\EC$.\n",
        "$\\BM13/3&13/3&143/2\\\\2/3&-4/3&8/3\\EM‚Üí\\BM1&0&37/3\\\\0&1&25/6\\EM$\n",
        "$‚áí\\v{x}^*=\\BM37/3\\\\25/6\\EM$. We know $(\\v{x}^*,\\v{Œª}^*)$ is the optimal solution because all 4 KKT conditions are met.\n",
        "\n",
        "  - Finding $\\v{Œª}$ that meets dual feasibility is a combinatorial process. The **Simplex Method** iteratively swaps columns of the basis (before RREF) until a valid active set is found."
      ],
      "metadata": {
        "id": "AfBGCwQO2p6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quadratic programming**: $f$ quadratic, $g_i$ linear: $\\BC\\min_{\\v{x}‚àà‚Ñù^d}&\\/{1}{2}\\v{x}^‚ä§\\v{Qx}+\\v{c}^‚ä§\\v{x}\\\\\\t{subject to}&\\v{Ax}‚â§\\v{b}\\EC$ where $\\v{A}‚àà‚Ñù^{m√ód}$ and $\\v{Q}‚àà‚Ñù^{d√ód}$ is symmetric positive-definite.\n",
        "\n",
        "- Lagrangian $\\m{L}(\\v{x},\\v{Œª})=\\/{1}{2}\\v{x}^‚ä§\\v{Qx}+\\v{c}^‚ä§\\v{x}+\\v{Œª}^‚ä§(\\v{Ax}-\\v{b})$\n",
        "$=\\/{1}{2}\\v{x}^‚ä§\\v{Qx}+(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{x}-\\v{Œª}^‚ä§\\v{b}$. Stationarity\n",
        "$‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})=\\v{x}^‚ä§\\v{Q}+\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A}=\\v{0}‚áí\\green{\\v{x}^‚ä§=-(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{Q}^{-1}}$ and $\\green{\\v{x}=\\v{Q}^{-1}(\\v{c}+\\v{A}^‚ä§\\v{Œª})}$. Substitute back to get dual function\n",
        "$\\m{D}(\\v{Œª})=\\/{1}{2}(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{Q}^{-1}(\\v{c}+\\v{A}^‚ä§\\v{Œª})-(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{Q}^{-1}(\\v{c}+\\v{A}^‚ä§\\v{Œª})-\\v{Œª}^‚ä§\\v{b}$\n",
        "$=-\\/{1}{2}(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{Q}^{-1}(\\v{c}+\\v{A}^‚ä§\\v{Œª})-\\v{Œª}^‚ä§\\v{b}$.\n",
        "\n",
        "  - Dual program $\\BC\\max_{\\v{Œª}‚àà‚Ñù^m}&-\\/{1}{2}(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{Q}^{-1}(\\v{c}+\\v{A}^‚ä§\\v{Œª})-\\v{Œª}^‚ä§\\v{b}\\\\\\t{subject to}&\\v{Œª}‚â•\\v{0}\\EC$\n",
        "\n",
        "- 7.6: $f(\\v{x})=\\/{1}{2}\\v{x}^‚ä§\\BM2&1\\\\1&4\\EM\\v{x}+\\BM5\\\\3\\EM^‚ä§\\v{x}$. Find $\\min_\\v{x}f(\\v{x})$ subject to $\\BM1&0\\\\-1&0\\\\0&1\\\\0&-1\\EM\\v{x}‚â§\\BM1\\\\1\\\\1\\\\1\\EM$. The constraint is $|x_1|‚â§1$ and $|x_2|‚â§1$. We solve unconstrained minimum first then apply constraints retrospectively.\n",
        "$‚àá_\\v{x}f(\\v{x})=\\v{x}^‚ä§\\BM2&1\\\\1&4\\EM+\\BM5\\\\3\\EM^‚ä§=\\v{0}$\n",
        "$‚áí\\v{x}=-\\BM2&1\\\\1&4\\EM^{-1}\\BM5\\\\3\\EM$\n",
        "$=\\BM-4/7&1/7\\\\1/7&-2/7\\EM\\BM5\\\\3\\EM$\n",
        "$=\\BM-17/7\\\\-1/7\\EM$, which fails constraint #2: $\\BM-1&0\\EM\\v{x}=\\/{17}{7}\\not‚â§1$. We apply constraint by setting $x_1=-1$ and resolve for unconstrained minimum. $f(-1,x_2)=\\/{1}{2}[2(-1)^2+2(-1)x_2+4x_2^2]-5+3x_2$\n",
        "$=2x_2^2+2x_2-4$. $\\/{df}{dx_2}=4x_2+2=0‚áíx_2=-\\/{1}{2}$.\n",
        "Therefore $\\v{x}^*=\\BM-1\\\\-1/2\\EM$ primal feasibility. Stationarity $‚àá_\\v{x}f(\\v{x}^*)=-\\v{Œª}^{*‚ä§}‚àá_\\v{x}\\v{g}(\\v{x}^*)$\n",
        "$‚áí\\BM5/2\\\\0\\EM=\\BM-1&1&0&0\\\\0&0&-1&1\\EM\\v{Œª}^{*}$\n",
        "$‚áíŒª_2=\\/{5}{2}$ dual feasibility.\n",
        "KKT conditions met."
      ],
      "metadata": {
        "id": "owBbXv13Cfck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Legendre-Fenchel Transform** or **convex conjugate** of $f:‚Ñù^D‚Üí‚Ñù$ is $\\red{f^*(\\v{s})=\\sup_{\\v{x}‚àà‚Ñù^D}(‚ü®\\v{s},\\v{x}‚ü©-f(\\v{x}))}$.\n",
        "\n",
        "- **Epigraph**: A convex function $f$ is a \"bowl\" that can be filled with water, and the result is the epigraph of $f$. A convex function becomes a convex set through its epigraph. **Supporting hyperplane** of a convex set is a tangential hyperplane. A convex set or convex function epigraph can be described in terms of its supporting hyperplanes.\n",
        "\n",
        "- On the $xy$-plane, $y=‚ü®s,x‚ü©=sx$ is line with slope $s$ passing through origin. $f^*(s)=\\sup_x(‚ü®s,x‚ü©-f(x))$ is the greatest vertical gap between the line $y=sx$ and $y=f(x)$ below it. If $f$ is convex then this is located at $(x_0,f(x_0))$ where $\\/{df}{dx}|_{x=x_0}=s$. Convex conjugate is the magnitude of this gap, which is equal to the negative of the y-intercept of the tangent with slope $s$.\n",
        "\n",
        "  - Proof: Let $g(x)=sx-f(x)$, then $\\/{dg}{dx}=s-\\/{df}{dx}=0‚áís=\\/{df}{dx}$. Second derivative check $\\/{d^2g}{dx^2}=-\\/{d^2f}{dx^2}< 0$. This critical point is supremum because $g$ is concave $‚áî-f$ is concave $‚áîf$ is convex. Therefore $x_0=\\arg\\sup_x(sx-f(x))=x|_{f'(x)=s}$. Consider the tangent line at $(x_0,f(x_0))$, which has slope $s$.\n",
        "  Then $y=mx+c‚áíf(x_0)=sx_0+c$\n",
        "  $‚áíc=-(sx_0-f(x_0))=-f^*(s)$.\n",
        "\n",
        "- $\\red{f^{**}=f}$ for closed convex sets and differentiable functions. Furthermore, $\\red{\\/{df}{dx}|_{x=x_0}=s_0‚áî\\/{df^*}{ds}|_{s=s_0}=x_0}$.\n",
        "\n",
        "  - Proof: $f^*(s)=\\sup_x(sx-f(x))$.\n",
        "  Let $x_0(s)=\\arg\\sup_x(sx-f(x))$ be a function of $s$. Let $\\/{df}{dx}|_{x=x_0}=s_0$, then $f^*(s_0)=s_0x_0-f(x_0)$.\n",
        "  By chain rule,\n",
        "  $\\/{df^*}{ds}|_{s=s_0}=x_0+s_0\\/{dx_0}{ds}-\\/{df}{dx}|_{x=x_0}\\/{dx_0}{ds}$\n",
        "  $=x_0+s_0\\/{dx_0}{ds}-s_0\\/{dx_0}{ds}=x_0$.\n",
        "\n",
        "- $\\BC\\t{Write}&g(x)=sx-f(x)\\\\\n",
        "\\t{Derivative}&g'(x)=s-f'(x)\\\\\n",
        "\\t{Express}&x_\\min(s)\\t{: $x$ as a function of $s$}\\\\\n",
        "\\t{Substitute}&f^*(s)=g(x_\\min(s))\\EC$\n",
        "\n",
        "**Convex conjugate of Lagrangian**: $\\m{L}_\\v{x}^*(\\v{s},\\v{Œª})=\\sup_\\v{x}(‚ü®\\v{s},\\v{x}‚ü©-\\m{L}(\\v{x},\\v{Œª}))$ is the convex conjugate of the Lagrangian wrt $\\v{x}$. Then\n",
        "$\\m{L}_\\v{x}^*(\\v{0},\\v{Œª})=\\sup_\\v{x}(‚ü®\\v{0},\\v{x}‚ü©-\\m{L}(\\v{x},\\v{Œª}))$\n",
        "$=\\sup_\\v{x}(-\\m{L}(\\v{x},\\v{Œª}))$\n",
        "$=-\\inf_\\v{x}\\m{L}(\\v{x},\\v{Œª})$.\n",
        "That is, the convex conjugate of the Lagrangian at $\\v{s}=\\v{0}$ is the negative of the Lagrangian dual function: $\\red{\\m{L}_\\v{x}^*(\\v{0},\\v{Œª})=-\\m{D}(\\v{Œª})}$.\n",
        "\n",
        "- B&V uses $f^*$ instead of $\\m{L}^*$. For $\\BC\n",
        "\\min_\\v{x}&f(\\v{x})\\\\\n",
        "\\t{subject to}&\\v{Ax}=\\v{b}\\EC$,\n",
        "Lagrangian $\\m{L}(\\v{x},\\v{Œª})=f(\\v{x})+\\v{Œª}^‚ä§(\\v{Ax}-\\v{b})$ has dual function\n",
        "$\\m{D}(\\v{Œª})=\\inf_\\v{x}\\m{L}(\\v{x},\\v{Œª})$\n",
        "$=\\inf_\\v{x}[-(-\\v{Œª}^‚ä§(\\v{Ax}-\\v{b})-f(\\v{x}))]$\n",
        "$=\\inf_\\v{x}[-(‚ü®-\\v{A}^‚ä§\\v{Œª},\\v{x}‚ü©-f(\\v{x}))]-\\v{Œª}^‚ä§\\v{b}$\n",
        "$=-\\sup_\\v{x}(‚ü®-\\v{A}^‚ä§\\v{Œª},\\v{x}‚ü©-f(\\v{x}))-\\v{Œª}^‚ä§\\v{b}$\n",
        "$=-f^*(-\\v{A}^‚ä§\\v{Œª})-\\v{Œª}^‚ä§\\v{b}$.\n",
        "\n",
        "  - $\\blue{\\inf(-f)=-\\sup f}$ and $\\blue{\\max f=-\\min(-f)}$\n",
        "\n",
        "- Example: $f(x)=x^2$. Let $g(x)=sx-x^2$ then $g(x)$ is greatest when $g'(x)=0=s-2x‚áíx=\\/{s}{2}$. $f^*(s)=\\sup_x(sx-f(x))=\\/{s^2}{2}-f(\\/{s}{2})=\\/{s^2}{4}$.\n",
        "\n",
        "  - $g^*(s)=st-\\/{s^2}{4}‚áí\\/{dg^*}{ds}=t-\\/{s}{2}=0‚áís=2t$.\n",
        "  $f^{**}(t)=t^2$.\n",
        "\n",
        "- Example: $f(x)=e^x$. Let $g(x)=sx-e^x$ then $g'(x)=0=s-e^x‚áíx=\\ln s$. Then $f^*(s)=s\\ln s-s$\n",
        "\n",
        "  - $g^*(s)=st-s\\ln s+s‚áí\\/{dg^*}{ds}=t-\\ln s-1+1=0‚áís=e^t$.\n",
        "  $f^{**}(t)=te^t-te^t+e^t=e^t$\n",
        "\n",
        "- 7.7: $f(\\v{y})=\\/{Œª}{2}\\v{y}^‚ä§\\v{K}^{-1}\\v{y}$.\n",
        "Convex conjugate $g(\\v{y})=‚ü®\\v{s},\\v{y}‚ü©-\\/{Œª}{2}\\v{y}^‚ä§\\v{K}^{-1}\\v{y}$\n",
        "$‚áí‚àá_\\v{y}g(\\v{y})=\\v{s}^‚ä§-Œª\\v{y}^‚ä§\\v{K}^{-1}=\\v{0}$\n",
        "$‚áí\\v{y}=\\/{1}{Œª}\\v{Ks}$.\n",
        "Then $f^*(\\v{s})=\\/{1}{Œª}\\v{s}^‚ä§\\v{Ks}-\\/{1}{2Œª}\\v{s}^‚ä§\\v{Ks}=\\/{1}{2Œª}\\v{s}^‚ä§\\v{Ks}$\n",
        "\n",
        "- 7.8: $L(\\v{t})=\\sum_il_i(t_i)$.\n",
        "Then $L^*(\\v{z})=\\sup_\\v{t}‚ü®\\v{z},\\v{t}‚ü©-\\sum_il_i(t_i)$\n",
        "$=\\sup_\\v{t}\\sum_i(z_it_i-l_i(t_i))$\n",
        "$=\\sum_i\\sup_\\v{t}(z_it_i-l_i(t_i))$\n",
        "$=\\sum_il^*(z_i)$\n",
        "\n",
        "- 7.9: Let $f,g$ be convex. Find $\\min_\\v{x}f(\\v{Ax})+g(\\v{x})$. Let $\\v{y}=\\v{Ax}$ then $\\BC\n",
        "\\min_\\v{x}&f(\\v{y})+g(\\v{x})\\\\\n",
        "\\t{subject to}&\\v{Ax}=\\v{y}\n",
        "\\EC$.\n",
        "The Lagrangian is $\\m{L}(\\v{x},\\v{y},\\v{Œª})=f(\\v{y})+g(\\v{x})+\\v{Œª}^‚ä§(\\v{Ax}-\\v{y})$ and\n",
        "$\\min_\\v{x}f(\\v{Ax})+g(\\v{x})=\\min_{\\v{x},\\v{y}}\\max_{\\v{Œª}‚â•\\v{0}}\\m{L}$\n",
        "$=\\max_{\\v{Œª}‚â•\\v{0}}\\min_{\\v{x},\\v{y}}\\m{L}$\n",
        "$=\\max_{\\v{Œª}‚â•\\v{0}}[\\min_\\v{y}f(\\v{y})-\\v{Œª}^‚ä§\\v{y}]+[\\min_\\v{x}g(\\v{x})+\\v{Œª}^‚ä§\\v{Ax}]$\n",
        "$=\\max_{\\v{Œª}‚â•\\v{0}}[-\\max_\\v{y}\\v{Œª}^‚ä§\\v{y}-f(\\v{y})]+[-\\max_\\v{y}(-\\v{A}^‚ä§\\v{Œª})^‚ä§\\v{x}-g(\\v{x})]$\n",
        "$=\\max_{\\v{Œª}‚â•\\v{0}}[-f^*(\\v{Œª})-g^*(-\\v{A}^‚ä§\\v{Œª})]$"
      ],
      "metadata": {
        "id": "WxxrvbtRZpac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 7.5: $\\BC\\max_{\\v{x}‚àà‚Ñù^2,Œæ‚àà‚Ñù}&\\v{p}^‚ä§\\v{x}+Œæ\\\\\n",
        "\\t{subject to}&Œæ‚â•0,x_0‚â§0,x_1‚â§3\\EC$.\n",
        "Let $\\v{z}=\\BM x_0\\\\x_1\\\\Œæ\\EM$ with objective function $\\max\\v{p}^‚ä§\\v{x}+Œæ=\\max\\BM p_0&p_1&1\\EM\\v{z}$\n",
        "$=-\\min\\BM -p_0&-p_1&-1\\EM\\v{z}$.\n",
        "The constraint is $\\BM1&0&0\\\\0&1&0\\\\0&0&-1\\EM\\v{z}‚â§\\BM0\\\\3\\\\0\\EM$.\n",
        "Therefore the linear program standard form is $\\BC-\\min_{\\v{x}‚àà‚Ñù^2,Œæ‚àà‚Ñù}&\\BM -p_0&-p_1&-1\\EM\\v{z}\\\\\n",
        "\\t{subject to}&\\BM1&0&0\\\\0&1&0\\\\0&0&-1\\EM\\v{z}‚â§\\BM0\\\\3\\\\0\\EM\\EC$\n",
        "\n",
        "- 7.8: $\\BC\\min_{\\v{w}‚àà‚Ñù^D}&\\/{1}{2}\\v{w}^‚ä§\\v{w}\\\\\n",
        "\\t{subject to}&\\v{w}^‚ä§\\v{x}‚â•1\\EC$.\n",
        "Lagrangian $\\m{L}(\\v{w},Œª)=\\/{1}{2}\\v{w}^‚ä§\\v{w}-Œª(\\v{w}^‚ä§\\v{x}-1)$.\n",
        "$‚àá_\\v{w}\\m{L}=\\v{w}^‚ä§-Œª\\v{x}^‚ä§=\\v{0}$\n",
        "$‚áí\\v{w}_\\min=Œª\\v{x}$.\n",
        "Dual $\\m{D}(Œª)=\\min_\\v{w}\\m{L}=\\m{L}(\\v{w}_\\min,Œª)$\n",
        "$=\\/{Œª^2}{2}\\v{x}^‚ä§\\v{x}-Œª^2\\v{x}^‚ä§\\v{x}+Œª$\n",
        "$=-\\/{Œª^2}{2}\\v{x}^‚ä§\\v{x}+Œª$.\n",
        "\n",
        "- 7.9: $f(\\v{x})=\\sum_dx_d\\ln x_d=\\sum_df_d(x_d)$.\n",
        "$f^*(\\v{s})=\\sup_\\v{x}\\sum_ds_dx_d-\\sum_dx_d\\ln x_d$\n",
        "$=\\sum_d\\sup_{x_d}‚ü®x_d,s_d‚ü©-x_d\\ln x_d$\n",
        "$=\\sum_df_d^*(s_d)$\n",
        "\n",
        "- 7.10: $f(\\v{x})=\\/{1}{2}\\v{x}^‚ä§\\v{Ax}+\\v{b}^‚ä§\\v{x}+c$.\n",
        "Let gap $g(\\v{x})=\\v{s}^‚ä§\\v{x}-\\/{1}{2}\\v{x}^‚ä§\\v{Ax}-\\v{b}^‚ä§\\v{x}-c$.\n",
        "Then $‚àá_\\v{x}g=\\v{s}^‚ä§-\\v{x}^‚ä§\\v{A}-\\v{b}^‚ä§=\\v{0}$\n",
        "$‚áí\\v{x}=\\v{A}^{-1}(\\v{s}-\\v{b})$.\n",
        "Then $f^*(\\v{s})=\\v{s}^‚ä§\\v{A}^{-1}(\\v{s}-\\v{b})-\\/{1}{2}(\\v{s}-\\v{b})^‚ä§\\v{A}^{-1}(\\v{s}-\\v{b})-\\v{b}^‚ä§\\v{A}^{-1}(\\v{s}-\\v{b})-c$\n",
        "$=\\/{1}{2}(\\v{s}-\\v{b})^‚ä§\\v{A}^{-1}(\\v{s}-\\v{b})-c$."
      ],
      "metadata": {
        "id": "diDjuqA_-xQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function**: given supervised training set $\\{(\\v{x}_1,y_1),...,(\\v{x}_N,y_N)\\}$, we wish to obtain $\\v{Œ∏}^*$ for predictor $f$ such that the output $\\hat{y}=f(\\v{x}_n,\\v{Œ∏}^*)=\\v{Œ∏}^{*‚ä§}\\v{X}‚âày_n$. Let $\\v{X}:=[\\v{x}_1,...,\\v{x}_N]^‚ä§‚àà‚Ñù^{N√óD}$, labels $\\v{y}:=[y_1,...,y_n]^‚ä§‚àà‚Ñù^N$, and loss function $l(y_n,\\hat{y}_n)$, the empirical risk (or average empirical loss) is $\\v{R}_\\t{emp}(f,\\v{X},\\v{y})=\\/{1}{N}\\sum_il(y_n,\\hat{y}_n)$ whereas if infinite data is available then $\\v{R}_\\t{true}(f)=\\E_{\\v{x},\\v{y}}[l(y,f(\\v{x}))]$.\n",
        "\n",
        "- **Least square loss**: $l(y_n,\\hat{y}_n)=(y_n-\\hat{y}_n)^2$. We're solving $\\min_\\v{Œ∏}\\/{1}{N}\\n{\\v{y}-\\v{XŒ∏}}^2$.\n",
        "\n",
        "- **Regularization**: to prevent overfitting, we solve $\\min_\\v{Œ∏}\\/{1}{N}\\n{\\v{y}-\\v{XŒ∏}}^2+Œª\\vn{Œ∏}^2$, which adds a Lagrangian constraint to keep $\\v{Œ∏}$ close to the origin.\n",
        "\n",
        "- **Cross validation**: $K$-fold validation partitions data $\\m{D}$ into $K$ chunks, $K-1$ of which form the training set $\\m{R}$ and last chunk serves as validation set $\\m{V}$ such that $\\m{D}=\\m{R}‚à™\\m{V}$ and $\\m{R}‚à©\\m{V}=‚àÖ$. For each partition $k=1,...,K$, we use $\\m{R}^{(k)}$ to produce $f^{(k)}$ and compute empirical risk $R(f^{(k)},\\m{V}^{(k)})$. Cross validation approximates the expected generalization error $\\E_\\m{V}[R(f,\\m{V})]‚âà\\/{1}{K}\\sum_{k=1}^KR(f^{(k)},\\m{V}^{(k)})$"
      ],
      "metadata": {
        "id": "mdgNgzAMc0Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Linear Regression"
      ],
      "metadata": {
        "id": "SnCzrLICYbWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression**: $p(y|\\v{x})=\\Normal(y|f(\\v{x}),œÉ^2)$ where $\\v{x}‚àà‚Ñù^D$, $y=f(\\v{x})+œµ$, and $œµ‚àº\\Normal(0,œÉ^2)$ iid. The objective is to find a function similar to the unknown $f$ that generated the data. **Linear regression** uses linear parameters $\\v{Œ∏}$ in the model $y=\\v{œï}(\\v{x})^‚ä§\\v{Œ∏}+œµ$ where $œµ‚àº\\Normal(0,œÉ^2)$ iid is the only source of uncertainty. Training set $\\{(\\v{x}_1,y_1),...,(\\v{x}_N,y_N)\\}$ is split into $\\v{y}=[y_1,...,y_N]^‚ä§‚àà‚Ñù^N$ and $\\v{X}=[\\v{x}_1,...,\\v{x}_N]^‚ä§‚àà‚Ñù^{N√óD}$.\n",
        "\n",
        "- $p(\\v{y}|\\v{X},\\v{Œ∏})=\\prodl_{n=1}^N\\Normal(y_n|\\green{\\v{x}_n^‚ä§\\v{Œ∏}},œÉ^2)$ where $\\v{Œ∏}‚àà‚Ñù^D$ has negative log-likelihood $l(\\v{Œ∏})=-\\ln p(\\v{y}|\\v{X},\\v{Œ∏})$. The MLE is $\\blue{\\v{Œ∏}_\\t{MLE}‚àà\\arg\\max_\\v{Œ∏}p(\\v{y}|\\v{X},\\v{Œ∏})=\\arg\\min_\\v{Œ∏}l(\\v{Œ∏})}$.\n",
        "\n",
        "  - $l(\\v{Œ∏})=\\/{1}{2œÉ^2}\\sum_n(y_n-\\v{x}_n^‚ä§\\v{Œ∏})^2$\n",
        "  $=\\/{1}{2œÉ^2}(\\v{y}-\\v{XŒ∏})^‚ä§(\\v{y}-\\v{XŒ∏})$\n",
        "  $=\\/{1}{2œÉ^2}\\green{\\n{\\v{y}-\\v{XŒ∏}}^2}$ thus the MLE is also the least minimum squares error solution.\n",
        "\n",
        "  - $‚àá_\\v{Œ∏}l(\\v{Œ∏})=\\/{1}{2œÉ^2}2(\\v{y}-\\v{XŒ∏})^‚ä§(-\\v{X})$\n",
        "  $=\\/{-1}{œÉ^2}(\\v{y}^‚ä§\\v{X}-\\v{Œ∏}^‚ä§\\v{X}^‚ä§\\v{X})=\\v{0}$\n",
        "  $‚áí(\\v{X}^‚ä§\\v{X})\\v{Œ∏}_\\t{MLE}=\\v{X}^‚ä§\\v{y}$.\n",
        "  Solving this system of equations gives the normal equation\n",
        "  $\\red{\\v{Œ∏}_\\t{MLE}=(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§\\v{y}}$ with requirement $\\blue{\\rk(\\v{X})=D}$.\n",
        "  Then $l(\\v{Œ∏})$ is convex, $‚àá_\\v{Œ∏}^2l(\\v{Œ∏})=\\v{X}^‚ä§\\v{X}‚àà‚Ñù^{D√óD}$ is positive definite and therefore invertible.\n",
        "\n",
        "- With features $p(\\v{y}|\\v{X},\\v{Œ∏})=\\prodl_{n=1}^N\\Normal(y_n|\\green{\\v{œï}(\\v{x}_n)^‚ä§\\v{Œ∏}},œÉ^2)$ where $\\v{Œ∏}‚àà‚Ñù^K$.\n",
        "Non-linear transformations are included in the **feature vector**\n",
        "$\\v{œï}(\\v{x}_n)=\\BMœï_0(\\v{x}_n)\\\\\\vdots\\\\œï_{K-1}(\\v{x}_n)\\EM$\n",
        "and the **feature matrix**\n",
        "$\\v{Œ¶}:=\\BMœï_0(\\v{x}_1)&...&œï_{K-1}(\\v{x}_1)\\\\\\vdots&\\ddots&\\vdots\\\\œï_0(\\v{x}_N)&...&œï_{K-1}(\\v{x}_N)\\EM‚àà‚Ñù^{N√óK}$.\n",
        "\n",
        "  - $l(\\v{Œ∏})=\\/{1}{2œÉ^2}\\sum_n(y_n-\\v{œï}(\\v{x}_n)^‚ä§\\v{Œ∏})^2$\n",
        "  $=\\/{1}{2œÉ^2}\\sum_n(\\v{y}-\\v{Œ¶Œ∏})^‚ä§(\\v{y}-\\v{Œ¶Œ∏})$\n",
        "  $=\\/{1}{2œÉ^2}\\green{\\n{\\v{y}-\\v{Œ¶Œ∏}}^2}$. Similarly to the simple linear regression, $\\red{\\v{Œ∏}_\\t{MLE}=(\\v{Œ¶}^‚ä§\\v{Œ¶})^{-1}\\v{Œ¶}^‚ä§\\v{y}}$\n",
        "  with requirement\n",
        "  $\\blue{\\rk(\\v{Œ¶})=K}$.\n",
        "\n",
        "- $œÉ_\\t{MLE}^2$ uses $l(œÉ^2)=\\/{N}{2}\\ln œÉ^2+\\/{1}{2œÉ^2}\\n{\\v{y}-\\v{Œ¶Œ∏}}^2$.\n",
        "Then $\\/{‚àÇl}{‚àÇœÉ^2}=\\/{N}{2œÉ^2}-\\/{\\n{\\v{y}-\\v{Œ¶Œ∏}}^2}{2œÉ^4}=0$\n",
        "$‚áí\\red{œÉ_\\t{MLE}^2=\\/{1}{N}\\n{\\v{y}-\\v{Œ¶Œ∏}_\\t{MLE}}^2}$\n",
        "is the mean square error (MSE) of $\\v{Œ¶Œ∏}_\\t{MLE}$ against observed $\\v{y}$. As a noise variance estimator it underestimates the true variance because of $\\v{Œ∏}_\\t{MLE}$ over-fitting: $œÉ_\\t{unbiased}^2=\\/{1}{N-K}\\n{\\v{y}-\\v{Œ¶Œ∏}_\\t{MLE}}^2$."
      ],
      "metadata": {
        "id": "OCvp9j2RHSfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting**: $\\t{MSE}(\\v{Œ∏}^*)=\\E[(\\v{Œ∏}^*-\\v{Œ∏}_\\t{true})^2]=\\V(\\v{Œ∏}^*)+\\t{Bias}(\\v{Œ∏}^*)^2$ where $\\t{Bias}(\\v{Œ∏}^*)=\\E[\\v{Œ∏}^*]-\\v{Œ∏}_\\t{true}$.\n",
        "\n",
        "- If $\\v{Œ∏}^*$ degree is too low, the model is biased away from the truth with high systematic error no matter how much data it receives in the training set. If $\\v{Œ∏}^*$ degree is too high, the model memorizes the noise in the training set, and results in high variations when given a new training set.\n",
        "\n",
        "- **Maximum a posteriori**: Parameter magnitude tends to be larger with overfitting. Using prior $p(\\v{Œ∏})$ anchors a parameter. Then posterior $p(\\v{Œ∏}|\\v{X},\\v{y})$ gives the MAP estimator, which is a compromise between prior $p(\\v{Œ∏})$ and MLE from likelihood $p(\\v{y}|\\v{X},\\v{Œ∏})$. The MAP is $\\blue{\\v{Œ∏}_\\t{MAP}‚àà\\arg\\max_\\v{Œ∏}p(\\v{y}|\\v{X},\\v{Œ∏})p(\\v{Œ∏})=\\arg\\min_\\v{Œ∏}[-\\ln p(\\v{y}|\\v{X},\\v{Œ∏})-\\ln p(\\v{Œ∏})]}$\n",
        "\n",
        "  - Let $p(\\v{Œ∏})=\\Normal(\\v{0},b^2\\v{I})$ be the parameters prior. Solve $\\min_\\v{Œ∏}l(\\v{Œ∏})$ where\n",
        "  $\\green{l(\\v{Œ∏})=\\/{1}{2œÉ^2}\\n{\\v{y}-\\v{Œ¶Œ∏}}^2+\\/{1}{2b^2}\\vn{Œ∏}^2}$.\n",
        "  $‚àá_\\v{Œ∏}l(\\v{Œ∏})=\\/{1}{œÉ^2}(\\v{y}-\\v{Œ¶Œ∏})^‚ä§(-\\v{Œ¶})+\\/{1}{b^2}\\v{Œ∏}^‚ä§=\\v{0}$\n",
        "  $‚áí\\/{1}{œÉ^2}\\v{Œ¶}^‚ä§\\v{y}=\\/{1}{b^2}\\v{Œ∏}+\\/{1}{œÉ^2}\\v{Œ¶}^‚ä§\\v{Œ¶Œ∏}$\n",
        "  $‚áí\\v{Œ∏}=\\/{1}{œÉ^2}(\\/{1}{b^2}\\v{I}+\\/{1}{œÉ^2}\\v{Œ¶}^‚ä§\\v{Œ¶})^{-1}\\v{Œ¶}^‚ä§\\v{y}$\n",
        "  $‚áí\\red{\\v{Œ∏}_\\t{MAP}=(\\v{Œ¶}^‚ä§\\v{Œ¶}+\\blue{\\/{œÉ^2}{b^2}\\v{I}})^{-1}\\v{Œ¶}^‚ä§\\v{y}}$.\n",
        "  \n",
        "  - $\\v{Œ¶}^‚ä§\\v{Œ¶}$ is positive-semidefinite. Therefore $\\v{Œ¶}^‚ä§\\v{Œ¶}+\\/{œÉ^2}{b^2}\\v{I}$ is strictly positive-definite and invertible.\n",
        "\n",
        "- **Regularization**: loss function $\\n{\\v{y}-\\v{Œ¶Œ∏}}_2^2+Œª\\vn{Œ∏}_p^p$. The first term is the data-fit (misfit) term. The second term is the regularizer where parameter $Œª$ is the shadow price of the regularization. Smaller values of $p$ lead to sparser $\\v{Œ∏}^*$ with higher frequency of $Œ∏_d=0$.\n",
        "\n",
        "  - **Ridge regression** (Gaussian prior): Using $Œª\\vn{Œ∏}_2^2$ leads to $\\v{Œ∏}^*=(\\v{Œ¶}^‚ä§\\v{Œ¶}+Œª\\v{I})^{-1}\\v{Œ¶}^‚ä§\\v{y}$, which is identical to prior $p(\\v{Œ∏})=\\Normal(\\v{0},\\/{œÉ^2}{Œª}\\v{I})$ where $Œª=\\/{œÉ^2}{b^2}$.\n",
        "\n",
        "  - **Lasso regression** (Laplace prior): Using $Œª\\vn{Œ∏}_1$ corresponds to prior\n",
        "  $p(\\v{Œ∏})=\\prodl_{j=1}^D\\/{1}{2b}\\e{-\\/{|Œ∏_j|}{b}}$ and leads to\n",
        "  $\\green{l(\\v{Œ∏})=\\/{1}{2œÉ^2}\\n{\\v{y}-\\v{Œ¶Œ∏}}_2^2+\\/{1}{b}\\vn{Œ∏}_1}$ where $Œª=\\/{œÉ^2}{b}$."
      ],
      "metadata": {
        "id": "bQ_eQOdYNRHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bayesian linear regression** does not compute $\\v{Œ∏}^*$ and instead relies on full posterior."
      ],
      "metadata": {
        "id": "c92OsYU-Nd7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Flops Count"
      ],
      "metadata": {
        "id": "Mptw5z72zfPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix structure**: speed of an optimization solver is determined by how fast it can solve $Ax=b$.\n",
        "Exploitable characteristics include sparsity, diagonality, and structures such as Toeplitz, Hankel, etc.\n",
        "**Flop** is one addition, subtraction, multiplication, or division of two floating point numbers. We take leading polynomial time to determine $\\m{O}(‚ãÖ)$ asymptotics.\n",
        "\n",
        "- Vector operations: $\\BC\n",
        "x^‚ä§y&2n\\\\\n",
        "x+y&n\\\\\n",
        "Œ±x&n\n",
        "\\EC$. If $x$ is sparse with $N$ non-zero elements then $x^‚ä§y$ costs $\\purple{2N}$.\n",
        "\n",
        "- Matrix-vector: $y=Ax,\\ A‚àà‚Ñù^{m√ón}$ costs $\\purple{2mn}$. If $A$ is diagonal then cost is $\\purple{n}$. If $A$ is sparse with $N$ non-zero elements then cost is $\\purple{2N}$.\n",
        "\n",
        "  - IF $\\rk(A)=p‚â™\\min(m,n)$ and is stored as $A=UV,\\ U‚àà‚Ñù^{m√óp},V‚àà‚Ñù^{p√ón}$, Then $Vx$ costs $\\purple{2pn}$ and $U(Vx)$ costs $\\purple{2mp}$. Total cost $\\purple{2p(m+n)}< 2mn$.\n",
        "\n",
        "- Matrix-matrix: $C=AB,\\ A‚àà‚Ñù^{m√ón},B‚àà‚Ñù^{n√óp}$. Then each element of $C‚àà‚Ñù^{m√óp}$ is dot product, total cost $\\purple{2nmp}$. If $C‚àà‚Ñù^{m√óm}$ is symmetric, then total cost $\\purple{m^2n}$.\n",
        "\n",
        "**Solving $Ax=b$** with generic Gaussian elimination costs $\\purple{n^3}$\n",
        "\n",
        "- **Diagonal** nonsingular $A‚àà‚Ñù^{n√ón}$ the solution is $x_i=\\/{b_i}{a_{ii}}$ with cost $\\purple{n}$.\n",
        "\n",
        "- **Lower triangular** nonsingular $A‚àà‚Ñù^{n√ón}$, by **forward substitution** $x_1=\\/{b_1}{a_{11}}$, $x_2=\\/{b_2-a_{21}x_1}{a_{22}}$, and $x_i=\\/{b_i-\\sum_{j=1}^{i-1}a_{ij}x_j}{a_{ii}}$ with cost $\\sum_{i=1}^n(2i-1)=1+3+...+(2n-1)=\\purple{n^2}$\n",
        "\n",
        "  - **Sparsity**: if $A$ has at most $k$ non-zero enetries per row, then substitution costs $\\purple{2kn}$.\n",
        "\n",
        "  - **Banded**: $a_{ij}=0,\\ ‚àÄ|i-j|>k$ only non-zero near diagonal\n",
        "\n",
        "  - **Upper triangular** nonsingular $A‚àà‚Ñù^{n√ón}$, by backward substitution costs are the same as lower triangular.\n",
        "\n",
        "- **Orthogonal matrix** $A^‚ä§A=I‚àà‚Ñù^{n√ón}$. Then $x=A^‚ä§b$. Total cost $\\purple{2n^2}$.\n",
        "\n",
        "  - Householder: If $A=I-2uu^‚ä§,\\ \\n{u}_2=1$ then $x=b-2(u^‚ä§b)u$ costing $\\purple{4n}$.\n",
        "\n",
        "- **Permutation matrix**: Let $p$ be a permutation of $(1,...,n)$, and $A_{ij}=I_{j=p_i}$ has one '1' entry per row and column, and 0 everywhere else.\n",
        "Total cost $0$.\n",
        "\n",
        "- **Factor-Solve method**: Recursively factor $A=A_1...A_k$ and solve $x=A_k^{-1}...A_1^{-1}b$. The factorization cost $\\purple{f}$ dominates the solve cost. So total cost is $\\purple{kf}$. Savings stack up if there are multiple RHS $b$-s to solve for.\n",
        "\n",
        "  - This is used for finding $A^{-1}=\\BM x_1&...&x_n\\EM$ where the problem is $Ax_i=e_i,\\ i=1,...,n$ with $n$ RHS."
      ],
      "metadata": {
        "id": "eRSFMx0szhFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LU factorization**: $A‚àà‚Ñù^{n√ón}$ decomposed via Gaussian elimination $\\red{A=PLU}$ where $P$ is permutation matrix reordering rows to ensure numerical stability, $L$ has '1' on the diagonal, and $U$ is nonsingular.\n",
        "Generic matrix costs $\\purple{\\/{2}{3}n^3}$.\n",
        "\n",
        "- $Ax=b$: $Pz_1=b$, $Lz_2=z_1$, $Ux=z_2$.\n",
        "Total cost $\\purple{\\/{2}{3}n^3+2n^2}$.\n",
        "\n",
        "- $Ax_i=b_i,\\ i=1,...,m$: total cost $\\purple{\\/{2}{3}n^3+2mn^2}$.\n",
        "\n",
        "- **Banded**: $L,U$ also banded. Factorization costs $\\purple{4nk^2}$. Solving costs $\\purple{6nk}$.\n",
        "\n",
        "- **Sparse**: preserve sparsity with $A=P_1LUP_2$ where $P_1,P_2$ are permutation matrices.\n",
        "\n",
        "**Cholesky factorization**: Positive-definite $A‚àà\\v{S}_{++}^n$ can be decomposed $\\red{A=LL^‚ä§}$ where $L$ is nonsingular with positive diagonals. Generic matrix costs $\\purple{\\/{1}{3}n^3}$.\n",
        "\n",
        "- $Ax=b$: $Lz=b$, $L^‚ä§x=z$. Total cost $\\purple{\\/{1}{3}n^3+2n^2}$.\n",
        "\n",
        "- **Banded**: $L$ also banded with $k$. Factorization costs $\\purple{nk^2}$. Solving costs $\\purple{4nk}$.\n",
        "\n",
        "- **Sparse**: $\\blue{A=PLL^‚ä§P^‚ä§}$. Permutation $P^‚ä§AP=LL^‚ä§$ is determined by the sparsity pattern of $A$.\n",
        "\n",
        "**$LDL^‚ä§$ factorization**: Symmetric nonsingular $A‚àà\\v{S}^n$ can be decomposed $\\red{A=PLDL^‚ä§P^‚ä§}$. Cholesky is special case with $P=D=I$. Generic matrix costs $\\purple{\\/{1}{3}n^3}$."
      ],
      "metadata": {
        "id": "8p7q7p7yJsKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block elimination (Schur complement)**: Let $x=\\BM x_1\\\\x_2\\EM,\\ x_1‚àà‚Ñù^{n_1},x_2‚àà‚Ñù^{n_2}$.\n",
        "Then $Ax=b‚áí\\BM A_{11}&A_{12}\\\\A_{21}&A_{22}\\EM\\BM x_1\\\\x_2\\EM=\\BM b_1\\\\b_2\\EM$.\n",
        "We solve $x_2$ first because $A_{11}‚âª0$ is easy to invert (diagonal or banded).\n",
        "$x_1=A_{11}^{-1}(b_1-A_{12}x_2)$\n",
        "$‚áíA_{22}x_2=b_2-A_{21}x_1$\n",
        "$=b_2-A_{21}A_{11}^{-1}(b_1-A_{12}x_2)$\n",
        "$‚áí\\green{(A_{22}-A_{21}A_{11}^{-1}A_{12})x_2=b_2-A_{21}A_{11}^{-1}b_1}$.\n",
        "Then substitute back to solve $x_1$.\n",
        "Matrix $\\red{S=A_{22}-A_{21}A_{11}^{-1}A_{12}}$ is **Schur complement** of $A_{11}$ in $A$ satisfying $S$ nonsingular $‚áîA$ nonsingular.\n",
        "\n",
        "- **Block LU** $\\blue{\\BM A_{11}&A_{12}\\\\A_{21}&A_{22}\\EM=\\BM A_{11}&0\\\\A_{21}&S\\EM\\BM I&A_{11}^{-1}A_{12}\\\\0&I\\EM}$.\n",
        "$\\BM A_{11}&A_{12}\\\\A_{21}&A_{22}\\EM\\BM x_1\\\\x_2\\EM=\\BM b_1\\\\b_2\\EM‚áí\\BM A_{11}&0\\\\A_{21}&S\\EM\\BM z_1\\\\z_2\\EM=\\BM b_1\\\\b_2\\EM‚áí\\BM I&A_{11}^{-1}A_{12}\\\\0&I\\EM\\BM x_1\\\\x_2\\EM=\\BM z_1\\\\z_2\\EM$\n",
        "\n",
        "- Let $\\purple{f}$ be cost of factoring $A_{11}$, and $\\purple{s}$ be cost of solving $A_{11}$. Calculating $A_{11}^{-1}A_{12}$ and $A_{11}^{-1}b_1$ costs $\\purple{f+n_2s}$. Calculating $S=A_{22}-A_{21}A_{11}^{-1}A_{12}$ costs $\\purple{2n_2^2n_1}$. Solving $Sx_2=b_2-A_{21}A_{11}^{-1}b_1$ costs $\\purple{\\/{2}{3}n_2^3}$. Final back substitution to calculate $x_1$ is negligible.\n",
        "Therefore total cost to solve $\\BM x_1\\\\x_2\\EM$ is $\\purple{f+n_2s+2n_2^2n_1+\\/{2}{3}n_2^3}$."
      ],
      "metadata": {
        "id": "vVqg0YuGyVTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Convex Sets"
      ],
      "metadata": {
        "id": "3XRLm8J7Zd27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Affine sets**: let $\\green{\\v{1}^‚ä§Œ∏=1}$ and $\\green{Œ∏‚àà‚Ñù^k}$, then $\\blue{Œ∏^‚ä§x}$ is an **affine combination** of the points $x_1,...,x_k$. Set $C$ is called an affine set if $x_1,...,x_k‚ààC$ then all affine combinations of them are in $C$.\n",
        "\n",
        "- Set $C$ is affine if $x_1,x_2‚ààC$ then the straight line through them $Œ∏x_1+(1-Œ∏)x_2‚ààC$ for $Œ∏‚àà‚Ñù$.\n",
        "\n",
        "- If $C$ is an affine set, then for $x_0‚ààC$ the set $V=C-x_0=\\{x-x_0|x‚ààC\\}$ is a subspace: $x_0‚ààC$ gets mapped to origin $0‚ààV$, and the affine set is translated to pass through origin. Thus $C=V+x_0=\\{v+x_0|v‚ààV\\}$.\n",
        "\n",
        "  - Proof: Suppose $v_1,v_2‚ààV$ and $a,b‚àà‚Ñù$. Then $V$ is a subspace $‚áî\\green{av_1+bv_2‚ààV}‚áîav_1+bv_2+x_0‚ààC$.\n",
        "  RHS\n",
        "  $av_1+bv_2+x_0$\n",
        "  $=a(v_1+x_0)+b(v_2+x_0)+(1-a-b)x_0$ is an affine combination of $v_1+x_0‚ààC$, $v_2+x_0‚ààC$, and $x_0‚ààC$ because $a+b+(1-a-b)=1$. Therefore $av_1+bv_2+x_0‚ààC$ and $V$ is a subspace.\n",
        "\n",
        "  - $\\dim(C)=\\dim(V)$ for affine set $C$ and subspace $V=C-x_0$. Dimension of a subspace is the number of spanning basis vectors.\n",
        "\n",
        "- **Affine hull** $\\t{aff}(C):=\\{Œ∏^‚ä§x|x_i‚ààC,\\v{1}^‚ä§Œ∏=1,Œ∏‚àà‚Ñù^k\\}$ is the smallest affine set containing an arbitrary set $C$: if $S$ is any affine set satisfying $C‚äÜS$, then $\\t{aff}(C)‚äÜS$.\n",
        "\n",
        "  - **Affine dimension** of $C$ is the dimension of $\\t{aff}(C)$.\n",
        "\n",
        "  - **Relative interior** $\\t{relint}(C):=\\{x‚ààC|B(x,r)‚à©\\t{aff}(C)‚äÜC\\t{ for some }r>0\\}$. If the affine dimension of $C$ is less than $n$ i.e., flat affine hull, then its volume is 0 and $\\t{int}(C)=‚àÖ$. $B(x,r)‚à©\\t{aff}(C)$ refers to a slice of a ball (with parts sticking out into the empty space shaved off) in the flat affine hull.\n",
        "\n",
        "  - **Relative boundary**: $\\t{cl}(C)\\backslash\\t{relint}(C)$, where the closure $\\t{cl}(C)$ refers to $C$ plus all its limit edge points.\n",
        "\n",
        "- 2.1 (solution set of linear equations): $C=\\{x|Ax=b\\}$ where $A‚àà‚Ñù^{m√ón}$ and $b‚àà‚Ñù^m$ is an affine set. Suppose $x_1,x_2‚ààC$, then $A(Œ∏x_1+(1-Œ∏)x_2)=Œ∏Ax_1+(1-Œ∏)Ax_2=Œ∏b+(1-Œ∏)b=b$\n",
        "$‚áíŒ∏x_1+(1-Œ∏)x_2‚ààC$.\n",
        "\n",
        "  - Every affine set can be expressed as a solution set of a system of linear equations.\n",
        "\n",
        "**Convex sets**: let $\\green{\\v{1}^‚ä§Œ∏=1}$ and $\\green{Œ∏‚âΩ0}$, the sum $\\blue{Œ∏^‚ä§x}$ is a **convex combination** of points $x_1,...,x_k$. Suppose $p:‚Ñù^n‚Üí‚Ñù$ satisfies $p(x)‚â•0$ and $‚à´_Cp(x)\\ dx=1$, then $\\E[x]=‚à´_Cxp(x)\\ dx$ is a convex combination. Set $C$ is called a convex set if $x_1,...,x_k‚ààC$ then all convex combinations of them are in $C$.\n",
        "\n",
        "- **If $x_1,x_2‚ààC$ then a line segment between them $\\red{Œ∏x_1+(1-Œ∏)x_2‚ààC}$ for $Œ∏‚àà[0,1]$**. Where affine set must extend infinitely, a convex set can be bounded. Every pair of two points in the set have a clear line of sight to each other.\n",
        "\n",
        "- **Convex hull** $\\t{conv}(C):=\\{Œ∏^‚ä§x|x_i‚ààC,\\v{1}^‚ä§Œ∏=1,Œ∏‚âΩ0\\}$ is the smallest convex set that contains arbitrary set $C$: if $S$ is any convex set satisfying $C‚äÜS$, then $\\t{conv}(C)‚äÜS$.\n",
        "\n",
        "**Convex cone**: Set $C$ is called a cone if it is nonnegative homogeneous: for every $x‚ààC$ and $Œ∏‚â•0$, we have $Œ∏x‚ààC$. Set $C$ is a convex cone if it is a convex set and a cone.\n",
        "\n",
        "- let $\\green{Œ∏‚âΩ0}$, then $\\blue{Œ∏^‚ä§x}$ is called **conic combination** (nonnegative linear combination) of $x_1,...,x_k$. Set $C$ is a called convex cone if $x_1,...,x_k‚ààC$ then all conic combinations of them are in $C$.\n",
        "\n",
        "- **Conic hull** $\\{Œ∏^‚ä§x|x_i‚ààC,Œ∏‚âΩ0\\}$ is the smallest conic cone that contains an arbitrary set $C$"
      ],
      "metadata": {
        "id": "k6j8We4TZiiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important examples**:\n",
        "\n",
        "- $‚àÖ$, $\\{x_0\\}$, and $‚Ñù^n$ are affine and convex subsets of $‚Ñù^n$.\n",
        "\n",
        "- A line is affine. If it passes through 0 it is a subspace and a convex cone.\n",
        "\n",
        "  - A line segment is convex, but not affine (unless it has 0 length).\n",
        "\n",
        "  - A ray $\\{x_0+Œ∏v|Œ∏‚â•0,v\\neq0\\}$ is convex but not affine. It is a convex cone if $x_0=0$.\n",
        "\n",
        "- A subspace is affine and convex cone.\n",
        "\n",
        "**Hyperplane**: $\\red{\\{x|a^‚ä§x=b\\}=\\{x|a^‚ä§(x-x_0)=0\\}}=x_0+a^\\perp$ where $a^\\perp=\\{v|a^‚ä§v=0\\}$ is orthogonal complement. A hyperplane has dimension $n-1$, is defined by normal $a‚àà‚Ñù^n$ where the inner product is 0, and is offset from the origin determined by $b=a^‚ä§x_0‚àà‚Ñù$. Hyperplane can also be defined as $\\{Fu+g|u‚àà‚Ñù^m\\}$ where $g$ is a particular solution and $Fu$ is the homogenous solution with direction $F$. Hyperplane is affine and convex.\n",
        "\n",
        "- $a^‚ä§x_0$ is the magnitude of $\\n{a}\\t{proj}_a(x_0)$.\n",
        "Set $\\{x|a^‚ä§x=b\\}$ contains all points that project onto $a$ with magnitude $\\/{b}{\\n{a}}$.\n",
        "$\\blue{\\t{dist}(x_0,C)=b-a^‚ä§x_0}$.\n",
        "Here $a$ is the normal, and $b$ is the original distance (along vector $a$).\n",
        "\n",
        "- $\\blue{x_0+a^\\perp}$: Let $b=a^‚ä§x_0$. Then hyperplane $a^‚ä§x=b$ passes through $x_0$ and has normal $a$.\n",
        "\n",
        "- **Halfspace**: $\\red{\\{x|a^‚ä§x‚â§b\\}}$ extends in the direction $-a$, whereas $\\{x|a^‚ä§x‚â•b\\}$ extends in the direction of $a$. Halfspace is convex, but not affine.\n",
        "\n",
        "**Polyhedron**: $\\red{\\{x|Ax‚âºb,Cx=d\\}}=\\{x|a_i^‚ä§x‚â§b_i,i=1,...,m,c_j^‚ä§x=d_j,j=1,...,p\\}$ is the intersection of a finite number of halfspaces and hyperplanes.\n",
        "\n",
        "- **Simplex**: Let $v_0,...,v_k‚àà‚Ñù^n$ be affinely independent so that $v_1-v_0,...,v_k-v_0$ are linearly independent. Then the $k$-dimensional simplex is $\\t{conv}\\{v_0,...,v_k\\}=\\{Œ∏^‚ä§v|\\v{1}^‚ä§Œ∏=1,Œ∏‚âΩ0\\}$, and is equivalent to a polyhedron.\n",
        "\n",
        "  - Simplex is the simplest possible shape in the $‚Ñù^k$, a convex hull of $k+1$ grid points. In $‚Ñù^0$ it is a point. In $‚Ñù^1$ it is a line connecting 2 vertices. In $‚Ñù^2$ it is a triangle connecting 3 vertices. In $‚Ñù^3$ it is a tetrahedron connecting 4 vertices.\n",
        "\n",
        "  - Simplex‚ÜíPolyhedron: Define vector $y=(Œ∏_1,...,Œ∏_k)$ and matrix $B=[v_1-v_0,...,v_k-v_0]‚àà‚Ñù^{n√ók}$. Then\n",
        "  $\\BC\n",
        "  By=Œ∏^‚ä§v-Œ∏_0v_0-v_0\\v{1}^‚ä§y=Œ∏^‚ä§v-Œ∏_0v_0-v_0(1-Œ∏_0)‚áíŒ∏^‚ä§v=v_0+By \\\\\n",
        "  \\t{conv}\\{v_0,...,v_k\\}=\\{x|x=v_0+By,y‚âΩ0,\\v{1}^‚ä§y+Œ∏_0=1\\}\\EC$. Because $\\rk(B)=k$, there exists $A=\\BM A_1\\\\A_2\\EM‚àà‚Ñù^{n√ón}$ such that $AB=\\BM I_k\\\\0_{n-k}\\EM‚àà‚Ñù^{n√ók}$\n",
        "  and\n",
        "  $\\BC\n",
        "  A_1x=A_1v_0+A_1By=A_1v_0+y&‚áíy=A_1x-A_1v_0\\\\\n",
        "  A_2x=A_2v_0+A_2By=A_2v&‚áíA_2x=A_2v_0\n",
        "  \\EC$.\n",
        "  The simplex can be written in polyhedron form $\\{x|A_2x=A_2v_0,A_1x‚â•A_1v_0,\\v{1}^‚ä§A_1x‚â§1+\\v{1}^‚ä§A_1v_0\\}$\n",
        "\n",
        "- **Probability simplex** $\\blue{\\{x‚àà‚Ñù^n|x‚âΩ0,\\v{1}^‚ä§x=1\\}}$ is **$‚Ñù^{n-1}$ convex hull of $‚Ñù^n$ unit vectors** $\\t{conv}\\{e_1,...,e_n|e_i‚ààI_n\\}$. Each vector $x=(x_1,...,x_n)=x_1e_1+...+x_ne_n$ in the probability simplex is a discrete probability distribution.\n",
        "\n",
        "  - Probability simplex is a convex hull. In $‚Ñù^1$ it is a line segment connecting vertices $(1,0)$ and $(0,1)$. In $‚Ñù^2$ it is the triangle connecting vertices $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$\n",
        "\n",
        "- The convex hull of set $\\{v_1,...,v_k\\}$ is $\\t{conv}(v_1,...,v_k)=\\{Œ∏^‚ä§v|\\v{1}^‚ä§Œ∏=1,Œ∏‚âΩ0\\}$ written in its generator form. However it is difficult to express it in linear constraints form unless it is simplex or a unit-ball.\n",
        "\n",
        "  - $‚Ñì_‚àû$-norm $\\n{x}_‚àû:=\\max_{x_i}\\{|x_i||x_i‚ààx\\}$\n",
        "\n",
        "- **Every linear inequality defines a halfspace ($a^‚ä§x‚â§b$) or a polyhedron ($Ax‚âºb$).**\n",
        "\n",
        "**Euclidean/norm ball**: $\\red{B(x_c,r)=\\{x|\\n{x-x_c}_2‚â§r\\}}=\\{x|(x-x_c)^‚ä§(x-x_c)‚â§r^2\\}=\\{x_c+u|\\ \\n{u}_2‚â§r\\}$.\n",
        "\n",
        "- $B(x_c,r)$ is a convex set.\n",
        "\n",
        "  - Proof: $\\n{x_1-x_c}‚â§r$ and $\\n{x_2-x_c}‚â§r$ then by triangle inequality\n",
        "  $\\n{Œ∏x_1+(1-Œ∏)x_2-x_c}$\n",
        "  $=\\n{Œ∏(x_1-x_c)+(1-Œ∏)(x_2-x_c)}$\n",
        "  $‚â§Œ∏\\n{(x_1-x_c)}+(1-Œ∏)\\n{(x_2-x_c)}$\n",
        "  $‚â§r$\n",
        "\n",
        "- **Ellipsoid**: $\\red{\\{x|(x-x_c)^‚ä§P(x-x_c)‚â§1\\}}$ where $P‚àà‚Ñù^{n√ón}_{++}$ (symmetric positive-definite), where the axes of the ellipsoid are the eigenvectors of $P$ and stretched by magnitude equal to the sqrt eigenvalues of $P$.\n",
        "\n",
        "- **Second-order norm cone**: $\\{(x,t)|\\n{x}‚â§t\\}‚äÜ‚Ñù^{n+1}$ is a convex cone. This is the definition of $\\epi(\\n{x})$. Therefore $f(x)=Œ±\\n{x}$ describes the \"skin\" of a cone whose height is scaled by $Œ±$.\n",
        "\n",
        "  - The **second-order cone** is the $‚Ñì_2$-norm cone $\\{(x,t)|\\n{x}_2‚â§t\\}=\\{\\BM x\\\\t\\EM|\\BM x\\\\t\\EM^‚ä§\\BM I&0\\\\0&-1\\EM\\BM x\\\\t\\EM‚â§0, t‚â•0\\}$. It is the epigraph of the Euclidean norm.\n",
        "\n",
        "**Proper cones**: Convex cone $K‚äÜ‚Ñù^n$ is proper if\n",
        "\n",
        "  - $\\BC\n",
        "  1.&K\\t{ is closed}\\\\\n",
        "  2.&K\\t{ is solid}\\\\\n",
        "  3.&K\\t{ is pointed (contains no line)}\\EC$\n",
        "\n",
        "- Nonnegative orthant $‚Ñù_+^n=\\{x‚àà‚Ñù^n|X_i‚â•0,i=1,...,n\\}$\n",
        "\n",
        "- Positive semidefinite cones: $\\v{S}^n=\\{X‚àà‚Ñù^{n√ón}|X=X^‚ä§\\}$ are symmetric matrices, $\\v{S}_+^n=\\{X‚àà\\v{S}^n|X‚âΩ0\\}$ are positive-semidefinite (nonnegative eigenvalues), and $\\v{S}_{++}^n=\\{X‚àà\\v{S}^n|X‚âª0\\}$ are positive-definite.\n",
        "\n",
        "- **Generalized inequalities**: $\\red{x‚âº_Ky ‚áî y-x‚ààK}$ and $\\red{x‚â∫_Ky‚áîy-x‚àà\\t{int}(K)}$, where $K$ is proper cone. B&V uses notation $\\preceq$ and $\\succeq$\n",
        "\n",
        "  - $\\BC\n",
        "  K=‚Ñù_+&‚âº_{‚Ñù_+}\\t{ is identical to }‚â§\\t{ for }‚Ñù\\\\\n",
        "  K=‚Ñù_+^n&‚âº_{‚Ñù_+^n}\\t{ is vector element-wise inequality}\\\\\n",
        "  K=\\v{S}_+^n&‚âº_{\\v{S}_+^n}\\t{ is matrix inequality}\n",
        "  \\EC$\n",
        "\n",
        "  - $x‚ààS$ is the **minimum** element of $S$ wrt $‚âº_K$ if for every $y‚ààS$ we have $x‚âº_Ky$. Everything else in the set is comparable and bigger. Written as $S‚äÜx+K$ to mean all points comparable and $‚â•$.\n",
        "\n",
        "  - $x‚ààS$ is the **minimal** element of $S$ wrt $‚âº_K$ if $y‚ààS$ and $y‚âº_Kx$ $‚áíy=x$. No other point is strictly smaller. Other points $y‚ààS$ may be different and incomparable, but not necessarily $y‚âº_Kx$. Written as $(x-K)‚à©S=\\{x\\}$ to mean the only thing comparable and $‚â§$ is $x$ it self.\n",
        "\n",
        "  - A cone defines what positive means. For point $x$ in a set, $y‚â§_Kx$ means $x-y=k$ for some vector $k‚ààK$. Or $y=x-k$ for that some vector $k‚ààK$. Therefore the set of points $y‚â§_Kx$ must exist in the cone pointing in opposite direction radiating from $x$."
      ],
      "metadata": {
        "id": "i2rnUOIehAnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preservation of convexity**:\n",
        "\n",
        "- If $S_1$ and $S_2$ are convex, then $S_1‚à©S_2$ is convex.\n",
        "\n",
        "- Suppose $S‚äÜ‚Ñù^n$ is convex and $f:‚Ñù^n‚Üí‚Ñù^m$ is affine function $\\{f(x)=Ax+b|A‚àà‚Ñù^{m√ón},b‚àà‚Ñù^m\\}$, then image $f(S)=\\{f(x)|x‚ààS\\}$ and inverse image $f^{-1}(S)=\\{x|f(x)‚ààS\\}$ are convex.\n",
        "\n",
        "- Affine combinations of convex sets are convex.\n",
        "\n",
        "- Projection of a convex set onto a lower dimension uses affine function\n",
        "$f(\\BM x_1\\\\x_2\\EM)=\\BM 1&0\\EM\\BM x_1\\\\x_2\\EM=x_1$.\n",
        "If $S‚äÜ‚Ñù^m√ó‚Ñù^n$ is convex, then\n",
        "$T=\\{x_1‚àà‚Ñù^m|(x_1,x_2)‚ààS\\t{ for some }x_2‚àà‚Ñù^n\\}$ is convex.\n",
        "\n",
        "- **Cartesian product** of lower dimension convex sets onto a higher dimension is convex using inverse image of previous affine function. if $S_1$ and $S_2$ are convex, then $\\blue{S_1√óS_2:=\\{(x_1,x_2)|x_1‚ààS_1,x_2‚ààS_2\\}}$ is convex. Here the notation $(x_1,x_2)=\\BM x_1\\\\x_2\\EM$ is a longitudinal stacking of two vectors.\n",
        "\n",
        "- **Partial sums** of convex $S_1,S_2$ as $S=\\{(x,y_1+y_2)|(x,y_1)‚ààS_1,(x,y_2)‚ààS_2,x‚àà‚Ñù^m,y_1,y_2‚àà‚Ñù^n\\}$ are convex.\n",
        "\n",
        "- **Linear matrix inequality** (LMI) is an equality $\\blue{F(x)=F_0+x_1F_1+...+x_nF_n‚âΩ0}$, which is an affine combination of symmetric matrices. $\\blue{\\{x|F(x)‚âΩ0\\}}$ (or $F(x)‚âº0$) is always a convex set because it is the inverse image of $F(x)‚àà\\v{S}_{+}^n$.\n",
        "\n",
        "- **Perspective function** $P:‚Ñù^{n+1}‚Üí‚Ñù^n$ with $\\dom(P)=‚Ñù^n√ó‚Ñù_{++}$ is defined as $\\blue{P(z,t)=\\/{z}{t}}$, which scales the vector so its last component is one, then drops it from the vector. If $C$ is convex, then its image $P(C)=\\{P(x)|x‚ààC\\}$ and inverse image $P^{-1}(C)=\\{x|P(x)‚ààC\\}$ are convex.\n",
        "\n",
        "  - Proof: Suppose\n",
        "  $x=(\\tilde{x},x_{n+1})$ and\n",
        "  $y=(\\tilde{y},y_{n+1})$. Then\n",
        "  $P(Œ∏x+(1-Œ∏)y)=\\/{Œ∏\\tilde{x}+(1-Œ∏)\\tilde{y}}{Œ∏x_{n+1}+(1-Œ∏)y_{n+1}}$\n",
        "  $=ŒºP(x)+(1-Œº)P(y)$ where\n",
        "  $Œº=\\/{Œ∏x_{n+1}}{Œ∏x_{n+1}+(1-Œ∏)y_{n+1}}‚àà[0,1]$.\n",
        "\n",
        "- **Linear fractional function**: Suppose $g:‚Ñù^n‚Üí‚Ñù^{m+1}$ is affine such that $g(x)=\\BM A\\\\c^‚ä§\\EM x+\\BM b\\\\d\\EM$ where $A‚àà‚Ñù^{m√ón}$, $c‚àà‚Ñù^n$, $b‚àà‚Ñù^m$, $d‚àà‚Ñù$. Then the non-linear $f:‚Ñù^n‚Üí‚Ñù^m$ where $\\blue{f(x)=P‚àòg(x)=\\/{Ax+b}{c^‚ä§x+d}}$ with $\\dom(f)=\\{x|c^‚ä§x+d>0\\}$ is called a linear-fractional (or projective) function.\n",
        "\n",
        "  - $f(x)=P‚àòg(x)=\\/{Ax+b}{c^‚ä§x+d}$ says $f(x)$ is equivalent to perspective $P(z,t)$ where $z=Ax+b$ and $t=c^‚ä§x+d$.\n",
        "  In general, linear-over-linear functions preserve convexity: image and inverse image are convex.\n",
        "  If $C$ is convex, then image $f(C)$ and inverse image $f^{-1}(C)$ are also convex.\n",
        "\n",
        "  - Projective geometry: We convert this non-linear problem in $‚Ñù^n‚Üí‚Ñù^m$ into a linear one in $‚Ñù^{n+1}‚Üí‚Ñù^{m+1}$.\n",
        "  Create block matrix $Q=\\BM A&b\\\\c^‚ä§&d\\EM‚àà‚Ñù^{(m+1)√ó(n+1)}$ and augment $x‚Ü¶\\BM x\\\\1\\EM$.\n",
        "  The augmented point is a ray in $‚Ñù^{n+1}$ to which we apply linear transformation and normalize\n",
        "  $Q\\BM x\\\\1\\EM=\\BM Ax+b\\\\c^‚ä§x+d\\EM‚àà‚Ñù^{m+1}$\n",
        "  $‚Ü¶\\BM (Ax+b)/(c^‚ä§x+d)\\\\1\\EM=\\BM f(x)\\\\1\\EM$.\n",
        "\n",
        "- 2.7: Show $\\v{S}_+^n$ is a convex set. $\\v{S}_+^n=\\bigcap_{z\\neq0}\\{X‚àà\\v{S}^n|z^‚ä§Xz‚â•0\\}$ is an intersection of the test $z^‚ä§Xz‚â•0$ for every possible vector $z\\neq0$. However $z^‚ä§Xz‚â•0$ is a linear inequality of $X$ therefore represents a halfspace. Intersection of halfspaces is a polyhedron and convex.\n",
        "\n",
        "- 2.8: Let $p(t)=\\sum_{k=1}^mx_k\\cos(kt)$. Show the set $\\{x‚àà‚Ñù^m||p(t)|‚â§1, |t|‚â§\\/{œÄ}{3}\\}$ is convex. The set is equivalent to $\\{x‚àà‚Ñù^m||-1‚â§[\\cos(t),...,\\cos(mt)]^‚ä§x‚â§1,|t|‚â§\\/{œÄ}{3}\\}$, where the test is a intersection of two linear inequalities (two halfspaces in opposite directions). Therefore it is convex.\n",
        "\n",
        "- 2.9 (polyhedron): $\\{x|Ax‚âºb,Cx=d\\}$ is inverse image of affine function $f(x)=(b-Ax,d-Cx)=\\BM b-Ax\\\\d-Cx\\EM‚àà\\BM ‚Ñù_+^m\\\\\\{0\\}\\EM$, which is a Cartesian product. Therefore $\\{x|Ax‚âºb,Cx=d\\}=\\{x|f(x)‚àà‚Ñù_+^m√ó\\{0\\}\\}$ is convex.\n",
        "\n",
        "- 2.10 (LMI): $A(x)=\\sum_{i=1}^nx_iA_i‚âºB$ where $B,A_i‚àà\\v{S}^m$ is called a linear matrix inequality in $x$. Its solution set $\\{x|A(x)‚âºB\\}$ is inverse image of affine function $f(x)=B-A(x)‚àà\\v{S}_+^m$ and is therefore convex.\n",
        "\n",
        "  - Ordinary linear inequality $a^‚ä§x=\\sum_{i=1}^na_ix_i‚â§b$ where $a_i,b‚àà‚Ñù$ is just a special case.\n",
        "\n",
        "- 2.13 (conditional probabilities): Let $U=\\{1,...,n\\}$ and $V=\\{1,...,m\\}$ be two discrete random variables.  Then $P(U=i|V=j)=\\/{P(U=i,V=j)}{\\sum_{k=1}^nP(U=k,V=j)}$ is a linear fractional function."
      ],
      "metadata": {
        "id": "SZfeSVUE03Bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Separating hyperplane theorem**: Suppose $C$ and $D$ are nonempty disjoint convex sets $C‚à©D=‚àÖ$. Then there exists $a\\neq0$ and $b‚àà‚Ñù$ such that $\\BC a^‚ä§x‚â§b\\ ‚àÄx‚ààC\\\\a^‚ä§x‚â•b\\ ‚àÄx‚ààD\\EC$. The hyperplane $\\{x|a^‚ä§x=b\\}$ is a **separating hyperplane** for $C,D$.\n",
        "\n",
        "- $\\t{dist}(C,D)=\\inf\\{\\n{u-v}_2|u‚ààC,v‚ààD\\}$. There exists $c‚ààC$ and $d‚ààD$ such that $\\n{c-d}_2=\\t{dist}(C,D)$. Then the separating plane $f(x)=a^‚ä§(x-x_0)$ passes through midpoint $x_0=\\/{c+d}{2}$ and has normal $a=d-c$ (pointing toward $D$). I.e., $\\green{f(x)=(d-c)^‚ä§(x-\\/{c+d}{2})}$.\n",
        "\n",
        "- Existence of a separating hyperplane does not mean $C,D$ are disjoint as defined by $‚â§$ and $‚â•$, as $C=\\{0\\}$ and $D=\\{0\\}$ is a counterexample.\n",
        "\n",
        "  - Any two convex sets $C,D$ at least one of which is open, is disjoint iff there exists a separating hyperplane.\n",
        "\n",
        "**Supporting hyperplanes**: Suppose $C‚äÜ‚Ñù^n$ and $x_0$ is a point in its boundary $x_0‚àà\\t{boundary}(C)=\\t{closure}(C)\\backslash\\t{int}(C)$. If $a\\neq0$ satisfies $a^‚ä§(x-x_0)‚â§0$ for all $x‚ààC$, then $\\red{\\{x|a^‚ä§(x-x_0)=0\\}}$ is the supporting hyperplane to $C$ at $x_0$.\n",
        "\n",
        "- Tangent. But unlike tangent, supporting hyperplanes also exist for $x_0$ at a sharp corner."
      ],
      "metadata": {
        "id": "6YMPCYJY-TGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dual cones**: Let $K$ be a cone. Then $K^*=\\{y|x^‚ä§y‚â•0\\ ‚àÄx‚ààK\\}$ is the dual cone of $K$. Every $(x‚ààK,y‚ààK^*)$ pair shares an angle no more than 90¬∞. **Every vector $y‚ààK^*$ is the normal of a supporting hyperplane of $K$ at the origin.**\n",
        "\n",
        "- $\\BC\n",
        "K^*\\t{ is closed and convex}\\\\\n",
        "K_1‚äÜK_2‚áíK_2^*‚äÜK_1^*\\\\\n",
        "\\t{int}(K)\\neq‚àÖ‚áíK^*\\t{ is pointed}\\\\\n",
        "\\t{cl}(K)\\t{ is pointed}‚áí\\t{int}(K^*)\\neq‚àÖ\\\\\n",
        "K^{**}=\\t{cl}(\\t{conv}(K)) \\\\\n",
        "\\red{K\\t{ is proper}‚áîK^*\\t{ is proper}‚áîK^{**}=K}\n",
        "\\EC$\n",
        "\n",
        "  - Geometrically a symmetric cone and its dual share the same central axis because of the angular requirement.\n",
        "\n",
        "- Proper $K$ induces primal inequality $‚âº_K$, then $‚âº_{K^*}$ is its dual inequality.\n",
        "\n",
        "  - Primal inequality from $y-x‚ààK$ has a dual inequality $Œª^‚ä§x‚â§Œª^‚ä§y\\ ‚àÄŒª‚âΩ_{K^*}0$ $\\red{\\BC\n",
        "  x‚âº_Ky ‚áî Œª^‚ä§x‚â§Œª^‚ä§y\\ ‚àÄŒª‚âΩ_{K^*}0\\\\\n",
        "  x‚â∫_Ky ‚áî Œª^‚ä§x< Œª^‚ä§y\\ ‚àÄŒª‚âΩ_{K^*}0, Œª\\neq0\n",
        "  \\EC}$\n",
        "\n",
        "  - Proof ($‚âº$): Suppose $x‚âº_Ky$. Then $y-x‚ààK$ and $Œª^‚ä§(y-x)‚â•0$ for all $Œª‚ààK^*$ by definition of dual cone.\n",
        "\n",
        "  - Proof ($‚â∫$): Suppose $x‚â∫_Ky$. Then $y-x‚àà\\t{int}(K)$. The vectors in $K^*$ that hold 90¬∞ angle with border of $K$ necessarily hold less than 90¬∞ with interior of $K$. Therefore $Œª^‚ä§(y-z)>0$ for all $Œª‚ààK^*$, which also implies $Œª\\neq0$.\n",
        "\n",
        "- Example: Why $x‚â∫_Ky ‚áî Œª^‚ä§x< Œª^‚ä§y\\ ‚àÄ\\green{Œª‚âΩ_{K^*}0, Œª\\neq0}$ and not $\\green{Œª‚âª_{K^*}0}$? Suppose $K=‚Ñù_+^2=K^*$, then $x=(0,5)‚ààK$ does not satisfy $x‚âª_K0$.\n",
        "\n",
        "  - $Œª‚âª_{K^*}0$: Let $Œª=(1,1)$, then $Œª^‚ä§x=5$. Therefore $x‚âª_K0$. Because $Œª‚àà\\t{int}(K)$, we have $Œª^‚ä§x>0\\ ‚àÄx‚ààK$. They cannot correctly distinguish $x‚ààK$ located on cone boundary and reject $x‚âª_K0$.\n",
        "\n",
        "  - $Œª‚âΩ_{K^*}0, Œª\\neq0$: Let $Œª=(1,0)$, then $Œª^‚ä§x=0$. Therefore $x\\not‚âª_K0$. We can check that $x‚ààK$ is located on boundary, and correctly reject $x‚âª_K0$.\n",
        "\n",
        "- $x‚ààS$ is the **minimum** element wrt $‚âº_K$ iff $x=\\arg\\min_zŒª^‚ä§z$ for all $Œª‚ààK^*$ and that hyperplane $\\{z|Œª^‚ä§(z-x)=0\\}$ supports $S$ at $x$.\n",
        "\n",
        "- If $x=\\arg\\min_zŒª^‚ä§z$ for all $Œª‚ààK^*$ then $x‚ààS$ is the **minimal** element wrt $‚âº_K$. However $x$ can be minimal, but not a minimizer of $Œª^‚ä§$ unless $S$ is convex.\n",
        "\n",
        "**Dual norm** of a vector $u$ wrt $\\n{‚ãÖ}_p$ is $\\red{\\n{u}_q:=\\sup\\{u^‚ä§x|\\ \\n{x}_p‚â§1\\}}$.\n",
        "\n",
        "- For any $p‚â•1$, the dual of $‚Ñì_p$ norm is $‚Ñì_q$ norm defined by H√∂lder conjugate: $\\blue{\\/{1}{p}+\\/{1}{q}=1}$. The dual norm max dot product is defined by the H√∂lder inequality: $\\blue{u^‚ä§x‚â§\\n{x}_p\\n{u}_q}‚áí\\sup u^‚ä§x=(1)\\n{u}_q$.\n",
        "\n",
        "- Dual norm of $u$ is the max dot product between $u$ and any collinear unit vector $x$ that satisfies $\\n{x}_p=1$ (because $\\sup u^‚ä§x=\\n{u}_2\\sup(\\n{x}_2\\cos œâ)$). The condition is written as $\\{x|\\ \\n{x}‚â§ 1\\}$ instead of $\\{x|\\ \\n{x}=1\\}$ because a filled ball is convex and allows convex analysis tools like support functions, while a hollow shell is non-convex.\n",
        "\n",
        "- **Alignment trick**: $\\blue{\\sup\\{u^‚ä§v|\\ \\n{u}_2‚â§1\\}=\\sup\\{\\n{u}_2\\n{v}_2\\cos œâ|\\ \\n{u}_2‚â§1\\}=\\n{v}_2}$.\n",
        "\n",
        "- 2.22: Dual cone of subspace $V‚äÜ‚Ñù^n$ is $V^\\perp=\\{y|v^‚ä§y=0\\ ‚àÄv‚ààV\\}$. If $x‚ààV$ then $-x‚ààV$.\n",
        "\n",
        "- 2.23: Dual cone of nonnegative orthant $‚Ñù_+^n$ is $‚Ñù_+^n$ itself (self-dual).\n",
        "\n",
        "- 2.24: Positive semidefinite cone $\\v{S}_+^n$ is self-dual. Proof:\n",
        "$(\\v{S}_+^n)^*=\\{Y|‚ü®X,Y‚ü©‚â•0\\ ‚àÄX‚ààK\\}$\n",
        "$=\\{Y|\\tr(X^‚ä§Y)‚â•0\\ ‚àÄX‚ààK\\}$\n",
        "$=\\{Y|\\sum_iŒª_i\\tr(u_iu_i^‚ä§Y)‚â•0\\ ‚àÄUDU^{-1}‚ààK\\}$\n",
        "$=\\{Y|\\sum_iŒª_i\\tr(u_i^‚ä§Yu_i)‚â•0\\ ‚àÄUDU^{-1}‚ààK\\}$\n",
        "$=\\{Y|Y‚àà\\v{S}_+^n\\}$.\n",
        "\n",
        "- 2.25: Dual cone of norm cone $\\{(x,t)‚àà‚Ñù^{n+1}|\\ \\n{x}‚â§t\\}$ is $\\{(u,v)‚àà‚Ñù^{n+1}|\\ \\n{u}_*‚â§v\\}$ where $\\n{u}_*$ is dual norm\n",
        "\n",
        "  - $\\{(x,t)‚àà‚Ñù^{n+1}|\\ \\n{x}_2‚â§t\\}$ is self-dual, as Euclidean norm is dual norm of itself."
      ],
      "metadata": {
        "id": "Q5mKhF-sEblu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2.5: What is the distance between two parallel hyperplanes $\\{x‚àà‚Ñù^n|a^‚ä§x=b_1\\}$ and $\\{x‚àà‚Ñù^n|a^‚ä§x=b_2\\}$?\n",
        "$b_1=a^‚ä§x_1$ and $b_2=a^‚ä§x_2$ where $x_1$ and $x_2$ are offsets from origin for the two planes. Project them onto $a$ and the displacement between them is $\\/{a^‚ä§x_1}{\\n{a}_2^2}a-\\/{a^‚ä§x_2}{\\n{a}_2^2}a=\\/{(b_1-b_2)a}{\\n{a}_2^2}$. Therefore the distance is $\\/{|b_1-b_2|}{\\n{a}_2}$.\n",
        "\n",
        "- 2.6: Two halfspaces $H_1=\\{x|a_\\v{1}^‚ä§x‚â§b_1\\}$ and $H_2=\\{x|a_2^‚ä§x‚â§b_2\\}$. Give conditions when $H_1‚äÜH_2$ and when $H_1=H_2$. Let $Œª>0$. Then $H_1=\\{x|Œªa_\\v{1}^‚ä§x‚â§Œªb_1\\}$. $H_1‚äÜH_2$ if $a_2=Œªa_1$ (collinear) and $b_2‚â•Œªb_1$. $H_1=H_2$ if $a_2=Œªa_1$ and $b_2=Œªb_1$.\n",
        "\n",
        "- 2.8: Polyhedron?\n",
        "\n",
        "  - $S=\\{y_1a_1+y_2a_2|y_1‚àà[-1,1], y_2‚àà[-1,1]\\}$ where $a_1,a_2‚àà‚Ñù^n$ is a parallelogram therefore polyhedron.\n",
        "\n",
        "  - $S=\\{x‚àà‚Ñù^n|x‚âΩ0,\\v{1}^‚ä§x=1,a^‚ä§x=b_1,a^‚ä§xa=b_2\\}$ is the set of all discrete PMFs with $P(X=a_i)=x_i$. All constraints are linear therefore polyhedron\n",
        "\n",
        "  - $S=\\{x‚àà‚Ñù^n|x‚âΩ0,x^‚ä§y‚â§1\\ ‚àÄy:\\n{y}_2=1\\}=\\{x‚àà‚Ñù^n|x‚âΩ0,\\n{x}_2‚â§1\\}$ is the intersection of the nonnegative orthant and a unit ball, which cannot be described by finite number of linear inequalities.\n",
        "\n",
        "  - $S=\\{x‚àà‚Ñù^n|x‚âΩ0,x^‚ä§y‚â§1\\ ‚àÄy:\\n{y}_1=1\\}=\\{x‚àà‚Ñù^n|x‚âΩ0,\\n{x}_‚àû‚â§1\\}$ is the intersection of the nonnegative orthant and a unit square and therefore polyhedron.\n",
        "\n",
        "- 2.10: Show $C=\\{x‚àà‚Ñù^n|x^‚ä§Ax+b^‚ä§x+c‚â§0\\}$ is convex if $A‚âΩ0$. $f(x)=x^‚ä§Ax+b^‚ä§x+c$ has Hessian $2A$. Therefore if $A‚âΩ0$ then $f(x)‚â§0$ is the **sublevel set of a convex function**, which is convex.\n",
        "\n",
        "- 2.12: Convex?\n",
        "\n",
        "  - Slab $\\{x‚àà‚Ñù^n|Œ±‚â§a^‚ä§x‚â§Œ≤\\}$ is the intersection of two halfplanes. Therefore convex.\n",
        "\n",
        "  - Rectangle $\\{x‚àà‚Ñù^n|a‚âºx‚âºb\\}$ is the intersection of $n$ slabs. Therefore convex.\n",
        "\n",
        "  - Wedge $\\{x‚àà‚Ñù^n|a_\\v{1}^‚ä§x‚â§b_1,a_2^‚ä§x‚â§b_2\\}$ is the intersection of two halfplanes. Therefore convex.\n",
        "\n",
        "  - $\\{x|\\ \\n{x-x_0}_2‚â§\\n{x-y}_2\\ ‚àÄy‚ààS\\}$.\n",
        "  Here $\\n{x-x_0}_2‚â§\\n{x-y}_2$\n",
        "  $‚áí\\n{x-x_0}^2‚â§\\n{x-y}^2$\n",
        "  $‚áí\\n{x}^2-2x_0^‚ä§x+\\n{x_0}^2‚â§\\n{x}^2-2x^‚ä§y+\\n{y}^2$\n",
        "  $‚áí2x^‚ä§(y-x_0)‚â§\\n{y}^2-\\n{x_0}^2$ is a linear inequality on $x$. Halfplane therefore convex.\n",
        "\n",
        "  - $\\{x|\\t{dist}(x,S)‚â§\\t{dist}(x,T)\\}$. Counter-example $S=\\{-1,1\\}$ and $T=\\{0\\}$. Not convex.\n",
        "\n",
        "  - $\\{x|x+S_2‚äÜS_1\\}$ where $S_1,S_2‚äÜ‚Ñù^n$ and $S_1$ is convex.\n",
        "  $x+S_2‚äÜS_1$ means $x+y‚ààS_1\\ ‚àÄy‚ààS_2$\n",
        "  $‚áíx‚ààS_1-y\\ ‚àÄy‚ààS_2$\n",
        "  $‚áíx‚àà\\bigcap_{y‚ààS_2}S_1-\\{y\\}$, the intersection of translations of $S_1$.\n",
        "  Therefore convex.\n",
        "\n",
        "- 2.15: Let $x$ be random variable with $P(x=a_1)=p_i$ where $a_1< ...< a_n$ and $p‚àà‚Ñù^n$ lies in the probability simplex $P=\\{p|\\v{1}^‚ä§p=1,p‚âΩ0\\}$. Which of these conditions on a set of $p‚ààP$ is convex?\n",
        "\n",
        "  - $Œ±‚â§\\E[f(x)]‚â§Œ≤$. Here $\\E[f(x)]=f(x)^‚ä§p$ is a linear inequality of $p$. The set $\\{p|Œ±‚â§f(x)^‚ä§p‚â§Œ≤\\}$ is a slab therefore convex.\n",
        "\n",
        "  - $P(x>Œ±)‚â§Œ≤$. Equivalent to $\\sum_{i:a_i>Œ±}p_i‚â§Œ≤$ is a linear inequality of $p$. It is a halfplane therefore convex.\n",
        "\n",
        "  - $\\E|x^3|‚â§Œ±\\E|x|$. Equivalent to $\\sum_i(|a_i^3|-Œ±|a_i|)p_i‚â§0$. Halfplane therefore convex.\n",
        "\n",
        "  - $\\Var(x)‚â§Œ±$. Equivalent to $a^‚ä§pa-(a^‚ä§p)^2‚â§Œ±$\n",
        "  $‚áí(a^‚ä§p)^2-a^‚ä§pa+Œ±‚â•0$. The **superlevel set of a convex function is NOT convex** (disjoint epigraph).\n",
        "\n",
        "  - $\\Var(x)‚â•Œ±$. Equivalent to $(a^‚ä§p)^2-a^‚ä§pa+Œ±‚â§0$. The sublevel set of a convex function is convex.\n",
        "\n",
        "- 2.16: Prove partial sums $S=\\{(x,y_1+y_2)|x‚àà‚Ñù^m,y_1,y_2‚àà‚Ñù^n,(x,y_1)‚ààS_1,(x,y_2)‚ààS_2\\}$ is convex if $S_1,S_2$ are convex sets. Let $(a,b_1)‚ààS_1$, $(a,b_2)‚ààS_2$ then $(a,b_1+b_2)‚ààS$. Let $(c,d_1)‚ààS_1$, $(c,d_2)‚ààS_2$ then $(c,d_1+d_2)‚ààS$. Fix $Œ∏‚àà[0,1]$. Then $(Œ∏a+(1-Œ∏)c,Œ∏b_1+(1-Œ∏)d_1)‚ààS_1$, $(Œ∏a+(1-Œ∏)c,Œ∏b_2+(1-Œ∏)d_2)‚ààS_2$, and $(Œ∏a+(1-Œ∏)c,Œ∏(b_1+b_2)+(1-Œ∏)(d_1+d_2))‚ààS$.\n",
        "\n",
        "- 2.17: Describe $P(C)=\\{\\/{v}{t}|(v,t)‚ààC,t>0\\}$.\n",
        "\n",
        "  - $C=\\t{conv}\\{(v_1,t_1),...,(v_K,t_K)\\}$. Then $P(C)=\\t{conv}\\{\\/{v_1}{t_1},...,\\/{v_K}{t_K}\\}$.\n",
        "\n",
        "  - $C=\\{(v,t)|f^‚ä§v+gt=h\\}$. Then $P(C)=\\{x|f^‚ä§x+g=\\/{h}{t}\\}$ is a hyperplane."
      ],
      "metadata": {
        "id": "5OX-xGpk_37i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Convex Functions\n",
        "\n",
        "- Denominator layout $‚àáf(x)^‚ä§(y-x)$"
      ],
      "metadata": {
        "id": "lCXdVkbcqRO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convex function**: $f:‚Ñù^n‚Üí‚Ñù$ is convex if $\\dom(f)$ is a convex set and $‚àÄx,y‚àà\\dom(f)$ and $Œ∏‚àà[0,1]$ we have f(mixture)‚â§mixture(f) or $\\red{f(Œ∏x_1+(1-Œ∏)x_2)‚â§Œ∏f(x_1)+(1-Œ∏)f(x_2)}$.\n",
        "\n",
        "- **Extended-value extension**: $\\tilde{f}(x)=\\BC f(x)&x‚àà\\dom(f)\\\\‚àû&x‚àâ\\dom(f)\\EC$ is assumed for all functions in the book.\n",
        "\n",
        "- First order condition: $\\red{f(y)‚â•f(x)+‚àáf(x)^‚ä§(y-x)}$ holds for all $y,x‚àà\\dom(f)$. Denominator layout.\n",
        "\n",
        "- Second order condition: $\\red{‚àá^2f(x)‚âΩ0}$ (positive-semidefinite).\n",
        "\n",
        "- Examples:\n",
        "\n",
        "  - **Affine**: $f(x)=a^‚ä§x+b$ or $f(X)=\\tr(A^‚ä§X)+b$\n",
        "\n",
        "  - **Norm**: $\\n{‚ãÖ}:‚Ñù^n‚Üí‚Ñù$ is convex, including matrix spectral norm (max singular value). Proof: $\\n{Œ∏x+(1-Œ∏)y}‚â§Œ∏\\n{x}+(1-Œ∏)\\n{y}$ by triangle inequality.\n",
        "\n",
        "  - **Max**: $\\max(Œ∏x+(1-Œ∏)y)‚â§Œ∏\\max(x)+(1-Œ∏)\\max(y)$ is convex. Triangle inequality applies as well.\n",
        "\n",
        "  - **Quadratic-over-linear**: $f(x,y)=\\/{x^2}{y}$ is convex on $y>0$. Hessian $‚àá^2f=\\/{2}{y^3}\\BM y^2&-xy\\\\-xy&x^2\\EM$. $\\det(‚àá^2f)=0$. All principal minors are $‚â•0‚áí‚àá^2f‚âΩ0$.\n",
        "\n",
        "  - **Log-sum-exp**: $f(x)=\\ln(\\sum_ie^{x_i})$ is convex.\n",
        "  Let $z=(e^{x_1},...,e^{x_n})$,\n",
        "  $S=\\v{1}^‚ä§z$, and\n",
        "  $f(x)=\\ln(S)$. Then\n",
        "  $‚àÇ_{x_i}f=\\/{z_i}{S}$.\n",
        "  Off-diagonal\n",
        "  $‚àÇ_{x_i,x_j}^2f=-\\/{z_iz_j}{S^2}$.\n",
        "  On-diagonal\n",
        "  $‚àÇ_{x_i,x_i}^2f=\\/{z_i}{S}-\\/{z_iz_i}{S^2}$.\n",
        "  Hessian $‚àá^2f(x)=\\/{\\diag(z_i)}{S}-\\/{zz^‚ä§}{S^2}$\n",
        "  $=\\/{1}{(\\v{1}^‚ä§z)^2}((\\v{1}^‚ä§z)\\diag(z_i)-zz^‚ä§)$.\n",
        "  Check positive-semidefinite\n",
        "  $v^‚ä§‚àá^2fv=\\/{1}{(\\v{1}^‚ä§z)^2}((\\v{1}^‚ä§z)v^‚ä§\\diag(z_i)v-v^‚ä§zz^‚ä§v)$\n",
        "  $=\\/{1}{(\\v{1}^‚ä§z)^2}((\\sum_iz_i)(\\sum_iz_iv_i^2)-(\\sum_iv_iz_i)^2)‚â•0$.\n",
        "  $(\\sum_i(\\sqrt{z_i})^2)(\\sum_i(\\sqrt{z_i}v_i)^2)‚â•(\\sum_i(\\sqrt{z_i})(\\sqrt{z_i}v_i))^2$ by\n",
        "  Cauchy-Schwarz, therefore Hessian positive-semidefinite and convex.\n",
        "\n",
        "  - **Geometric mean**: $f(x)=[\\prod_{i=1}^nx_i]^{1/n}$ is concave on $‚Ñù_{++}^n$.\n",
        "  Let $S=\\prod_ix_i$,\n",
        "  $z=(\\/{1}{x_1},...,\\/{1}{x_n})$. Then\n",
        "  $‚àÇ_{x_i}f=\\/{1}{n}(\\/{S}{x_i})^{1/n}x_i^{1/n-1}$\n",
        "  $=\\/{S^{1/n}}{n}z_i$.\n",
        "  Off-diagonal\n",
        "  $‚àÇ_{x_ix_j}^2f=\\/{S^{1/n}}{n^2}z_iz_j$.\n",
        "  On-diagonal\n",
        "  $‚àÇ_{x_ix_i}^2f=\\/{S^{1/n}}{n^2}z_i^2-\\/{S^{1/n}}{n}z_i^2$.\n",
        "  Hessian $‚àá^2f(x)=-\\/{S^{1/n}}{n^2}(n\\diag(z_i^2)-zz^‚ä§)$.\n",
        "  Check negative-semidefinite\n",
        "  $v‚ä§‚àá^2fv=-\\/{S^{1/n}}{n^2}(nv^‚ä§\\diag(z_i^2)v-v^‚ä§zz^‚ä§v)$\n",
        "  $=-\\/{S^{1/n}}{n^2}((\\sum_i1)(\\sum_iz_i^2v_i^2)-(\\sum_iz_iv_i)^2)‚â§0$ by Cauchy-Schwarz, therefore Hessian negative-semidefinite and concave.\n",
        "\n",
        "- **Restriction of convex function to a line** (lecture): $f:‚Ñù^n‚Üí‚Ñù$ is convex iff $g:‚Ñù‚Üí‚Ñù,g(t)=f(x+tv)$ with $\\dom(g)=\\{t|x+tv‚àà\\dom(f)\\}$ is convex in $t$ for any $x‚àà\\dom(f),v‚àà‚Ñù^n$. This is used to prove convexity of $f:\\v{S}_{++}^n‚Üí‚Ñù$ where $f(X)=f(Z+tV)$.\n",
        "\n",
        "  - **Log-determinant**: Let $f(x)=\\ln(\\det(X))$ where $\\dom(f)=\\v{S}_{++}^n$, then\n",
        "  $g(t)=f(X+tV)=\\ln\\det(X+tV)$\n",
        "  $=\\ln\\det(X^{1/2}(I+tX^{-1/2}VX^{-1/2})X^{1/2})$\n",
        "  $=\\ln\\det(X)+\\ln \\prod_i(1+tŒª_i)$\n",
        "  $=\\ln\\det(X)+\\sum_i\\ln(1+tŒª_i)$\n",
        "  where $Œª_i$ are eigenvalues of $X^{-1}V$. Concave.\n",
        "\n",
        "- **Sublevel sets**: The $Œ±$-sublevel set of $f:‚Ñù^n‚Üí‚Ñù$ is $C_Œ±=\\{x‚àà\\dom(f)|f(x)‚â§Œ±\\}$. Convex function $‚áí$ convex sublevel sets. Converse not true.\n",
        "\n",
        "  - Convex ‚áí quasiconvex, concave ‚áí quasiconcave, converse not true.\n",
        "\n",
        "- **Epigraph**: Graph of $f:‚Ñù^n‚Üí‚Ñù$ is $\\{(x,f(x))‚àà‚Ñù^{n+1}|x‚àà\\dom(f)\\}$. Epigraph is the filled area above the graph $\\epi(f)=\\{(x,y)‚àà‚Ñù^{n+1}|x‚àà\\dom(f),f(x)‚â§y\\}$. **Function is convex ‚áî epigraph is convex set**.\n",
        "\n",
        "  - $(x,y)‚àà\\epi(f)$\n",
        "  $‚áíy‚â•f(x)‚â•f(x_0)+‚àáf(x_0)^‚ä§(x-x_0)$\n",
        "  $‚áíy-f(x_0)‚â•‚àáf(x_0)^‚ä§(x-x_0)$\n",
        "  at the point $(x_0,f(x_0))$.\n",
        "  Rewrite in matrix form\n",
        "  $\\BM ‚àáf(x_0)\\\\-1\\EM^‚ä§\\BM x-x_0\\\\y-f(x_0)\\EM‚â§0$\n",
        "  $‚áí\\red{\\BM ‚àáf(x_0)\\\\-1\\EM^‚ä§(\\BM x\\\\y\\EM-\\BM x_0\\\\f(x_0)\\EM)‚â§0}$ is a supporting hyperplane whose normal is $\\BM ‚àáf(x_0)\\\\-1\\EM$.\n",
        "  \n",
        "  - **Supporting hyperplane**: Consider $g(x,y)=f(x)-y$ then $‚àág(x,y)=(‚àáf(x),-1)$ points in the direction of steepest ascent and is perpendicular to the level set $g(x,y)=0$.\n",
        "  This trick allows us to find the normal vector at $(x_0,f(x_0))$, which is $‚àág(x,y)=\\blue{\\BM ‚àáf(x_0)\\\\-1\\EM}$. Example: $f(x)=x^2$, $‚àáf(x)=2x$. At $x=2$, $‚àáf(2)=2(2)=4$. The normal vector is $(4,-1)$.\n",
        "\n",
        "- **Jensen's inequality**: $\\BC\n",
        "f(Œ∏x_1+(1-Œ∏)x_2)‚â§Œ∏f(x_1)+(1-Œ∏)f(x_2) \\\\\n",
        "f(\\sum_iŒ∏_ix_i)‚â§\\sum_iŒ∏_if(x_i) \\\\\n",
        "f(‚à´_pp(x)x\\ dx)‚â§‚à´_pf(x)p(x)\\ dx \\\\\n",
        "f(\\E[x])‚â§\\E[f(x)]\n",
        "\\EC$\n",
        "\n",
        "- 3.4 (**matrix fractional function**): $f:‚Ñù^n√ó\\v{S}^n‚Üí‚Ñù,f(x,P)=x^‚ä§P^{-1}x$ is convex on $\\dom(f)=‚Ñù^n√ó\\v{S}_{++}^n$.\n",
        "\n",
        "  - Proof: $\\epi(f)=\\{(x,P,y)|P‚âª0,x^‚ä§P^{-1}x‚â§y\\}$.\n",
        "  By Schur's complement $y-x^‚ä§P^{-1}x‚â•0$\n",
        "  $‚áî\\BM P&x\\\\x^‚ä§&y\\EM‚âΩ0$, therefore\n",
        "  $\\epi(f)=\\{(x,P,y)|P‚âª0,\\BM P&x\\\\x^‚ä§&y\\EM‚âΩ0\\}$,\n",
        "  which is a linear matrix inequality on $(x,P,y)$:\n",
        "  $\\BM P&x\\\\x^‚ä§&y\\EM=P\\BM I&0\\\\0&0\\EM+x\\BM0&I\\\\0&0\\EM+x^‚ä§\\BM0&0\\\\I&0\\EM+y\\BM0&0\\\\0&1\\EM$ therefore $\\epi(f)$ convex and $f$ convex.\n"
      ],
      "metadata": {
        "id": "wogUsgC_qWwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preservation of convexity**:\n",
        "\n",
        "- **Nonnegative weighted combination**: If $f_i$ is convex then $f=\\sum_iw_if_i$ is convex where $w_i>0$.\n",
        "\n",
        "  - Proof: $\\epi(w_if_i)=\\BM I&0\\\\0&w_i\\EM\\epi(f_i)$\n",
        "  $=\\{\\BM I&0\\\\0&w_i\\EM\\BM x\\\\y\\EM|(x,y)‚àà\\epi(f_i)\\}$. Therefore $\\epi(f)$ is the linear image of convex sets $\\epi(f_i)$. Negative weight $w_i< 0$ converts convex $f_i$ into concave.\n",
        "\n",
        "  - $wf_i$ can be negative. The rule says summation preserves convexity if summants are convex.\n",
        "\n",
        "- **Integration**: If $f(x,y)$ is convex in $x\\ ‚àÄy‚àà\\m{A}$, and $w(y)‚â•0\\ ‚àÄy‚àà\\m{A}$, then $g(x)=‚à´_\\m{A}w(y)f(x,y)\\ dy$ is convex.\n",
        "\n",
        "  - $w(y)f(x,y)$ can be negative if convex. The rule says integration preserves convexity if integrant is convex.\n",
        "\n",
        "- **Affine pre-composition** (basis change): If $f$ is convex then $f(Ax+b)$ is convex. If $f$ is concave then $f(Ax+b)$ is concave.\n",
        "\n",
        "- **Pointwise maximum**: If $f_1,...,f_m$ are convex, then $f(x)=\\max\\{f_1(x),...,f_m(x)\\}$ is convex.\n",
        "\n",
        "  - Proof: $\\epi(f)=\\bigcap_{i=1}^m\\epi(f_i)$\n",
        "\n",
        "- **Pointwise supremum**: $f(x)=\\sup_{y‚àà\\m{A}}g(x,y)$ is convex if $g(x,y)$ is convex in $x$ for all $y‚àà\\m{A}$.\n",
        "\n",
        "  - Proof: $\\epi(f)=\\bigcap_{y‚àà\\m{A}}\\epi(g(‚ãÖ,y))$\n",
        "\n",
        "- **Partial infimum**: $f(x)=\\inf_{y‚ààC}g(x,y)$ is convex if $g(x,y)$ is jointly convex in both $x,y$ and set $C$ is convex.\n",
        "\n",
        "  - Although the definition is pointwise, $g$ is jointly convex in $(x,y)$ so that it is shaped like a \"bowl\" and $\\epi(f)$ is the projection (shadow) of $\\epi(g)‚àà(x,y,t)$ onto $(x,t)$, which is still convex (like the keel of a ship's hull).\n",
        "\n",
        "- **Upper envelope principle**: $\\red{f(x)=\\sup\\{g(x)|g\\t{ affine},g(z)‚â§f(z)\\ ‚àÄz‚àà‚Ñù^n\\}}$. $f$ is convex iff the upper envelope of all lower affine functions $g$ perfectly recreates the function $f$. Almost every convex function can be expressed as the pointwise supremum of a famly of affine functions.\n",
        "\n",
        "  - Proof: Because $\\epi(f)$ is convex, each $x_0‚àà‚Ñù^n$ has a supporting hyperplane\n",
        "  $\\BM‚àáf(x_0)\\\\-1\\EM^‚ä§\\BM z-x_0\\\\t-f(x_0)\\EM‚â§0$\n",
        "  for all $(z,t)‚àà\\epi(f)$.\n",
        "  I.e., $‚àáf(x_0)(z-x_0)-(t-f(x_0))‚â§0$\n",
        "  $‚áít‚â•f(x_0)+‚àáf(x_0)^‚ä§(z-x_0)$.\n",
        "  For $t=f(z)$ on the graph,\n",
        "  $f(z)‚â•f(x_0)+‚àáf(x_0)^‚ä§(z-x_0)=g(z)$.\n",
        "\n",
        "  - This principle requires that $f$ is closed ($\\epi(f)$ is a closed set) to prevent vertical supporting hyperplane.\n",
        "\n",
        "- **Composition**: $h:‚Ñù^k‚Üí‚Ñù$ and $g:‚Ñù^n‚Üí‚Ñù^k$ and $f=h‚àòg=h(g(x))$\n",
        "\n",
        "  - **Scalar** ($k=1$): Let $h,g:‚Ñù‚Üí‚Ñù‚ààC^2$. Then\n",
        "  $f''(x)=h''(g(x))g'(x)^2+h'(g(x))g''(x)$.\n",
        "  In general for $h:‚Ñù‚Üí‚Ñù$, $g:‚Ñù^n‚Üí‚Ñù$, even if $g,h$ are not twice differentiable: $f$ is $\\BC\n",
        "  \\t{convex if }h\\t{ convex}&\\t{&}&g\\t{ affine or} \\BC\n",
        "  h'(x)‚â•0\\t{ ‚áë nondecreasing}&\\t{&}&g\\t{ convex}\\\\\n",
        "  h'(x)‚â§0\\t{ ‚áì nonincreasing}&\\t{&}&g\\t{ concave}\n",
        "  \\EC \\\\\n",
        "  \\t{concave if }h\\t{ concave}&\\t{&}&g\\t{ affine or} \\BC\n",
        "  h'(x)‚â•0\\t{ ‚áë nondecreasing}&\\t{&}&g\\t{ concave}\\\\\n",
        "  h'(x)‚â§0\\t{ ‚áì nonincreasing}&\\t{&}&g\\t{ convex}\\\\\n",
        "  \\EC\n",
        "  \\EC$\n",
        "\n",
        "  - **Vector** ($k>1$): $‚àá^2f(x)=‚àág(x)^‚ä§‚àá^2h(g(x))‚àág(x)+‚àáh(g(x))^‚ä§‚àá^2g(x)$. Then $f$ is $\\red{\\BC\n",
        "  \\t{convex if }h\\t{ convex}&\\t{&}&\\t{each }g_i\\t{ affine or} \\BC\n",
        "  ‚àÇ_{g_i}h‚â•0\\t{ ‚áë nondecreasing}&\\t{&}&g_i\\t{ convex}\\\\\n",
        "  ‚àÇ_{g_i}h‚â§0\\t{ ‚áì nonincreasing}&\\t{&}&g_i\\t{ concave}\\\\\n",
        "  \\EC\\\\\n",
        "  \\t{concave if }h\\t{ concave}&\\t{&}&\\t{each }g_i\\t{ affine or} \\BC\n",
        "  ‚àÇ_{g_i}h‚â•0\\t{ ‚áë nondecreasing}&\\t{&}&g_i\\t{ concave}\\\\\n",
        "  ‚àÇ_{g_i}h‚â§0\\t{ ‚áì nonincreasing}&\\t{&}&g_i\\t{ convex}\\\\\n",
        "  \\EC\n",
        "  \\EC}$\n",
        "  in mix-and-match for each $g_i$. This generalizes all other rules (including pointwise max, nonnegative weighted, etc).\n",
        "\n",
        "  - Softwares (CVX, CVXPY) use disciplined convex programming as one engine of convex analysis. DCP maintains a library of known convex/concave \"atoms\", and recursively checks against the composition rules.\n",
        "\n",
        "- **Perspective function**: if $f:‚Ñù^n‚Üí‚Ñù$, then $g:‚Ñù^{n+1}‚Üí‚Ñù$ where $g(x,t)=tf(x/t)$ is the perspective of $f$ where $\\dom(g)=\\{(x,t)|x/t‚àà\\dom(f),t>0\\}$. If $f$ is convex then so is $g$. If $f$ is concave then so is $g$.\n",
        "\n",
        "- 3.7 (**support function of a set**): the support function associated with a set $C‚äÜ‚Ñù^n$ is $\\red{S_C(x)=\\sup\\{x^‚ä§y|y‚ààC\\}}$ is the distance from origin out to the furthest (most positive) frontier of the set along positive $x$. It is convex because it is pointwise supremum of linear functions of $x$ (even if $C$ is not convex).\n",
        "\n",
        "  - Support function of set $C$ for a given $x$ returns the supporting hyperplane for $C$ in the direction of $x$. The supporting halfplane is $\\{y|x^‚ä§y‚â§S_C(x)\\}$.\n",
        "\n",
        "- 3.8 (**distance to farthest point of a set**): $f(x)=\\sup_{y‚ààC}\\n{x-y}=\\sup_{y‚ààC}\\{(x-y)^‚ä§(x-y)\\}$\n",
        "$=\\n{x}^2+\\sup_{y‚ààC}\\{-2x^‚ä§y+\\n{y}^2\\}$ is pointwise supremum of family of convex functions.\n",
        "\n",
        "- 3.9 (**weighted least squares**): Let\n",
        "$g(w)=\\inf_x\\sum_{i=1}^nw_i(a_i^‚ä§x-b_i)^2=\\inf_x(Ax-b)^‚ä§W(Ax-b)$ where\n",
        "$W=\\diag(w)$. Then\n",
        "$g(W)=b^‚ä§Wb-b^‚ä§WA(A^‚ä§WA)^{-1}A^‚ä§Wb=D-E^‚ä§P^{-1}E$\n",
        "where $D$ is linear and $E^‚ä§P^{-1}E$ is a matrix fractional function and therefore convex only when $P‚âª0$. Therefore $g(W)$ is concave.\n",
        "\n",
        "- 3.10 (**max eigenvalue of symmetric matrix**): $f(X)=Œª_\\max(X)=\\sup\\{y^‚ä§Xy|\\n{y}_2=1\\}$ is pointwise supremum of linear functions of $X$ therefore $f$ is convex.\n",
        "$Xy=Œªy‚áíŒª(y^‚ä§y)=y^‚ä§Xy$.\n",
        "\n",
        "  - Proof: Let $X=PDP^‚ä§$ where $D$ is sorted $Œª_1‚â•...‚â•Œª_n$. Then $y^‚ä§Xy=y^‚ä§PDP^‚ä§y=(P^‚ä§y)^‚ä§D(P^‚ä§y)$.\n",
        "  Let $z=(P^‚ä§y)$, then $\\sup_{\\n{y}_2=1}\\{y^‚ä§Xy\\}=\\sup_{\\n{z}_2=1}\\{z^‚ä§Dz\\}=Œª_1$ where $z=(1,0,...,0)$.\n",
        "\n",
        "- 3.11 (**spectral norm of matrix**): $f(X)=\\n{X}_2=\\sup\\{u^‚ä§Xv|\\n{u}_2=1,\\n{v}_2=1\\}$ where $\\dom(f)=‚Ñù^{p√óq}$ is pointwise supremum of linear functions of $X$ therefore $f$ is convex.\n",
        "\n",
        "  - Proof: $u^‚ä§Xv=u^‚ä§UŒ£V^‚ä§v=\\tilde{u}^‚ä§Œ£\\tilde{v}$.\n",
        "  For rank-$k$ matrix,\n",
        "  $\\sup_{\\n{u}_2=\\n{v}_2=1}\\{u^‚ä§Xv\\}$\n",
        "  $=\\sup_{\\n{\\tilde{u}}_2=\\n{\\tilde{v}}_2=1}\\{\\tilde{u}^‚ä§Œ£\\tilde{v}\\}$\n",
        "  $=\\sup_{\\n{\\tilde{u}}_2=\\n{\\tilde{v}}_2=1}\\sum_{i=1}^kœÉ_i\\tilde{u}_i\\tilde{v}_i$\n",
        "  $=œÉ_1$ where $\\tilde{u}=(1,0,...,0)$ and $\\tilde{v}=(1,0,...,0)$.\n",
        "\n",
        "- 3.15 (**Schur complement**): Let $f(u,v)=\\BM u\\\\v\\EM^‚ä§\\BM A&B\\\\B^‚ä§&C\\EM\\BM u\\\\v\\EM$\n",
        "$=u^‚ä§Au+2u^‚ä§Bv+v^‚ä§Cv$, then $f$ is convex iff $‚àá_{u,v}^2f‚âΩ0$. However $‚àá_{u,v}^2f=2\\BM A&B\\\\B^‚ä§&C\\EM$ therefore by definition of positive-semidefinite $f(u,v)‚â•0\\ ‚àÄ(u,v)$. Let $f_\\min(u)=\\inf_vf(u,v)$. Then $‚àá_vf=2u^‚ä§B+2v^‚ä§C=0‚áív^*=-C^{-1}B^‚ä§u$.\n",
        "Then $f_\\min(u)=f(u,v^*)=u^‚ä§Au-2u^‚ä§BC^{-1}B^‚ä§u+u^‚ä§BC^{-1}CC^{-1}B^‚ä§u$\n",
        "$=u^‚ä§Au-u^‚ä§BC^{-1}B^‚ä§u$\n",
        "$=u^‚ä§(A-BC^{-1}B^‚ä§)u‚â•0$.\n",
        "Therefore $f_\\min(u)$ exists iff $C‚âª0$ strictly, and $A-BC^{-1}B^‚ä§‚âΩ0$.\n",
        "\n",
        "  - Schur complement lemma says if $C‚âª0$ then $\\BM A&B\\\\B^‚ä§&C\\EM‚âΩ0‚áîA-BC^{-1}B^‚ä§‚âΩ0$.\n",
        "\n",
        "- 3.16 (**Distance to a set**): $\\t{dist}(x,S)=\\inf_{y‚ààS}\\n{x-y}$. Here $\\n{x-y}$ is convex in $(x,y)$. Then $\\t{dist}(x,S)$ is convex if $S$ is convex.\n",
        "\n",
        "- 3.17 (**relative entropy**): Let $f(x)=-\\ln x$ on $‚Ñù_{++}$. Then $g(x,t)=-t\\ln\\/{x}{t}=t\\ln t-t\\ln x$ is the relative entropy of $t$ and $x$. It is the perspective of convex $f$ and is convex on $‚Ñù_{++}^2$.\n",
        "\n",
        "  - Let $u,v‚àà‚Ñù_{++}^n$, then sum of relative entropy $\\sum_iu_i\\ln(\\/{u_i}{v_i})=\\sum_ig(v_i,u_i)$ is convex in $(u,v)$.\n",
        "\n",
        "  - **Kullback-Leibler divergence** between $u,v‚àà‚Ñù_{++}^n$ is $\\blue{D_\\t{kl}(u,v)=\\sum_i(u_i\\ln(\\/{u_i}{v_i})-u_i+v_i)}$ is convex in $(u,v)$. It is a measure of two positive vectors. $D_\\t{kl}(u,v)‚â•0$ and $D_\\t{kl}(u,v)=0‚áîu=v$. When $\\v{1}^‚ä§u=\\v{1}^‚ä§v=1$ are probability vectors then $D_\\t{kl}(u,v)=\\sum_iu_i\\ln(\\/{u_i}{v_i})$ is equal to the relative entropy.\n",
        "\n",
        "  - If $\\v{1}^‚ä§u\\neq1$, **normalized entropy** is $\\blue{\\sum_iu_i\\ln(\\/{\\v{1}^‚ä§u}{u_i})=(\\v{1}^‚ä§u)\\sum_i\\/{u_i}{\\v{1}^‚ä§u}\\ln(\\/{\\v{1}^‚ä§u}{u_i})}$, which is a sum of concave perspective functions $(\\v{1}^‚ä§u)\\sum_iz_i\\ln(\\/{1}{z_i})$ therefore concave.\n",
        "\n",
        "- Lecture example: $f(x)=\\/{(\\v{1}^‚ä§x)^2}{\\min(2,\\sqrt{x_3})}$.\n",
        "The outer function $h(u(x),v(x))=\\/{u(x)^2}{v(x)}$ quadratic-over-linear is convex.\n",
        "$‚àÇ_uh>0$ and $u(x)$ is convex.\n",
        "$‚àÇ_vh< 0$ and $v(x)$ is concave.\n",
        "Therefore $f$ is convex.\n"
      ],
      "metadata": {
        "id": "tpqHEMZsWeHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Legendre-Fenchel Conjugate**: Let $f:‚Ñù^n‚Üí‚Ñù$. Then $f^*:‚Ñù^n‚Üí‚Ñù,\\red{f^*(y)=\\sup_{x‚àà\\dom(f)}(y^‚ä§x-f(x))}$ is the conjugate of $f$ where $\\red{\\dom(f^*)=\\{y‚àà‚Ñù^n|y^‚ä§x-f(x)< ‚àû\\ ‚àÄx‚àà\\dom(f)\\}}$.\n",
        "The conjugate function $f^*(y)$ is only defined for $y$ where $y^‚ä§x-f(x)$ is bounded above $\\dom(f)$.\n",
        "It is a pointwise supremum of affine functions of $y$. Conjugate function $f^*(y)$ is always convex, even if $f(x)$ is concave.\n",
        "\n",
        "- **Fenchel's inequality**: $\\blue{f(x)+f^*(y)‚â•x^‚ä§y}$ for all $x,y$.\n",
        "\n",
        "- **Conjugate of conjugate**: If $f$ is convex and $\\epi(f)$ is closed, then $\\blue{f^{**}=f}$.\n",
        "\n",
        "  - Proof: $f^{**}(x)=\\sup_y(x^‚ä§y-f^*(y))$, which is always convex (thereby requiring $f$ to be convex).\n",
        "  Fenchel's inequality says $f(x)‚â•x^‚ä§y-f^*(y)=h(x,y)$.\n",
        "  I.e., $h(x,y)$ is an affine underestimator of $f(x)$,\n",
        "  and $f^{**}(x)=\\sup_yh(x,y)$ is a pointwise supremum of all affine underestimators. Since $\\epi(f)$ is closed, $f$ is equal to the supremum of its affine underestimators $f=f^{**}$.\n",
        "\n",
        "- **Legendre transform**: If $f$ is differentiable then the conjugate is called Legendre transform (as opposed to Fenchel conjugate for non-differentiable). Let $h(x)=y^‚ä§x-f(x)‚áí‚àá_xh(x)=y-‚àá_xf(x)=0$. Then $\\blue{y=‚àá_xf(x^*)}$ and $\\BC\n",
        "y=‚àá_xf(x^*)&\\t{is the slope at }(x^*,f(x^*)) \\\\\n",
        "f^*(y)=‚àá_xf(x^*)^‚ä§x^*-f(x^*)&\\t{is the gap between a straight line from origin and }f(x^*)\n",
        "\\EC$\n",
        "\n",
        "- **Affine**: Let $g(x)=af(x)+b$ where $a>0,b‚àà‚Ñù$, then $g^*(y)=af^*(y/a)-b$.\n",
        "\n",
        "  - Let $g(x)=f(Ax+b)$ then $\\red{g^*(y)=f^*(A^{-‚ä§}y)-b^‚ä§A^{-‚ä§}y}$.\n",
        "\n",
        "- **Support function vs Indicator**: If $f(x)$ is the support function $\\red{f(x)=\\sup_{z‚ààS}z^‚ä§x}$ of a closed convex set $S‚äÜ‚Ñù^n$, then $f^*(y)$ is the indicator function of set $S$.\n",
        "\n",
        "  - 3.24 (**indicator**): Indicator of $S‚äÜ‚Ñù^n$ is defined to be $\\red{I_S(x)=\\BC\n",
        "  0&x‚ààS\\\\\n",
        "  ‚àû&\\t{else}\n",
        "  \\EC}$ on $\\dom(I_S)=S$.\n",
        "  Its conjugate is $I_S^*(y)=\\sup_{x‚ààS}y^‚ä§x$, which is the support function of $S$.\n",
        "\n",
        "  - $f(x)$ is a support function of set $\\red{S=\\{y|y^‚ä§x‚â§f(x)\\ ‚àÄx‚àà\\dom(f)\\}}$ iff it satisfies 3 conditions: $\\BC\n",
        "  1. &\\t{positively homogeneous}&f(tx)=tf(x)\\t{ for }t>0\\\\\n",
        "  2. &\\t{convex}&f(Œ∏x_1+(1-Œ∏)x_2)‚â§Œ∏f(x_1)+(1-Œ∏)f(x_2)\\\\\n",
        "  3. &\\t{closed}&f\\t{ is lower semicontinuous}\n",
        "  \\EC$\n",
        "\n",
        "- **Two variable conjugate**: Let $f(x,z)$. Then $\\red{f^*(y,u)=\\sup_{x,z‚àà\\dom(f)}(y^‚ä§x+u^‚ä§z-f(x,z))}$. (exercise 3.39)\n",
        "\n",
        "- 3.21: Examples\n",
        "\n",
        "  - **Affine**: $f(x)=ax+b$. Then $‚àÇ_x(xy-ax-b)=y-a=0‚áíy=a$ with singular domain\n",
        "  $\\dom(f^*)=\\{y|yx-ax-b< ‚àû\\ ‚àÄx\\}=\\{a\\}$\n",
        "  and $f^*(a)=-b$.\n",
        "\n",
        "  - **Negative log**: $f(x)=-\\ln x$ on $\\dom(f)=‚Ñù_{++}$. Differentiate $‚àÇ_x(yx+\\ln(x))=y+\\/{1}{x}=0‚áíx^*=-\\/{1}{y}$.\n",
        "  Then $f^*(y)=-\\ln(-y)-1$ where $y< 0$.\n",
        "\n",
        "  - **Exponential**: $f(x)=e^x$. Then $‚àÇ_x(xy-e^x)=y-e^x=0‚áíx^*=\\ln(y)$.\n",
        "  Therefore $f^*(y)=y(\\ln y-1)$ on $\\dom(f^*)=‚Ñù_+$.\n",
        "\n",
        "  - **Negative entropy**: $f(x)=x\\ln x$. Then $‚àÇ_x(xy-x\\ln x)=y-\\ln x-1=0‚áíx^*=e^{y-1}$.\n",
        "  Therefore $f^*(y)=ye^{y-1}-(y-1)e^{y-1}=e^{y-1}$ on $\\dom(f^*)=‚Ñù$.\n",
        "\n",
        "  - **Inverse**: $f(x)=1/x$. Then $‚àÇ_x(xy-1/x)=y+1/x^2=0‚áíx^*=\\/{1}{\\sqrt{-y}}$.\n",
        "  Therefore $f^*(y)=\\/{y}{\\sqrt{-y}}-\\sqrt{-y}=-2\\sqrt{-y}$ on $\\dom(f^*)=‚Ñù_-$.\n",
        "\n",
        "- 3.22 (**strictly convex quadratic function**): $\\blue{f(x)=\\/{1}{2}x^‚ä§Qx}$ where $Q‚àà\\v{S}_{++}^n$.\n",
        "Then\n",
        "$‚àá_x(x^‚ä§y-\\/{1}{2}x^‚ä§Qx)$\n",
        "$=y^‚ä§-x^‚ä§Q=0$\n",
        "$‚áíx^*=Q^{-1}y$.\n",
        "Therefore $f^*(y)=y^‚ä§Q^{-1}y-\\/{1}{2}y^‚ä§Q^{-1}QQ^{-1}y$\n",
        "$‚áí\\blue{f^*(y)=\\/{1}{2}y^‚ä§Q^{-1}y}$ on $\\dom(f^*)=‚Ñù^n$.\n",
        "\n",
        "- 3.23 (**log-determinant**): $\\blue{f(X)=\\ln\\det(X^{-1})=-\\ln\\det(X)}$ on $\\dom(f)=\\v{S}_{++}^n$. Then\n",
        "$f^*(Y)=\\sup_{X‚àà\\dom(f)}(\\tr(XY)+\\ln\\det(X))$ where $Y‚àà\\v{S}^n$. Let $h(X)=\\tr(XY)+\\ln\\det(X)$.\n",
        "\n",
        "  - If $Y‚âª0$ has a positive eigenvalue then $h(X)$ is unbounded.\n",
        "  Proof: Let $Œª>0$ be an eigenvalue of $Y$ with eigenvector $v$ where $\\n{v}_2=1$.\n",
        "  Let $X=I+tvv^‚ä§$.\n",
        "  Then\n",
        "  $\\tr(XY)=\\tr(Y+tvv^‚ä§Y)$\n",
        "  $=\\tr(Y)+t\\tr(vv^‚ä§Y)$\n",
        "  $=\\tr(Y)+t\\tr(Yvv^‚ä§)$\n",
        "  $=\\tr(Y)+tŒª\\tr(vv^‚ä§)$\n",
        "  $=\\tr(Y)+tŒª\\n{v}_2^2$.\n",
        "  By matrix determinant lemma\n",
        "  $\\det(X)$\n",
        "  $=\\det(I+tvv^‚ä§)$\n",
        "  $=1+t\\n{v}_2^2$.\n",
        "  Therefore $h(X)=\\tr(Y)+tŒª+\\ln(1+t)$\n",
        "  $‚Üí‚àû$ with $t‚Üí‚àû$.\n",
        "\n",
        "  - Let $Y‚â∫0$ with negative eigenvalues. Then\n",
        "  $‚àá_Xh(X)=Y^‚ä§+\\/{1}{\\det(X)}\\det(X)X^{-1}$\n",
        "  $=Y^‚ä§+X^{-1}=0$\n",
        "  $‚áíX^*=-Y^{-1}$.\n",
        "  Then\n",
        "  $f^*(Y)=h(X^*)=-\\tr(YY^{-1})+\\ln\\det(-Y^{-1})$\n",
        "  $‚áí\\blue{f^*(Y)=-n+\\ln\\det(-Y^{-1})}$ on $\\dom(f^*)=\\v{S}_{--}^n$.\n",
        "\n",
        "- 3.25 (**log-sum-exp**): $\\blue{f(x)=\\ln(\\sum_ie^{x_i})}$.\n",
        "Let $h(x)=y^‚ä§x-\\ln(\\sum_ie^{x_i})$.\n",
        "Then\n",
        "$‚àá_xh(x)=y^‚ä§-\\/{(e^x)^‚ä§}{\\sum_ie^{x_i}}=0$\n",
        "$‚áíy_j=\\/{e^{x_j}}{\\sum_ie^{x_i}}$.\n",
        "Therefore $\\dom(f^*)=\\{y|y‚âΩ0,\\v{1}^‚ä§y=1\\}$ is probability simplex.\n",
        "$x_j^*=\\ln(\\sum_ie^{x_i}y_j)$\n",
        "$=\\ln(\\sum_ie^{x_i})+\\ln(y_j)$.\n",
        "And\n",
        "$f^*(y)=\\sum_jy_i(\\ln(\\sum_ie^{x_i})+\\ln(y_j))-\\ln(\\sum_ie^{x_i})$\n",
        "$=(\\sum_jy_i)\\ln(\\sum_ie^{x_i})+\\sum_jy_i\\ln(y_j)-\\ln(\\sum_ie^{x_i})$\n",
        "$‚áí\\blue{f^*(y)=\\sum_jy_i\\ln(y_j)}$ is sum of negative entropy on $\\dom(f^*)=\\{y|y‚âΩ0,\\v{1}^‚ä§y=1\\}$.\n",
        "\n",
        "- 3.26 (**norm**): Let $\\blue{f(x)=\\n{x}}$ with dual norm $\\n{y}_*=\\sup\\{y^‚ä§x|\\n{x}‚â§1\\}$. Similarly $\\n{x}=\\sup\\{y^‚ä§x|\\n{y}_*‚â§1\\}$ is the support function of set $\\{\\n{y}_*‚â§1\\}$. Therefore $\\blue{f^*(y)=\\BC\n",
        "0&\\n{y}_*‚â§1\\\\\n",
        "‚àû&\\t{otherwise}\n",
        "\\EC}$.\n",
        "\n",
        "  - Proof: Holder inequality $\\red{y^‚ä§x‚â§\\n{x}\\n{y}_*}$\n",
        "  $‚áíy^‚ä§x-\\n{x}‚â§\\n{x}(\\n{y}_*-1)$.\n",
        "  Now $y^‚ä§x-\\n{x}‚â§0\\ ‚àÄ\\n{y}‚â§1$. Specifically $\\sup_x(y^‚ä§x-\\n{x})=0$ when $\\n{y}_*=1$. If $\\n{y}_*>1$, then $\\n{x}‚Üí‚àû‚áí\\n{x}(\\n{y}_*-1)‚Üí‚àû$.\n",
        "\n",
        "- 3.27 (**norm-squared**): Let $\\blue{f(x)=\\/{1}{2}\\n{x}^2}$.\n",
        "Then $y^‚ä§x-\\/{1}{2}\\n{x}^2‚â§\\n{x}\\n{y}_*-\\/{1}{2}\\n{x}^2=h(\\n{x})$.\n",
        "Then $‚àÇ_th(t)=\\n{y}_*-t=0$\n",
        "$‚áít^*=\\n{y}_*$.\n",
        "Therefore\n",
        "$\\blue{f^*(y)=\\/{1}{2}\\n{y}_*^2}$.\n",
        "\n",
        "  - $\\blue{f(x)=\\n{x}^2}$. Then $y^‚ä§x-\\n{x}^2‚â§\\n{x}\\n{y}_*-\\n{x}^2$.\n",
        "  $‚àÇ_{\\n{x}}(\\n{x}\\n{y}_*-\\n{x}^2)=\\n{y}_*-2\\n{x}=0$\n",
        "  $‚áí\\n{x}_\\min=\\/{1}{2}\\n{y}_*$.\n",
        "  Therefore\n",
        "  $\\sup_x(y^‚ä§x-\\n{x}^2)=\\/{1}{4}\\n{y}_*^2$ and\n",
        "  $\\blue{f^*(y)=\\/{1}{4}\\n{y}_*^2}$.\n",
        "\n",
        "- Lecture example: Let $x‚àà‚Ñù_{+}^n$ be the quantities of $n$ widgets produced over a time period. Let $f(x)$ be the cost of production. Let $y‚àà‚Ñù_{+}^n$ be the market prices of the widgets produced. Then $f^*(y)=\\sup_xy^‚ä§x-f(x)$ is the maximum profit made by adjusting the quantity of production as a function of market prices."
      ],
      "metadata": {
        "id": "nrpFUY-0OD6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quasiconvex**: $f:‚Ñù^n‚Üí‚Ñù$ is quasiconvex if its domain and all its sublevel sets $S_Œ±=\\{x‚àà\\dom(f)|f(x)‚â§Œ±\\}$ are convex sets. $f$ is quasiconcave if $-f$ is quasiconvex, and is quasilinear if both quasiconcave and quasiconvex. Many convex function properties have analogs for quasiconvex. Quasiconvex functions are **unimodal**: single valley but may have flat spots.\n",
        "\n",
        "- **Jensen's inequality**: function $f$ is consiconvex iff $\\dom(f)$ is convex and for any $x,y‚àà\\dom(f)$ and $Œ∏‚àà[0,1]$ we have f(mixture)‚â§max f(endpoints) $\\blue{f(Œ∏x+(1-Œ∏)y)‚â§\\max\\{f(x),f(y)\\}}$.\n",
        "\n",
        "  - Proof: let $x,y$ be two points in $S_Œ±$, then $\\max\\{f(x),f(y)\\}‚â§Œ±$. Then any $z=Œ∏x+(1-Œ∏)y$ will satisfy $f(z)‚â§Œ±$. Therefore $S_Œ±$ convex. Similar logic for converse.\n",
        "\n",
        "- **Monotonicity**: Continuous $f$ is quasiconvex on $‚Ñù$ iff one of the conditions hold: $\\BC\n",
        "1.&f\\t{ ‚áë nondecreasing}\\\\\n",
        "2.&f\\t{ ‚áì nonincreasing}\\\\\n",
        "3.&‚àÉc‚àà\\dom\\small(f):f\\t{ ‚áì nonincreasing }‚àÄt‚â§c\\t{ and }f\\t{ ‚áë nondecreasing }‚àÄt‚â•c\n",
        "\\EC$\n",
        "\n",
        "- **First order condition**: If $f$ is differentiable, then $f$ is quasiconvex iff $\\dom(f)$ is convex and for all $x,y‚àà\\dom(f)$ we have $\\blue{f(y)‚â§f(x)‚áí‚àáf(x)^‚ä§(y-x)‚â§0}$.\n",
        "\n",
        "  - If you're standing at $f(x)$ and $f(y)$ is downhills from you, then the path leading toward $y$ must initially go down.\n",
        "\n",
        "  - Proof: Let $Œ±=f(x)$ and $f(y)‚â§f(x)$ then for small $Œ∏‚àà(0,1]$ we have\n",
        "  $f(x+Œ∏(y-x))‚â§f(x)$\n",
        "  $‚áíf(x+Œ∏(y-x))-f(x)‚â§0$\n",
        "  $‚áí\\/{f(x+Œ∏(y-x))-f(x)}{Œ∏}‚â§0$\n",
        "  $‚áí‚àáf(x)^‚ä§(y-x)‚â§0$.\n",
        "  Similar logic for converse.\n",
        "\n",
        "  - $‚àáf(x)$ is the supporting hyperplane normal for the convex sublevel set $\\{z|f(z)‚â§f(x)\\}$.\n",
        "\n",
        "- **Second order condition**: If $f$ is quasiconvex, then for all $x‚àà\\dom(f)$ and $y‚àà‚Ñù^n$ we have $\\blue{y^‚ä§‚àáf(x)=0‚áíy^‚ä§‚àá^2f(x)y‚â•0}$.\n",
        "\n",
        "  - Second order Taylor's\n",
        "  $f(x+ty)=f(x)+t‚àáf(x)^‚ä§y+\\/{t^2}{2}y^‚ä§‚àá^2f(x)y$\n",
        "  $=\\green{f(x)+\\/{t^2}{2}y^‚ä§‚àá^2f(x)y‚â•f(x)}$. Starting at location $x$, moving straight along tangent $y$ off the contour by $t$ amount results in going uphills.\n",
        "\n",
        "- **Preservation of quasiconvexity**:\n",
        "\n",
        "  - Nonnegative weighted maximum: $f=\\max\\{w_1f_1,...,w_mf_m\\}$\n",
        "\n",
        "  - If $g:‚Ñù^n‚Üí‚Ñù$ is quasiconvex and $h:‚Ñù‚Üí‚Ñù$ is nondecreasing $‚áë$, then $f=h‚àòg$ is quasiconvex.\n",
        "\n",
        "  - If $f:‚Ñù^n‚Üí‚Ñù$ is quasiconvex then $g(x)=f(Ax+b)$ is quasiconvex.\n",
        "\n",
        "  - If $f(x,y)$ is quasiconvex jointly in $(x,y)$ and $C$ is convex, then $g(x)=\\inf_{y‚ààC}f(x,y)$ is quasiconvex.\n",
        "\n",
        "- **Convex representation**: to check if $x$ is inside sublevel set $f(x)‚â§t‚áîœï_t(x)‚â§0$ where proxy function $œï_t(x)$ is strictly convex and monotonous: $s‚â•t‚áíœï_s(x)‚â§œï_t(x)$.\n",
        "\n",
        "  - Indicator $œï_t(x)=\\BC 0&f(x)‚â§t\\\\‚àû&\\t{otherwise}\\EC$ is not differentiable. So we use $œï_t(x)=\\t{dist}(x,\\{z|f(z)‚â§t\\})$.\n",
        "\n",
        "- 3.29: $f(x)=\\ln(x)$ is quasiconvex and quasiconcave. $f(x)=\\t{ceil}(x)=\\inf\\{z‚àà‚Ñ§|z‚â•x\\}$ is quasiconvex and quasiconcave.\n",
        "\n",
        "- 3.30: length of a vector $f(x)=\\max\\{i|x_i\\neq0\\}$ has $\\dom(f)=‚Ñù^n$ being a subspace, and all its sublevels are subspaces. It is therefore quasiconvex.\n",
        "\n",
        "- 3.32: **linear-fractional** $f(x)=\\/{a^‚ä§x+b}{c^‚ä§x+d}$ with $\\dom(f)=\\{x|c^‚ä§x+d>0\\}$ is not convex but quasilinear. Sublevels $S_Œ±=\\{x|c^‚ä§x+d>0,a^‚ä§x+b‚â§Œ±c^‚ä§x+Œ±d\\}$ are convex sets therefore linear-fractional functions are quasilinear."
      ],
      "metadata": {
        "id": "DoTPqWdMJBLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log-convexity and log-concavity**: $f:‚Ñù^n‚Üí‚Ñù$ is log-concave if $f(x)>0$ for all $x‚àà\\dom(f)$ and $\\ln f$ is concave. $f$ is log-convex if $\\ln f$ is convex. $f$ is log-convex if $1/f$ is log-concave.\n",
        "\n",
        "- $f:‚Ñù^n‚Üí‚Ñù$  with convex domain and $f(x)>0$ for all $x‚àà\\dom(f)$ is log-concave iff for all $x,y‚àà\\dom(f)$ and $Œ∏‚àà[0,1]$ we have $\\red{f(Œ∏x+(1-Œ∏)y)‚â•f(x)^Œ∏f(y)^{1-Œ∏}}$. And $f$ is log-convex iff $\\red{f(Œ∏x+(1-Œ∏)y)‚â§f(x)^Œ∏f(y)^{1-Œ∏}}$.\n",
        "\n",
        "- Let $f‚ààC^2$ with $\\dom(f)$ convex. Then $‚àá^2\\ln f(x)=\\/{1}{f(x)}‚àá^2f(x)-\\/{1}{f(x)^2}‚àáf(x)‚àáf(x)^‚ä§$ by denom-layout. Then $f$ is $\\blue{\\BC\n",
        "\\t{log-convex }‚áî&f(x)‚àá^2f(x)‚âΩ‚àáf(x)‚àáf(x)^‚ä§\\\\\n",
        "\\t{log-concave }‚áî&f(x)‚àá^2f(x)‚âº‚àáf(x)‚àáf(x)^‚ä§\\\\\n",
        "\\EC}$\n",
        "\n",
        "- Log-convexity is closed under pointwise multiplication and positive-scaling. If $f,g$ are log-concave, then so is pointwise product $h(x)=f(x)g(x)$.\n",
        "\n",
        "- Sums of log-concave functions is NOT in general log-concave. Sums of log-convex functions is log-convex. If $f(x,y)$ is log-convex in $x$ for each $y‚ààC$ then $g(x)=‚à´_Cf(x,y)\\ dy$ is log-convex.\n",
        "\n",
        "  - **Log-sum-exp**: $f(x)=\\ln(\\sum_ie^{x_i})$.\n",
        "  Inner $e^{x_i}$ is log-convex, which is closed under summation and $\\sum_ie^{x_i}$ is log-convex. Therefore $\\ln(\\sum_ie^{x_i})$ is convex.\n",
        "\n",
        "- **Prekopa's theorem**: If $f:‚Ñù^n√ó‚Ñù^m‚Üí‚Ñù$ is log-concave, then $g(x)=‚à´f(x,y)\\ dy$ is log-concave.\n",
        "\n",
        "  - Marginal PDFs of log-concave joint PDFs are log-concave.\n",
        "\n",
        "  - Log-concavity is closed under convolution: If $f,g$ are log-concave, then $(f‚äïg)(x)=‚à´f(x-y)g(y)\\ dy$ is log-concave.\n",
        "\n",
        "  - CDFs of log-concave PDFs are log-concave.\n",
        "\n",
        "- 3.39: Examples\n",
        "\n",
        "  - **Affine functions**: $f(x)=a^‚ä§x+b$ is log-concave on $\\{x|a^‚ä§x+b>0\\}$.\n",
        "\n",
        "  - **Powers**: $f(x)=x^a$ is log-convex for $a‚â§0$ and log-concave for $a‚â•0$.\n",
        "\n",
        "  - **Exponential**: $f(x)=e^{ax}$ is log-convex and log-concave.\n",
        "\n",
        "  - **Normal CDF**: $Œ¶(x)=\\/{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^xe^{-u^2/2}\\ du$ is log-concave (integration preservation)\n",
        "\n",
        "  - **Gamma function**: $Œì(x)=‚à´_0^‚àûu^{x-1}e^{-u}\\ du$ is log-convex for $x‚â•1$ (integration preservation)\n",
        "\n",
        "  - **Determinant**: $f(X)=\\det(X)$ is log-concave on $\\v{S}_{++}^n$\n",
        "\n",
        "- 3.40: **PDFs**: Many common PDFs are log-concave."
      ],
      "metadata": {
        "id": "tThPy89H3ybe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Monotonocity on general inequalities**: wrt proper cone $K‚äÜ‚Ñù^n$, function $f:‚Ñù^n‚Üí‚Ñù$ is K-nondecreasing if $x‚âº_Ky‚áíf(x)‚â§f(y)$ and K-increasing if $x‚âº_Ky‚áíf(x)< f(y)$\n",
        "\n",
        "- $f$ with convex domain is **K-nondecreasing** iff $‚àáf(x)‚âΩ_{K^*}0$ (i.e., $‚àáf(x)^‚ä§(y-x)‚â•0$).\n",
        "\n",
        "  - Proof: Assuming $‚àáf(x)^‚ä§(y-x)‚â•0$ show $y-x‚ààK‚áíf(y)-f(x)‚â•0$.\n",
        "  Let $x,y‚ààD$ convex, then $z(t)=x+t(y-x)‚ààD$ for $t‚àà[0,1]$.\n",
        "  By fundamental theorem of calculus,\n",
        "  $f(y)-f(x)=‚à´_y^x‚àáf(z)\\ dz$\n",
        "  $=‚à´_0^1‚àáf(z(t))^‚ä§(y-x)\\ dt$\n",
        "  $‚â•0$.\n",
        "\n",
        "  - Proof: Assuming $y-x‚ààK‚áíf(y)-f(x)‚â•0$ show $‚àáf(y)^‚ä§v‚â•0$ for all $v‚ààK$. Let $x‚ààD,v‚ààK$, then $x+tv‚ààD,tv‚ààK‚áíf(x+tv)-f(x)‚â•0$\n",
        "  $‚áí‚àáf(x)^‚ä§v=\\lim\\limits_{t‚Üí0}\\/{f(x+tv)-f(x)}{t}‚â•0$\n",
        "\n",
        "- If $‚àáf(x)‚âª_{K^*}0$ then $f$ is **K-increasing**.\n",
        "\n",
        "**Convexity on generalized inequalities**: $f$ is K-convex if $f(Œ∏x+(1-Œ∏)y)‚âº_KŒ∏f(x)+(1-Œ∏)f(y)$.\n",
        "\n",
        "- $f$ is strictly K-convex if $f(Œ∏x+(1-Œ∏)y)‚â∫_KŒ∏f(x)+(1-Œ∏)f(y)$.\n",
        "\n",
        "- Suppose $f:‚Ñù^p‚Üí‚Ñù^q$ then $f(x)$ produces a $q$-dimensional vector with sclar components $F_1,...,F_q$. Then $f$ is K-convex if $F_1(x),...,F_q(x)$ are all convex.\n",
        "\n",
        "- Composition rules on convexity apply to K-convexity as well.\n",
        "\n",
        "- **Dual characterization of K-convexity**: $f$ is K-convex iff $‚àÄŒª‚âΩ_{K^*}0$, we have $Œª^‚ä§f$ is convex.\n",
        "\n",
        "  - Proof: from definition of dual inequality: $x‚âº_Ky$ iff $Œªx‚â§Œªy$ for all $Œª‚ààK^*$.\n",
        "\n",
        "- Example (**Matrix convexity**): function $g(F)‚àà\\v{S}_+^n$ is convex iff $v^‚ä§g(F)v$ is convex for every possible $v‚àà‚Ñù^n$\n",
        "\n",
        "  - Proof: Cone $\\v{S}_+^n$ is self dual. Then $g$ is $\\v{S}_+^n$-convex iff $\\tr(W^‚ä§g(F))$ is convex for every $W‚àà\\v{S}_+^n$.\n",
        "  Decompose $W=PDP^‚ä§=\\sum_iŒª_ip_ip_i^‚ä§=\\sum_iv_iv_i^‚ä§$.\n",
        "  Therefore $g$ is $\\v{S}_+^n$-convex iff $v^‚ä§g(F)v$ is convex for all $v$.\n",
        "\n",
        "- 3.45 (monotone vector function): $f:‚Ñù^n‚Üí‚Ñù$ is nondecreasing wrt $‚Ñù_+^n$ iff $x‚âº_{‚Ñù_+^n}y‚áíf(x)‚â§f(y)$.\n",
        "\n",
        "- 3.46 (monotone matrix functions): $f:\\v{S}^n‚Üí‚Ñù$ can be monotone wrt $\\v{S}_{+}^n$.\n",
        "\n",
        "  - $\\tr(X^{-1})$ is matrix decreasing on $\\v{S}_{++}^n$.\n",
        "  \n",
        "  - $\\det(X)$ is matrix increasing on $\\v{S}_{++}^n$ and matrix nondecreasing on $\\v{S}_+^n$.\n",
        "\n",
        "- 3.47 (componentwise convexity): $f:‚Ñù^n‚Üí‚Ñù^m$ is convex wrt $‚Ñù_+^m$ iff each component $f_i(z)$ is convex $f(Œ∏x+(1-Œ∏)y)‚âºŒ∏f(x)+(1-Œ∏)f(y)$"
      ],
      "metadata": {
        "id": "xUrae60JDV48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3.16: Hessian and convexity\n",
        "\n",
        "  - $f(x)=e^x-1$ on $‚Ñù$ convex, quasiconvex, and quasiconcave.\n",
        "\n",
        "  - $f(x_1,x_2)=x_1x_2$ on $‚Ñù_{++}^2$. $‚àáf=\\BM x_2&x_1\\EM$. $‚àá^2f=\\BM 0&1\\\\1&0\\EM$. It looks like a saddle. Not convex/concave. The level set $\\{x_1,x_2|x_1x_2=1\\}$ looks like a hyperbola, which is quasiconcave, and not quasiconvex.\n",
        "\n",
        "  - $f(x_1,x_2)=1/(x_1x_2)$ on $‚Ñù_{++}^2$. $‚àáf=\\BM -\\/{1}{x_1^2x_2}&-\\/{1}{x_1x_2^2}\\EM$. $‚àá^2f=\\BM\\/{2}{x_1^3x_2}&\\/{1}{x_1^2x_2^2}\\\\\\/{1}{x_1^2x_2^2}&\\/{2}{x_1x_2^3}\\EM$. $\\det(‚àá^2f)=\\/{3}{x_1^4x_2^4}$ convex. The level set $\\{x_1,x_2|1/(x_1x_2)=1\\}$ looks like a hyperbola. It is quasiconvex, and not quasiconcave.\n",
        "\n",
        "  - $f(x_1,x_2)=\\blue{x_1/x_2}$ on $‚Ñù_{++}^2$. Linear-fractional $‚àáf=\\BM1/x_2&-x_1/x_2^2\\EM$. $‚àá^2f=\\BM0&\\/{-1}{x_2^2}\\\\\\/{-1}{x_2^2}&\\/{2x_1}{x_2^3}\\EM$. Not convex/concave. The level set $\\{x_1,x_2|x_1/x_2=1\\}$ is a line. Quasilinear.\n",
        "\n",
        "  - $f(x_1,x_2)=\\blue{x_1^2/x_2}$ on $‚Ñù√ó‚Ñù_{++}$. Quadratic-over-linear $‚àáf=\\BM2x_1/x_2&-x_1^2/x_2^2\\EM$. $‚àá^2f=\\BM 2/x_2&-2x_1/x_2^2\\\\-2x_1/x_2^2&2x_1^2/x_2^3\\EM$.\n",
        "  $\\det(‚àá^2f)=4x_1^2/x_2^4-4x_1^2/x_2^4=0$ positive-semidefinite. Convex and quasiconvex.\n",
        "\n",
        "  - $f(x_1,x_2)=\\blue{x_1^Œ±x_2^{1-Œ±}}$ on $‚Ñù_{++}^2$ and $Œ±‚àà[0,1]$. $‚àáf=\\BM Œ±x_1^{Œ±-1}x_2^{1-Œ±}&(1-Œ±)x_1^Œ±x_2^{-Œ±}\\EM$.\n",
        "  $‚àá^2f=\\BM Œ±(Œ±-1)x_1^{Œ±-2}x_2^{1-Œ±}&Œ±(1-Œ±)x_1^{Œ±-1}x_2^{-Œ±}\\\\\n",
        "  Œ±(1-Œ±)x_1^{Œ±-1}x_2^{-Œ±}&-Œ±(1-Œ±)x_1^Œ±x_2^{-Œ±-1}\\EM$.\n",
        "  $\\det(‚àá^2f)=0$. Negative-semidefinite. Concave and quasiconcave.\n",
        "\n",
        "- 3.17: Show $f(x)=\\blue{\\n{x}_p=(\\sum_ix_i^p)^{1/p}}$ where $\\blue{p< 1,p\\neq0}$ is **concave** on $‚Ñù_{++}^n$.\n",
        "Let $S=\\sum_ix_i^p$ then\n",
        "$‚àÇ_{x_j}f=S^{\\/{1}{p}-1}x_j^{p-1}$.\n",
        "Off-diagonal $‚àÇ_{x_i,x_j}^2=(1-p)S^{\\/{1}{p}-2}x_i^{p-1}x_j^{p-1}$.\n",
        "On-diagonal $‚àÇ_{x_i,x_i}^2=(1-p)S^{\\/{1}{p}-2}x_i^{2p-2}+(p-1)S^{\\/{1}{p}-1}x_i^{p-2}$.\n",
        "Let $z=(x_1^{p-1},...,x_n^{p-1})$\n",
        "Hessian $‚àá^2f=(p-1)S^{\\/{1}{p}-2}(S\\diag(x_i^{p-2})-zz^‚ä§)$.\n",
        "Check negative-semidefinite\n",
        "$v^‚ä§‚àá^2fv=(p-1)S^{\\/{1}{p}-2}((\\sum_ix_i^p)(\\sum_iv_i^2x_i^{p-2})-(\\sum_iv_ix_i^{p-1})^2)$\n",
        "$=(p-1)S^{\\/{1}{p}-2}(\\sum_i(x_i^{p/2})^2\\sum_i(v_ix_i^{p/2-1})^2-(\\sum_iv_ix_i^{p-1})^2)‚â§0$ by Cauchy-Schwarz, therefore concave.\n",
        "\n",
        "- 3.18: matrix functions convexity\n",
        "\n",
        "  - $f(X)=\\blue{\\tr(X^{-1})}$ on $\\v{S}_{++}^n$.\n",
        "  Let $g(t)=\\tr((Z+tV)^{-1})$ where $Z,Z+tV‚àà\\v{S}_{++}^n$.\n",
        "  We only need $g''(0)$ to prove convexity of $f$. Let $Y(t)=(Z+tV)^{-1}$.\n",
        "  Then $Y'(t)=-YVY$ and\n",
        "  $g(t)=\\tr(Y)$\n",
        "  $‚áíg'(t)=\\tr(-YVY)=-\\tr(YVY)$.\n",
        "  $g''(t)=-\\tr(‚àáYVY+YV‚àáY)$\n",
        "  $=2\\tr(YVYVY)$.\n",
        "  Then\n",
        "  $g''(0)=2\\tr(Z^{-1}VZ^{-1}VZ^{-1})$\n",
        "  $=2\\tr((Z^{-1}VZ^{-1/2})(Z^{-1/2}VZ^{-1}))$\n",
        "  $=2\\tr((Z^{-1/2}VZ^{-1})^‚ä§(Z^{-1/2}VZ^{-1}))$\n",
        "  $=2\\n{(Z^{-1/2}VZ^{-1})}_F^2‚â•0$\n",
        "  therefore convex.\n",
        "\n",
        "  - $f(X)=\\blue{(\\det(X))^{1/n}}$ on $\\v{S}_{++}^n$.\n",
        "  Let $g(t)=(\\det(Z+tV))^{1/n}$ where $Z,Z+tV‚àà\\v{S}_{++}^n$.\n",
        "  $\\det(Z+tV)=\\det(Z)\\det(I+tZ^{-1/2}VZ^{-1/2})$\n",
        "  $=\\det(Z)\\prod_i(1+tŒª_i)$ where $Œª_i‚àà‚Ñù$ are eigenvalues of $Z^{-1/2}VZ^{-1/2}$.\n",
        "  Therefore\n",
        "  $g(t)=\\det(Z)^{1/n}(\\prod_{i=1}^n(1+tŒª_i))^{1/n}$,\n",
        "  which is a geometric mean, concave.\n",
        "\n",
        "- 3.19: nonnegative weighted combinations\n",
        "\n",
        "  - $f(x)=\\sum_{i=1}^rŒ±_ix_{[i]}$ given $Œ±_1‚â•...‚â•Œ±_r‚â•0$ and $x_{[1]}‚â•...‚â•x_{[r]}$.\n",
        "  $f(x)$ is the pointwise max of $\\binom{n}{r}$ possible sums of $r$ of $n$ elements. Therefore it is convex.\n",
        "\n",
        "  - Let $T(x,œâ)=x_1+x_2\\cos œâ+...+x_n\\cos (n-1)œâ$. Show $f(x)=-‚à´_{0}^{2œÄ}\\ln T(x,œâ)\\ dœâ$ is convex on $\\{x|T(x,œâ)>0,0‚â§œâ‚â§2œÄ\\}$.\n",
        "  Each $T(x,œâ)$ is an affine combination of $x$ and is convex.\n",
        "  Therefore $-\\ln T(x,œâ)$ is convex.\n",
        "  $f(x)$ is an integration of convex functions of $x$ and is convex.\n",
        "\n",
        "- 3.20: composition with affine\n",
        "\n",
        "  - $f(x)=\\n{Ax-b}$. $h(y)=\\n{y}$ convex. Therefore $f$ is convex.\n",
        "\n",
        "  - $f(x)=-\\det(A_0+x_1A_1+...+x_mA_m)^{1/m}$ where  linear matrix combination is positive-definite. From 3.18 $\\det(X)^{1/n}$ is concave, therefore $f(x)$ is convex.\n",
        "\n",
        "  - $f(x)=\\tr((A_0+x_1A_1+...+x_nA_n)^{-1})$ where linear matrix combination is positive-definite. From 3.18 $\\tr(X^{-1})$ is convex, therefore $f(x)$ is convex.\n",
        "\n",
        "- 3.21: pointwise max/supremum\n",
        "\n",
        "  - $f(x)=\\max_{i=1,...,k}\\n{A^{(i)}x-b^{(i)}}$ where $A^{(i)}‚àà‚Ñù^{m√ón}$, $b‚àà‚Ñù^m$.\n",
        "  $f(x)$ is pointwise maximum of convex functions. Therefore convex.\n",
        "\n",
        "  - $f(x)=\\sum_{i=1}^r|x|_{[i]}$ on $‚Ñù^n$ where $|x|_{[i]}$ is the $i$th largest absolute value of vector $x$.\n",
        "  $f(x)$ is the pointwise max of $\\binom{n}{r}$ possible sums and is therefore convex.\n",
        "\n",
        "- 3.22: composition rules:\n",
        "\n",
        "  - $f(x)=-\\ln(-\\ln(\\sum_ie^{a_i^‚ä§x+b_i}))$ on $\\{x|S< 1\\}$ where $S=\\sum_ie^{a_i^‚ä§x+b_i}$.\n",
        "  Here $S(x)$ is convex.\n",
        "  $-\\ln(S)>0$ is concave ($h$ convex and nonincreasing, $g$ convex).\n",
        "  Therefore $f(x)$ is convex.\n",
        "\n",
        "  - $f(x,u,v)=-\\sqrt{uv-x^‚ä§x}$ on $\\{x,u,v|uv>x^‚ä§x,u,v>0\\}$.\n",
        "  $f(x,u,v)=-\\sqrt{u(v-\\/{x^‚ä§x}{u})}$.\n",
        "  Here\n",
        "  $\\/{x^‚ä§x}{u}$ is quadratic-over-linear convex,\n",
        "  $v-\\/{x^‚ä§x}{u}$ is concave.\n",
        "  From 3.16\n",
        "  $\\sqrt{x_1x_2}$ is concave,\n",
        "  $-\\sqrt{x_1x_2}$ is convex and decreasing.\n",
        "  Therefore $f(x)$ is convex.\n",
        "\n",
        "  - $f(x,u,v)=-\\ln(uv-x^‚ä§x)$ on $\\{x,u,v|uv>x^‚ä§x,u,v>0\\}$.\n",
        "  $f(x,u,v)=-\\ln(u)-\\ln(v-\\/{x^‚ä§x}{u})$.\n",
        "  Here $-\\ln(u)$ is convex.\n",
        "  $v-\\/{x^‚ä§x}{u}$ is concave,\n",
        "  and $-\\ln(‚ãÖ)$ is convex and decreasing,\n",
        "  then $-\\ln(v-\\/{x^‚ä§x}{u})$ is convex.\n",
        "  Therefore $f(x,u,v)$ is convex.\n",
        "\n",
        "  - $f(x,t)=-(t^p-\\n{x}_p^p)^{1/p}$ where $p>1$ on $\\{x,t|t‚â•\\n{x}_p\\}$.\n",
        "  $f(x,t)=-t^{1-\\/{1}{p}}(t-\\/{\\n{x}_p^p}{t^{p-1}})^{\\/{1}{p}}$.\n",
        "  Here $\\/{\\n{x}_p^p}{t^{p-1}}$ is convex,\n",
        "  $t-\\/{\\n{x}_p^p}{t^{p-1}}$ is concave.\n",
        "  From 3.16\n",
        "  $x_1^{1-\\/{1}{p}}x_2^{\\/{1}{p}}$ is concave,\n",
        "  $-x_1^{1-\\/{1}{p}}x_2^{\\/{1}{p}}$ is convex and decreasing,\n",
        "  therefore $f(x,t)$ is convex.\n",
        "\n",
        "  - $f(x,t)=-\\ln(t^p-\\n{x}_p^p)$ where $p>1$ on $\\{x,t|t>\\n{x}_p\\}$.\n",
        "  $f(x,t)=-\\ln(t^{1-\\/{1}{p}})-\\ln(t-\\/{\\n{x}_p^p}{t^{p-1}})$.\n",
        "  Here $t-\\/{\\n{x}_p^p}{t^{p-1}}$ is concave,\n",
        "  $-\\ln(t-\\/{\\n{x}_p^p}{t^{p-1}})$ is convex.\n",
        "  $t^{1-\\/{1}{p}}$ is concave, $-\\ln(t^{1-\\/{1}{p}})$ is convex.\n",
        "  Therefore $f(x)$ is convex.\n",
        "\n",
        "- 3.23: Perspective: if $g(x)$ is convex then so is $tg(x/t)$.\n",
        "\n",
        "  - Show $p>1$ then $f(x,t)=\\blue{\\/{\\n{x}_p^p}{t^{p-1}}}$ is convex on $\\{x,t|t>0\\}$.\n",
        "  Base function $g(x)=\\n{x}_p^p=\\sum_i|x_i|^p$ is convex.\n",
        "  Then $tg(\\/{x}{t})=t\\sum_i|\\/{x_i}{t}|^p$\n",
        "  $=\\/{1}{t^{p-1}}\\sum_i|x_i|^p$\n",
        "  $=\\/{\\n{x}_p^p}{t^{p-1}}$ is convex.\n",
        "\n",
        "  - Show $f(x)=\\/{\\n{Ax+b}_2^2}{c^‚ä§x+d}$ is convex on $\\{x|c^‚ä§x+d>0\\}$.\n",
        "  By affine composition $g(x)=\\n{Ax+b}_2^2$ is convex.\n",
        "  Path #1: $(c^‚ä§x+d)g(\\/{x}{c^‚ä§x+d})$\n",
        "  $=(c^‚ä§x+d)\\sum_i|\\/{Ax_i+b}{c^‚ä§x+d}|^2$\n",
        "  $=\\/{1}{c^‚ä§x+d}\\sum_i|Ax_i+b|^2$\n",
        "  $=\\/{\\n{Ax+b}_2^2}{c^‚ä§x+d}$\n",
        "  is convex.\n",
        "  Path #2: $f(x)=\\/{(Ax+b)^‚ä§(Ax+b)}{c^‚ä§x+d}$ is quadratic-over-linear, which is convex.\n",
        "\n",
        "- 3.24: Probability simplex: Let $x$ be random variable with support $\\{a_1< ...< a_n\\}$ and probability $P(x=a_i)=p_i$. Determine convexity and quasiconvexity of functions on simplex $\\{p‚àà‚Ñù_+^n|\\v{1}^‚ä§p=1\\}$.\n",
        "\n",
        "  - $f(p)=\\E[x]=\\sum_ia_ip_i$ is a linear function of $p$. Linear and therefore quasilinear.\n",
        "\n",
        "  - $f(p)=P(x‚â•Œ±)=\\sum_{i‚â•k}p_i$ is a linear function of $p$. Linear and quasilinear.\n",
        "\n",
        "  - $f(p)=P(Œ±‚â§x‚â§Œ≤)=\\sum_{k‚â§i‚â§l}p_i$ is a linear function of $p$. Linear and quasilinear.\n",
        "\n",
        "  - **Negative entropy** $f(p)=\\blue{\\sum_ip_i\\ln(p_i)}=p^‚ä§\\ln(p)$.\n",
        "  $‚àáf=\\ln(p)+n$.\n",
        "  $‚àá^2f=\\/{1}{p}‚âª0$\n",
        "  therefore convex and quasiconvex.\n",
        "\n",
        "  - $f(p)=\\Var(x)=\\E[x^2]-\\E[x]^2$.\n",
        "  $\\E[x^2]=\\sum_ia_i^2p_i$ is a linear function of $p$.\n",
        "  $-\\E[x]^2$ is a concave function of $p$.\n",
        "  Together $f(p)$ is negative quadratic function of $p$ and is concave and quasiconcave.\n",
        "\n",
        "- 3.26: **Eigenvalues**\n",
        "\n",
        "  - Largest eigenvalue $Œª_1(X)=\\sup\\{y^‚ä§Xy|\\n{y}_2=1\\}$ is convex.\n",
        "\n",
        "  - Smallest eigenvalue $Œª_n(X)=\\inf\\{y^‚ä§Xy|\\n{y}_2=1\\}$ is concave.\n",
        "\n",
        "  - Sum of eigenvalues $\\tr(X)$ is linear.\n",
        "\n",
        "  - Sum of inverse of eigenvalues $\\tr(X^{-1})$ is convex (3.18)\n",
        "\n",
        "  - Geometric mean of eigenvalues $\\det(X)^{1/n}$ is concave (3.18)\n",
        "\n",
        "  - Sum of $k$ largest eigenvalues $\\sum_{i=1}^kŒª_i(X)=\\sup\\{\\tr(V^‚ä§XV)|V‚àà‚Ñù^{n√ók},V^‚ä§V=I\\}$.\n",
        "  $\\tr(V^‚ä§XV)=\\tr(VV^‚ä§X)=‚ü®VV^‚ä§,X‚ü©$ is linear function of $X$.\n",
        "  Pointwise maximum of convex functions is convex.\n",
        "  \n",
        "- 3.36: Conjugate functions\n",
        "\n",
        "  - **Max**: $\\blue{f(x)=\\max_{i=1,...,n}x_i}$ on $‚Ñù^n$.\n",
        "  Variational characterization\n",
        "  $f(x)=\\sup\\{z^‚ä§x|0‚âºz‚âº\\v{1},\\v{1}^‚ä§z=1\\}$, which forces $z=(1,0,...,0)$ for $x_1=\\max_ix_i$.\n",
        "  This means $f$ is the support function of $S=\\{0‚âºz‚âº\\v{1},\\v{1}^‚ä§z=1\\}$.\n",
        "  Therefore $f^*(y)=\\BC\n",
        "  0&\\v{1}^‚ä§y=1,y‚âΩ0 \\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$ is indicator function for probability simplex.\n",
        "\n",
        "  - **Sum of largest**: $\\blue{f(x)=\\sum_{i=1}^rx_{[i]}}$ on $‚Ñù^n$.\n",
        "  Variational characterization\n",
        "  $f(x)=\\sup\\{z^‚ä§x|0‚âºz‚âº\\v{1},\\v{1}^‚ä§z=r\\}$, which forces $z_i$ to be either 0 or 1.\n",
        "  This means $f$ is the support function $f(x)=\\sup_{z‚ààS}z^‚ä§x$ where $S=\\{z|0‚âºz‚âº\\v{1},\\v{1}^‚ä§z=r\\}$.\n",
        "  Therefore $f^*(y)=\\BC\n",
        "  0&0‚âºy‚âº\\v{1},\\v{1}^‚ä§y=r\\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$.\n",
        "\n",
        "  - **Piecewise-linear**: $\\blue{f(x)=\\max_i(a_ix+b_i)}$ on $‚Ñù$. No piece is redundant: $f(x)=f_i(x)=a_ix+b_i$ for some $x$. The slopes are ordered $a_1‚â§...‚â§a_m$.\n",
        "  $\\BC\n",
        "  \\dom(f^*)=[a_1,a_n] &\\t{are slopes of }f_i(x)\\\\\n",
        "  f^*(a_k)=-b_k &\\t{are $f$-intersept of }f_i(x)\n",
        "  \\EC$.\n",
        "  $f^*(y)$ is also piecewise-linear, with lines joining at vertices $(a_k,-b_k)$.\n",
        "  For $y‚àà(a_k,a_{k+1})$, we have $f_k$ and $f_{k+1}$ joining at $x_k$ and satisfy\n",
        "  $f_k(x_k)=f_{k+1}(x_k)$\n",
        "  $‚áía_kx_k+b_k=a_{k+1}x_k+b_{k+1}$\n",
        "  $‚áíx_k=\\/{b_k-b_{k+1}}{a_{k+1}-a_k}$ is the slope of the conjugate line.\n",
        "  The sharp corner at $x_k$ between $f_k$ and $f_{k+1}$ has spawned the entire set of slopes $y‚àà[a_k,a_{k+1}]$ in the conjugate domain forming a line segment whose slope is $x_k$.\n",
        "  We have\n",
        "  $f^*(y)=yx_k-f(x_k)$\n",
        "  $=x_k(y-a_k)-b_k$\n",
        "  $=\\/{b_k-b_{k+1}}{a_{k+1}-a_k}(y-a_k)-b_k$.\n",
        "  Duality\n",
        "  $\\blue{\\BC\n",
        "  f(x)&\\t{line segment slope }a_k&\\t{sharp corner }x=x_k\\\\\n",
        "  f^*(y)&\\t{sharp corner at }y=a_k&\\t{line segment slope }x_k\n",
        "  \\EC}$\n",
        "\n",
        "  - **Power**: $\\blue{f(x)=x^p}$ on $‚Ñù_{++}$.\n",
        "  $h(x)=y^‚ä§x-x^p$.\n",
        "  $h'(x)=y-px^{p-1}=0$\n",
        "  $‚áíx^*=(\\/{y}{p})^{\\/{1}{p-1}}$.\n",
        "  $\\dom(f)=‚Ñù_{++}‚áí\\BC\n",
        "  \\dom(f^*)=‚Ñù_{++} &\\t{if }p>1\\\\\n",
        "  \\dom(f^*)=‚Ñù_{--} &\\t{if }p< 0\n",
        "  \\EC$.\n",
        "  $f^*(y)=h(x^*)=y(\\/{y}{p})^{\\/{1}{p-1}}-(\\/{y}{p})^{\\/{p}{p-1}}$\n",
        "  $=(p-1)(\\/{y}{p})^{\\/{p}{p-1}}$\n",
        "\n",
        "  - **Negative geometric mean**: $\\blue{f(x)=-(\\prod_ix_i)^{1/n}}$ on $‚Ñù_{++}^n$.\n",
        "  $f$ is convex and satisfies $f(tx)=tf(x)$, therefore is support function of $S=\\{y|y^‚ä§x‚â§f(x)\\ ‚àÄx‚àà\\dom(f)\\}$\n",
        "  $=\\{y|y^‚ä§x+(\\prod_ix_i)^{1/n}‚â§0\\ ‚àÄx‚àà‚Ñù_{++}^n\\}$.\n",
        "  $\\BC\n",
        "  1. &\\t{immediately clear requirement}&\\green{y_i‚â§0}\\\\\n",
        "  2. &\\t{Arithmetic-geometric inequality}&\\/{1}{n}\\sum_iz_i‚â•(\\prod_iz_i)^{1/n}\\\\\n",
        "  &\\t{Let }z_i=-y_ix_i&\\sum_i-y_ix_i‚â•(\\prod_ix_i)^{1/n}\\\\\n",
        "  &&\\sum_iz_i‚â•(\\prod_i\\/{z_i}{-y_i})^{1/n}\\\\\n",
        "  &&(\\prod_i-y_i)^{1/n}\\sum_iz_i‚â•(\\prod_iz_i)^{1/n}\\\\\n",
        "  &\\t{AG inequality is tightest}&(\\prod_i-y_i)^{1/n}\\sum_iz_i‚â•\\/{1}{n}\\sum_iz_i‚â•(\\prod_iz_i)^{1/n}\\\\\n",
        "  &&\\green{(\\prod_i-y_i)^{1/n}‚â•\\/{1}{n}}\n",
        "  \\EC$.\n",
        "  $S=\\{y|y‚âº0,(\\prod_i-y_i)^{1/n}‚â•\\/{1}{n}\\}$.\n",
        "  $f^*(y)=\\BC\n",
        "  0&y‚âº0,(\\prod_i-y_i)^{1/n}‚â•\\/{1}{n}\\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$.\n",
        "\n",
        "- 3.37: $f(X)=\\tr(X^{-1})$ on $\\v{S}_{++}^n$.\n",
        "$h(X)=\\tr(XY)-\\tr(X^{-1})$.\n",
        "$‚àáh(X)=Y+X^{-2}=0$\n",
        "$‚áíX^*=(-Y)^{-1/2}$.\n",
        "$Y‚àà‚Ñù_{-}^n$.\n",
        "$f^*(Y)=\\tr((-Y)^{-1/2}Y)-\\tr((-Y)^{1/2})$\n",
        "$=-\\tr((-Y)^{-1/2}(-Y))-\\tr((-Y)^{1/2})$\n",
        "$=-\\tr((-Y)^{1/2})-\\tr((-Y)^{1/2})$\n",
        "$=-2\\tr((-Y)^{1/2})$\n",
        "on $‚Ñù_{-}^n$.\n",
        "\n",
        "- 3.39 Properties of conjugate:\n",
        "\n",
        "  - **Convex + affine**: $\\blue{g(x)=f(x)+c^‚ä§x+d}$ where $f$ convex.\n",
        "  $g^*(y)=\\sup_xy^‚ä§x-f(x)-c^‚ä§x-d$\n",
        "  $=\\sup_x(y-c)^‚ä§x-f(x)-d$.\n",
        "  $\\blue{g^*(y)=f^*(y-c)-d}$ on\n",
        "  $\\dom(g^*)=\\{y|y-c‚àà\\dom(f^*)\\}$\n",
        "\n",
        "  - **Perspective**: $\\blue{g(x,t)=tf(\\/{x}{t})}$ is perspective function of $f(x)$ on $\\{x,t|t>0,x/t‚àà\\dom(f)\\}$.\n",
        "  Let $z=x/t$ then\n",
        "  $g^*(y,s)=\\sup_{x,t}y^‚ä§x+st-tf(x/t)$\n",
        "  $=\\sup_{z,t}y^‚ä§(tz)+st-tf(z)$\n",
        "  $=\\sup_tt\\sup_z(y^‚ä§z+s-f(z))$\n",
        "  $=\\sup_tt(f^*(y)+s)$\n",
        "  $‚áíg^*(y,s)=\\BC\n",
        "  0&f^*(y)+s‚â§0\\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$\n",
        "\n",
        "  - **Partial infimum**: $\\blue{g(x)=\\inf_zf(x,z)}$ where $f(x,z)$ is convex in $(x,z)$.\n",
        "  $g^*(y)=\\sup_x(y^‚ä§x-\\inf_zf(x,z))$\n",
        "  $=\\sup_x(y^‚ä§x+\\sup_z(-f(x,z)))$\n",
        "  $=\\sup_{x,z}y^‚ä§x-f(x,z)$\n",
        "  $=\\sup_{x,z}y^‚ä§x+0z-f(x,z)$\n",
        "  $‚áí\\blue{g^*(y)=f^*(y,0)}$\n",
        "\n",
        "  - **Partial infimum**: $\\blue{g(x)=\\inf_z\\{f(z)|Az+b=x\\}}$ where $f(z)$ is convex.\n",
        "  Let $F(x,z)=\\BC\n",
        "  f(z)&Az+b=x\\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$, then\n",
        "  $g(x)=\\inf_zF(x,z)$.\n",
        "  $F^*(y,v)=\\sup_{x,z}y^‚ä§x+v^‚ä§z-F(x,z)$\n",
        "  $=\\sup_zy^‚ä§(Az+b)+v^‚ä§z-f(z)$\n",
        "  $=y^‚ä§b+\\sup_z(A^‚ä§y+v)^‚ä§z-f(z)$\n",
        "  $‚áí\\green{F^*(y,v)=y^‚ä§b+f^*(A^‚ä§y+v)}$.\n",
        "  $g^*(y)=F^*(y,0)=y^‚ä§b+f^*(A^‚ä§y)$.\n",
        "\n",
        "- 3.40 **Gradient and Hessian of conjugate**: $f:‚Ñù^n‚Üí‚Ñù$ is convex and $C^2$. Suppose $\\bar{y}=‚àáf(\\bar{x})$ and $‚àá^2f(\\bar{x})‚âª0$.\n",
        "\n",
        "  - Show $\\blue{‚àáf^*(\\bar{y})=\\bar{x}}$.\n",
        "  For $f^*(y)=\\sup_xy^‚ä§x-f(x)$, at supremum $x^*=\\arg\\sup_xy^‚ä§x-f(x)$ must satisfy\n",
        "  $\\green{y=‚àáf(x^*)}$.\n",
        "  Let $x^*(y)$ be the supremum $x^*$ for given $y$ satisfying $y=‚àáf(x^*(y))$, then\n",
        "  $f^*(y)=y^‚ä§x^*(y)-f(x^*(y))$\n",
        "  $‚áí‚àáf^*(y)=x^*(y)+y^‚ä§‚àáx^*(y)-‚àáf(x^*(y))^‚ä§‚àáx^*(y)$\n",
        "  $=x^*(y)+(y-‚àáf(x^*(y)))^‚ä§‚àáx^*(y)$\n",
        "  $‚áí\\green{‚àáf^*(y)=x^*(y)}$.\n",
        "\n",
        "  - Show $\\blue{‚àá^2f^*(\\bar{y})=‚àá^2f(\\bar{x})^{-1}}$.\n",
        "  $\\BC\n",
        "  y=‚àáf(x^*(y))&‚áí\n",
        "  I=‚àá^2f(x^*(y))‚àáx^*(y)\\\\\n",
        "  ‚àáf^*(y)=x^*(y)&‚áí\n",
        "  ‚àá^2f^*(y)=‚àáx^*(y)\n",
        "  \\EC$\n",
        "  $‚áí\\green{I=‚àá^2f(x^*(y))‚àá^2f^*(y)}$\n",
        "\n",
        "- 3.41 **Negative normalized entropy**: $\\blue{f(x)=\\sum_{i=1}^nx_i\\ln(\\/{x_i}{\\v{1}^‚ä§x})}$ on $‚Ñù_{++}^n$.\n",
        "Normalized entropy $\\sum_ix_i\\ln(\\/{\\v{1}^‚ä§x}{x_i})$ is concave therefore $f(x)$ is convex. Furthermore $f(x)$ is positively homogeneous $f(tx)=tf(x)$ therefore $f$ is a support function for set $S=\\{y|y^‚ä§x‚â§f(x)\\}$.\n",
        "Let $p_i=\\/{x_i}{\\v{1}^‚ä§x}$ be probability simplex, then\n",
        "$y^‚ä§x‚â§\\sum_ix_i\\ln(\\/{x_i}{\\v{1}^‚ä§x})$\n",
        "$‚áí\\sum_iy_ix_i‚â§\\sum_ix_i\\ln(\\/{x_i}{S})$\n",
        "$‚áí\\sum_ix_i(y_i-\\ln(\\/{x_i}{S}))‚â§0$\n",
        "$‚áíS\\sum_ip_i(y_i-\\ln(p_i))‚â§0$\n",
        "$‚áí\\sum_ip_iy_i-\\sum_ip_i\\ln(p_i)‚â§0$\n",
        "$‚áí\\sup_pp^‚ä§y-p\\ln(p)‚â§0$\n",
        "$‚áíg^*(y)‚â§0$ where convex negative entropy $\\green{g(p)=p\\ln(p)}$ is conjugate of convex log-sum-exp $\\green{g^*(y)=\\ln(\\sum_ie^{y_i})}$ (example 3.25).\n",
        "Therefore $f$ is support function for\n",
        "$S=\\{y|\\ln(\\sum_ie^{y_i})‚â§0\\}$.\n",
        "$f^*(y)=\\BC\n",
        "0&\\sum_ie^{y_i}‚â§1\\\\\n",
        "‚àû&\\t{otherwise}\n",
        "\\EC$.\n",
        "\n",
        "- 3.52 **moment functions**: $f$ is nonnegative on $‚Ñù_+$. Show $\\blue{œï(x)=‚à´_0^‚àûu^xf(u)\\ du}$ is log-convex.\n",
        "By Holder inequality $\\/{1}{p}=Œ∏$ and $\\/{1}{q}=1-Œ∏$ we have\n",
        "$œï(Œ∏x+(1-Œ∏)y)=‚à´_0^‚àûu^{Œ∏x+(1-Œ∏)y}f(u)\\ du$\n",
        "$=‚à´_0^‚àû(u^xf(u))^Œ∏(u^yf(u))^{1-Œ∏}\\ du$\n",
        "$‚â§(‚à´_0^‚àû(u^xf(u))^{Œ∏\\/{1}{Œ∏}}\\ du)^Œ∏(‚à´_0^‚àû(u^xf(u))^{(1-Œ∏)\\/{1}{1-Œ∏}}\\ du)^{1-Œ∏}$\n",
        "$‚áíœï(Œ∏x+(1-Œ∏)y)‚â§œï(x)^Œ∏œï(x)^{1-Œ∏}$.\n",
        "\n",
        "- 3.53: Suppose $x,y$ are independent random variables with log-concave PDF $f,g$. Show $z=x+y$ is log-concave.\n",
        "Let $h(z)$ be the PDF of $z$, then\n",
        "$h(z)=(f‚äïg)(z)=‚à´_tf(z-t)g(t)\\ dt$.\n",
        "If $f,g$ are log-concave, then log-concavity is closed under pointwise product $f(z-t)g(t)$ and integral $‚à´_tf(z-t)g(t)\\ dt$. Therefore $h(z)$ is log-concave."
      ],
      "metadata": {
        "id": "wtAlAljSXaJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Convex Optimization Problems"
      ],
      "metadata": {
        "id": "V4sbAQwnPNNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimization problem** standard form $p^*=\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0,&i=1,...,m\\\\\n",
        "&h_j(x)=0,&j=1,...,p\n",
        "\\EC$\n",
        "\n",
        "- $f_0:‚Ñù^n‚Üí‚Ñù$ is the objective function, $x‚àà‚Ñù^n$ is the optimization variable. $f_i(x)‚â§0,h_j(x)=0$ are inequality and equality functions and constraints.\n",
        "$\\m{D}=\\bigcap_i\\dom(f_i)‚à©\\bigcap_j\\dom(h_i)$ is the domain of the problem.\n",
        "Point $x$ is **feasible** if it satisfies all constraints. Feasible set $\\m{F}=\\{x‚àà\\m{D}|f_i(x)‚â§0\\ ‚àÄi,h_j(x)=0\\ ‚àÄj\\}$.\n",
        "\n",
        "- If $\\m{F}\\neq‚àÖ$ then the problem is feasible and **optimal value** $p*=\\inf_x\\{f_0(x)|x‚àà\\m{F}\\}$.\n",
        "Point $x^*‚àà\\m{F}$ is an **optimal point** if $f_0(x^*)=p^*$.\n",
        "Optimal set\n",
        "$X_\\t{opt}=\\{x‚àà\\m{F}|,f_0(x)=p^*\\}$.\n",
        "If $\\m{F}=‚àÖ$ then $p^*=‚àû$.\n",
        "\n",
        "  - Problem is unbounded below if there are $x_k‚àà\\m{F}$ such that $f_0(x_k)‚Üí-‚àû$.\n",
        "  The nature of the problem is minimization. If the objective function is downward sloping, and there is no constraint limiting movements toward $-‚àáf_0$, then it is unbounded below and the optimal set is empty.\n",
        "\n",
        "  - $x‚àà\\m{F}$ satisfying $f_0(x)‚â§p^*+œµ$ is $œµ$-suboptimal.\n",
        "\n",
        "  - Point $z‚àà\\m{F}$ is **locally optimal** if $f_0(z)=\\inf\\{f_0(x)|f_i(x)‚â§0\\ ‚àÄi,h_j(x)=0\\ ‚àÄj,\\n{x-z}_2‚â§R\\}$ for $R>0$.\n",
        "  Optimization problem with an added constraint $\\n{x-z}_2‚â§R$ for a smaller $\\m{F}$.\n",
        "\n",
        "- **Feasibility problem**: $\\BC\n",
        "\\t{find}&x\\\\\n",
        "\\t{subject to}&x‚àà\\m{F}\\EC‚áí\\BC\n",
        "\\t{minimize}&0\\\\\n",
        "\\t{subject to}&x‚àà\\m{F}\\EC$ returns $p^*=\\BC0&x^*‚àà\\m{F}\\\\‚àû&x^*‚àà‚àÖ\\EC$.\n",
        "\n",
        "- If $x‚àà\\m{F}$ and $f_i(x)=0$, then the $i$-th inequality constraint is **active** at $x$.\n",
        "If $f_i(x)< 0$ then constraint $f_i(x)‚â§0$ is inactive. Constraint is redundant if deleting it does not change $\\m{F}$.\n",
        "\n",
        "**Equivalent problems**: solution of one problem is readily obtained from the solution of another.\n",
        "\n",
        "- **Maximizing** $f_0(x)$ is equivalent to minimizing $-f_0(x)$.\n",
        "\n",
        "- **Change of variable**: Let $œï:‚Ñù^n‚Üí‚Ñù^n$, $\\m{D}‚äÜœï(\\dom(œï))$ such that $\\tilde{f}_i(x)=f_i(œï(x))$ and $\\tilde{h}_j(x)=h_j(œï(x))$.\n",
        "\n",
        "  - $\\BC \\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0\\\\\n",
        "  &h_j(x)=0\\EC‚áí\\BC \\t{minimize}&\\tilde{f}_0(x)\\\\\n",
        "  \\t{subject to}&\\tilde{f}_i(x)‚â§0\\\\\n",
        "  &\\tilde{h}_j(x)=0 \\EC$\n",
        "\n",
        "- **Transformation of functions**: $œà_0$ is monotone increasing, $œà_i(u)‚â§0‚áîu‚â§0$, and $œà_{m+j}(u)=0‚áîu=0$, such that $\\tilde{f}_i(x)=œà_i(f_i(x))$ and $\\tilde{h}_j(x)=œà_{m+j}(h_j(x))$.\n",
        "\n",
        "  - $\\BC \\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0\\\\\n",
        "  &h_j(x)=0 \\EC‚áí\\BC \\t{minimize}&\\tilde{f}_0(x)\\\\\n",
        "  \\t{subject to}&\\tilde{f}_i(x)‚â§0\\\\\n",
        "  &\\tilde{h}_j(x)=0 \\EC$\n",
        "\n",
        "- **Slack variables**: $f_i(x)‚â§0$ becomes $f_i(x)+s_i=0$ and $s_i‚â•0$.\n",
        "\n",
        "  - $\\BC \\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0\\\\\n",
        "  &h_j(x)=0 \\EC‚áí\\BC \\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&s_i‚â•0\\\\\n",
        "  &f_i(x)+s_i=0\\\\\n",
        "  &h_j(x)=0 \\EC$\n",
        "\n",
        "- **Eliminating equality constraints**: Let $z‚àà‚Ñù^k$ and $œï:‚Ñù^k‚Üí‚Ñù^n$ such that $h_j(œï(z))=0$ is always true.\n",
        "\n",
        "  - $\\BC \\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0\\\\\n",
        "  &h_j(x)=0 \\EC‚áí\\BC \\t{minimize}&f_0(œï(z))\\\\\n",
        "  \\t{subject to}&f_i(œï(z))‚â§0 \\EC$\n",
        "\n",
        "  - Linear example:\n",
        "  $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0 \\\\\n",
        "  &Ax-b=0\\EC‚áí\\BC \\t{minimize}&f_0(z_0+Fz) \\\\\n",
        "  \\t{subject to}&f_i(z_0+Fz)‚â§0\\EC$.\n",
        "  Let $A‚àà‚Ñù^{m√ón}$ and $F‚àà‚Ñù^{n√ók}$ such that $b‚àà\\span(A)$ and $\\span(F)=\\null(A)$.\n",
        "  Solutions to $Ax=b$ are $x‚àà\\{x_0+\\null(A)\\}$ where $x_0$ is one solution.\n",
        "  Now the dimensionality of the solution has been reduced by $\\rk(A)$ from $n$ variables to $k$ variables.\n",
        "\n",
        "- **Introducing equality constraints**: Create a slack variable $y_i$ for each affine pre-composition $A_ix+b_i$. The objective and constraints become independent with different optimization variables. This is for parallel computing.\n",
        "\n",
        "  - $\\BC\\t{minimize}&f_0(A_0x+b_0)\\\\\n",
        "  \\t{subject to}&f_i(A_ix+b_i)‚â§0\\\\\n",
        "  &h_j(x)=0\\EC ‚áí \\BC\\t{minimize }(y_i,x)&f_0(y_0)\\\\\n",
        "  \\t{subject to}&f_i(y_i)‚â§0\\\\\n",
        "  &y_i=A_ix+b\\\\\n",
        "  &h_j(x)=0\\EC$\n",
        "\n",
        "- **Partial optimization**: $\\inf_{x,y}f(x,y)=\\inf_x(\\inf_yf(x,y))$. Useful when $\\inf_yf(x,y)$ has closed form or easy to solve.\n",
        "\n",
        "  - Let $x‚àà‚Ñù^{n_1+n_2},x_1‚àà‚Ñù^{n_1},x_2‚àà‚Ñù^{n_2}$, and $\\tilde{f}_0(x_1)=\\inf_z\\{f_0(x_1,z)|g_j(z)‚â§0\\}$. Then\n",
        "  $\\BC\\t{minimize}&f_0(x_1,x_2)\\\\\n",
        "  \\t{subject to}&f_i(x_1)‚â§0\\\\\n",
        "  &g_j(x_2)‚â§0\\EC ‚áí \\BC\\t{minimize}&\\tilde{f}_0(x_1)\\\\\n",
        "  \\t{subject to}&f_i(x_1)‚â§0\\EC$\n",
        "\n",
        "- **Epigraph form**: $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&h_j(x)=0\n",
        "\\EC ‚áí\\BC\\t{minimize }(x,t)&\\red{t}\\\\\n",
        "\\t{subject to}&\\red{f_0(x)-t‚â§0}\\\\\n",
        "&f_i(x)‚â§0\\\\\n",
        "&h_j(x)=0\\EC$\n",
        "\n",
        "  - All convex optimization problems can be rewritten in epigraph form.\n",
        "\n",
        "- **Implicit constraints**: $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&h_j(x)=0\n",
        "\\EC ‚áí \\BC\\t{minimize}&F(x)\\EC$ where\n",
        "$F(x)=\\BC f_0(x)&f_i(x)‚â§0,h_j(x)=0\\\\\n",
        "‚àû&\\t{otherwise}\n",
        "\\EC$\n",
        "\n",
        "- 4.4 (Partial optimization): $\\BC\\t{minimize}&\\BM x_1\\\\x_2\\EM^‚ä§\\BM P_{11}&P_{12}\\\\P_{12}^‚ä§&P_{22}\\EM\\BM x_1\\\\x_2\\EM\\\\\n",
        "\\t{subject to}&f_i(x_1)‚â§0\\EC ‚áí \\BC\n",
        "\\t{minimize}&x_1^‚ä§(P_{11}-P_{12}P_{22}^{-1}P_{12}^‚ä§)x_1\\\\\n",
        "\\t{subject to}&f_i(x_1)‚â§0\\EC$.\n",
        "By 3.15 Schur Complement, $\\inf_{x_2}x_1^‚ä§P_{11}x_1+2x_1^‚ä§P_{12}x_2+x_2^‚ä§P_{22}x_2=x_1^‚ä§(P_{11}-P_{12}P_{22}^{-1}P_{12}^‚ä§)x_1$"
      ],
      "metadata": {
        "id": "yxjQWlkaPULn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convex optimization problem**\n",
        "$\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0,&i=1,...,m&f_i\\t{ convex}\\\\\n",
        "&Ax=b,&&h_j\\t{ affine}\\EC$\n",
        "\n",
        "- $\\m{D}=\\bigcap_i\\dom(f_i)$ is the intersection of $m$ sublevel sets and $p$ hyperplanes. $a_i\\neq0$ otherwise the problem is infeasible. Therefore convex optimization is minimizing convex objective function over a convex set.\n",
        "\n",
        "  - If $f_0$ is quasiconvex, the problem is quasiconvex optimization problem where its sublevel $œµ$-suboptimal set $f_0(x)‚â§p^*+œµ$ and optimal set are still convex.\n",
        "\n",
        "  - Strictly convex objective $f(Œ∏x+(1-Œ∏)y)< Œ∏f(x)+(1-Œ∏)f(y)$ optimal set contains at most 1 point.\n",
        "\n",
        "  - Any locally optimal point is also globally optimal.\n",
        "\n",
        "**Convex optimality criterion**: $f_0‚ààC^1$ is convex, then $\\red{f_0(y)‚â•f_0(x^*)‚áî‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0}\\ ‚àÄy‚àà\\m{F}$.\n",
        "\n",
        "- This is used by solvers to decide whether convergence has been reached for differentiable objective. Proof:\n",
        "\n",
        "  - Show $‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0‚áíf_0(y)‚â•f_0(x^*)$. This is evident from the first-order convexity condition\n",
        "  $f_0(y)‚â•f_0(x^*)+‚àáf_0(x^*)^‚ä§(y-x^*)$.\n",
        "\n",
        "  - Show $f_0(y)‚â•f_0(x^*)‚áí‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0$. This relies on convexity of the feasible set. Let $z(t)=x^*+t(y-x^*)$ be a line segment in the feasible set between the optimal $x^*$ and any $y‚àà\\m{F}$, then\n",
        "  $f_0(x^*+t(y-x^*))‚â•f_0(x^*)\\ ‚àÄt‚àà[0,1]$\n",
        "  $‚áí\\lim\\limits_{t‚Üí0^+}\\/{f_0(x^*+t(y-x^*))-f_0(x^*)}{t}‚â•0$\n",
        "  $‚áí‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0$.\n",
        "\n",
        "- Intuition: $f_0(x)$ is a surface floating in the air. The feasible set and $‚àáf_0(x)$ are both parallel to the ground. $‚àáf_0$ points toward a direction that results in the steepest ascent in the floating surface.\n",
        "$‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0$ says moving away from $x^*$ toward any $y‚àà\\m{F}$ results in $f_0(y)‚â•f_0(x^*)$.\n",
        "It also says geometrically $‚àáf_0(x^*)$ and $y-x^*$ form an angle no more than 90¬∞.\n",
        "\n",
        "  - If $x^*$ is unconstrained, it sits at the bottom of the bowl where $‚àáf_0(x^*)=0$ and $‚àáf_0(x^*)^‚ä§(y-x)=0$. Suppose $x^*$ is on boundary,\n",
        "  then $‚àáf_0(x^*)$ points toward steepest ascent and into the feasible set thus $f_0(y)‚â•f_0(x^*)$ for all $y‚àà\\m{F}$.\n",
        "  The feasible set is convex thus $‚àáf_0(x^*)(y-x^*)>0$ with all $y‚àà\\m{F}$.\n",
        "  Finally $-‚àáf_0(x^*)\\neq0$ points away from feasible set and therefore is normal to the supporting hyperplane of the feasible set at $x^*$.\n",
        "\n",
        "- **Unconstrained** (sanity check): Let $y=x^*-t‚àáf_0(x^*),\\ t>0$ in a small step toward $-‚àáf_0(x^*)$.\n",
        "Then optimality $‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0$\n",
        "$‚áí‚àáf_0(x^*)^‚ä§(-t‚àáf_0(x^*))$\n",
        "$=-t\\n{‚àáf_0(x^*)}_2^2‚â•0$\n",
        "$‚áí‚àáf_0(x^*)=0$, which agrees with unconstrained optimization.\n",
        "\n",
        "- **Equality only constraint**: $\\BC\n",
        "\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&\\green{Ax=b}\n",
        "\\EC$.\n",
        "Feasible set is affine $\\{x_0+\\null(A)\\}$ assuming $b‚àà\\span(A)$.\n",
        "Every $y‚àà\\m{F}$ satisfies $y-x^*‚àà\\null(A)$.\n",
        "Optimal point $x^*$ satisfies $\\green{‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0}$.\n",
        "\n",
        "  - If linear function $g(v)=‚àáf_0(x^*)^‚ä§v$ is nonnegative on a subspace $\\dom(g)=S$ then it must be zero on the subspace.\n",
        "  $‚àáf_0(x^*)^‚ä§v‚â•0\\ ‚àÄv‚ààS‚áí‚àáf_0(x^*)^‚ä§v=0$, because if $v‚ààS$ then $-v‚ààS$.\n",
        "  \n",
        "  - Stationarity: $‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0\\ ‚àÄy-x^*‚àà\\null(A)$\n",
        "  $‚áí‚àáf_0(x^*)^‚ä§(y-x^*)=0$\n",
        "  $‚áí‚àáf_0(x^*)\\perp\\null(A)$\n",
        "  $‚áí\\blue{‚àáf_0(x^*)‚àà\\null(A)^\\perp=\\span(A^‚ä§)}$\n",
        "  $‚áí‚àáf_0(x^*)=A^‚ä§ŒΩ$ for some $ŒΩ‚àà‚Ñù^p$.\n",
        "  Therefore\n",
        "  $‚àáf_0(x)+A^‚ä§ŒΩ=0$.\n",
        "  \n",
        "- **Nonnegative orthant**: $\\BC\n",
        "\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&\\green{x‚âΩ0}\n",
        "\\EC$.\n",
        "Optimality $‚àáf_0(x^*)^‚ä§y-‚àáf_0(x^*)^‚ä§x^*‚â•0\\ ‚àÄy‚âΩ0$ implies 2 requirements:\n",
        "\n",
        "  - Dual feasibility: $\\green{‚àáf_0(x^*)‚âΩ0}$ otherwise $‚àáf_0(x^*)^‚ä§y$ is unbounded below when $y‚âΩ0$ is arbitrarily large.\n",
        "  From stationarity\n",
        "  $‚àá_xL(x,Œª)=‚àá_xf_0(x)-‚àá_x(Œª^‚ä§x)=0$\n",
        "  $‚áíŒª^*=‚àáf_0(x^*)‚âΩ0$.\n",
        "\n",
        "  - Complementary slackness: $-‚àáf_0(x^*)^‚ä§x^*‚â•0$\n",
        "  $‚áí‚àáf_0(x^*)=0$ because $x^*‚âΩ0$.\n",
        "  Therefore $‚àáf_0(x^*)^‚ä§x^*=0$\n",
        "  $‚áí\\sum_i‚àáf_0(x^*)_ix^*_i=0$\n",
        "  $‚áí\\green{‚àáf_0(x^*)_ix^*_i=0\\ ‚àÄi}$\n",
        "  $‚áíŒª_i^*f_i(x^*)=0$.\n",
        "  The indices corresponding to nonzero components of $x^*$ and $‚àáf_0(x^*)$ are complements and have zero intersection: it's zero in either one or the other.\n",
        "\n",
        "**Convex equivalent problems**\n",
        "\n",
        "- Maximizing concave objective function with convex constraints is equivalent problem.\n",
        "\n",
        "- Some non-convex optimization problems are convex in disguise:\n",
        "$\\BC\\t{minimize}&f_0(x)=x_1^2+x_2^2\\\\\n",
        "\\t{subject to}&f_1(x)=x_1/(1+x_2^2)‚â§0\\\\\n",
        "&h_1(x)=(x_1+x_2)^2=0\\EC ‚áí\n",
        "\\BC\\t{minimize}&f_0(x)=x_1^2+x_2^2\\\\\n",
        "\\t{subject to}&\\tilde{f}_1(x)=x_1‚â§0&\\t{- denom is a positive scalar}\\\\\n",
        "&\\tilde{h}_1(x)=x_1+x_2=0&\\t{- square-root both sides}\n",
        "\\EC$\n",
        "\n",
        "- 4.5 (unconstrained): $f_0(x)=\\/{1}{2}x^‚ä§Px+q^‚ä§x+r$. $‚àáf_0(x)=Px+q=0$.\n",
        "\n",
        "  - If $q‚àâ\\span(P)$ then there is no solution and $f_0$ is unbounded below. This happens when $P$ is singular (has a 0 eigenvalue), and $f_0$ is linear along that eigenvector E.g., $f_0(x_1,x_2)=\\/{1}{2}x_1^2+x_2$ where $P=\\BM1&0\\\\0&0\\EM$ and $q=\\BM0\\\\1\\EM$.\n",
        "\n",
        "  - If $P‚âª0$ then $f_0$ is strictly convex, $P$ is invertible, and $x^*=-P^{-1}q$\n",
        "\n",
        "  - If $P$ is singular (non-invertible) but $q‚àà\\span(P)$, then $X_\\t{opt}=\\{-P^‚Ä†q+\\null(P)\\}$.\n",
        "\n",
        "- 4.6 **analytic centering**: $f_0(x)=-\\sum_i\\ln(b_i-a_i^‚ä§x)$ on $\\{x|Ax‚â∫b\\}$. Here $‚àáf_0=\\sum_i\\/{a_i}{b_i-a_i^‚ä§x}=0$.\n",
        "\n",
        "  - $b-Ax$ forms $m$ walls. $\\ln(b_i-a_i^‚ä§x)$ finds an analytic center point that simultaneously maximizes distance to all walls. $‚àáf_0‚àù\\/{1}{b_i-a_i^‚ä§x}$ represents the force of repulsion from each wall. At optimality, the pushing forces cancel out each other in equilibrium.\n",
        "\n",
        "  - If the space between the walls has an opening like letter 'C', then there's no solution to $‚àáf_0=0$ and $f_0$ is unbounded below. You can move infinitely through the opening to reduce $f_0$ against that one far wall.\n",
        "\n",
        "  - If the space is fully enclosed in a polygon (soccer ball), then there is exactly 1 analytic center as solution.\n",
        "\n",
        "  - If the walls form a tube/tunnel axis with 2 openings, then that has infinite solutions."
      ],
      "metadata": {
        "id": "8jaa28W7nnDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quasiconvex optimization**: $\\BC\\t{minimize}&f_0(x)&f_0\\t{ quasiconvex}\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0,&f_i\\t{ convex, }i=1,...,m\\\\\n",
        "&Ax=b&\\t{affine}\\EC$\n",
        "\n",
        "- Quasiconvex means unimodal but possibly with flat spots half-way down the valley. Locally optimal solutions are not guaranteed to be globally optimal.\n",
        "\n",
        "- **Quasiconvex optimality criterion**: $x^*‚àà\\m{F}$ is globally optimal if $\\blue{‚àáf_0(x)^‚ä§(y-x^*)>0\\ ‚àÄy‚àà\\m{F}\\backslash\\{x\\}}$ as sufficient but not necessary for $x^*$ to be optimal.\n",
        "\n",
        "  - If you move to any other $y‚àà\\m{F}$ you immediately go strictly uphill, then that's sufficient to be global optimal. However there could be multiple points in $X_\\t{opt}$, therefore criterion not necessary for $x^*$ to be global optimal.\n",
        "\n",
        "  - Criterion requires $\\blue{‚àáf_0(x^*)\\neq0}$, which means this criterion is **designed for constrained boundaries**, and does not work for unconstrained minima where the bottom is flat.\n",
        "\n",
        "- **Equivalent convex constraint**: If $f_i(x)$ constraint is quasiconvex, use equivalent 0-sublevel convex constraint $œï_0(x)=\\t{dist}(x,\\{z|f_i(z)‚â§0\\})‚â§0$ is an all-purpose example, but often something simpler is used.\n",
        "\n",
        "- **Convex feasibility algorithm**: $x_{\\m{F}_t}=\\BC\\t{find}&x\\\\\n",
        "\\t{subject to}&œï_t(x)‚â§0\\\\\n",
        "&f_i(x)‚â§0\\\\\n",
        "&Ax=b\\EC$ where $\\BC\n",
        "œï_t(x)\\t{ is convex}\\\\\n",
        "œï_t(x)‚â§0‚áîf_0(x)‚â§t\\\\\n",
        "s‚â•t‚áîœï_s(x)‚â§œï_t(x)\\EC$ returns whether $\\m{F}_t$ is empty for a given $t$.\n",
        "Then $\\BC\n",
        "p^*‚â§t&\\t{if }\\m{F}_t\\neq‚àÖ\\\\\n",
        "p^*>t&\\t{if }\\m{F}_t=‚àÖ\n",
        "\\EC$. Binary search on $t$ to find the $œµ$-suboptimal value.\n",
        "\n",
        "**Linear fractional programming**: $f_0(x)=\\/{c^‚ä§x+d}{e^‚ä§x+f}$ on $\\dom(f_0)=\\{x|e^‚ä§x+f>0\\}$ is quasilinear.\n",
        "\n",
        "- Let $y=\\/{x}{e^‚ä§x=f}$ and $z=\\/{1}{e^‚ä§x=f}$ then $\\BC\\t{minimize}&\\/{c^‚ä§x+d}{e^‚ä§x+f}\\\\\n",
        "\\t{subject to}&Gx‚âºh\\\\\n",
        "&Ax=b\\\\\n",
        "&e^‚ä§x+f>0\n",
        "\\EC ‚áí \\BC\\t{minimize}&c^‚ä§y+dz\\\\\n",
        "\\t{subject to}&Gy-hz‚âº0\\\\\n",
        "&Ay-bz=0\\\\\n",
        "&e^‚ä§y+fz=1\\\\\n",
        "&z‚â•0\\EC$\n",
        "\n",
        "- Generalized linear fractional programming: $f_0(x)=\\max_i\\/{c_i^‚ä§x+d_i}{e_i^‚ä§x+f_i}$.\n",
        "E.g., Von Neumann growth problem"
      ],
      "metadata": {
        "id": "pw5hQ-0CpKc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Programming (LP)** $\\BC\\t{minimize}&c^‚ä§x+d&\\red{\\t{affine }f_0(x)}\\\\\n",
        "\\t{subject to}&Gx‚âºh&\\red{\\t{affine }f_i(x)}\\\\\n",
        "&Ax=b\\EC$\n",
        "where $d$ is redundant and often dropped.\n",
        "$\\m{F}$ is a polyhedron.\n",
        "LP is finding the lowest point on a tilted hyperplane in a polyhedron. If the LP is not unbounded and the polyhdron has at least one vertex then **at least one optimal point is at a vertex**.\n",
        "\n",
        "- Two common forms: **standard form LP** $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax=b\\\\\n",
        "&x‚âΩ0\\EC$ and\n",
        "**inequality form LP** $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax‚âºb\n",
        "\\EC$. These are the standard formats to write LPs for solvers.\n",
        "\n",
        "  - Converting to standard form: use slack variables $s$ and use the **nonnegative variables trick** by splitting $x=x^+-x^-$ such that $x^+‚âΩ0$ and $x^-‚âΩ0$.\n",
        "  $\\BC\\t{minimize}&c^‚ä§x+d\\\\\n",
        "  \\t{subject to}&Gx‚âºh\\\\\n",
        "  &Ax=b\\EC ‚áí \\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&Gx+s=h\\\\\n",
        "  &Ax=b\\\\\n",
        "  &s‚âΩ0\\EC ‚áí \\BC\\t{minimize}&c^‚ä§x^+-c^‚ä§x^-\\\\\n",
        "  \\t{subject to}&Gx^+-Gx^-+s=h\\\\\n",
        "  &Ax^+-Ax^-=b\\\\\n",
        "  &x^+‚âΩ0,\\ x^-‚âΩ0,\\ s‚âΩ0\\EC$\n",
        "\n",
        "- **Separable problem**: When there is no constraint linking $x_i$ and $x_j$ together such as $x_i+x_j‚â§1$, then the optimization problem $c^‚ä§x=\\sum_ic_ix_i$ can be separated into $n$ different problems $c_ix_i$.\n",
        "\n",
        "- **Diet problem**: A healthy diet contains at least $b_1,...,b_m$ amounts of $m$ nutrient types. One consumes $x_1,...,x_n$ units of $n$ food types. Each unit of food $j$ contains $a_{ij}$ amount of type $i$ nutrient, and costs $c_j$ amount.\n",
        "$\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax‚âΩb\\\\\n",
        "&x‚âΩ0\\EC‚áí\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&\\BM b\\\\0\\EM-\\BM A\\\\I\\EM x‚âº0\n",
        "\\EC$\n",
        "\n",
        "- **Piecewise-minimization**: non-LP minimization of piecewise $f(x)=\\max_i(a_i^‚ä§x+b_i)$ can be written in epigraph form\n",
        "$\\BC\n",
        "\\t{minimize}&\\max_i(a_i^‚ä§x+b_i)\n",
        "\\EC‚áí\\BC\n",
        "\\t{minimize}&t\\\\\n",
        "\\t{subject to}&\\max_i(a_i^‚ä§x+b_i)‚â§t\n",
        "\\EC‚áí\\BC\n",
        "\\t{minimize}&t\\\\\n",
        "\\t{subject to}&a_i^‚ä§x+b_i‚â§t\n",
        "\\EC$\n",
        "\n",
        "- **Chebyshev center of polyhedron**: largest Euclidean ball $\\m{B}=\\{x_c+u|\\ \\n{u}_2‚â§r\\}$ to fit in polyhedron $\\m{P}=\\{x‚àà‚Ñù^n|a_i^‚ä§x‚â§b_i,\\ i=1,...,m\\}$ has closest point to each wall given by\n",
        "$\\blue{x_c+r\\/{a_i}{\\n{a_i}_2}}$ satisfying\n",
        "$a_i^‚ä§(x_c+r\\/{a_i}{\\n{a_i}_2})‚â§b_i$\n",
        "$‚áía_i^‚ä§x_c+r\\n{a_i}_2‚â§b_i$.\n",
        "$\\BC\\t{maximize}&r\\\\\n",
        "\\t{subject to}&a_i^‚ä§x_c+r\\n{a_i}_2‚â§b_i\\ ‚àÄi\n",
        "\\EC$\n",
        "\n",
        "  - Using Cauchy-Schwarz: $\\m{B}$ fits in $\\m{P}$ iff\n",
        "  $\\sup\\{a_i^‚ä§(x_c+u)|\\ \\n{u}_2‚â§r\\}‚â§b_i$\n",
        "  $‚áía_i^‚ä§x_c+\\sup\\{a_i^‚ä§u|\\ \\n{u}_2‚â§r\\}‚â§b_i$\n",
        "  $‚áía_i^‚ä§x_c+r\\n{a_i}_2‚â§b_i\\ ‚àÄi$\n",
        "\n",
        "- **Activity planning**: There are $n$ activities over $N$ time periods.\n",
        "Activities consume and produce products in proportion to activity levels.\n",
        "Activity $j$ produces $a_{ij}$ and consumes $b_{ij}$ amounts of product $i$ per unit of activity level.\n",
        "$x_j(t)‚â•0,\\ t=1,...,N$ denotes the activity level of activity $j$ in period $t$.\n",
        "Initial products are provided in $g_0$ so that $\\BC\n",
        "Bx(1)‚âºg_0\\\\\n",
        "Bx(t+1)‚âºAx(t)\n",
        "\\EC$.\n",
        "Excess products not consumed are $\\BC\n",
        "s(0)=g_0-Bx(1)\\\\\n",
        "s(t)=Ax(t)-Bx(t+1)\\\\\n",
        "s(N)=Ax(N)\n",
        "\\EC$.\n",
        "Let $c$ be the values of products and $Œ≥$ be discount rate, then $\\BC\\t{maximize}&c^‚ä§s(0)+Œ≥c^‚ä§s(1)+...+Œ≥^NC^‚ä§s(N)\\\\\n",
        "\\t{subject to}&x(t)‚âΩ0,\\ s(t)‚âΩ0\\\\\n",
        "&s(0)=g_0-Bx(1),\\ s(t)=Ax(t)-Bx(t+1),\\ s(N)=Ax(N)\\EC$\n"
      ],
      "metadata": {
        "id": "fzFmAC1vpvTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quadratic programming (QP)** $\\BC\n",
        "\\t{minimize}&\\/{1}{2}x^‚ä§Px+q^‚ä§x+r,&\\red{\\t{convex quadratic }f_0(x)}\\\\\n",
        "\\t{subject to}&Gx‚âºh,&\\red{\\t{affine }f_i(x)}\\\\\n",
        "&Ax=b\\EC$\n",
        "where $\\m{F}$ is polyhedron like LP.\n",
        "\n",
        "- Quadratically constrained quadratic program (QCQP): quadratic convex objective over quadratic constraints.\n",
        "**Any quadratic objective can be transformed into a linear objective using epigraph transformation.**\n",
        "\n",
        "- **Least squares**: $\\n{Ax-b}_2^2=x^‚ä§A^‚ä§Ax-2b^‚ä§Ax+b^‚ä§b$. Unconstrained least squares has analytical solution $x=A^‚Ä†b$ where $A^‚Ä†$ is pseudoinverse of $A$.\n",
        "\n",
        "- **Distance between polyhedra**: $\\m{P}_1=\\{x|A_1x‚âºb_1\\}$ and $\\m{P}_2=\\{x|A_2x‚âºb_2\\}$.\n",
        "Then $\\t{dist}(\\m{P}_1,\\m{P}_2)=\\inf\\{\\n{x_1-x_2}_2|x_1‚àà\\m{P}_1,x_2‚àà\\m{P}_2\\}$. Then\n",
        "$\\BC\n",
        "\\t{minimize}&\\n{x_1-x_2}_2^2\\\\\n",
        "\\t{subject to}&A_1x_1‚âºb_1,\\ A_2x_2‚âºb_2\n",
        "\\EC$\n",
        "\n",
        "- **LP with stochastic objective**: Suppose LP with $f_0(x)=c^‚ä§x$ where $c‚àº(\\bar{c},Œ£)$ where $Œ£=\\E(c-\\bar{c})(c-\\bar{c})^‚ä§$. In trade off between minimizing expected cost vs cost variance, we minimize $\\E[c^‚ä§x]+Œ≥\\Var(c^‚ä§x)$ using $Œ≥$ risk aversion parameter\n",
        "$\\BC\\t{minimize}&\\bar{c}^‚ä§x+Œ≥x^‚ä§Œ£x\\\\\n",
        "\\t{subject to}&Gx‚âºh\\\\\n",
        "&Ax=b\\EC$\n",
        "\n",
        "- **Markowitz portfolio optimization**: Let $x‚àà‚Ñù^n$ be portfolio in dollars, and $p‚àº(\\bar{p},Œ£)$ be relative price change over a period. Then dollar return is $r=p^‚ä§x‚àº(\\bar{p}^‚ä§x,x^‚ä§Œ£x)$. Classical portfolio optimization\n",
        "$\\BC\\t{minimize}&x^‚ä§Œ£x\\\\\n",
        "\\t{subject to}&\\bar{p}^‚ä§x‚â•r_\\min\\\\\n",
        "&\\v{1}^‚ä§x=1,\\ x‚âΩ0\n",
        "\\EC$\n",
        "\n",
        "  - To allow shorting, use $x_\\t{long}‚âΩ0$, $x_\\t{short}‚âΩ0$, $x=x_\\t{long}-x_\\t{short}$, and $\\v{1}^‚ä§x_\\t{short}‚â§Œ∑\\v{1}^‚ä§x_\\t{long}$.\n",
        "\n",
        "  - To have transaction costs, let $u_\\t{buy}‚âΩ0$ and $u_\\t{sell}‚âΩ0$ be transactions in current period so that $x=x_0+u_\\t{buy}-u_\\t{sell}$. Let $f_\\t{buy}‚â•0$ and $f_\\t{sell}‚â•0$ be transaction fee rates, then total transation fees are $f_\\t{buy}u_\\t{buy}+f_\\t{sell}u_\\t{sell}$. If applying self-financing, then $(1-f_\\t{sell})\\v{1}^‚ä§u_\\t{sell}=(1+f_\\t{buy})\\v{1}^‚ä§u_\\t{buy}$.\n",
        "\n",
        "**Second-order cone program (SOCP)**: $\\BC\n",
        "\\t{minimize}&f^‚ä§x,&\\red{\\t{affine }f_0(x)}\\\\\n",
        "\\t{subject to}&\\red{\\n{A_ix+b_i}_2‚â§c_i^‚ä§x+d_i}&‚àÄi=1,...,m\\\\\n",
        "&Fx=g\n",
        "\\EC$\n",
        "where the second-order cone constraint $\\n{A_ix+b_i}_2‚â§c_i^‚ä§x+d_i$ fits $(Ax+b,c^‚ä§x+d)$ into a second-order cone $\\{(x,t)|\\n{x}_2‚â§t\\}$.\n",
        "\n",
        "- LP generalizes to SOCP with cone width $A=0$. QCQP generalizes to SOCP with cone height $c=0$. QP generalizes to SOCP with epigraph transformation.\n",
        "\n",
        "- **Robust LP**: suppose $\\BC\n",
        "\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&a_i^‚ä§x‚â§b_i,&a_i‚àà\\{\\bar{a}_i+P_iu|\\n{u}_2‚â§1\\}\\\\\n",
        "\\EC$.\n",
        "Then $\\sup\\{a_i^‚ä§x|a_i‚àà\\{\\bar{a}_i+P_iu|\\n{u}_2‚â§1\\}\\}$\n",
        "$=\\bar{a}_i^‚ä§x+\\sup\\{(P_iu)^‚ä§x|\\n{u}_2‚â§1\\}$\n",
        "$=\\bar{a}_i^‚ä§x+\\sup\\{u^‚ä§(P_i^‚ä§x)|\\n{u}_2‚â§1\\}$\n",
        "$=\\bar{a}_i^‚ä§x+\\n{P_i^‚ä§x}_2$ by Cauchy-Schwarz.\n",
        "Then\n",
        "$\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&\\bar{a}_i^‚ä§x+\\n{P_i^‚ä§x}_2‚â§b_i\n",
        "\\EC$\n",
        "\n",
        "  - $\\n{P_i^‚ä§x}_2$ is regularization that provides fixed safety margin from boundary. It allows wiggle room on $a$ because real-life LP constraints often have variations.\n",
        "\n",
        "- **LP with stochastic constraints**: In robust LP, suppose $a_i‚àº\\Normal(\\bar{a}_i,Œ£_i)$ and $a_i^‚ä§x‚àº\\Normal(\\bar{a}_i^‚ä§x,x^‚ä§Œ£_ix)$ where\n",
        "$\\blue{x^‚ä§Œ£_ix=x^‚ä§Œ£_i^{1/2}Œ£_i^{1/2}x=\\n{Œ£_i^{1/2}x}_2^2}$ for Cholesky decomposition $Œ£_i^{1/2}$, then the $m$ constraints become\n",
        "$P(a_i^‚ä§x‚â§b_i)‚â•Œ∑$\n",
        "$‚áíŒ¶(\\/{b_i-\\bar{a}_i^‚ä§x}{\\n{Œ£_i^{1/2}x}_2})‚â•Œ∑$\n",
        "$‚áí\\bar{a}_i^‚ä§x+Œ¶^{-1}(Œ∑)\\n{Œ£_i^{1/2}x}_2‚â§b_i$.\n",
        "$\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&P(a_i^‚ä§x‚â§b_i)‚â•Œ∑\n",
        "\\EC ‚áí \\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&\\bar{a}_i^‚ä§x+Œ¶^{-1}(Œ∑)\\n{Œ£_i^{1/2}x}_2‚â§b_i\n",
        "\\EC$\n",
        "\n",
        "  - This only works if $Œ∑>0.5‚áíŒ¶^{-1}(Œ∑)>0$. If $Œ∑‚â§0.5$ the constraint is no longer conic.\n",
        "\n",
        "- **Chebyschev inequality**: $P(f(x)‚â•r)‚â§\\/{\\E[f(x)]}{r}$. Let discrete random variable $x$ have support $\\{u_1,...,u_n\\}‚äÜ‚Ñù$ with probability simplex $p‚àà‚Ñù^n$. Let $\\E[f_i(x)]=[f_i(x_j),\\ j=1,...,n]^‚ä§p$ be a function of prior $p$. We wish to find lowerbound on $\\E[f_0(x)]$ with optimization variable $p$.\n",
        "$\\BC\\t{minimize}&\\E[f_0(x)]\\\\\n",
        "\\t{subject to}&p‚âΩ0,\\ \\v{1}^‚ä§p=1\\\\\n",
        "&Œ±_i‚â§\\E[f_i(x)]‚â§Œ≤_i\n",
        "\\EC$\n",
        "\n",
        "- **Chebyschev inequality**: $P(|D|‚â•t)‚â§\\/{\\Var(D)}{t^2}$. Then $\\BC\\t{maximize}&[f_0(x_j)^2,\\ j=1,...,n]^‚ä§p-(\\E[f_0(x)])^2\\\\\n",
        "\\t{subject to}&p‚âΩ0,\\ \\v{1}^‚ä§p=1\\\\\n",
        "&Œ±_i‚â§\\E[f_i(x)]‚â§Œ≤_i\n",
        "\\EC$\n",
        "\n",
        "- 4.8: **Markowitz with loss risk**: $p‚àº\\Normal(\\bar{p},Œ£)$ and $r‚àº\\Normal(\\bar{p}^‚ä§x,x^‚ä§Œ£x)$. Then loss risk constraint $P(r‚â§Œ±)‚â§Œ≤‚áí\\bar{p}^‚ä§x+Œ¶^{-1}(Œ≤)\\n{Œ£^{1/2}x}_2‚â•Œ±$. $\\BC\\t{minimize}&\\bar{p}^‚ä§x\\\\\n",
        "\\t{subject to}&\\bar{p}^‚ä§x+Œ¶^{-1}(Œ≤)\\n{Œ£^{1/2}x}_2‚â•Œ±\\\\\n",
        "&x‚âΩ0,\\ \\v{1}^‚ä§x=1\n",
        "\\EC$"
      ],
      "metadata": {
        "id": "SdCfpBpe_sLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geometric programming (GP)**: $\\BC\\t{minimize}&f_0(x)&\\t{posynomial }f_0\\\\\n",
        "\\t{subject to}&f_i(x)‚â§1&\\t{posynomial }f_i\\\\\n",
        "&h_i(x)=1&\\t{monomial }h_i\n",
        "\\EC$ on domain $\\m{D}=‚Ñù_{++}^n$.\n",
        "\n",
        "- Monomial $f(x)=cx_1^{a_1}...x_n^{a_n},\\ c>0,\\ a_i‚àà‚Ñù$, and posynomial $f(x)=\\sum_kc_kx_1^{a_{1k}}...x_n^{a_{nk}}$.\n",
        "\n",
        "  - $f(x)=cx_1^{a_1}...c_n^{a_n}$\n",
        "  $=f(e^y)=c({e^{y_1}})^{a_1}...({e^{y_n}})^{a_n}$\n",
        "  $=e^{a^‚ä§y+\\ln(c)}$ where $y_i=\\ln(x_i)$\n",
        "\n",
        "  - $f(x)=\\sum_kc_kx_1^{a_{1k}}...x_n^{a_{nk}}$\n",
        "  $=\\sum_kc_k({e^{y_1}})^{a_{1k}}...({e^{y_n}})^{a_{nk}}$\n",
        "  $=\\sum_ke^{a_k^‚ä§y+\\ln(c_k)}$ where $a_k=(a_{1k},...,a_{nk})$\n",
        "\n",
        "- Let $\\tilde{f}_i(y)=\\ln(f_i(x))$ be convex log-sum-exp and\n",
        "$\\tilde{h}_j(y)=\\ln(h_j(x))$ be affine then GP can be re-expressed in convex form\n",
        "$\\BC\\t{minimize}&\\tilde{f}_0(y)\\\\\n",
        "\\t{subject to}&\\tilde{f}_i(y)‚â§0,\\ \\tilde{h}_j(y)=0\\EC$.\n",
        "\n",
        "**Generalized inequality constraints**: $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚âº_{K_i}0&f_i\\t{ is }K_i\\t{-convex}\\\\\n",
        "&Ax=b\\EC$ - vector-valued constraint functions. These problems are often separable into minimizing each $x_i$.\n",
        "\n",
        "- **Conic programming**: $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Fx+g‚âº_K0\\\\\n",
        "&Ax=b\\EC$.\n",
        "Std $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&x‚âΩ_K0\\\\\n",
        "&Ax=b\\EC$.\n",
        "Ineq $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Fx+g‚âº_K0\\EC$.\n",
        "\n",
        "- **Semidefinite programming (SDP)**: $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&x_1F_1+...+x_nF_n+G‚âº0,&\\t{LMI; }G,F_i‚àà\\v{S}^k\\\\\n",
        "&Ax=b,&A‚àà‚Ñù^{p√ón}\\EC$.\n",
        "\n",
        "  - Standard form $\\BC\\t{minimize}&\\tr(CX)\\\\\n",
        "  \\t{subject to}&\\tr(A_iX)=b_i\\\\\n",
        "  &X‚âΩ0\n",
        "  \\EC$.\n",
        "  Inequality form $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&x_1A_1+...+x_nA_n‚âºB\n",
        "  \\EC$.\n",
        "\n",
        "  - LPs generalize into SDPs as diagonals: $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&Ax‚âºb\\EC‚áí\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&\\diag(Ax-b)‚âº0\\EC$\n",
        "\n",
        "  - SOCPs generalize into SDPs as Schur's complement: $\\BC\\t{minimize}&f^‚ä§x\\\\\n",
        "  \\t{subject to}&\\n{A_ix+b_i}_2‚â§c_i^‚ä§x+d_i\n",
        "  \\EC‚áí\\BC\\t{minimize}&f^‚ä§x\\\\\n",
        "  \\t{subject to}&\\BM (c_i^‚ä§x+d_i)I&A_ix+b_i\\\\(A_ix+b_i)^‚ä§&c_i^‚ä§x+d_i\\EM‚âΩ0,\\ ‚àÄi\n",
        "  \\EC$.\n",
        "\n",
        "  - LP ‚äÇ QP ‚äÇ QCQP ‚äÇ SOCP ‚äÇ SDP"
      ],
      "metadata": {
        "id": "WVKpVUc_5wfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-criterion optimization**: $\\BC\\t{minimize wrt }K&f_0(x),&f_0:‚Ñù^n‚Üí‚Ñù^q\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\n",
        "&h_i(x)=0\n",
        "\\EC$ with $K=‚Ñù_+^q$ where $f_0(x)=(F_1(x),...,F_q(x))$ represent $q$ different scalar objectives. Problem is convex if $F_j,f_i$ convex and $h_i$ affine.\n",
        "\n",
        "- Minimize wrt proper cone $K$: if $K=‚Ñù_+^2$ then $f_0(x)$ is a vector. If $K=\\v{S}_+^n$ then $f_0(x)$ is a matrix.\n",
        "\n",
        "- **Achievable objective values** $\\red{\\m{O}=\\{f_0(x)|‚àÉx‚àà\\m{F}\\}‚äÜ‚Ñù^q}$ are objective values of feasible points.\n",
        "\n",
        "  - **Optimality**: $\\m{O}$ has an unambiguous minimum $x^*‚àà\\m{F}$ such that\n",
        "  $f_0(x^*)‚âº_Kf_0(y)\\ ‚àÄy‚àà\\m{F}$\n",
        "  $‚áíf_0(y)-f_0(x^*)‚ààK$\n",
        "  $‚áíf_0(y)‚ààf_0(x^*)+K\\ ‚àÄf_0(y)‚àà\\m{O}$\n",
        "  $‚áí\\m{O}‚äÜf_0(x^*)+K$.\n",
        "  The existence of $x^*$ dominates all other points $F_i(x^*)‚â§F_i(y)\\ ‚àÄy,i$ and $‚àÉj:F_j(x^*)< F_j(y)$. The objectives are noncompeting.\n",
        "\n",
        "- **Pareto Optimality**: $\\m{O}$ does not have a minimum, but does have minimal elements with trade-offs.\n",
        "  \n",
        "  - Point $x‚àà\\m{F}$ is Pareto optimal if $f_0(x)$ is a minimal element of $\\m{O}$.\n",
        "  \n",
        "  - Point $x‚àà\\m{F}$ is Pareto optimal if $‚àÄy‚àà\\m{F}$ satisfying $f_0(y)‚âº_Kf_0(x)$ implies $f_0(y)=f_0(x)$. Any $y$ better than or equal to $x$ has the same objective value as $x$. No other point dominates $x$.\n",
        "\n",
        "  - Point $x‚àà\\m{F}$ is Pareto optimal iff $(f_0(x)-K)‚à©\\m{O}=\\{f_0(x)\\}$.\n",
        "\n",
        "  - Pareto optimal values set $\\m{P}$ satisfies: $\\red{\\m{P}‚äÜ\\m{O}‚à©\\t{boundary}(\\m{O})}$\n",
        "\n",
        "- **Scalarization**:\n",
        "$\\BC\\t{minimize wrt }K&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&h_i(x)=0\n",
        "\\EC ‚áí \\BC\\t{minimize}&\\red{Œª^‚ä§f_0(x)},&\\red{Œª‚âª_{K^*}0}\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&h_i(x)=0\\EC$.\n",
        "Then optimal point $x^*$ in the ordinary scalar problem is Pareto optimal in the vector problem. Here $Œª$ are called **weights**.\n",
        "\n",
        "  1. **Pick a $Œª‚àà\\t{int}(K^*)$ and minimize $Œª^‚ä§f_0(x)$.** Minimizing $Œª^‚ä§f_0(x)$ is finding the supporting hyperplane of $\\m{O}$ with normal $Œª$.\n",
        "  Point $x$ is optimal iff $Œª^‚ä§f_0(y)‚â•Œª^‚ä§f_0(x)‚áíŒª^‚ä§(f_0(y)-f_0(x))‚â•0\\ ‚àÄy‚àà\\m{F}$\n",
        "  $‚áí\\blue{Œª^‚ä§(u-f(x))‚â•0\\ ‚àÄu‚àà\\m{O}}$, a hyperplane.\n",
        "\n",
        "  - If $Œª‚àà\\t{boundary}(K^*)$ is used instead of $Œª‚àà\\t{int}(K^*)$, then $Œª^‚ä§v=0$ for some $v‚àà\\t{boundary}(K)$.\n",
        "  Suppose $f_0(x)‚àà\\m{O}$ and $f_0(y)=f_0(x)-v‚àà\\m{O}$ such that $f_0(y)< f_0(x)$.\n",
        "  Then\n",
        "  $Œª^‚ä§f_0(y)=Œª^‚ä§(f_0(x)-v)$\n",
        "  $=Œª^‚ä§f_0(x)$ and the solver stops at\n",
        "  $f_0(x)$, which is not a true Pareto optimal point but is at a flat horizontal or vertical edge of $\\m{O}$.\n",
        "  Using $Œª‚àà\\t{int}(K^*)$ ensures $v‚àà\\t{boundary}(K)$ does not stop the solver.\n",
        "\n",
        "  2. **Iterate $Œª$ and collect minimal $f_0(x)$ to build the Pareto frontier $\\m{P}$**. All elements in $\\t{boundary}(\\m{O})‚à©\\t{conv}(\\m{O})$ along the lower left side is minimal $f_0(x)\\ ‚àÄx‚àà\\m{F}$. This boundary is the scalarizable part of Pareto frontier $\\m{P}$. Pareto optimal solutions in concave dips are not reachable by scalarization.\n",
        "\n",
        "- **Scalarization of convex problems**: Let $f_0(x)$ be convex, then $Œª^‚ä§x$ is an affine transformation and is convex. Furthermore $\\m{D}$ and $\\m{A}=\\m{O}+‚Ñù_+^q$ ($\\m{O}$ plus everything worse) is a convex set. This guarantees no concave dips on the lower left boundary and scalarization finds the entire Pareto frontier.\n",
        "\n",
        "  - Proof: $\\epi(f_0)=\\{(x,t)‚àà‚Ñù^n√ó‚Ñù^q|x‚àà\\m{D},f_0(x)‚âºt\\}$.\n",
        "  Then\n",
        "  $\\m{A}=\\{t‚àà‚Ñù^q|‚àÉx‚àà\\m{D}\\t{ such that }f_0(x)‚âºt\\}$\n",
        "  $=\\{t|(x,t)‚àà\\epi(f_0)\\}$\n",
        "  is the projection of the epigraph onto the objective space of $f_0(x)$, which is an affine mapping whose image preserves convexity.\n",
        "  I.e., $\\m{A}$ is the projection (shadow) of $\\epi(f_0)$ onto a lower dimension and therefore preserves convexity of $f_0$.\n",
        "\n",
        "  - After scalarizing with $Œª‚âª_{K^*}0$ to find Pareto frontier, we then scalarize $Œª‚âΩ_{K^*}0$ to find end-points. These results we have to check they are minimal.\n",
        "\n",
        "  - $Œª^‚ä§f_0(x)=\\sum_{i=1}^qŒª_iF_i(x)$ where $Œª‚âª0$ are the weights attached to the objectives. $Œª_i/Œª_j$ should be large if we care more about wanting $F_i$ to be small than $F_j$.\n",
        "\n",
        "- **Regularized least-squares**: We optimize against two criteria $\\BC\n",
        "F_1(x)=\\n{Ax-b}_2^2&\\t{goodness of fit} \\\\\n",
        "F_2(x)=\\n{x}_2^2&\\t{minimal size}\\EC$\n",
        "unconstrained\n",
        "$\\BC\n",
        "\\t{minimize wrt }‚Ñù_+^2&f_0(x)=(F_1(x),F_2(x))\n",
        "\\EC$\n",
        "$‚áí\\BC\n",
        "\\t{minimize}&Œª^‚ä§f_0(x)\n",
        "\\EC$.\n",
        "Then\n",
        "$Œª^‚ä§f_0(x)$\n",
        "$=Œª_1F_1(x)+Œª_2F_2(x)$\n",
        "$=Œª_1\\n{Ax-b}_2^2+Œª_2\\n{x}_2^2$.\n",
        "$‚àá_x=2Œª_1(Ax-b)^‚ä§A+2Œª_2x^‚ä§$\n",
        "$=2Œª_1x^‚ä§A^‚ä§A-2Œª_1b^‚ä§A+2Œª_2x^‚ä§$\n",
        "$=2x^‚ä§(Œª_1A^‚ä§A+Œª_2I)-2Œª_1b^‚ä§A=0$\n",
        "$‚áí\\blue{x=(A^‚ä§A+\\/{Œª_2}{Œª_1}I)^{-1}A^‚ä§b}$,\n",
        "which produces all Pareto optimal points except two associated with $\\/{Œª_2}{Œª_1}‚Üí‚àû$ and $\\/{Œª_2}{Œª_1}‚Üí0$, which are produced by $Œª‚âΩ_{‚Ñù_+^2}0$, namely $Œª=(0,1)$ and $Œª=(1,0)$.\n",
        "\n",
        "- **Markowitz portfolio risk-return trade-off**: $p‚àº(\\bar{p},Œ£)$, dollar $r=p^‚ä§x$. We want to maximize returns and minimize variance\n",
        "$\\BC\n",
        "\\t{minimize wrt }‚Ñù_+^2&(F_1(x),F_2(x))=(-\\bar{p}^‚ä§x,x^‚ä§Œ£x)\\\\\n",
        "\\t{subject to}&\\v{1}^‚ä§x=1,\\ x‚âΩ0\n",
        "\\EC ‚áí \\BC\n",
        "\\t{minimize}&-\\bar{p}^‚ä§x+Œºx^‚ä§Œ£x&Œª_1=1,\\ Œª_2=Œº\\\\\n",
        "\\t{subject to}&\\v{1}^‚ä§x=1,\\ x‚âΩ0\n",
        "\\EC$\n",
        "\n",
        "- 4.9 **BLUE**: $y=Ax+v$ where $A‚àà‚Ñù^{m√ón}$ has rank $n$. Assume $v‚àº(0,I)$.\n",
        "We want to estimate $x$ given $y$, then linear estimator of $x$ has form $\\hat{x}=Fy$ where $FA=I$ and $\\E[\\hat{x}]=x$.\n",
        "Then $x=F(y-v)$, $\\hat{x}-x=Fv$, and\n",
        "$\\E[(\\hat{x}-x)(\\hat{x}-x)^‚ä§]$\n",
        "$=\\E[Fvv^‚ä§F^‚ä§]$\n",
        "$=FF^‚ä§$.\n",
        "Then the BLUE estimator is $\\BC\\t{minimize wrt }\\v{S}_+^n&FF^‚ä§\\\\\n",
        "\\t{subject to}&FA=I\\EC$.\n",
        "Here $FF^‚ä§$ is convex because $v^‚ä§FF^‚ä§v=\\n{F^‚ä§v}_2^2$ is convex for all $v$.\n",
        "\n",
        "  - Solution is the least-squares estimator $F^*=A^‚Ä†=(A^‚ä§A)^{-1}A^‚ä§$ and $F^*F^{*‚ä§}=(A^‚ä§A)^{-1}$\n",
        "\n",
        "- 4.10 (minimal upperbound): $\\BC\\t{minimize wrt }\\v{S}_+^n&X\\\\\n",
        "\\t{subject to}&X‚âΩA_i\\EC$. Choose $W‚àà\\v{S}_{++}^n$ and run $\\BC\\t{minimize}&\\tr(WX)\\\\\n",
        "\\t{subject to}&X‚âΩA_i\\EC$,\n",
        "which is standard form SDP. Geometrically each $A_i$ is an ellipsoid, and the Pareto frontier is the set of envelope ellipsoids."
      ],
      "metadata": {
        "id": "9Nik1ilE3_gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 4.1: $\\BC\\t{minimize}&f_0(x_1,x_2)\\\\\n",
        "\\t{subject to}&2x_1+x_2‚â•1\\\\\n",
        "&x_1+3x_2‚â•1\\\\\n",
        "&x_1‚â•0,\\ x_2‚â•0\\EC$.\n",
        "$\\m{F}$ is above the line $2x_1+x_2=1$ through $(1/2,0)$ and $(0,1)$ and above the line $x_1+3x_2=1$ through $(1,0)$ and $(0,1/3)$ in $‚Ñù_+^2$.\n",
        "\n",
        "  - $f_0(x_1,x_2)=x_1+x_2$. LP, the optimal point is at the intersection of the two lines, which offers lowest point along the diagonal.\n",
        "  $\\/{1}{2}-\\/{x_2}{2}=1-3x_2$\n",
        "  $‚áíx_2=\\/{1}{5}$ and\n",
        "  $x_1=\\/{2}{5}$.\n",
        "\n",
        "  - $f_0(x_1,x_2)=-x_1-x_2$. LP, the problem is unbounded below.\n",
        "\n",
        "  - $f_0(x_1,x_2)=x_1$. LP, the optimal point is the ray $(0,1)+s(0,1)\\ ‚àÄs‚àà‚Ñù_+$\n",
        "\n",
        "  - $f_0(x_1,x_2)=\\max\\{x_1,x_2\\}$.\n",
        "  Then $\\min\\max\\{x_1,x_2\\}$ occurs at $x_1=x_2$.\n",
        "\n",
        "  - $f_0(x_1,x_2)=x_1^2+9x_2^2$. QP.\n",
        "\n",
        "- 4.3: Show $x^*=(1,\\/{1}{2},-1)$ is optimal for $\\BC\\t{minimize}&\\/{1}{2}x^‚ä§\\BM13&12&-2\\\\12&17&6\\\\-2&6&12\\EM x+\\BM-22\\\\-14.5\\\\13\\EM^‚ä§x+1\\\\\n",
        "\\t{subject to}&-1‚â§x_i‚â§1\n",
        "\\EC$.\n",
        "$‚àáf_0^2=P‚âª0$ therefore $f_0$ convex.\n",
        "$‚àáf_0(x^*)^‚ä§=x^{*‚ä§}P+q^‚ä§$\n",
        "$=\\BM1&\\/{1}{2}&-1\\EM\\BM13&12&-2\\\\12&17&6\\\\-2&6&12\\EM+\\BM-22&-14.5&13\\EM$\n",
        "$=\\BM-1&0&2\\EM$.\n",
        "Then\n",
        "$‚àáf_0(x^*)^‚ä§(y-x^*)$\n",
        "$=-(y_1-1)+2(y_3+1)$\n",
        "$=-y_1+2y_3+3$\n",
        "$=-(1)+2(-1)+3=0$. Therefore $x^*$ is optimal.\n",
        "\n",
        "- 4.5 (**Huber penalty**): $\\BC\\t{minimize}&\\sum_iœï(r_i)\\EC$ where $r_i=a_i^‚ä§x-b_i$ and $\\blue{œï(u)=\\BC\n",
        "u^2&|u|‚â§M\\\\\n",
        "M(2|u|-M)&|u|>M\\EC}$.\n",
        "\n",
        "  - **Weighted least squares** $\\BC\\t{minimize}&\\sum_i(\\/{r_i^2}{w_i+1}+M^2w_i)\\\\\n",
        "  \\t{subject to}&w‚âΩ0\\EC$ is equivalent.\n",
        "  Let $f(w_i)=\\/{r_i^2}{w_i+1}+M^2w_i$.\n",
        "  Differentiate subject to $w_i‚â•0$ we have\n",
        "  $‚àÇ_{w_i}f(w_i)=-\\/{r_i^2}{(w_i+1)^2}+M^2=0$\n",
        "  $‚áí(w_i+1)^2=\\/{r_i^2}{M^2}$\n",
        "  $‚áíw_i^*=\\/{|r_i|}{M}-1$.\n",
        "  Therefore $\\green{w_i=\\max(0,\\/{|r_i|}{M}-1)}$ and $\\green{\\BC\n",
        "  \\t{if }|r_i|‚â§M&w_i=0&f(w_i)=r_i^2\\\\\n",
        "  \\t{if }|r_i|>M&w_i=\\/{|r_i|}{M}-1&f(w_i)=\\/{r_i^2}{|r_i|/M}+M^2(\\/{|r_i|}{M}-1)=2M|r_i|-M^2\n",
        "  \\EC}$.\n",
        "  $\\BC F_1(x)=\\sum_i(a_i^‚ä§x-b_i)^2/(w_i+1)\\\\F_2(x)=M^2\\v{1}^‚ä§w\\EC$ is modified $\\n{Ax-b}_2^2$ with weights and regularization where residual $r_i^2$ is weighted less (and therefore less influence on solver) if it is a large outlier.\n",
        "\n",
        "  - **QP** $\\BC\\t{minimize}&\\sum_i(u_i^2+2Mv_i)\\\\\n",
        "  \\t{subject to}&-u-v‚âºAx-b‚âºu+v\\\\\n",
        "  &0‚âºu‚âºM\\v{1},\\ v‚âΩ0\n",
        "  \\EC$ is equivalent.\n",
        "  The constraints say $\\BC\n",
        "  |r_i|‚â§u_i+v_i\\\\\n",
        "  0‚â§u_i‚â§M\\\\\n",
        "  v_i=\\max(0,|r_i|-u_i)\n",
        "  \\EC$.\n",
        "  Then $\\green{\\BC\n",
        "  \\t{if }|r_i|‚â§M&u_i=|r_i|&v_i=0&r^2\\\\\n",
        "  \\t{if }|r_i|>M&u_i=M&v_i=|r_i|-M&M^2+2M(|r_i|-M)=2M|r_i|-M^2\n",
        "  \\EC}$\n",
        "\n",
        "- 4.8: LPs\n",
        "\n",
        "  - **LP over affine**: $\\blue{\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&Ax=b\\EC}$.\n",
        "  Solution to $Ax=b$ is $x=x_p+\\null(A)$.\n",
        "  Then $c^‚ä§x=c^‚ä§x_p+c^‚ä§z\\ ‚àÄz‚àà\\null(A)$.\n",
        "  If $c^‚ä§z\\neq0$ then $‚àÉz:c^‚ä§z=-‚àû$. Therefore $c^‚ä§z=0\\ ‚àÄz‚àà\\null(A)$\n",
        "  $‚áí\\blue{c‚àà\\null(A)^\\perp=\\span(A^‚ä§)}$ any any $x_p$ is an optimal solution. Otherwise problem is unbounded below.\n",
        "\n",
        "  - **LP over halfspace**: $\\blue{\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&a^‚ä§x‚â§b\\EC}$. $c^‚ä§x$ is a tilted hyperplane through origin and $c$ is the gradient pointing towards ascent.\n",
        "  If $c\\perp a$ then minimizing along $-c$ moves toward $-‚àû$.\n",
        "  If $a$ is codirected with $-c$ (i.e., $a$ and $c$ point in opposite directions $a=kc,\\ k< 0$), then the halfspace cuts off the descent of $c^‚ä§x$.\n",
        "\n",
        "  - **LP over rectangle**: $\\blue{\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&l‚âºx‚âºu\\EC}$. Conic program.\n",
        "  $\\BC\\t{minimize}&\\sum_ic_ix_i\\\\\n",
        "  \\t{subject to}&l_i‚â§x_i‚â§u_i\\EC$.\n",
        "  Separable problem:\n",
        "  $x_i^*=\\BC l_i&c_i>0\\\\\n",
        "  u_i&c_i< 0\n",
        "  \\EC$.\n",
        "  If $c_i=0$ then $x_i^*‚àà[l_i,u_i]$ any value can be used.\n",
        "\n",
        "  - **LP over probability simplex**: $\\blue{\\BC\n",
        "  \\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&\\v{1}^‚ä§x=1,\\ x‚âΩ0\n",
        "  \\EC}$.\n",
        "  Portfolio optimization problem with $-c$ return. $\\m{F}$ is a convex hull of Cartesian basis vectors. Like LP over rectangle, we look at separate components of gradient $c$, the lowest $c_i$ is the Cartesian of greatest descent. Then $x_i^*=1$ and $x_j^*=0\\ ‚àÄj\\neq i$.\n",
        "\n",
        "  - **LP over a unit box**: $\\BC\n",
        "  \\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&\\v{1}^‚ä§x=Œ±,\\ 0‚âºx‚âº\\v{1}\n",
        "  \\EC$.\n",
        "  Filling up a $Œ±$-bucket using $n$ ingredients, each with price $c_i$. Greedy algorithm use the cheapest $c_i$ ingredients first in ascending order.\n",
        "\n",
        "  - **LP with weighted budget constraint**: $\\BC\n",
        "  \\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&d^‚ä§x=Œ±,\\ 0‚âºx‚âº\\v{1}\n",
        "  \\EC$ with $d‚âª0$ and $0‚â§Œ±‚â§\\v{1}^‚ä§d$. Each ingredient has volume $d_i$. Use highest $\\/{d_i}{c_i}$ ratio ingredients first in descending order.\n",
        "\n",
        "- 4.9: **Square LP**: $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax‚âºb\\EC$ where $A‚àà\\v{S}_{++}^n$.\n",
        "Let $y=Ax$ then $\\BC\\t{minimize}&c^‚ä§A^{-1}y\\\\\n",
        "\\t{subject to}&y‚âºb\\EC$. If $c^‚ä§A^{-1}‚âª0$ then $y‚Üí-‚àû$ and the problem is unbounded below. If $c^‚ä§A^{-1}‚âº0$ then $y^*=b$.\n",
        "Therefore\n",
        "$p^*=\\BC\n",
        "c^‚ä§A^{-1}b&c^‚ä§A^{-1}‚âº0\\\\\n",
        "-‚àû&\\t{otherwise}\n",
        "\\EC$.\n",
        "\n",
        "- 4.11: formulate LP\n",
        "\n",
        "  - Minimize $\\blue{\\n{Ax-b}_‚àû}$ where $\\n{y}_‚àû=\\max_i|y_i|$.\n",
        "  $\\BC\\t{minimize}&t\\\\\n",
        "  \\t{subject to}&-t\\v{1}‚âºAx-b‚âºt\\v{1}\\EC$ LP $(x,t)$.\n",
        "  Here $(x^*,t^*)$ is the optimal solution to the LP.\n",
        "\n",
        "  - Minimize $\\blue{\\n{Ax-b}_1}$ where $\\n{y}_1=\\sum_i|y_i|$.\n",
        "  $\\BC\\t{minimize}&\\v{1}^‚ä§u\\\\\n",
        "  \\t{subject to}&-u‚âºAx-b‚âºu\\EC$ LP $(x,u)$.\n",
        "\n",
        "  - Minimize $\\n{Ax-b}_1$ subject to $\\n{x}_‚àû‚â§1$.\n",
        "  $\\BC\\t{minimize}&\\v{1}^‚ä§u\\\\\n",
        "  \\t{subject to}&-u‚âºAx-b‚âºu\\\\\n",
        "  &-\\v{1}‚âºx‚âº\\v{1}\\EC$.\n",
        "\n",
        "  - Minimize $\\n{x}_1$ subject to $\\n{Ax-b}_‚àû‚â§1$.\n",
        "  $\\BC\\t{minimize}&\\v{1}^‚ä§u\\\\\n",
        "  \\t{subject to}&-u‚âºx‚âºu\\\\\n",
        "  &-\\v{1}‚âºAx-b‚âº1\\EC$.\n",
        "\n",
        "- 4.21 QCQP:\n",
        "\n",
        "  - **Linear over centered ellipsoid**: $\\blue{\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&x^‚ä§Ax‚â§1,&A‚àà\\v{S}_{++}^n\\EC}$.\n",
        "  The solution will lie on the boundary of the ellipsoid $x^‚ä§Ax=1$ (levelset) in the direction of $-c$. I.e., the ellipsoid gradient is codirectional with $-c$:\n",
        "  $‚àá_x=2x^‚ä§A=-Œªc^‚ä§$\n",
        "  $‚áíx^*=\\/{-Œª}{2}A^{-1}c$.\n",
        "  Then substitute back into ellipsoid to solve for $Œª>0$:\n",
        "  $\\/{Œª^2}{4}(A^{-1}c)^‚ä§A(A^{-1}c)=1$\n",
        "  $‚áíŒª=\\/{2}{\\sqrt{c^‚ä§A^{-1}c}}$\n",
        "  $‚áíx^*=\\/{-A^{-1}c}{\\sqrt{c^‚ä§A^{-1}c}}$\n",
        "\n",
        "  - **Linear over shifted ellipsoid**: $\\blue{\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&(x-x_c)^‚ä§A(x-x_c)‚â§1,&A‚àà\\v{S}_{++}^n\\EC}$.\n",
        "  Let $y=x-x_c$ and $\\BC\\t{minimize}&c^‚ä§y\\\\\n",
        "  \\t{subject to}&y^‚ä§Ay‚â§1\\EC$ is identical to origin-centered.\n",
        "\n",
        "  - **Quadratic over centered ellipsoid**: $\\blue{\\BC\\t{minimize}&x^‚ä§Bx,&B‚àà\\v{S}_+^n\\\\\n",
        "  \\t{subject to}&x^‚ä§Ax‚â§1,&A‚àà\\v{S}_{++}^n\\EC}$.\n",
        "  $x^*=0$."
      ],
      "metadata": {
        "id": "2sKdlxC2QkEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Lagrangian Duality"
      ],
      "metadata": {
        "id": "uQhCxXgBKC74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lagrangian**: $p^*=\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0,&i=1,...,m\\\\\n",
        "&h_i(x)=0,&i=1,...,p\n",
        "\\EC$ with primal variable $x‚àà‚Ñù^n$, nonempty $\\m{D}$. Then the Lagrangian $L:‚Ñù^n√ó‚Ñù^m√ó‚Ñù^p‚Üí‚Ñù$ is $\\red{L(x,Œª,ŒΩ)=f_0(x)+\\sum_{i=1}^mŒª_if_i(x)+\\sum_{i=1}^pŒΩ_ih_i(x)}$ where $Œª_i$ is Lagrange multiplier associated with $i$-th inequality constraint and $ŒΩ_i$ is Lagrange multiplier associated with $i$-th equality constraint. $Œª$ and $ŒΩ$ are dual variables.\n",
        "\n",
        "- **Infinity walls**: Equivalent problem $\\green{p^*=\\BC\n",
        "\\t{minimize}&f_0(x)+\\sum_{i=1}^mI(f_i(x)‚â§0)+\\sum_{i=1}^pI(h_i(x)=0)\n",
        "\\EC}$\n",
        "where\n",
        "$I(u)=\\BC0&u\\t{ true}\\\\‚àû&u\\t{ false}\\EC$\n",
        "act as infinity wall against any constraint violations.\n",
        "\n",
        "  - **Linear relaxation**: $\\green{\\BC\n",
        "  \\t{minimize}&L(x,Œª,ŒΩ)=f_0(x)+\\sum_{i=1}^mŒª_if_i(x)+\\sum_{i=1}^pŒΩ_ih_i(x),\\ Œª_i‚â•0\n",
        "  \\EC}$ is relaxed where $I(f_i(x)‚â§0)‚ÜíŒª_if_i(x),\\ Œª_i‚â•0$ and $I(h_i(x)=0)‚ÜíŒΩ_ih_i(x)$ are linear underestimators of the infinity walls, which can be re-erected by\n",
        "  $\\sup_{Œª‚âΩ0,ŒΩ}L(x,Œª,ŒΩ)=f_0(x)+\\sup_{Œª‚âΩ0}\\sum_{i=1}^mŒª_if_i(x)+\\sup_ŒΩ\\sum_{i=1}^pŒΩ_ih_i(x))$.\n",
        "\n",
        "  - The Lagrangian reframes a constrained optimization primal problem as a dual problem where hard constraint violations are treated as linear costs. Its usefulness depends on duality.\n",
        "\n",
        "- **Lagrange dual function** $g:‚Ñù^m√ó‚Ñù^p‚Üí‚Ñù$ is\n",
        "$\\red{g(Œª,ŒΩ)=\\inf_{x‚àà\\m{D}}L(x,Œª,ŒΩ)}$,\n",
        "which is pointwise infimum of affine functions of $(Œª,ŒΩ)$ and is therefore concave regardless if $f_i$ are concave or convex.\n",
        "It satisfies the **dual lower bound**: $\\red{g(Œª,ŒΩ)‚â§p^*\\ ‚àÄ(Œª,ŒΩ)‚àà\\m{F}_D}$, where $\\m{F}_D=\\{(Œª,ŒΩ)‚àà\\dom(g)|Œª‚âΩ0,Œª‚àà‚Ñù^m,ŒΩ‚àà‚Ñù^p\\}$ is the dual feasible set.\n",
        "\n",
        "  - Proof: Let $\\tilde{x}‚àà\\m{F}$, then $\\sum_{i=1}^mŒª_if_i(\\tilde{x})+\\sum_{i=1}^pŒΩ_ih_i(\\tilde{x})‚â§0$\n",
        "  $‚áí\\green{L(\\tilde{x},Œª,ŒΩ)‚â§f_0(\\tilde{x})}$.\n",
        "  Therefore\n",
        "  $g(Œª,ŒΩ)=\\inf_{x‚àà\\m{D}}L(x,Œª,ŒΩ)$\n",
        "  $‚â§L(\\tilde{x},Œª,ŒΩ)$\n",
        "  $‚â§f_0(\\tilde{x})$.\n",
        "  Therefore $\\blue{g(Œª,ŒΩ)‚â§f_0(\\tilde{x})\\ ‚àÄ\\tilde{x}‚àà\\m{F}}$.\n",
        "\n",
        "  - Numerical solver outputs $x^*$ as well as $(Œª^*,ŒΩ^*)$ as a dual certificate to verify $g(Œª^*,ŒΩ^*)‚â§f_0(x^*)$.\n",
        "\n",
        "- **Least-squares**: $\\green{\\BC\n",
        "\\t{minimize}&x^‚ä§x\\\\\n",
        "\\t{subject to}&Ax=b,&A‚àà‚Ñù^{p√ón}\n",
        "\\EC}$.\n",
        "Lagrangian $\\green{L(x,ŒΩ)=x^‚ä§x+ŒΩ^‚ä§(Ax-b)}$.\n",
        "Dual $g(ŒΩ)=\\inf_x(x^‚ä§x+ŒΩ^‚ä§(Ax-b))$.\n",
        "$‚àá_xL=2x^‚ä§+ŒΩ^‚ä§A=0$\n",
        "$‚áíx=-\\/{1}{2}A^‚ä§ŒΩ$.\n",
        "Substitute back\n",
        "$g(ŒΩ)=\\/{1}{4}ŒΩ^‚ä§AA^‚ä§ŒΩ-\\/{1}{2}ŒΩ^‚ä§AA^‚ä§ŒΩ-ŒΩ^‚ä§b$\n",
        "$=\\green{-\\/{1}{4}ŒΩ^‚ä§AA^‚ä§ŒΩ-ŒΩ^‚ä§b}$\n",
        "which is a concave negative quadratic.\n",
        "\n",
        "  - Alternatively by conjugate, dual $g(ŒΩ)=\\inf_x(\\n{x}_2^2+ŒΩ^‚ä§(Ax-b))$\n",
        "  $=-ŒΩ^‚ä§b-\\sup_x(-(A^‚ä§ŒΩ)^‚ä§x-\\n{x}_2^2)$\n",
        "  $=-ŒΩ^‚ä§b-\\/{1}{4}\\n{A^‚ä§ŒΩ}_2^2$.\n",
        "\n",
        "- **Standard form LP**: $\\green{\\BC\n",
        "\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax=b\\\\\n",
        "&x‚âΩ0\n",
        "\\EC}$.\n",
        "Lagrangian $\\green{L(x,Œª,ŒΩ)=c^‚ä§x-Œª^‚ä§x+ŒΩ^‚ä§(Ax-b)}$\n",
        "$=(c-Œª+A^‚ä§ŒΩ)^‚ä§x-b^‚ä§ŒΩ$ is affine function of $x$, which minimizes at $-‚àû$ unless zero slope.\n",
        "Therefore\n",
        "$\\green{g(Œª,ŒΩ)=\\BC-b^‚ä§ŒΩ&(c-Œª+A^‚ä§ŒΩ)=0\\\\\n",
        "-‚àû&\\t{otherwise}\n",
        "\\EC}$ and the dual LP is $\\BC\\t{maximize}&-b^‚ä§ŒΩ\\\\\n",
        "\\t{subject to}&A^‚ä§ŒΩ+c=Œª,&Œª‚âΩ0\\EC$.\n",
        "\n",
        "  - Another perspective: $A^‚ä§ŒΩ+c‚âΩ0$ and $x‚âΩ0$ give $(A^‚ä§ŒΩ)^‚ä§x+c^‚ä§x‚âΩ0$. Furthermore $Ax=b‚áí(A^‚ä§ŒΩ)^‚ä§x=ŒΩ^‚ä§b$.\n",
        "  Therefore $ŒΩ^‚ä§b+c^‚ä§x‚âΩ0‚áíc^‚ä§x‚âΩ-ŒΩ^‚ä§b$\n",
        "\n",
        "- **Two-way partitioning**: $\\BC\n",
        "\\t{minimize}&x^‚ä§Wx,&W‚àà\\v{S}^n\\\\\n",
        "\\t{subject to}&x_i^2=1\n",
        "\\EC$.\n",
        "Here $x_i=¬±1$, and the NP-hard problem is searching amongst $2^n$ combinations for the optimal one, which would have $x_iW_{ij}x_j< 0$ for as many pairs as possible, where $W_{ij}$ encodes a preference for the $i$ and $j$ from same team vs opposite teams.\n",
        "Instead of solving exactly we find lower bound using duality, which minimizes the Lagrangian over all real $x$ and not just $¬±1$.\n",
        "$\\green{L(x,ŒΩ)=x^‚ä§Wx+\\sum_iŒΩ_i(x_i^2-1)}$\n",
        "$=x^‚ä§Wx+x^‚ä§\\diag(ŒΩ_i)x-\\v{1}^‚ä§ŒΩ$\n",
        "$=x^‚ä§(W+\\diag(ŒΩ_i))x-\\v{1}^‚ä§ŒΩ$.\n",
        "$‚àá_xL=2x^‚ä§(W+\\diag(ŒΩ_i))$.\n",
        "$g(ŒΩ)=\\BC\n",
        "-\\v{1}^‚ä§ŒΩ&W+\\diag(ŒΩ_i)‚âΩ0\\\\\n",
        "-‚àû&\\t{otherwise}\n",
        "\\EC$.\n",
        "If $W+\\diag(ŒΩ_i)$ had negative eigenvalues then $x‚Üí¬±‚àû$ along that eigenvector will lead $L‚Üí-‚àû$.\n",
        "We guarantee $W+\\diag(ŒΩ_i)‚âΩ0$ nonnegative eigenvalues by choosing $ŒΩ_i=-Œª_\\min(W)\\ ‚àÄi$.\n",
        "Then $g(v)=-\\sum_iŒΩ_i=nŒª_\\min$\n",
        "and $p^*‚â•nŒª_\\min$.\n",
        "\n",
        "  - **Two-way partitioning (Rayleigh Quotient)**: $\\BC\n",
        "  \\t{minimize}&x^‚ä§Wx\\\\\n",
        "  \\t{subject to}&x_i^2=1\n",
        "  \\EC ‚áí \\BC\n",
        "  \\t{minimize}&x^‚ä§Wx\\\\\n",
        "  \\t{subject to}&x^‚ä§x=n\n",
        "  \\EC$\n",
        "  via constraint relaxation $x_i^2=1‚áí\\sum_ix_i^2=n$\n",
        "  gives classic Rayleigh Quotient where $p^*=nŒª_\\min(W)$.\n",
        "  \n"
      ],
      "metadata": {
        "id": "wtRmEIEGKHxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lagrange dual problem**: $d^*=\\BC\n",
        "\\t{maximize}&g(Œª,ŒΩ)\\\\\n",
        "\\t{subject to}&Œª‚âΩ0\n",
        "\\EC$, whose solution is the dual optimal point $(Œª^*,ŒΩ^*)$.\n",
        "\n",
        "- **Weak duality**: The dual optimal value is $d^*‚â§p^*$, which holds regardless if $f_i$ are convex or concave. $p^*-d^*$ is the **optimal duality gap**. The dual problem is always a convex optimization problem.\n",
        "\n",
        "  - Max-min inequality: $\\sup_{Œª‚âΩ0}\\inf_xL(x,Œª)‚â§\\inf_x\\sup_{Œª‚âΩ0}L(x,Œª)$\n",
        "\n",
        "- **Strong duality**: $d^*=p^*$. Attained if the primal problem is convex\n",
        "$\\BC\\t{minimize}&f_0(x)&\\t{convex}\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0&\\t{convex}\\\\\n",
        "&Ax=b\\EC$\n",
        "and satisfies **Slater's condition**:\n",
        "$\\red{‚àÉx‚àà\\t{relint}(\\m{D}):\\t{non-affine }f_i(x)< 0\\ ‚àÄi,\\ \\t{affine }f_i(x)‚â§0\\ ‚àÄi,\\ Ax=b}$,\n",
        "which is a point that satisfies non-strict feasibility for affine inequalities and strict feasibility for non-affine inequalities.\n",
        "\n",
        "  - Lecture: **Strong duality holds in all practical convex problems. All.** There are people who say any problem you can solve in optimization is because they're convex.\n",
        "  \n",
        "  - $\\BC\\t{minimize}&x^‚ä§Ax+2b^‚ä§x\\\\\n",
        "  \\t{subject to}&x^‚ä§x‚â§1\n",
        "  \\EC$ is one of few examples of non-convex problems with strong duality.\n",
        "\n",
        "**Conjugate vs. Lagrange dual**: $\\BC\n",
        "\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&Ax‚âºb\\\\\n",
        "&Cx=d\n",
        "\\EC$\n",
        "Then $g(Œª,ŒΩ)=\\inf_x(f_0(x)+Œª^‚ä§(Ax-b)+ŒΩ^‚ä§(Cx-d))$\n",
        "$=-\\sup_x(-f_0(x)-(A^‚ä§Œª+C^‚ä§ŒΩ)^‚ä§x+Œª^‚ä§b+ŒΩ^‚ä§d)$\n",
        "$‚áí\\red{g(Œª,ŒΩ)=-Œª^‚ä§b-ŒΩ^‚ä§d-f_0^*(-A^‚ä§Œª-C^‚ä§ŒΩ)}$\n",
        "with $\\dom(g)=\\{(Œª,ŒΩ)|-A^‚ä§Œª-C^‚ä§ŒΩ‚àà\\dom(f_0^*)\\}$.\n",
        "\n",
        "- **Norm** (lecture): $\\green{\\BC\n",
        "\\t{minimize}&\\n{x}\\\\\n",
        "\\t{subject to}&Ax=b\n",
        "\\EC}$.\n",
        "Dual $g(ŒΩ)=\\inf_x(\\n{x}+ŒΩ(Ax-b))$\n",
        "$=-ŒΩ^‚ä§b-\\sup_x(-(A^‚ä§ŒΩ)^‚ä§x-\\n{x})$\n",
        "$=\\green{\\BC-ŒΩ^‚ä§b&\\n{A^‚ä§ŒΩ}_*‚â§1\\\\\n",
        "-‚àû&\\t{otherwise}\n",
        "\\EC}$\n",
        "with dual problem\n",
        "$\\BC\\t{maximize}&-ŒΩ^‚ä§b\\\\\n",
        "\\t{subject to}&\\n{A^‚ä§ŒΩ}_*‚â§1\n",
        "\\EC$\n",
        "\n",
        "  - Because $ŒΩ$ has no $¬±$ sign restriction, some texts would use $ŒΩ^‚ä§b$ instead of $-ŒΩ^‚ä§b$.\n",
        "\n",
        "- **Max entropy**: $\\green{\\BC\\t{minimize}&\\sum_ix_i\\ln(x_i)\\\\\n",
        "\\t{subject to}&Ax‚âºb,\\ \\v{1}^‚ä§x=1\\EC}$.\n",
        "$h(x)=yx-x\\ln(x)$\n",
        "$‚áí‚àÇh=y-\\ln(x)-1$\n",
        "$‚áíx^*=e^{y-1}$.\n",
        "$f_0^*(y)=\\sum_ie^{y_i-1}$.\n",
        "Then\n",
        "$g(Œª,ŒΩ)=-Œª^‚ä§b-ŒΩ-f_0^*(-A^‚ä§Œª-\\v{1}^‚ä§ŒΩ)$\n",
        "$=\\green{-Œª^‚ä§b-ŒΩ-\\sum_ie^{-a_i^‚ä§Œª-ŒΩ-1}}$\n",
        "\n",
        "- **Standard form LP**: $\\green{\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax=b,\\ x‚âΩ0\\EC}$.\n",
        "$g(Œª,ŒΩ)=\\BC-b^‚ä§ŒΩ&A^‚ä§ŒΩ-Œª+c=0\\\\\n",
        "-‚àû&\\t{otherwise}\n",
        "\\EC$ and\n",
        "$\\green{\\BC\\t{maximize}&-b^‚ä§ŒΩ\\\\\n",
        "\\t{subject to}&A^‚ä§ŒΩ+c‚âΩ0\\EC}$\n",
        "is the dual problem.\n",
        "\n",
        "- **Inequality form LP**: $\\green{\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax‚âºb\\EC}$.\n",
        "$L(x,Œª)=c^‚ä§x+Œª^‚ä§(Ax-b)$\n",
        "$=(c+A^‚ä§Œª)^‚ä§x-Œª^‚ä§b$.\n",
        "$g(Œª)=\\BC-Œª^‚ä§b&c+A^‚ä§Œª=0\\\\\n",
        "-‚àû&\\t{otherwise}\n",
        "\\EC$.\n",
        "Dual problem\n",
        "$\\green{\\BC\\t{maximize}&-Œª^‚ä§b\\\\\n",
        "\\t{subject to}&c+A^‚ä§Œª=0,\\ Œª‚âΩ0\\EC}$.\n",
        "\n",
        "- **QCQP**: $\\BC\\t{minimize}&\\/{1}{2}x^‚ä§P_0x+q_0^‚ä§x+r_0&P_0‚àà\\v{S}_{++}^n\\\\\n",
        "\\t{subject to}&\\/{1}{2}x^‚ä§P_ix+q_i^‚ä§x+r_i‚â§0&P_i‚àà\\v{S}_+^n\n",
        "\\EC$.\n",
        "$L(x,Œª)=\\/{1}{2}x^‚ä§P_0x+q_0^‚ä§x+r_0+\\sum_iŒª_i(\\/{1}{2}x^‚ä§P_ix+q_i^‚ä§x+r_i)$\n",
        "$=\\/{1}{2}x^‚ä§P(Œª)x+q(Œª)^‚ä§x+r(Œª)$\n",
        "where $P(Œª)=P_0+\\sum_iŒª_iP_i‚âª0$.\n",
        "Then $‚àáL_x^‚ä§=x^‚ä§P(Œª)+q(Œª)^‚ä§=0$\n",
        "$‚áíx=-P(Œª)^{-1}q(Œª)$.\n",
        "$g(Œª)=\\/{1}{2}q(Œª)^‚ä§P(Œª)^{-1}q(Œª)-q(Œª)^‚ä§P(Œª)^{-1}q(Œª)+r(Œª)$\n",
        "$=-\\/{1}{2}q(Œª)^‚ä§P(Œª)^{-1}q(Œª)+r(Œª)$.\n",
        "Therefore\n",
        "$\\BC\\t{maximize}&-\\/{1}{2}q(Œª)^‚ä§P(Œª)^{-1}q(Œª)+r(Œª)\\\\\n",
        "\\t{subject to}&Œª‚âΩ0\n",
        "\\EC$.\n",
        "\n",
        "- **Max entropy**: $\\green{\\BC\\t{minimize}&\\sum_ix_i\\ln(x_i)\\\\\n",
        "\\t{subject to}&Ax‚âºb,\\ \\v{1}^‚ä§x=1\n",
        "\\EC}$ with $\\m{D}=‚Ñù_+^n$\n",
        "$‚áí\\BC\\t{maximize}&-b^‚ä§Œª-ŒΩ-\\sum_ie^{-a_i^‚ä§Œª-ŒΩ-1}\\\\\n",
        "\\t{subject to}&Œª‚âΩ0\n",
        "\\EC$\n",
        "Slater condition says duality gap is zero if $‚àÉx‚àà‚Ñù_{++}^n:Ax‚âºb,\\ \\v{1}^‚ä§x=1$.\n",
        "Optimize $ŒΩ$ out:\n",
        "$‚àÇ_ŒΩg(Œª,ŒΩ)=-1+\\sum_ie^{-a_i^‚ä§Œª-ŒΩ-1}=0$\n",
        "$‚áí\\ln(\\sum_ie^{-a_i^‚ä§Œª})-ŒΩ-1=0$\n",
        "$‚áíŒΩ^*=\\ln(\\sum_ie^{-a_i^‚ä§Œª})-1$.\n",
        "$\\green{\\BC\\t{maximize}&-b^‚ä§Œª-\\ln(\\sum_ie^{-a_i^‚ä§Œª})\\\\\n",
        "\\t{subject to}&Œª‚âΩ0\n",
        "\\EC}$, which is convex geometric program.\n",
        "\n"
      ],
      "metadata": {
        "id": "8zDRlBYmR1yP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geometric interpretation**: Let $\\m{G}=\\{(f_1(x),...,f_m(x),h_1(x),...,h_p(x),f_0(x))‚àà‚Ñù^m√ó‚Ñù^p√ó‚Ñù|x‚àà\\m{D}\\}$. Then\n",
        "$p^*=\\inf\\{t|(u,v,t)‚àà\\m{G},u‚âº0,v=0\\}$\n",
        "and\n",
        "$g(Œª,ŒΩ)=\\inf\\{(Œª,ŒΩ,1)^‚ä§(u,v,t)|(u,v,t)‚àà\\m{G}\\}$.\n",
        "Define $\\m{A}=\\m{G}+(‚Ñù_+^m√ó\\{0\\}√ó‚Ñù_+)=\\{(u,v,t)|‚àÉx‚àà\\m{D},f_i(x)‚â§u_i\\ ‚àÄi,h_i(x)=v_i\\ ‚àÄi,f_0(x)‚â§t\\}$ be the epigraph that includes all points in $\\m{G}$ and worse.\n",
        "Visualize simple case $\\m{G}=\\{(f_1(x),f_0(x))|x‚àà\\m{D}\\}$ in a 2D space where the vertical axis $t$ represents $f_0(x)$ and the horizontal axis $u$ represent $f_1(x)$.\n",
        "\n",
        "- $\\m{A}=\\{(u,t)|‚àÉx‚àà\\m{D},f_1(x)‚â§u,f_0(x)‚â§t\\}$ is an epigraph that includes all points in $\\m{G}$ and worse (greater costs along $t$ and higher violations along $u$). Then $\\m{A}$ is convex iff the problem is convex.\n",
        "\n",
        "  - Proof: $F=\\{(x,u,t):f_0(x)‚â§t,f_1(x)‚â§u\\}$ is the convex intersection of convex epigraphs.\n",
        "  Then $\\m{A}=\\{(u,t)|‚àÉx‚àà\\m{D},(x,u,t)‚ààF\\}$ is a projection mapping of $\\m{F}$, and is convex.\n",
        "\n",
        "- $p^*=\\inf\\{t|(u,t)‚àà\\m{G},u‚âº0\\}=\\inf\\{t|(0,t)‚àà\\m{A}\\}$ is the lowest point of $\\m{G}$ that lies in $u‚â§0$.\n",
        "\n",
        "- $g(Œª)=\\inf_{(u,t)‚àà\\m{G}}(Œª,1)^‚ä§(u,t)‚áí\\green{g(Œª)‚â§Œªu+t}\\ ‚àÄ(u,t)‚àà\\m{G}$ defines a supporting hyperplane of $\\m{G}$.\n",
        "The lower tangent $\\green{t=g(Œª,ŒΩ)-Œªu}$ has negative slope ($Œª‚â•0$) and intersects the $t$-axis at $g(Œª)$.\n",
        "\n",
        "- $d^*$ is the highest $t$-intersect for all non-positive sloping tangent lines of the lower-left frontier of $\\m{G}$. Therefore $d^*‚â§p^*$.\n",
        "If the lower-left frontier of $\\m{G}$ has a dent, then the duality gap is nonzero $d^*< p^*$.\n",
        "\n",
        "**Slater's condition proof**: Assume convex problem. Slater's point $\\tilde{x}‚àà\\t{relint}(\\m{D}):f_i(\\tilde{x})< 0\\ ‚àÄi,\\ A\\tilde{x}=b$ satisfy Slater's condition. Assume $\\rk(A)=p$ (no redundancy in $Ax=b$).\n",
        "Define $\\m{B}=\\{(0,0,s)‚àà‚Ñù^m√ó‚Ñù^p√ó‚Ñù|s< p^*\\}$\n",
        "then $\\m{A}‚à©\\m{B}=‚àÖ$ are disjoint and are separable by a hyperplane\n",
        "$\\tilde{Œª}^‚ä§u+\\tilde{ŒΩ}^‚ä§v+Œºt=Œ±$\n",
        "where $(\\tilde{Œª},\\tilde{ŒΩ},Œº)\\neq0$.\n",
        "\n",
        "- $\\BC\n",
        "(u,v,t)‚àà\\m{A} ‚áí (\\tilde{Œª},\\tilde{ŒΩ},Œº)^‚ä§(u,v,t)‚â•Œ±\n",
        "& (0,0,p^*)‚àà\\m{A} & ‚áí Œºp^*‚â•Œ±,\\ \\tilde{Œª}‚âΩ0,\\ Œº‚â•0 \\\\\n",
        "(u,v,t)‚àà\\m{B} ‚áí (\\tilde{Œª},\\tilde{ŒΩ},Œº)^‚ä§(u,v,t)‚â§Œ±\n",
        "& (0,0,s)‚àà\\m{B}\\ ‚àÄs< p^* & ‚áí Œºp^*‚â§Œ±\n",
        "\\EC$.\n",
        "\n",
        "  - $\\blue{\\sum_i\\tilde{Œª}_if_i(x)+\\tilde{ŒΩ}^‚ä§(Ax-b)+Œºf_0(x)‚â•Œºp^*}$: existence of the separating hyperplane $\\tilde{Œª}^‚ä§u+\\tilde{ŒΩ}^‚ä§v+Œºt=Œ±$ leads to $g(\\tilde{Œª},\\tilde{ŒΩ})‚â•Œºp^*$. Slater's condition will allow division of $Œº$ from both sides, as shown below.\n",
        "\n",
        "- Suppose $Œº=0$, then\n",
        "$\\green{\\sum_i\\tilde{Œª}_if_i(x)+\\tilde{ŒΩ}^‚ä§(Ax-b)‚â•0}$\n",
        "for all $x‚àà\\m{D}$. The hyperplane is vertical and separates $\\m{A}$ and $\\m{B}$ based on $u$ and $v$ while ignoring $t$\n",
        "\n",
        "  - The Slater point $\\tilde{x}$ satisfies\n",
        "  $f_i(\\tilde{x})< 0$ and $A\\tilde{x}-b=0$.\n",
        "  Substituting in green\n",
        "  $\\sum_i\\tilde{Œª}_if_i(\\tilde{x})+\\tilde{ŒΩ}^‚ä§0‚â•0$\n",
        "  $‚áí\\tilde{Œª}_i=0$.\n",
        "\n",
        "  - Since $(\\tilde{Œª},\\tilde{ŒΩ},Œº)\\neq0$ but $\\tilde{Œª}=0$ and $Œº=0$ then\n",
        "  $\\tilde{ŒΩ}\\neq0$.\n",
        "  Substituting in green\n",
        "  $\\tilde{ŒΩ}^‚ä§(Ax-b)‚â•0$ for all $x‚àà\\m{D}$.\n",
        "  However,\n",
        "  $\\tilde{ŒΩ}^‚ä§(Ax-b)$ is an affine function of $x$ whose slope is $A^‚ä§\\tilde{ŒΩ}$.\n",
        "\n",
        "  - Because\n",
        "  $\\tilde{x}‚àà\\t{relint}(\\m{D})$ satisfies\n",
        "  $\\tilde{ŒΩ}^‚ä§(A\\tilde{x}-b)=0$\n",
        "  then there must be another point $x'‚àà\\m{D}$ in the neighborhood of $\\tilde{x}$ such that\n",
        "  $\\tilde{ŒΩ}^‚ä§(Ax'-b)< 0$\n",
        "  unless $A^‚ä§\\tilde{ŒΩ}=0$\n",
        "  $‚áí\\tilde{ŒΩ}=0$\n",
        "  because $A$ is full rank.\n",
        "\n",
        "  - Therefore $Œº=0$ would lead to $\\tilde{Œª}=0$ and $\\tilde{ŒΩ}=0$, which contradicts green and the separating hyperplane.\n",
        "\n",
        "- Suppose $Œº>0$, then $L(x,Œª,ŒΩ)=\\sum_i\\/{\\tilde{Œª}_i}{Œº}f_i(x)+\\/{\\tilde{ŒΩ}}{Œº}^‚ä§(Ax-b)+f_0(x)‚â•p^*$\n",
        "$‚áíg(Œª,ŒΩ)=\\inf_xL(x,Œª,ŒΩ)‚â•p^*$.\n",
        "However, weak duality says $g(Œª,ŒΩ)‚â§p^*$.\n",
        "Therefore $g(Œª,ŒΩ)=p^*$.\n",
        "\n",
        "**Pareto interpretation**: Unconstrained multicriterion problem\n",
        "$\\BC\n",
        "\\t{minimize wrt }‚Ñù_+^{m+1}&F(x)=(f_1(x),...,f_m(x),f_0(x))\n",
        "\\EC$ can be scalarized to become\n",
        "$\\BC\n",
        "\\t{minimize}&(Œª,1)^‚ä§F(x)=f_0(x)+\\sum_iŒª_if_i(x)\n",
        "\\EC$, Lagrangian of\n",
        "$\\BC\\t{minimize}&f_0(x)&‚Ñù^n‚Üí‚Ñù\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0&i=1,...,m\\EC$.\n",
        "\n",
        "- $\\m{A}=\\m{O}+‚Ñù_+^{m+1}$\n",
        "$=\\{t‚àà‚Ñù^{m+1}|‚àÉx‚àà\\m{D},f_i(x)‚â§t_i\\ ‚àÄi\\}$ with achievable objective values $\\m{O}$ is same as the epigraph set $\\m{A}=\\m{G}+(‚Ñù_+^m√ó‚Ñù_+)=\\{(u,t)‚àà‚Ñù^{m+1}|‚àÉx‚àà\\m{D},f_i(x)‚â§u_i\\ ‚àÄi,f_0(x)‚â§t\\}$\n",
        "\n",
        "- Solving a Lagrangian constrained problem is same as solving a Pareto multicriterion trade-off problem where $Œª_i$ is the objective cost of violating constraint $i$."
      ],
      "metadata": {
        "id": "vaorA-lYlEbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimality conditions**: Many problems can be solved analytically through optimality conditions.\n",
        "\n",
        "- **Certificate of suboptimality**: $g(Œª,ŒΩ)$ is a certified lowerbound for $p^*$. **Duality gap** $\\red{œµ=f_0(x)-g(Œª,ŒΩ)}$ guarantees that solution $x‚àà\\m{F}$ is $œµ$-suboptimal. If the duality gap is zero then $p^*=f_0(x)$ and $d^*=g(Œª,ŒΩ)$. Optimization solvers use user-provided absolute accuracy $œµ_\\t{abs}$ or $œµ_\\t{rel}$ as stopping criterion $\\blue{f_0(x^{(k)})-g(Œª^{(k)},ŒΩ^{(k)})‚â§œµ_\\t{abs}}$\n",
        "\n",
        "- **Complementary slackness**: Suppose strong duality holds with $x^*$ and $(Œª^*,ŒΩ^*)$.\n",
        "Then $\\red{\\sum_{i=1}^mŒª^*_if_i(x^*)=0}$, and since each term is nonpositive, then $\\red{Œª_i^*f_i(x^*)=0\\ ‚àÄi}‚áí\\BC\n",
        "Œª_i^*>0 ‚áí f_i(x^*)=0 &\\t{active (taut) constraint}\\\\\n",
        "f_i(x^*)< 0 ‚áí Œª_i^*=0 &\\t{inactive (slack) constraint}\n",
        "\\EC$.\n",
        "\n",
        "  - Proof: $\\BC\n",
        "  f_0(x^*)&=g(Œª^*,ŒΩ^*)&\\t{strong duality forces equality unto the 3rd line}\\\\\n",
        "  &=\\inf_x(f_0(x)+\\sum_{i=1}^mŒª^*_if_i(x)+\\sum_{i=1}^pŒΩ_i^*h_i(x))&\\t{with forced equality, }x^*=\\arg\\inf_xL(x,Œª^*,ŒΩ^*)\\\\\n",
        "  &\\red{‚â§}f_0(x^*)+\\sum_{i=1}^mŒª^*_if_i(x^*)+\\sum_{i=1}^pŒΩ_i^*h_i(x^*)&‚â§\\t{ because inf}\\\\\n",
        "  &‚â§f_0(x^*)&‚â§\\t{ because }Œª_i‚â•0\\t{ and }f_i(x^*)‚â§0\n",
        "  \\EC$\n",
        "\n",
        "- **KKT optimality conditions**: Suppose $f_i,h_i$ are differentiable. Since $x^*=\\arg\\inf_xL(x,Œª^*,ŒΩ^*)$, Lagrangian gradient $\\red{‚àá_xL(x^*,Œª^*,ŒΩ^*)=‚àá_xf_0(x^*)+\\sum_{i=1}^mŒª_i^*‚àá_xf_i(x^*)+\\sum_{i=1}^pŒΩ_i^*‚àá_xh_i(x^*)=0}$ and\n",
        "$\\blue{‚àÇ_{x_i}L(x^*,Œª^*,ŒΩ^*)=0}$.\n",
        "\n",
        "  - Karush-Kuhn-Tucker:\n",
        "  $\\BC\n",
        "  \\t{Primal feasible}&f_i(x^*)‚â§0,\\ h_i(x^*)=0\\\\\n",
        "  \\t{Dual feasible}&Œª_i‚â•0\\\\\n",
        "  \\t{Complementary slackness}&Œª_i^*f_i(x^*)=0\\\\\n",
        "  \\t{Stationarity}&‚àá_xf_0(x^*)+\\sum_{i=1}^mŒª_i^*‚àá_xf_i(x^*)+\\sum_{i=1}^pŒΩ_i^*‚àá_xh_i(x^*)=0\\\\\n",
        "  \\EC$\n",
        "\n",
        "  - For non-convex problems, $x^*$ optimal $‚áí$ KKT satisfied. KKT is a necessary requirement but not sufficient to prove global optimality because of local critical points.\n",
        "\n",
        "  - For convex problems, $x^*$ optimal $‚áî$ KKT satisfied. **KKT is necessary and sufficient to prove $x^*$ and $(Œª^*,ŒΩ^*)$ are primal and dual optimal with zero duality gap**.\n",
        "  Proof (zero duality gap): $\\red{g(Œª^*,ŒΩ^*)=L(x^*,Œª^*,ŒΩ^*)=f(x^*)}$\n",
        "  because $Œª_i^*f_i(x^*)=0$ and $h_i(x^*)=0$\n",
        "\n",
        "- **Force analogy**: Energy = Force √ó Displacement. Let $f_0(x)$ be spring potential energy ($J$) in a horizontal walls-springs-blocks system, and $f_i(x)‚â§0$ be displacement constraints ($m$) between objects. Then $‚àá_xf_0(x)$ are spring forces ($N$) pushing blocks into equilibrium, $‚àá_xf_i(x)$ are unitless directional vectors, and $Œª_i$ are contact forces ($N$) exerted from walls. Internal spring potential energy is minimized at equilibrium, and all blocks are stationary.\n",
        "\n",
        "  - Stationarity: $‚àáf_0(x)+\\sum_iŒª_i‚àáf_i(x)=0$ says sum of spring forces plus wall contact forces equal to zero.\n",
        "\n",
        "  - Complementary slackness: $Œª_if_i(x)=0$ says if a block is not touching a wall and gap is nonzero ($f_i(x)< 0$), then $Œª_i=0$. If the gap is zero, then the wall exerts a positive force $Œª_i>0$.\n",
        "\n",
        "  - Dual feasibility: $Œª_i‚â•0$ says the walls can only push the blocks. Walls don't pull blocks.\n",
        "\n",
        "  - Primal feasibility: displacement of blocks relative to walls and other blocks\n",
        "\n",
        "- 5.1: $\\BC\n",
        "\\t{minimize}&\\/{1}{2}x^‚ä§Px+q^‚ä§x+r,\\ P‚àà\\v{S}_+^n\\\\\n",
        "\\t{subject to}&Ax=b\n",
        "\\EC$.\n",
        "The KKT conditions are $\\BC\n",
        "Ax-b=0&Ax^*=b\\\\\n",
        "x^‚ä§P+q^‚ä§+ŒΩ^‚ä§A=0&Px^*+A^‚ä§ŒΩ^*=-q\n",
        "\\EC$\n",
        "or\n",
        "$\\BM P&A^‚ä§\\\\A&0\\EM\\BM x^*\\\\ŒΩ^*\\EM=\\BM b\\\\-q\\EM$\n",
        "\n",
        "- 5.2 (water filling): $\\BC\n",
        "\\t{minimize}&-\\sum_i\\ln(Œ±_i+x_i)\\\\\n",
        "\\t{subject to}&x‚âΩ0,\\ \\v{1}^‚ä§x=1\n",
        "\\EC$.\n",
        "$L(x,Œª,ŒΩ)=-\\sum_i\\ln(Œ±_i+x_i)-\\sum_iŒª_ix_i+ŒΩ(\\sum_ix_i-1)$.\n",
        "Then KKT conditions are\n",
        "$‚àÇ_{x_i}L(x^*,Œª^*,ŒΩ^*)=\\/{-1}{a_i+x_i^*}-Œª_i^*+ŒΩ^*=0$,\n",
        "$Œª_i‚â•0$, $x_i^*‚â•0$, and $\\v{1}^‚ä§x^*=1$.\n",
        "Here we treat $Œª_i^*=ŒΩ^*-\\/{1}{a_i+x_i^*}$ as slack variable that satisfies $Œª_i^*=ŒΩ^*-1/(a_i+x_i^*)‚â•0$.\n",
        "Then\n",
        "$\\BC\n",
        "\\t{if }ŒΩ^*< 1/Œ±_i & x_i^*>0,\\ Œª_i=0 & ŒΩ^*=1/(Œ±_i+x_i^*)‚áíx_i^*=1/ŒΩ^*-Œ±_i\\\\\n",
        "\\t{if }ŒΩ^*‚â•1/Œ±_i & x_i^*=0,\\ Œª_i>0\n",
        "\\EC$ or $x_i^*=\\max(0,\\/{1}{ŒΩ^*}-Œ±_i)$\n",
        "\n",
        "- 5.3 (**max entropy**): $\\BC\\t{minimize}&\\sum_ix_i\\ln(x_i)\\\\\n",
        "\\t{subject to}&Ax‚âºb,\\ \\v{1}^‚ä§x=1\\EC$ on $‚Ñù_{++}^n$.\n",
        "Dual\n",
        "$\\BC\\t{maximize}&-b^‚ä§Œª-ŒΩ-e^{-ŒΩ-1}\\sum_ie^{-a_i^‚ä§Œª}\\\\\n",
        "\\t{subject to}&Œª‚âΩ0\\EC$\n",
        "solved to obtain dual optimal $(Œª^*,ŒΩ^*)$.\n",
        "Then\n",
        "$L(x,Œª^*,ŒΩ^*)=\\sum_ix_i\\ln(x_i)+Œª^{*‚ä§}(Ax-b)+ŒΩ(\\sum_ix_i-1)$\n",
        "$=\\sum_ix_i\\ln(x_i)+(A^‚ä§Œª^*)^‚ä§x+ŒΩ\\sum_ix_i-c$\n",
        "$=\\sum_i(x_i\\ln(x_i)+a_i^‚ä§Œª^*x_i+ŒΩx_i)-c$\n",
        "where $a_i^‚ä§$ is the $i$-th row of $A$.\n",
        "Unconstrained optimization\n",
        "$‚àÇ_{x_i}L=\\ln(x_i)+1+a_i^‚ä§Œª^*+ŒΩ=0$\n",
        "$‚áíx_i^*=e^{-(a_i^‚ä§Œª^*+ŒΩ+1)}$\n",
        "\n",
        "- 5.4 (separable): $\\BC\\t{minimize}&\\sum_if_i(x_i)&\\t{differentiable and convex}\\\\\n",
        "\\t{subject to}&a^‚ä§x=b\\EC$.\n",
        "Lagrangian\n",
        "$L(x,ŒΩ)=\\sum_if_i(x_i)+ŒΩ(a^‚ä§x-b)$\n",
        "$=-bŒΩ+\\sum_i(f_i(x_i)+ŒΩa_ix_i)$\n",
        "is also separable. So the dual function is\n",
        "$g(ŒΩ)=-bŒΩ+\\sum_i\\inf_x(f_i(x_i)+ŒΩa_ix_i)$\n",
        "$=-bŒΩ-\\sum_if_i^*(-ŒΩa_i)$\n",
        "and the dual problem is\n",
        "$\\BC\\t{maximize}&-bŒΩ-\\sum_if_i^*(-ŒΩa_i)\\EC$"
      ],
      "metadata": {
        "id": "9HEMf0SrCkN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purturbation analysis**: $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§\\red{u_i},&u_i\\t{ represents tightened/relaxed constraint}\\\\\n",
        "&h_i(x)=\\red{v_i}\\EC$.\n",
        "This perturbed problem has optimal value\n",
        "$p^*(u,v)=\\inf\\{f_0(x)|‚àÉx‚àà\\m{D},f_i(x)‚â§u_i\\ ‚àÄi,h_i(x)=v_i\\ ‚àÄi\\}$\n",
        "such that $p^*(0,0)=g(Œª^*,ŒΩ^*)$ under strong duality.\n",
        "The epigraph of $p^*(u,v)$ is the epigraph set $\\m{A}$, which is convex if the problem is convex.\n",
        "\n",
        "- **Under strong duality**,\n",
        "$p^*(0,0)=g(Œª^*,ŒΩ^*)$\n",
        "$‚â§f_0(x)+\\sum_iŒª_i^*f_i(x)+\\sum_iŒΩ_i^*h_i(x)$\n",
        "$‚â§f_0(x)+\\sum_iŒª_i^*u_i+\\sum_iŒΩ_i^*v_i$\n",
        "$‚áíf_0(x)‚â•p^*(0,0)-Œª^{*‚ä§}u-ŒΩ^{*‚ä§}v$\n",
        "$‚áí\\red{p^*(u,v)‚â•p^*(0,0)-Œª^{*‚ä§}u-ŒΩ^{*‚ä§}v}$\n",
        "\n",
        "  - The inequality introduces an asymmetry in conclusions that can be drawn:\n",
        "  If $Œª_i$ is large and we tighten $u_i< 0$ then $p^*(u,v)$ will increase greatly.\n",
        "  If $Œª_i$ is small and we loosen $u_i>0$ then $p^*(u,v)$ will not decrease too much.\n",
        "\n",
        "- **Under strong duality**, $p^*(u,v)-p^*(0,0)‚â•-Œª^{*‚ä§}u-ŒΩ^{*‚ä§}v$.\n",
        "Let $v=0,u=t$, then\n",
        "$p^*(t,0)-p^*(0,0)‚â•-Œª^{*‚ä§}t$\n",
        "and\n",
        "$\\lim\\limits_{t‚Üí0}\\/{p^*(t,0)-p^*(0,0)}{t}\\BC\n",
        "‚â•-Œª^*&\\t{if }t>0\\\\\n",
        "‚â§-Œª^*&\\t{if }t< 0\\EC$\n",
        "therefore $\\red{\\/{‚àÇp^*(0,0)}{‚àÇu_i}=-Œª_i^*}$ and\n",
        "$\\red{\\/{‚àÇp^*(0,0)}{‚àÇv_i}=-ŒΩ_i^*}$.\n",
        "\n",
        "  - $Œª_i^*$ measures how active the $i$-th constraint is in the magnitude of effect on $p^*$ from loosening or tightening."
      ],
      "metadata": {
        "id": "iq-_7CmSwO3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equivalent problems with better duals**: Equivalent problems often have unrecognizably different duals.\n",
        "\n",
        "- **Introducing new variables to objective**: $\\BC\n",
        "\\t{minimize}&f_0(Ax+b)\\EC$ has useless dual $p*$. Reformulate\n",
        "$\\BC\\t{minimize}&f_0(y)\\\\\n",
        "\\t{subject to}&Ax+b=y\\EC$\n",
        "then $g(ŒΩ)=b^‚ä§ŒΩ-f_0^*(ŒΩ)$\n",
        "and\n",
        "$\\blue{\\BC\\t{maximize}&b^‚ä§ŒΩ-f_0^*(ŒΩ)\\\\\n",
        "\\t{subject to}&A^‚ä§ŒΩ=0\n",
        "\\EC}$.\n",
        "\n",
        "  - Proof: $L(x,y,ŒΩ)=f_0(y)+ŒΩ^‚ä§(Ax+b-y)$.\n",
        "  Then\n",
        "  $g(ŒΩ)=\\inf_{x,y}L(x,y,ŒΩ)$\n",
        "  $=b^‚ä§ŒΩ+\\inf_{x,y}((A^‚ä§ŒΩ)^‚ä§x-ŒΩ^‚ä§y+f_0(y))$\n",
        "  $=b^‚ä§ŒΩ+\\inf_y(-ŒΩ^‚ä§y+f_0(y))$\n",
        "  $g(ŒΩ)=b^‚ä§ŒΩ-f_0^*(ŒΩ)$\n",
        "\n",
        "- **Introducing new variables to constraints**: $\\BC\n",
        "\\t{minimize}&f_0(A_0x+b_0)\\\\\n",
        "\\t{subject to}&f_i(A_ix+B_i)‚â§0\n",
        "\\EC$ where $f_i:‚Ñù^{k_i}‚Üí‚Ñù$ convex.\n",
        "Reformulate\n",
        "$\\BC\\t{minimize}&f_0(y_0)\\\\\n",
        "\\t{subject to}&f_i(y_i)‚â§0\\\\\n",
        "&A_ix+b_i=y_i\\EC$.\n",
        "Therefore $\\blue{\\BC\n",
        "\\t{maximize}&\\sum_{i=0}^mŒΩ_ib_i-f_0^*(ŒΩ_0)-\\sum_{i=1}^mŒª_if_i^*(\\/{ŒΩ_i}{Œª_i})\\\\\n",
        "\\t{subject to}&Œª‚âΩ0,\\ \\sum_{i=0}^mA_i^‚ä§ŒΩ_i=0\n",
        "\\EC}$.\n",
        "  \n",
        "  - Proof: $g(Œª,ŒΩ)=\\inf_{x,y}(f_0(y_0)+\\sum_{i=1}^mŒª_if_i(y_i)+\\sum_{i=0}^mŒΩ_i(A_ix+b_i-y_i))$\n",
        "  $=\\sum_{i=0}^mŒΩ_ib_i+\\inf_y(f_0(y_0)+\\sum_{i=1}^mŒª_if_i(y_i)-\\sum_{i=1}^mŒΩ_iy_i)$\n",
        "  $=\\sum_{i=0}^mŒΩ_ib_i-f_0^*(ŒΩ_0)-\\sum_{i=1}^mŒª_if_i^*(\\/{ŒΩ_i}{Œª_i})$\n",
        "\n",
        "- 5.5 (**log-sum-exp**): $\\BC\n",
        "\\t{minimize}&\\green{\\ln(\\sum_ie^{a_i^‚ä§x+b_i})}\n",
        "\\EC$. Reformulate $\\BC\n",
        "\\t{minimize}&f_0(y)=\\ln(\\sum_ie^{y_i})\\\\\n",
        "\\t{subject to}&Ax+b=y\n",
        "\\EC$.\n",
        "$f_0^*(ŒΩ)=\\sum_iŒΩ_i\\ln(ŒΩ_i)$ on $\\{ŒΩ‚âΩ0,\\v{1}^‚ä§ŒΩ=1\\}$.\n",
        "$L(x,y,ŒΩ)=\\ln(\\sum_ie^{y_i})+ŒΩ^‚ä§(Ax+b-y)$.\n",
        "And $\\green{\\BC\n",
        "\\t{maximize}&b^‚ä§ŒΩ-\\sum_iŒΩ_i\\ln(ŒΩ_i)\\\\\n",
        "\\t{subject to}&ŒΩ‚âΩ0,\\ \\v{1}^‚ä§ŒΩ=1,\\ A^‚ä§ŒΩ=0\n",
        "\\EC}$.\n",
        "\n",
        "- 5.6 (**norm**): $\\BC\\t{minimize}&\\n{Ax-b}\\EC$. Reformulate $\\BC\n",
        "\\t{minimize}&\\n{y}\\\\\n",
        "\\t{subject to}&Ax-b=y\n",
        "\\EC$.\n",
        "$L(x,y,ŒΩ)=\\n{y}+ŒΩ^‚ä§(Ax-b-y)$.\n",
        "And $\\BC\\t{maximize}&b^‚ä§ŒΩ\\\\\n",
        "\\t{subject to}&\\n{ŒΩ}_*‚â§1,\\ A^‚ä§ŒΩ=0\n",
        "\\EC$.\n",
        "\n",
        "- 5.8 (**norm**): $\\BC\\t{minimize}&\\n{Ax-b}\\EC$. Reformulate $\\BC\n",
        "\\t{minimize}&\\/{1}{2}\\n{y}^2\\\\\n",
        "\\t{subject to}&Ax-b=y\n",
        "\\EC$. Then dual\n",
        "$\\BC\\t{maximize}&b^‚ä§ŒΩ-\\/{1}{2}\\n{ŒΩ}_*^2\\\\\n",
        "\\t{subject to}&A^‚ä§ŒΩ=0\n",
        "\\EC$.\n",
        "\n",
        "- 5.9 (**box constraints**): $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax=b,\\ l‚âºx‚âºu\\EC$.\n",
        "Then $g(ŒΩ,Œª_1,Œª_2)=\\inf_x(c^‚ä§x+ŒΩ^‚ä§(Ax-b)+Œª_1^‚ä§(x-l)+Œª_2^‚ä§(u-x))$\n",
        "$=-ŒΩ^‚ä§b-Œª_1^‚ä§l+Œª_2^‚ä§u+\\inf_x(c+A^‚ä§ŒΩ+Œª_1-Œª_2)^‚ä§x$\n",
        "$=\\BC-ŒΩ^‚ä§b-Œª_1^‚ä§l+Œª_2^‚ä§u&c+A^‚ä§ŒΩ+Œª_1-Œª_2=0\\\\\n",
        "-‚àû&\\t{otherwise}\n",
        "\\EC$.\n",
        "Therefore the dual\n",
        "$\\BC\\t{maximize}&-ŒΩ^‚ä§b-Œª_1^‚ä§l+Œª_2^‚ä§u\\\\\n",
        "\\t{subject to}&c+A^‚ä§ŒΩ+Œª_1-Œª_2=0,\\ Œª_1‚âΩ0,\\ Œª_2‚âΩ0\n",
        "\\EC$.\n",
        "\n",
        "  - Alternatively, $f_0=\\BC c^‚ä§x&l‚âºx‚âºu\\\\‚àû&\\t{otherwise}\\EC$ and $\\BC\\t{minimize}&f_0(x)\\\\\\t{subject to}&Ax=b\\EC$.\n",
        "  Here $g(ŒΩ)=\\inf_x(c^‚ä§x+ŒΩ^‚ä§(Ax-b))$\n",
        "  $=-ŒΩ^‚ä§b+\\inf_{l‚âºx‚âºu}x^‚ä§(c+A^‚ä§ŒΩ)$\n",
        "  $=-ŒΩ^‚ä§b-u^‚ä§(c+A^‚ä§ŒΩ)^-+l^‚ä§(c+A^‚ä§ŒΩ)^+$.\n",
        "  Here notation $y^-=\\{\\max(-y_i,0)|‚àÄi\\}$ and $y^+=\\{\\max(y_i,0)|‚àÄi\\}$. I.e., multiply $u_i$ if $(c+A^‚ä§ŒΩ)_i< 0$ and multiply $l_i$ if $(c+A^‚ä§ŒΩ)_i>0$"
      ],
      "metadata": {
        "id": "-5VsYRQ-LtAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theorems of alternatives**: two systems of inequalities are called **weak alternatives** if at most one of them is feasible.\n",
        "\n",
        "- Feasibility problem $\\BC\n",
        "\\t{minimize}&0\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&h_i(x)=0\n",
        "\\EC$ has primal $p^*=\\BC 0&\\t{feasible}\\\\\n",
        "‚àû&\\t{infeasible}\n",
        "\\EC$.\n",
        "Because $f_0(x)=0$, the dual function\n",
        "$g(Œª,ŒΩ)=\\inf_x(\\sum_iŒª_if_i(x)+\\sum_iŒΩ_ih_i(x))$ is homogeneous ray $g(Œ±Œª,Œ±ŒΩ)=Œ±g(Œª,ŒΩ)$.\n",
        "The inequality $\\blue{\\{Œª‚âΩ0,g(Œª,ŒΩ)>0\\}}$ is special. If it is satisfied by any $(Œª,ŒΩ)$ (i.e., inequality is feasible) then it is also satisfied by $(Œ±Œª,Œ±ŒΩ)$ and\n",
        "$d^*=\\sup_{Œª,ŒΩ}g(Œª,ŒΩ)=\\BC\n",
        "‚àû&Œª‚âΩ0,\\ g(Œª,ŒΩ)>0\\t{ is feasible}\\\\\n",
        "0&\\t{otherwise}\\\\\n",
        "\\EC$.\n",
        "If $\\{Œª‚âΩ0,g(Œª,ŒΩ)>0\\}$ is feasible then $p^*=‚àû$ and the primal problem is infeasible.\n",
        "\n",
        "  - Therefore the pair of inequalities $\\red{\\BC\n",
        "  f_i(x)‚â§0,\\ h_i(x)=0\\\\\n",
        "  Œª‚âΩ0,\\ g(Œª,ŒΩ)>0\n",
        "  \\EC}$ are weak alternatives.\n",
        "\n",
        "- With strict inequalities $f_i(x)< 0,\\ h_i(x)=0$, suppose $\\tilde{x}$ satisfies primal inequalities, then $g(Œª,ŒΩ)=\\inf_x(\\sum_iŒª_if_i(x)+\\sum_iŒΩ_ih_i(x))$\n",
        "$‚â§\\sum_iŒª_if_i(\\tilde{x})+\\sum_iŒΩ_ih_i(\\tilde{x})\\red{<} 0$ if $Œª_i\\neq0$.\n",
        "\n",
        "  - Therefore the pair of inequlities $\\red{\\BC\n",
        "  f_i(x)< 0,\\ h_i(x)=0\\\\\n",
        "  Œª‚âΩ0,\\ g(Œª,ŒΩ)‚â•0,\\ Œª\\neq0\n",
        "  \\EC}$ are weak alternatives.\n",
        "\n",
        "- The pair of weak alternatives are **strong alternatives** if exactly one of them holds. This happens when the original feasibility problem is convex ($f_i$ convex and $h_i$ affine).\n",
        "\n",
        "- **Linear alternatives**: $Ax‚âºb$. The dual is $g(Œª)=\\inf_xŒª^‚ä§(Ax-b)=\\BC\n",
        "-b^‚ä§Œª&A^‚ä§Œª=0\\\\\n",
        "-‚àû&\\t{otherwise}\n",
        "\\EC$.\n",
        "$Œª‚âΩ0‚áíŒª^‚ä§(Ax-b)‚â§0$\n",
        "$‚áí\\green{Œª^‚ä§b‚â•0}$.\n",
        "Therefore $Ax‚âºb$ and $b^‚ä§Œª< 0$ are contradictory.\n",
        "Inequalities $\\BC\n",
        "Ax‚âºb\\\\\n",
        "b^‚ä§Œª< 0,\\ A^‚ä§Œª=0,\\ Œª‚âΩ0\n",
        "\\EC$ are strong alternatives.\n",
        "\n",
        "- **Farka's lemma**: $\\BC\n",
        "Ax‚âº0,\\ c^‚ä§x< 0\\\\\n",
        "A^‚ä§Œª+c=0,\\ Œª‚âΩ0\n",
        "\\EC$ are strong alternatives.\n",
        "\n",
        "  - Proof: Consider LP\n",
        "  $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&Ax‚âº0\\EC$ and its dual\n",
        "  $\\BC\\t{maximize}&0\\\\\n",
        "  \\t{subject to}&A^‚ä§Œª+c=0,\\ Œª‚âΩ0\\EC$.\n",
        "  The LP is homogeneous and has\n",
        "  $p^*=\\BC0&Ax‚âº0,\\ c^‚ä§x< 0\\t{ infeasible}\\\\\n",
        "  -‚àû&Ax‚âº0,\\ c^‚ä§x< 0\\t{ feasible}\\EC$.\n",
        "  The dual $d^*=\\BC0&A^‚ä§Œª+c=0,\\ Œª‚âΩ0\\t{ feasible}\\\\\n",
        "  -‚àû&A^‚ä§Œª+c=0,\\ Œª‚âΩ0\\t{ infeasible}\\EC$.\n",
        "  LPs always have strong duality unless when primal and dual are both infeasible. $\\tilde{x}=0$ meets Slater's condition, therefore $d^*=p^*$ and\n",
        "  $Ax‚âº0,\\ c^‚ä§x< 0$ feasible $‚áîA^‚ä§Œª+c=0,\\ Œª‚âΩ0$ infeasible and vice versa.\n",
        "\n",
        "- **Fredholm alternative**: $Ax=b$ has no solution iff $‚àÉy: A^‚ä§y=0,b^‚ä§y\\neq0$. $\\BC Ax=b\\\\A^‚ä§y=0,\\ b^‚ä§y\\neq0\\EC$ are strong alternatives.\n",
        "\n",
        "  - Proof: $\\BC\\t{minimize}&0\\\\\\t{subject to}&Ax=b\\EC$\n",
        "  has dual $g(ŒΩ)=\\inf_xŒΩ^‚ä§(Ax-b)$\n",
        "  $=\\BC-b^‚ä§ŒΩ&A^‚ä§ŒΩ=0\\\\-‚àû&\\t{othewise}\\EC$.\n",
        "  Affine problem has strong duality, therefore $p^*=0=d^*=-b^‚ä§ŒΩ$.\n",
        "  Therefore $Ax=b$ is feasible iff $‚àÉŒΩ: A^‚ä§ŒΩ=0,b^‚ä§ŒΩ=0$.\n",
        "\n",
        "- 5.10 **arbitrage-free pricing**: $n$ asset positions $x_1,...,x_n$ with prices at $t_0$ being $p=(p_1,...,p_n)$ and final value of investments at $t_1$ being $v^{(i)}=(v_1,...,v_n)$ if scenario $i=1,...,m$ had occurred. Arbitrage exists if $‚àÉx$ such that $p^‚ä§x< 0$ (positive cashflow at $t_0$) and $v^{(i)‚ä§}x‚â•0\\ ‚àÄi$ (nonnegative final value for all scenarios). Let $V_{ij}=v_j^{(i)}$, then no arbitrage corresponds to inequality $\\blue{Vx‚âΩ0,\\ p^‚ä§x< 0}$, which occurs iff $‚àÉŒª$ such that $\\blue{-V^‚ä§Œª+p=0,\\ Œª‚âΩ0}$.\n",
        "\n",
        "  - Suppose $V$ is known, and all $p_i$s are known except $p_n$, then we want to find the lowest no-arbitrage price $p_n$, then $\\BC\n",
        "  \\t{minimize}&p_n\\\\\n",
        "  \\t{subject to}&V^‚ä§Œª=p,\\ y‚âΩ0\n",
        "  \\EC$"
      ],
      "metadata": {
        "id": "tECIVWfk6pUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 5.1: $\\BC\\t{minimize}&x^2+1\\\\\n",
        "\\t{subject to}&(x-2)(x-4)‚â§0\\EC$ on $x‚àà‚Ñù$.\n",
        "\n",
        "  - Primal: $\\m{F}=\\{2‚â§x‚â§4\\}$. Then $x^*=2$ and $p^*=5$.\n",
        "\n",
        "  - Dual: $L(x,Œª)=x^2+1+Œª(x^2-6x+8)$\n",
        "  $=(Œª+1)x^2-6Œªx+8Œª+1$.\n",
        "  $‚àÇ_xL(x,Œª)=2(Œª+1)x-6Œª=0$\n",
        "  $‚áíx_\\min=\\/{3Œª}{Œª+1}$.\n",
        "  $g(Œª)=\\inf_xL(x,Œª)$\n",
        "  $=-\\/{9Œª^2}{Œª+1}+8Œª+1$.\n",
        "  $\\BC\\t{maximize}&-\\/{9Œª^2}{Œª+1}+8Œª+1\\\\\n",
        "  \\t{subject to}&Œª‚â•0\n",
        "  \\EC$.\n",
        "  $‚àÇ_Œªg(Œª)=-\\/{18Œª}{Œª+1}+\\/{9Œª^2}{(Œª+1)^2}+8=0$\n",
        "  $‚áí-18Œª(Œª+1)+9Œª^2+8(Œª+1)^2=0$\n",
        "  $‚áí-Œª^2-2Œª+8=0$\n",
        "  $‚áíŒª_\\max=\\/{2¬±\\sqrt{4+32}}{-2}$.\n",
        "  Then $Œª^*=2$ and\n",
        "  $d^*=5$.\n",
        "\n",
        "  - Sensitivity: $\\BC\\t{minimize}&x^2+1\\\\\n",
        "  \\t{subject to}&(x-2)(x-4)‚â§u\n",
        "  \\EC$ then\n",
        "  $g(Œª)=-\\/{9Œª^2}{Œª+1}+(8-u)Œª+1$.\n",
        "  $p^*(u)‚â•p^*(0)-Œª^*u$\n",
        "  $‚áí\\/{p^*(u)-p^*(0)}{u}‚â•-Œª^*$.\n",
        "  Therefore $\\/{dp^*(0)}{u}=-Œª^*$.\n",
        "\n",
        "- 5.2: $d^*‚â§p^*$ also works for unbounded ($p^*=-‚àû$) and infeasible ($p^*=‚àû$) primal problems as well.\n",
        "\n",
        "  - If unbounded below ($p^*=-‚àû$), then by lower bound property because $f_i(x)‚â§0$ constraints we still have $d^*‚â§p^*$.\n",
        "\n",
        "  - If infeasible ($p^*=‚àû$), then either $d^*=‚àû$ as dual is unbounded above, or interpret the Lagrangian as $L(x,Œª)=f_0+Œª^‚ä§\\t{constraint violations}$ which bypass primal feasibility by paying costs.\n",
        "\n",
        "- 5.3: $\\BC\\t{minimize}&c^‚ä§x&c\\neq0\\\\\n",
        "\\t{subject to}&f(x)‚â§0\\EC$.\n",
        "$g(Œª)=\\inf_x(c^‚ä§x+Œªf(x))$\n",
        "$=-\\sup_x(-c^‚ä§x-Œªf(x))$\n",
        "$=-Œª\\sup_x((\\/{-c}{Œª})^‚ä§x-f(x))$\n",
        "$=-Œªf^*(\\/{-c}{Œª})$.\n",
        "Dual problem $\\BC\\t{maximize}&-Œªf^*(\\/{-c}{Œª})\\\\\n",
        "\\t{subject to}&c\\neq0,\\ Œª‚âΩ0\\EC$\n",
        "\n",
        "- 5.4 **LP relaxation**: $p^*=\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax‚âºb\\EC$ has dual LP\n",
        "$\\BC\\t{maximize}&-Œª^‚ä§b\\\\\n",
        "\\t{subject to}&A^‚ä§Œª+c=0,\\ Œª‚âΩ0\n",
        "\\EC$.\n",
        "Let $w‚àà‚Ñù_+^m$, then $x‚àà\\m{F}$ satisfies $w^‚ä§Ax‚â§w^‚ä§b$, which relaxes $\\m{F}$ from polyhedron to halfspace $H_w=\\{x|w^‚ä§Ax‚â§w^‚ä§b,\\ w‚âΩ0\\}$, a wall that is a linear combination of the polyhedral walls weighted by $w_i$.\n",
        "\n",
        "  - $p_w^*=\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&w^‚ä§Ax‚â§w^‚ä§b,\\ w‚âΩ0\n",
        "  \\EC$.\n",
        "  Gradient $‚àáf_0^‚ä§=c$ requires $A^‚ä§w=-c$ to avoid unbounded below.\n",
        "  $-c^‚ä§x‚â§w^‚ä§b‚áíc^‚ä§x‚â•-w^‚ä§b$\n",
        "  therefore $p_w^*=\\BC\n",
        "  -b^‚ä§w&A^‚ä§w=-c\\\\\n",
        "  -‚àû&\\t{otherwise}\n",
        "  \\EC$.\n",
        "\n",
        "  - $x^*‚ààH_w\\ ‚àÄw‚âΩ0$ and $p_w^*‚â§p^*\\ ‚àÄw‚âΩ0$. Therefore $p^*=\\BC\\t{maximize}&-w^‚ä§b\\\\\n",
        "  \\t{subject to}&A^‚ä§w+c=0,\\ w‚âΩ0\n",
        "  \\EC$.\n",
        "  Therefore finding optimal $Œª^*$ is equivalent to finding the single wall that isn't unbounded below and gives the highest $-b^‚ä§Œª^*$.\n",
        "\n",
        "- 5.5 **general LP**: $\\BC\n",
        "\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Gx‚âºh,\\ Ax=b\n",
        "\\EC$.\n",
        "$g(Œª,ŒΩ)=\\inf_x(c^‚ä§x+Œª^‚ä§(Gx-h)+ŒΩ^‚ä§(Ax-b))$.\n",
        "$‚àá_xL^‚ä§=c^‚ä§+Œª^‚ä§G+ŒΩ^‚ä§A=0$.\n",
        "Therefore $g(Œª,ŒΩ)=\\BC-Œª^‚ä§h-ŒΩ^‚ä§b&c+G^‚ä§Œª+A^‚ä§ŒΩ=0\\\\\n",
        "-‚àû&\\t{otherwise}\\EC$.\n",
        "The dual problem is\n",
        "$\\BC\\t{maximize}&-h^‚ä§Œª-b^‚ä§ŒΩ\\\\\n",
        "\\t{subject to}&c+G^‚ä§Œª+A^‚ä§ŒΩ=0,\\ Œª‚âΩ0\n",
        "\\EC$.\n",
        "\n",
        "- 5.7 **Hardmax vs softmax**: piecewise-LP $p_\\t{pwl}^*=\\BC\n",
        "\\t{minimize}&\\max_ia_i^‚ä§x+b_i\n",
        "\\EC‚áí\\BC\n",
        "\\t{minimize}&\\max_iy_i\\\\\n",
        "\\t{subject to}&a_i^‚ä§x+b_i=y_i\n",
        "\\EC$ where $a_i^‚ä§$ is the $i$-th row of $A$.\n",
        "\n",
        "  - Derive dual problem: $f_0(y)=\\max_iy_i$\n",
        "  $=\\sup\\{z^‚ä§y|z‚âΩ0,\\v{1}^‚ä§z=1\\}$.\n",
        "  Conjugate\n",
        "  $f_0^*(z)=\\BC0&z‚âΩ0,\\v{1}^‚ä§z=1\\\\\n",
        "  ‚àû&\\t{otherwise}\\EC$.\n",
        "  Dual\n",
        "  $g(ŒΩ)=\\inf_{x,y}(f_0(y)+ŒΩ^‚ä§(Ax+b-y))$\n",
        "  $=\\inf_{y}(f_0(y)+ŒΩ^‚ä§(b-y))$\n",
        "  $=b^‚ä§ŒΩ-\\sup_y(ŒΩ^‚ä§y-f_0(y))$\n",
        "  $=\\BC b^‚ä§ŒΩ&A^‚ä§ŒΩ=0,ŒΩ‚âΩ0,\\v{1}^‚ä§ŒΩ=1\\\\\n",
        "  -‚àû&\\t{otherwise}\n",
        "  \\EC$.\n",
        "  Dual problem\n",
        "  $\\BC\\t{maximize}&b^‚ä§ŒΩ\\\\\n",
        "  \\t{subject to}&A^‚ä§ŒΩ=0,\\ ŒΩ‚âΩ0,\\ \\v{1}^‚ä§ŒΩ=1\n",
        "  \\EC$.\n",
        "\n",
        "  - Formulate as LP: $\\BC\n",
        "  \\t{minimize}&\\max_ia_i^‚ä§x+b_i\n",
        "  \\EC‚áí\\BC\n",
        "  \\t{minimize}&t\\\\\n",
        "  \\t{subject to}&a_i^‚ä§x+b_i‚â§t\n",
        "  \\EC$.\n",
        "  Dual\n",
        "  $g(Œª)=\\inf_{x,t}(t+Œª^‚ä§(Ax+b-t\\v{1}))$\n",
        "  $=b^‚ä§Œª+\\inf_tt(1-Œª^‚ä§\\v{1})$\n",
        "  $=\\BC b^‚ä§Œª&A^‚ä§Œª=0,\\ Œª‚âΩ0,\\ \\v{1}^‚ä§Œª=1\\\\-‚àû&\\t{otherwise}\\EC$.\n",
        "  Dual problem $\\BC\\t{maximize}&b^‚ä§Œª\\\\\\t{subject to}&A^‚ä§Œª=0,\\ Œª‚âΩ0,\\ \\v{1}^‚ä§Œª=1\\EC$.\n",
        "\n",
        "  - Use softmax: $p_\\t{pwl}^*=\\BC\n",
        "  \\t{minimize}&\\max_iy_i\\\\\n",
        "  \\t{subject to}&a_i^‚ä§x+b_i=y_i\n",
        "  \\EC‚áíp_\\t{gp}^*=\\BC\\t{minimize}&\\ln(\\sum_i\\e{y_i})\\\\\n",
        "  \\t{subject to}&a_i^‚ä§x+b_i=y_i\\EC$.\n",
        "  Dual problem $\\BC\\t{maximize}&b^‚ä§ŒΩ-\\sum_iŒΩ_i\\ln(ŒΩ_i)\\\\\\t{subject to}&A^‚ä§ŒΩ=0,\\ ŒΩ‚âΩ0,\\ \\v{1}^‚ä§ŒΩ=1\\EC$.\n",
        "  $\\sum_i\\e{\\max_iy_i}‚â•\\sum_i\\e{y_i}$\n",
        "  $‚áím\\e{\\max_iy_i}‚â•\\sum_i\\e{y_i}$\n",
        "  $‚áí\\ln(m)+\\max_iy_i‚â•\\ln(\\sum_i\\e{y_i})$\n",
        "  $‚áí\\ln(m)‚â•p_\\t{pwl}^*-p_\\t{gp}^*$.\n",
        "  $\\e{\\max_iy_i}‚â§\\sum_i\\e{y_i}$\n",
        "  $‚áí\\max_iy_i‚â§\\ln(\\sum_i\\e{y_i})$\n",
        "  $‚áíp_\\t{pwl}^*‚â§p_\\t{gp}^*$.\n",
        "  Therefore $\\green{0‚â§p_\\t{pwl}^*-p_\\t{gp}^*‚â§\\ln(m)}$.\n",
        "\n",
        "- 5.11 (**Regularization**): $\\blue{\\BC\\t{minimize}&\\sum_i\\n{A_ix+b_i}_2+\\/{1}{2}\\n{x-x_0}_2^2\\EC}‚áí\\BC\\t{minimize}&\\sum_i\\n{y_i}_2+\\/{1}{2}\\n{x-x_0}_2^2\\\\\n",
        "\\t{subject to}&A_ix+b_i=y_i\\EC$.\n",
        "$g(ŒΩ)=\\inf_{x,y}(\\sum_i\\n{y_i}_2+\\/{1}{2}\\n{x-x_0}_2^2+\\sum_iŒΩ_i^‚ä§(A_ix+b_i-y_i))$\n",
        "$=\\sum_iŒΩ_i^‚ä§b_i+\\sum_i\\inf_y(\\n{y_i}_2-ŒΩ_i^‚ä§y_i)+\\inf_x(\\/{1}{2}\\n{x-x_0}_2^2+\\sum_iŒΩ_i^‚ä§A_ix)$\n",
        "$=\\sum_iŒΩ_i^‚ä§b_i-\\sum_i\\sup_y(ŒΩ_i^‚ä§y_i-\\n{y_i}_2)-\\sup_x(-(\\sum_iA_i^‚ä§ŒΩ_i)^‚ä§x-\\/{1}{2}\\n{x-x_0}_2^2)$.\n",
        "  \n",
        "  - $\\sup_y(ŒΩ_i^‚ä§y_i-\\n{y_i}_2)$.\n",
        "  From Holder inequality $ŒΩ_i^‚ä§y_i‚â§\\n{y_i}_2\\n{ŒΩ_i}_2$\n",
        "  $‚áíŒΩ_i^‚ä§y_i-\\n{y_i}_2‚â§\\n{y_i}_2(\\n{ŒΩ_i}_2-1)$\n",
        "  $=\\BC\n",
        "  0&\\n{ŒΩ_i}_2‚â§1\\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$.\n",
        "  Therefore\n",
        "  $\\sum_i\\sup_y(ŒΩ_i^‚ä§y_i-\\n{y_i}_2)=\\BC\n",
        "  0&\\n{ŒΩ_i}_2‚â§1\\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$.\n",
        "\n",
        "  - $\\sup_x(-(\\sum_iA_i^‚ä§ŒΩ_i)^‚ä§x-\\/{1}{2}\\n{x-x_0}_2^2)$. Let $t=(-\\sum_iA_i^‚ä§ŒΩ_i)$. From Holder inequality\n",
        "  $t^‚ä§x‚â§\\n{t}_2\\n{x}_2$\n",
        "  $‚áít^‚ä§x-\\/{1}{2}\\n{x}_2^2‚â§\\n{t}_2\\n{x}_2-\\/{1}{2}\\n{x}_2^2$.\n",
        "  Differentiate\n",
        "  $‚àÇ_{\\n{x}_2}(\\n{t}_2\\n{x}_2-\\/{1}{2}\\n{x}_2^2)$\n",
        "  $=\\n{t}_2-\\n{x}_2=0$\n",
        "  $‚áí\\n{x}_{2,\\min}=\\n{t}_2$\n",
        "  $‚áít^‚ä§x-\\/{1}{2}\\n{x}_2^2‚â§\\n{t}_2^2-\\/{1}{2}\\n{t}_2^2$\n",
        "  $‚áí\\sup_x(t^‚ä§x-\\/{1}{2}\\n{x}_2^2)=\\/{1}{2}\\n{t}_2^2$\n",
        "  $‚áí\\sup_x(t^‚ä§x-\\/{1}{2}\\n{x-x_0}_2^2)=\\/{1}{2}\\n{t}_2^2+t^‚ä§x_0$.\n",
        "  Therefore\n",
        "  $\\sup_x(-(\\sum_iA_i^‚ä§ŒΩ_i)^‚ä§x-\\/{1}{2}\\n{x-x_0}_2^2)=\\/{1}{2}\\n{\\sum_iA_i^‚ä§ŒΩ_i}_2^2-\\sum_iŒΩ_i^‚ä§A_ix_0$\n",
        "\n",
        "  - Dual problem $\\blue{\\BC\\t{maximize}&\\sum_iŒΩ_i^‚ä§(A_ix_0+b_i)-\\/{1}{2}\\n{\\sum_iA_i^‚ä§ŒΩ_i}_2^2\\\\\n",
        "  \\t{subject to}&\\n{ŒΩ_i}_2‚â§1\n",
        "  \\EC}$\n",
        "\n",
        "- 5.12 (**Analytic centering**): $\\BC\n",
        "\\t{minimize}&-\\sum_i\\ln(b_i-a_i^‚ä§x)\n",
        "\\EC‚áí\\BC\n",
        "\\t{minimize}&-\\sum_i\\ln(y_i)\\\\\n",
        "\\t{subject to}&y_i=b_i-a_i^‚ä§x\n",
        "\\EC$ on $\\{x|a_i^‚ä§x< b_i\\}$ where $a_i^‚ä§$ is the $i$-th row of $A$.\n",
        "$g(ŒΩ)=\\inf_{x,y}(-\\sum_i\\ln(y_i)+\\sum_iŒΩ_i(b_i-a_i^‚ä§x-y_i))$\n",
        "$=\\sum_iŒΩ_ib_i+\\inf_y(-\\sum_i\\ln(y_i)+ŒΩ_iy_i)+\\inf_x(-\\sum_iŒΩ_ia_i^‚ä§x)$\n",
        "$=\\sum_iŒΩ_ib_i-\\sum_i\\sup_{y_i}(ŒΩ_iy_i+\\ln(y_i))$\n",
        "\n",
        "  - $\\sup_y(ty+\\ln(y))$: $‚àÇ_y(ty+\\ln(y))=t+\\/{1}{y}=0$\n",
        "  $‚áíy^*=-\\/{1}{t}$\n",
        "  $‚áí\\sup_y(ty+\\ln(y))=-1+\\ln(-\\/{1}{t})$ on $t< 0$.\n",
        "\n",
        "  - Dual problem $\\BC\\t{maximize}&\\sum_i(ŒΩ_ib_i+1-\\ln(-1/ŒΩ_i))\\\\\n",
        "  \\t{subject to}&ŒΩ_i< 0,\\ \\sum_iŒΩ_ia_i=0\\EC$\n",
        "\n",
        "- 5.14 **Quadratic penalty**: $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&Ax=b\\EC‚áí\\BC\\t{minimize}&œï(x)=f_0(x)+Œ±\\n{Ax-b}_2^2\\EC$\n",
        "to arrive at $\\tilde{x}‚âàx^*$.\n",
        "\n",
        "  - Differentiate\n",
        "  $‚àáœï^‚ä§=‚àáf_0(x)^‚ä§+2Œ±(Ax-b)^‚ä§A=0$\n",
        "  $‚áí‚àáf_0(\\tilde{x})+\\green{2Œ±A^‚ä§(A\\tilde{x}-b)}=0$\n",
        "\n",
        "  - Stationarity $‚àáL(x,ŒΩ)=‚àáf_0(x^*)+\\green{A^‚ä§ŒΩ^*}=0$.\n",
        "  Therefore $ŒΩ^*=2Œ±(A\\tilde{x}-b)$.\n",
        "\n",
        "  - $g(ŒΩ)=\\inf_x(f_0(x)+ŒΩ^‚ä§(Ax-b))$\n",
        "  $‚áíg(ŒΩ^*)=f_0(\\tilde{x})+2Œ±(A\\tilde{x}-b)^‚ä§(A\\tilde{x}-b)$\n",
        "  $=f_0(\\tilde{x})+2Œ±\\n{A\\tilde{x}-b}_2^2$.\n",
        "  Therefore $œï(\\tilde{x})‚â§f_0(\\tilde{x})+2Œ±\\n{A\\tilde{x}-b}_2^2‚â§p^*$.\n",
        "  Minimizing quadratic penalty $œï(\\tilde{x})$ underestimates $p^*$.\n",
        "  Because $ŒΩ^*$ is fixed, the penalty term $A\\tilde{x}-b=\\/{\\tilde{ŒΩ}}{2Œ±}‚Üí0$ and $œï(\\tilde{x})‚Üíp^*$ as $Œ±‚Üí‚àû$.\n",
        "\n",
        "- 5.20: $\\BC\\t{minimize}&-c^‚ä§x+\\sum_iy_i\\ln(y_i)\\\\\n",
        "\\t{subject to}&Px=y,\\ x‚âΩ0,\\ \\v{1}^‚ä§x=1\\EC$ where $P^‚ä§\\v{1}=1$.\n",
        "$g(Œª,ŒΩ,Œº)=\\inf_{x,y}(-c^‚ä§x+\\sum_iy_i\\ln(y_i)-Œª^‚ä§x+ŒΩ^‚ä§(Px-y)+Œº(\\v{1}^‚ä§x-1))$\n",
        "$=-Œº+\\inf_x(-c^‚ä§x-Œª^‚ä§x+(P^‚ä§ŒΩ)^‚ä§x+Œº\\v{1}^‚ä§x)+\\inf_y(\\sum_iy_i\\ln(y_i)-ŒΩ^‚ä§y)$\n",
        "$=-Œº+\\inf_x(-c-Œª+P^‚ä§ŒΩ+Œº\\v{1})^‚ä§x-\\sup_y(ŒΩ^‚ä§y-\\sum_iy_i\\ln(y_i))$\n",
        "$=\\BC-Œº-\\sum_ie^{ŒΩ_i-1}&-c-Œª+P^‚ä§ŒΩ+Œº\\v{1}=0\\\\\n",
        "-‚àû&\\t{otherwise}\n",
        "\\EC$\n",
        "$=\\BC-Œº-\\sum_ie^{ŒΩ_i-1}&P^‚ä§ŒΩ+Œº\\v{1}‚âΩc\\\\\n",
        "-‚àû&\\t{otherwise}\n",
        "\\EC$\n",
        "\n",
        "- 5.21: $\\BC\\t{minimize}&e^{-x}\\\\\n",
        "\\t{subject to}&x^2/y‚â§0\\EC$ on $\\m{D}=\\{(x,y)|y>0\\}$.\n",
        "The constraint and domain require that $x=0$ and $p^*=1$. Slater's condition does not hold.\n",
        "$g(Œª)=\\inf_{x,y}(e^{-x}+Œª(x^2/y))=0$. Therefore $d^*=0< 1=p^*$.\n",
        "\n",
        "- 5.27: $\\BC\\t{minimize}&\\n{Ax-b}_2^2\\\\\n",
        "\\t{subject to}&Gx=h\\EC$ where $\\rk(G‚àà‚Ñù^{p√ón})=p$.\n",
        "$L(x,ŒΩ)=\\n{Ax-b}_2^2+ŒΩ^‚ä§(Gx-h)$.\n",
        "\n",
        "  - Dual: $g(ŒΩ)=-ŒΩ^‚ä§h+\\inf_x(\\n{Ax-b}_2^2+ŒΩ^‚ä§Gx)$.\n",
        "  Differentiate\n",
        "  $‚àá_x(ŒΩ^‚ä§Gx+\\n{Ax-b}_2^2)$\n",
        "  $=G^‚ä§ŒΩ+2A^‚ä§Ax-2A^‚ä§b=0$\n",
        "  $‚áíx^*=(A^‚ä§A)^{-1}A^‚ä§b-\\/{1}{2}(A^‚ä§A)^{-1}G^‚ä§ŒΩ$.\n",
        "  Therefore $g(ŒΩ)=-\\/{1}{4}ŒΩ^‚ä§G(A^‚ä§A)^{-1}G^‚ä§ŒΩ+ŒΩ^‚ä§(Gx^*-h)+\\n{Ax^*-b}_2^2$.\n",
        "\n",
        "  - Primal feasibility: $Gx=h$\n",
        "\n",
        "  - Stationarity: $G^‚ä§ŒΩ+2A^‚ä§Ax-2A^‚ä§b=0$\n",
        "\n",
        "  - Optimal point: $x^*=(A^‚ä§A)^{-1}A^‚ä§b-\\/{1}{2}(A^‚ä§A)^{-1}G^‚ä§ŒΩ^*$."
      ],
      "metadata": {
        "id": "E_hNxvae7chw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Approximation and Fitting"
      ],
      "metadata": {
        "id": "kD8fcfg0p7tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Norm approximation** $\\red{\\BC\\t{minimize}&\\n{Ax-b},\\ A‚àà‚Ñù^{m√ón}\\t{ tall}\\EC}$ is a convex problem and approximates $Ax‚âàb$ with residual $r=Ax-b$.\n",
        "Columns of $A$ are independent. $Ax=x_1a_1+...+x_na_n$ where $a_j‚àà‚Ñù^m$ is the $j$-th column of $A$.\n",
        "Each residual $r_i=b_i-a_i^‚ä§x$ where $a_i^‚ä§‚àà‚Ñù^n$ is the $i$-th row of $A$.\n",
        "\n",
        "- Optimal value $\\n{r^*}=0$ iff $b‚àà\\span(A)$. Otherwise $r=u-b$ where $u$ is the projection of $b$ onto subspace $\\span(A)$. The norm approximation problem is equivalent to $\\BC\n",
        "\\t{minimize}&\\n{u-b}\\\\\n",
        "\\t{subject to}&u‚àà\\span(A)\n",
        "\\EC$.\n",
        "\n",
        "- Regression: $y=Ax+v$ where $v‚àà‚Ñù^m$ is some unknown measurement error. $v=y-A\\hat{x}$ where $\\hat{x}=\\arg\\min_x\\n{Ax-y}$.\n",
        "\n",
        "- **Weighted norm**: $\\BC\\t{minimize}&\\n{W(Ax-b)}\\EC$ where $W‚àà‚Ñù^{m√ón}$ is a diagonal matrix that gives relative emphasis to different components of $r=Ax-b$.\n",
        "\n",
        "- **Least squares**: $\\BC\\t{minimize}&\\n{Ax-b}_2^2=r_1^2+...+r_m^2\\EC$ is solved analytically $x=(A^‚ä§A)^{-1}A^‚ä§b$.\n",
        "\n",
        "  - Let $a_i^‚ä§‚àà‚Ñù^n$ be a row of $A$, then $\\red{\\hat{x}=(A^‚ä§A)^{-1}A^‚ä§b=(\\sum_{i=1}^ma_ia_i^‚ä§)^{-1}\\sum_{i=1}^mb_ia_i}$\n",
        "\n",
        "- **Chebyshev minimax**: $\\BC\\t{minimize}&\\n{Ax-b}_‚àû=\\max\\{|r_1|,...,|r_m|\\}\\EC‚áí\\BC\n",
        "\\t{minimize}&t\\\\\n",
        "\\t{subject to}&-t\\v{1}‚âºAx-b‚âºt\\v{1}\n",
        "\\EC$\n",
        "\n",
        "- **Sum of absolute**: $\\BC\\t{minimize}&\\n{Ax-b}_1=|r_1|+...+|r_m|\\EC‚áí\\BC\n",
        "\\t{minimize}&\\v{1}^‚ä§t\\\\\n",
        "\\t{subject to}&-t‚âºAx-b‚âºt\n",
        "\\EC$\n",
        "\n",
        "**Least norm** $\\red{\\BC\\t{minimize}&\\n{x}\\\\\n",
        "\\t{subject to}&Ax=b,\\ A‚àà‚Ñù^{m√ón}\\t{ fat}\n",
        "\\EC}‚áí\\BC\n",
        "\\t{minimize}&\\n{x_0+Zu}\n",
        "\\EC$ where $x_0‚àà‚Ñù^n$ is any solution, and $Z‚àà‚Ñù^{n√ók}$ satisfying $\\span(Z)=\\null(A)$ is a reformulation of least norm into norm approximation.\n",
        "\n",
        "- Problem represents the most efficient design with least resource consumption $\\n{x}$ to achieve constraints $Ax=b$.\n",
        "Geometrically the problem is the projection of origin onto affine set $\\{x|Ax=b\\}$: closest point to origin.\n",
        "\n",
        "- **Least squares**: $\\blue{\\BC\\t{minimize}&\\n{x}_2^2\\\\\n",
        "\\t{subject to}&Ax=b\\EC}$.\n",
        "This has an analytical solution.\n",
        "Differentiate\n",
        "$L=\\n{x}_2^2+ŒΩ^‚ä§(Ax-b)$\n",
        "$‚áí0=2x^‚ä§+ŒΩ^‚ä§A$\n",
        "$‚áíx^*=-\\/{1}{2}A^‚ä§ŒΩ$.\n",
        "Substitute\n",
        "$A(-\\/{1}{2}A^‚ä§ŒΩ)=b$\n",
        "$‚áíŒΩ^*=-2(AA^‚ä§)^{-1}b,\\ \\blue{x^*=A^‚ä§(AA^‚ä§)^{-1}b}$\n",
        "assuming $\\rk(A)=m$ so that $AA^‚ä§‚âª0$.\n",
        "\n",
        "- **Least $‚Ñì_1$-norm**: $\\BC\\t{minimize}&\\n{x}_1\\\\\n",
        "\\t{subject to}&Ax=b\\EC$ tends to produce a sparse solution with large number of zero components."
      ],
      "metadata": {
        "id": "ORgWWNrQqEzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penalty functions**: $\\green{\\BC\\t{minimize}&\\n{Ax-b}_p^p\\EC‚áí\\BC\\t{minimize}&\\n{r}_p^p=\\sum_i|r_i|^p\\\\\\t{subject to}&Ax=b\n",
        "\\EC‚áí\\BC\\t{minimize}&\\sum_i\\red{œï(r_i)}\\\\\n",
        "\\t{subject to}&r=Ax-b\\EC}$ is a separable function depending only on residual amplitudes.\n",
        "Penalty function $œï:‚Ñù‚Üí‚Ñù$ is convex, symmetric, and nonnegative.\n",
        "\n",
        "- $\\red{œï_1(u)=|u|}$ vs $\\red{œï_2(u)=u^2}$: $\\BC\n",
        "œï_1(u)‚â´œï_2(u)&u< 1\\\\\n",
        "œï_1(u)‚â™œï_2(u)&u\\t{ large}\\EC$.\n",
        "\n",
        "  - $‚Ñì_1$-norm places largest penalty weights on small residuals and produces sparse residual vector with many residuals at zero. Easy to overfit small residuals, but is the most robust convex penalty function for large outlier residuals.\n",
        "  \n",
        "  - $‚Ñì_2$-norm places largest penalty weights on large residuals and prevents any one error from standing out but almost no zero residual.\n",
        "  Easy to overfit large residuals, but is best for small and moderate residuals.\n",
        "\n",
        "- **Deadzone** $œï(u)=\\BC\n",
        "0&|u|‚â§a\\\\\n",
        "|u|-a&|u|>a\n",
        "\\EC$ and\n",
        "**log barrier** $œï(u)=\\BC\n",
        "-a^2\\ln(1-(u/a)^2)&|u|< a\\\\\n",
        "‚àû&|u|‚â•a\n",
        "\\EC$ only assess penalties for residuals larger than $a$. This prevents overfitting small residuals like in $‚Ñì_1$.\n",
        "\n",
        "- **Outlier** is when $r_i=b_i-a_i^‚ä§x$ is relative large. **Truncated penalty** $\\green{œï(u)=\\BC u^2&|u|‚â§M\\\\M^2&|u|>M\\EC}$ limits the influence of outliers on $x^*$ determination. Residuals larger than $M$ are ignored by the solver because $‚àÇ_xM^2=0$ does not contribute to the objective forces. Unfortunately it is non-convex and cannot use with SOCP or QP.\n",
        "\n",
        "  - **Huber penalty** $\\red{œï(u)=\\BC u^2&|u|‚â§M\\\\M(2|u|-M)&|u|>M\\EC}$ is the closest convex approximation to truncated penalty"
      ],
      "metadata": {
        "id": "YsU_PxlBuTHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization** $\\red{\\BC\\t{minimize}&\\n{Ax-b}+Œ≥\\n{x}\\EC}$ is scalarization with $Œª=(1,Œ≥)$.\n",
        "\n",
        "- We want $x$ small because either we have a prior on $x$ or $A$ is susceptible to noise.\n",
        "$\\/{‚àÇ(Ax)}{‚àÇa_i^‚ä§}=x$.\n",
        "Larger $x$ amplifies effect of errors in $A$.\n",
        "Where $A^‚ä§A‚âΩ0$, we guarantee $A^‚ä§A+Œ≥I‚âª0$ (invertible).\n",
        "\n",
        "- **Tikhonov $‚Ñì_2$ regularization**: $\\blue{\\BC\\t{minimize}&\\n{Ax-b}_2^2+Œ¥\\n{x}_2^2\\EC}‚áí\\BC\\t{minimize}&\n",
        "\\n{\\BM A\\\\\\sqrt{Œ¥}I\\EM x-\\BM b\\\\0\\EM}_2^2\\EC$ has analytical solution\n",
        "$‚àá_xL^‚ä§=2(Ax-b)^‚ä§A+2Œ¥x^‚ä§=0$\n",
        "$‚áí\\blue{x^*=(A^‚ä§A+Œ¥I)^{-1}A^‚ä§b}$.\n",
        "\n",
        "  - **Smoothing regularization**: $\\blue{\\BC\\t{minimize}&\\n{Ax-b}_2^2+Œ¥\\n{Dx}_2^2+Œ∑\\n{x}_2^2\\EC}$ where $D‚àà‚Ñù^{(n-2)√ón}$ is a **difference matrix** with pattern $[1,-2,1]$ to approximate second-order differentiation of $x$.\n",
        "\n",
        "- **$‚Ñì_1$ regularization**: $\\blue{\\BC\\t{minimize}&\\n{Ax-b}_2+Œ≥\\n{x}_1\\EC}$ as SOCP is used as a heuristic for finding a sparse $x$.\n",
        "\n",
        "**Stochastic robust approximation**: $\\blue{\\BC\\t{minimize}&\\E\\n{Ax-b},\\ A=\\bar{A}+U\\EC}$ with random noise matrix $U$ is convex but usually intractable because $\\E[‚ãÖ]$ objective evaluation is computationally hard in general.\n",
        "\n",
        "- **Sum of norms**: $P(A=A_i)=p_i,\\ i=1,...,k$ limits the support of $A$ and is tractable. Then the problem becomes SOCP ($‚Ñì_2$)\n",
        "$\\blue{\\BC\n",
        "\\t{minimize}&\\sum_ip_i\\n{A_ix-b}\\EC}‚áí\\BC\n",
        "\\t{minimize}&p^‚ä§t\\\\\n",
        "\\t{subject to}&\\n{A_ix-b}_2‚â§t_i,\\ i=1,...,k\n",
        "\\EC$\n",
        "or LP ($‚Ñì_1$ and $‚Ñì_‚àû$).\n",
        "\n",
        "  - Here $\\n{A_ix-b}_2‚â§t_i$ cone is a convex set. $\\n{A_ix-b}_2=t_i$ is not affine and is non-convex set.\n",
        "\n",
        "- **Stochastic least squares**: $\\BC\n",
        "\\t{minimize}&\\E\\n{Ax-b}_2^2\n",
        "\\EC$ is tractable.\n",
        "Objective\n",
        "$\\E\\n{Ax-b}_2^2=\\E\\n{\\bar{A}x+Ux-b}_2^2$\n",
        "$=\\E[(\\bar{A}x-b+Ux)^‚ä§(\\bar{A}x-b+Ux)]$\n",
        "$=(\\bar{A}x-b)^‚ä§(\\bar{A}x-b)+2(\\bar{A}x-b)^‚ä§\\E[U]x+\\E[(Ux)^‚ä§(Ux)]$\n",
        "$=\\n{\\bar{A}x-b}_2^2+x^‚ä§\\E[U^‚ä§U]x$.\n",
        "Let $P=\\E[U^‚ä§U]$ then\n",
        "$\\blue{\\BC\\t{minimize}&\\n{\\bar{A}x-b}_2^2+\\n{P^{1/2}x}_2^2\\EC}$.\n",
        "\n",
        "  - $‚àá_xL^‚ä§=2(\\bar{A}x-b)^‚ä§\\bar{A}+2(P^{1/2}x)^‚ä§P^{1/2}=0$\n",
        "  $‚áí\\bar{A}^‚ä§\\bar{A}x+Px=\\bar{A}^‚ä§b$\n",
        "  $‚áí\\blue{x^*=(\\bar{A}^‚ä§\\bar{A}+P)^{-1}\\bar{A}^‚ä§b}$.\n",
        "\n",
        "  - Tikhonov regularization is stochastic least squares with $Œ¥I=\\E[U^‚ä§U]$ where entry $\\blue{U_{ij}‚àº\\Normal(0,Œ¥/m)}$.\n",
        "  Proof: Each entry $(U^‚ä§U)_{ij}=\\sum_kU_{ki}U_{kj}$ is the dot product of columns $i,j$.\n",
        "  $\\BC\n",
        "  \\t{off diagonal}&(Œ¥I)_{ij}=0&\\E[(U^‚ä§U)_{ij}]=\\sum_k\\E[U_{ki}U_{kj}]=\\sum_k\\Cov(U_{ki},U_{kj})=0\\\\\n",
        "  \\t{on diagonal}&(Œ¥I)_{ii}=Œ¥&\\E[(U^‚ä§U)_{ii}]=\\sum_k\\E[U_{ki}^2]=\\sum_k\\Var(U_{ki})=m\\Var(U_{ij})\n",
        "  \\EC$\n",
        "\n",
        "**Worst-case robust approximation**: Let $\\m{A}‚äÜ‚Ñù^{m√ón}$ be possible values for $A$ amid uncertainty. Then worst-case error of solution is $e_\\t{wc}(x)=\\sup\\{\\n{Ax-b}|A‚àà\\m{A}\\}$, which is a convex support function. Then the worst-case robust approximation problem is\n",
        "$\\red{\\BC\\t{minimize}&e_\\t{wc}(x)=\\sup\\{\\n{Ax-b}|A‚àà\\m{A}\\}\\EC}$, which reduces to norm approximation if $\\m{A}=\\{A\\}$.\n",
        "\n",
        "- **Finite set**: $\\blue{\\m{A}=\\{A_1,...,A_k\\}}$ then\n",
        "$\\BC\\t{minimize}&\\max_i\\n{A_ix-b}\\EC$\n",
        "$‚áí\\BC\n",
        "\\t{minimize}&t\\\\\n",
        "\\t{subject to}&\\n{A_ix-b}‚â§t\n",
        "\\EC$ is a convex problem as SOCP ($‚Ñì_2$) or LP ($‚Ñì_1$ and $‚Ñì_‚àû$).\n",
        "\n",
        "- **Norm ball**: $\\blue{\\m{A}=\\{\\bar{A}+U|\\n{U}‚â§a\\}}$.\n",
        "Here $\\BC\\t{minimize}&e_\\t{wc}(x)=\\sup\\{\\n{Ax-b}|A‚àà\\m{A}\\}\\EC$.\n",
        "Expand $‚Ñì_2$ objective\n",
        "$e_\\t{wc}(x)=\\sup\\{\\n{(\\bar{A}+U)x-b}_2|\\n{U}_2‚â§a\\}$\n",
        "$=\\n{\\bar{A}x-b}_2+\\sup\\{\\n{Ux}_2|\\n{U}_2‚â§a\\}$\n",
        "$=\\n{\\bar{A}x-b}_2+a\\n{x}_2$\n",
        "where $\\n{U}_2=œÉ_\\max(U)$ is the spectral norm.\n",
        "Problem is equivalent to\n",
        "$\\blue{\\BC\\t{minimize}&\\n{\\bar{A}x-b}_2^2+a\\n{x}_2^2\\EC}$.\n",
        "\n",
        "  - Tikhonov regularization is robust approximation with worst-case adversarial noise of size $Œ¥$."
      ],
      "metadata": {
        "id": "WEeQe4vHTqRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function fitting and interpolation**: Let $f_1,...,f_n:‚Ñù^k‚Üí‚Ñù$ be basis functions spanning subspace $\\m{F}$. Let coefficient vector $x‚àà‚Ñù^n$ be the optimization variable, then $\\red{f(t)=x_1f_i(t)+...+x_nf_n(t)}$ on domain $\\dom(f_i)=\\dom(f)=\\m{D}$.\n",
        "\n",
        "- **Polynomial basis**: $f(t)=x_1+x_2t+x_3t^2+...$ are the standard Taylor series form. $f_i(t)=t^{i-1}$ is numerically unstable for higher $n$ because $t^n$ and $t^{n-1}$ look very similar for large $t$.\n",
        "\n",
        "  - Orthonormal basis $f_1,...,f_n$ satisfy\n",
        "  $‚à´f_i(t)f_j(t)œï(t)\\ dt=\\BC\n",
        "  1&i=j\\\\\n",
        "  0&i\\neq j\\EC$ for some positive $œï(t)$.\n",
        "  Lagrange basis $\\red{f_i(u_j)=\\BC\n",
        "  1&i=j\\\\\n",
        "  0&i\\neq j\\EC,\\ ‚àÄi,j=1,...,n}$.\n",
        "  Also can use $\\sin kt$ and $\\cos kt$ as in Fourier series.\n",
        "\n",
        "- **Piecewise-affine triangulation**: Grid $u_1,...,u_n‚àà\\m{D}‚äÜ‚Ñù^k$ partitions domain into simplexes $\\m{D}=S_1‚à™...‚à™S_m$ on that grid where simplex $S_j$ is convex hull of $k+1$ grid points and $\\t{int}(S_i‚à©S_j)=‚àÖ$.\n",
        "Define vertices $x_i=f(u_i)$ with Lagrange basis\n",
        "$f_i(u_j)=\\BC\n",
        "1&i=j\\\\\n",
        "0&i\\neq j\\EC$ and interpolate with affine segments.\n",
        "\n",
        "- **Constraints**: Let $f(t)=\\sum_{i=1}^nx_if_i(t)$. Then $f(v_j)=z_j$ are $m$ constraints on $x$.\n",
        "\n",
        "  - Lipschitz constraint $|f(v_j)-f(v_k)|‚â§L\\n{v_j-v_k}$\n",
        "\n",
        "  - Derivative constraints: If $f_i‚ààC^1$ then $\\n{‚àáf}=\\n{\\sum_{i=1}^nx_i‚àáf_i}‚â§M$ is a linear inequality constraint.\n",
        "\n",
        "  - Smoothness constraint: If $f_i‚ààC^2$ then $lI‚âº‚àá^2f‚â§uI$ is a LMI inequality constraint.\n",
        "\n",
        "- **Least squares fitting**: Given $(u_1,y_1),...,(u_m,y_m)$ where $u_i‚àà\\m{D}$ and $y_i‚àà‚Ñù$, we seek $f‚àà\\m{F}$ with $m‚â´n$. Then\n",
        "$\\BC\n",
        "\\t{minimize}&\\sum_{i=1}^m(f(u_i)-y_i)^2=\\sum_{i=1}^m(\\sum_{j=1}^nx_jf_j(u_i)-y_i)^2\n",
        "\\EC$\n",
        "\n",
        "  - **Least norm interpolation**: If $m‚â™n$, then $f(u_i)=y_i$ is an equality constraint."
      ],
      "metadata": {
        "id": "36uY04qZpNGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 6.3: $A‚àà‚Ñù^{m√ón}$. Formulate as LP, QP, SOCP, or SDP.\n",
        "\n",
        "  - **Deadzone-linear**: $\\BC\\t{minimize}&\\sum_iœï(a_i^‚ä§x-b_i)\\EC$ with\n",
        "  $\\green{œï(u)=\\BC0&|u|‚â§Œ±\\\\\n",
        "  |u|-Œ±&|u|>Œ±\\EC}$.\n",
        "  Here $œï(u)=\\max(0,|u|-Œ±)$.\n",
        "  LP\n",
        "  $\\BC\\t{minimize}_{(t,x)}&\\sum_it_i\\\\\n",
        "  \\t{subject to}&t_i‚â•0,\\ t_i‚â•|a_i^‚ä§x-b_i|-Œ±\\EC$\n",
        "  $‚áí\\BC\\t{minimize}_{(t,x)}&\\sum_it_i\\\\\n",
        "  \\t{subject to}&t_i‚â•0\\\\\n",
        "  &t_i+Œ±‚â•a_i^‚ä§x-b_i,\\ -t_i-Œ±‚â§a_i^‚ä§x-b_i\\EC$\n",
        "\n",
        "  - **Log-barrier**: $\\BC\\t{minimize}&\\sum_iœï(a_i^‚ä§x-b_i)\\EC$ with\n",
        "  $\\green{œï(u)=\\BC-Œ±^2\\ln(1-(u/Œ±)^2)&|u|< Œ±\\\\\n",
        "  ‚àû&|u|‚â•Œ±\\EC}$.\n",
        "  Logs cannot use SDP.\n",
        "\n",
        "  - **Huber** (e4.5): $\\BC\\t{minimize}&\\sum_iœï(a_i^‚ä§x-b_i)\\EC$ with\n",
        "  $\\green{œï(u)=\\BC u^2&|u|‚â§Œ±\\\\\n",
        "  Œ±(2|u|-Œ±)&|u|‚â•Œ±\\EC}$.\n",
        "  QP\n",
        "  $\\BC\\t{minimize}&\\sum_i(u_i^2+2Œ±v_i)\\\\\n",
        "  \\t{subject to}&-u_i-v_i‚â§a_i^‚ä§x-b_i‚â§u_i+v_i\\\\\n",
        "  &0‚â§u_i‚â§Œ±,\\ v_i‚â•0\\EC$.\n",
        "\n",
        "- 6.6 Conjugate:\n",
        "\n",
        "  - **Deadzone**: $\\blue{œï(u)=\\BC0&|u|‚â§Œ±\\\\\n",
        "  |u|-Œ±&|u|>Œ±\\EC}$.\n",
        "  Then $œï^*(v)=\\sup_u(uv-\\max(0,|u|-Œ±))$\n",
        "  $\\BC\n",
        "  -Œ±‚â§u‚â§Œ±&œï(u)=0&œï^*(v)=\\sup_u(uv)=\\BC Œ±v&\\t{ if }v>0‚áíu=Œ±\\\\-Œ±v&\\t{ if }v< 0‚áíu=-Œ±\\EC=Œ±|v|\\\\\n",
        "  u>Œ±&œï(u)=u-Œ±&œï^*(v)=Œ±+\\sup_uu(v-1)=\\BC ‚àû&\\t{if }v>1\\\\Œ±v&\\t{if }v‚â§1‚áíu=Œ±\\EC\\\\\n",
        "  u< -Œ±&œï(u)=-u-Œ±&œï^*(v)=Œ±+\\sup_uu(v+1)=\\BC ‚àû&\\t{if }v< -1\\\\-Œ±v&\\t{if }v‚â•-1‚áíu=-Œ±\\EC\n",
        "  \\EC$\n",
        "  $‚áí\\blue{œï^*(v)=\\BCŒ±|v|&|v|‚â§1\\\\‚àû&\\t{otherwise}\\EC}$\n",
        "\n",
        "  - **Huber**: $\\blue{œï(u)=\\BC u^2&|u|‚â§Œ±\\\\\n",
        "  Œ±(2|u|-Œ±)&|u|‚â•Œ±\\EC}$.\n",
        "  $\\BC\n",
        "  -Œ±‚â§u‚â§Œ±&œï(u)=u^2&œï^*(v)=\\sup_u(uv-u^2)=\\/{1}{4}v^2\\\\\n",
        "  u‚â•Œ±&œï(u)=2Œ±u-Œ±^2&œï^*(v)=Œ±^2+\\sup_uu(v-2Œ±)=\\BC ‚àû&\\t{if }v>2Œ±\\\\Œ±v-Œ±^2&\\t{if }v‚â§2Œ±‚áíu=Œ±\\EC\\\\\n",
        "  u‚â§-Œ±&œï(u)=-2Œ±u-Œ±^2&œï^*(v)=Œ±^2+\\sup_uu(v+2Œ±)=\\BC ‚àû&\\t{if }v< -2Œ±\\\\Œ±v-Œ±^2&\\t{if }v‚â•-2Œ±‚áíu=-Œ±\\EC\n",
        "  \\EC$\n",
        "  $‚áí\\/{1}{4}v^2-Œ±v+Œ±^2=(\\/{1}{2}v-Œ±)^2>0$\n",
        "  $‚áí\\max(\\/{1}{4}v^2,Œ±v-Œ±^2)=\\/{1}{4}v^2$\n",
        "  $‚áí\\blue{œï^*=\\BC\n",
        "  \\/{1}{4}v^2&|v|‚â§2Œ±\\\\\n",
        "  ‚àû&|v|>2Œ±\\EC}$.\n",
        "\n",
        "- 6.7 **SVD**: $\\BC\\t{minimize wrt }‚Ñù_+^2&(\\n{Ax-b}_2^2,\\n{x}_2^2)\\EC$ where $A=U\\diag(œÉ)V^‚ä§=\\sum_{i=1}^rœÉ_iu_iv_i^‚ä§$ has rank $r$.\n",
        "\n",
        "  - $A‚àà‚Ñù^{m√ón}$ therefore $U‚àà‚Ñù^{m√óm},V‚àà‚Ñù^{n√ón}$ are orthogonal matrices.\n",
        "  Let rotated $\\tilde{x}=V^‚ä§x$ and $\\tilde{b}=U^‚ä§b$, then by definition\n",
        "  $\\n{\\tilde{x}}_2^2=\\n{V^‚ä§x}_2^2$\n",
        "  $=\\n{x}_2^2$ norm is preserved.\n",
        "  Also by definition\n",
        "  $\\n{Ax-b}_2^2=\\n{UŒ£V^‚ä§x-b}_2^2$\n",
        "  $=\\n{U^‚ä§(UŒ£V^‚ä§x-b)}_2^2$\n",
        "  $=\\n{Œ£\\tilde{x}-\\tilde{b}}_2^2$\n",
        "  $=\\sum_{i=1}^r(œÉ_i\\tilde{x}_i-\\tilde{b}_i)^2+\\sum_{i=r+1}^m\\tilde{b}_i^2$\n",
        "\n",
        "  - $\\BC\\t{minimize}&\\n{Ax-b}_2^2+Œ¥\\n{x}_2^2\\EC$\n",
        "  $‚áí\\BC\\t{minimize}&\\sum_{i=1}^r(œÉ_i\\tilde{x}_i-\\tilde{b}_i)^2+Œ¥\\tilde{x}_i^2\\EC$.\n",
        "  The extra $i=r+1,...,m$ terms are differentiated out.\n",
        "  $2œÉ_i(œÉ_i\\tilde{x}_i-\\tilde{b}_i)+2Œ¥\\tilde{x}_i=0$\n",
        "  $‚áí\\tilde{x}_i=\\/{œÉ_i\\tilde{b}_i}{œÉ_i^2+Œ¥}$\n",
        "  $‚áíx_i=\\/{œÉ_i(u_i^‚ä§b)}{œÉ_i^2+Œ¥}v_i$\n",
        "\n",
        "  - $\\BC\\t{minimize}&\\n{Ax-b}_2^2\\\\\n",
        "  \\t{subject to}&\\n{x}_2^2=Œ≥\\EC$.\n",
        "  $L(x,ŒΩ)=\\n{Ax-b}_2^2+ŒΩ(\\n{x}_2^2-Œ≥)$\n",
        "  $=\\sum_{i=1}^r(œÉ_i\\tilde{x}_i-\\tilde{b}_i)^2+ŒΩ\\tilde{x}_i^2$ identical to regularization with extra $i=r+1,...,n$ terms differentiated out.\n",
        "  $x_i=\\/{œÉ_i(u_i^‚ä§b)}{œÉ_i^2+ŒΩ}v_i$.\n",
        "  Solve for $ŒΩ^*$ from\n",
        "  $\\sum_{i=1}^r(\\/{œÉ_i(u_i^‚ä§b)}{œÉ_i^2+ŒΩ})^2=Œ≥$.\n",
        "  Finally\n",
        "  $x_i=\\/{œÉ_i(u_i^‚ä§b)}{œÉ_i^2+ŒΩ^*}v_i$\n"
      ],
      "metadata": {
        "id": "Bl_mf6Ipv9sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: VMLS Least Squares"
      ],
      "metadata": {
        "id": "fNMuahN8qNzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 12: Least squares problem**: $Ax=b,\\ A‚àà‚Ñù^{m√ón}$ tall is an over-determined system with more equations than variables and no exact solution. For **residual** $r=Ax-b$ we want $\\red{\\BC\\t{minimize}_x&\\n{Ax-b}^2=\\n{r}^2=\\sum_{i=1}^mr_i^2\\EC}$. Residual is affine function of $x$ it is \"linear\" least squares. Assume columns of $A$ are linearly independent.\n",
        "\n",
        "- **Column interpretation**: $\\n{Ax-b}^2=\\n{\\sum_{i=1}^\\red{n}x_ia_i-b}^2$ **synthesize** $b$ by adding columns of $A$ weighted by $x$.\n",
        "\n",
        "- **Row interpretation**: $\\n{Ax-b}^2=\\sum_{i=1}^\\red{m}(a_i^‚ä§x-b_i)^2$ where $a_i^‚ä§$ is the $i$-th row of $A$ minimize errors from $m$ contradicting **constraints**.\n",
        "\n",
        "- **Least squares estimator** $\\hat{x}$ achieves smallest norm residual $\\n{A\\hat{x}-b}^2‚â§\\n{Ax-b}^2\\ ‚àÄx$. The value $\\n{A\\hat{x}-b}$ is the optimal residual norm. $\\hat{x}$ is the result of \"**regressing** $b$ onto columns of $A$\".\n",
        "\n",
        "  - Solution: $‚àá_x^‚ä§(\\n{Ax-b}^2)=2(Ax-b)^‚ä§A=0$\n",
        "  $‚áí\\red{\\hat{x}=(A^‚ä§A)^{-1}A^‚ä§b=A^‚Ä†b}$.\n",
        "  Gram matrix $A^‚ä§A$ is positive-definite and invertible iff $\\rk(A)=n$ (linearly independent columns).\n",
        "  Moore-Penrose pseudoinverse $A^‚Ä†$ is the left inverse of $A$. If $Ax=b$ is solvable then $\\hat{x}=A^‚Ä†b$ is an exact solution. Generally $A\\hat{x}\\neq b$.\n",
        "\n",
        "  - Proof $\\n{A\\hat{x}-b}^2‚â§\\n{Ax-b}^2\\ ‚àÄx$: Because $\\blue{\\n{u+v}^2=\\n{u}^2+\\n{v}^2+2u^‚ä§v}$ we have\n",
        "  $\\n{Ax-b}^2=\\n{Ax-A\\hat{x}+A\\hat{x}-b}^2$\n",
        "  $=\\n{Ax-A\\hat{x}}^2+\\n{A\\hat{x}-b}^2+2\\green{(Ax-A\\hat{x})^‚ä§(A\\hat{x}-b)}$.\n",
        "  The cross term\n",
        "  $(Ax-A\\hat{x})^‚ä§(A\\hat{x}-b)=(x-\\hat{x})^‚ä§A^‚ä§(A\\hat{x}-b)$\n",
        "  $=(x-\\hat{x})^‚ä§(A^‚ä§A\\hat{x}-A^‚ä§b)$\n",
        "  $=(x-\\hat{x})^‚ä§0$.\n",
        "  Therefore\n",
        "  $\\green{\\n{Ax-b}^2=\\n{Ax-A\\hat{x}}^2+\\n{A\\hat{x}-b}^2}$\n",
        "  $‚áí\\n{Ax-b}^2‚â•\\n{A\\hat{x}-b}^2$.\n",
        "\n",
        "  - Proof $\\hat{x}$ is unique optimal: We have $\\n{Ax-b}^2=\\n{Ax-A\\hat{x}}^2+\\n{A\\hat{x}-b}^2$. Suppose $\\n{Ax-b}^2=\\n{A\\hat{x}-b}^2$, then $\\n{Ax-A\\hat{x}}^2=0$\n",
        "  $‚áíx=\\hat{x}$.\n",
        "\n",
        "  - Geometric interpretation: Let $m=3,n=2$. Then $b$ is a point in the 3-D space and columns of $A$ span the floor. The optimal residual $\\hat{r}=b-A\\hat{x}$ is the orthogonal vector from the floor reaching up to point $b$. In this model\n",
        "  $\\span(A)\\perp\\hat{r}$\n",
        "  $‚áí\\hat{r}‚àà\\null(A)$\n",
        "  $‚áíA^‚ä§(A\\hat{x}-b)=0$\n",
        "  $‚áí\\hat{x}=(A^‚ä§A)^{-1}A^‚ä§b$.\n",
        "\n",
        "- **Computation**: Let $A=QR$ satisfying $Q^‚ä§Q=I$ and $R$ positive-definite, then $\\red{A^‚Ä†=R^{-1}Q^‚ä§}$. Then $\\red{\\hat{x}=R^{-1}(Q^‚ä§b)}$ is the most numerically stable algorithm.\n",
        "\n",
        "  - $\\hat{x}=R^{-1}(Q^‚ä§b)$ is identical to the formula for solving $Ax=b$ where $A$ is positive-definite.\n",
        "\n",
        "  - $A=QR$ costs $\\m{O}(2mn^2)$ flops. $Q^‚ä§b$ costs $\\m{O}(2mn)$ flops. $R^{-1}$ costs $n^2$ flops. Total cost $\\m{O}(2mn^2)$.\n",
        "\n",
        "  - $\\blue{\\BM\n",
        "  0&A^‚ä§\n",
        "  \\\\A&I\n",
        "  \\EM\\BM\n",
        "  \\hat{x}\\\\\\hat{r}\n",
        "  \\EM=\\BM\n",
        "  0\\\\b\n",
        "  \\EM}$ is a square set of $‚Ñù^{(m+n)√ó(m+n)}$ satisfying\n",
        "  $\\hat{r}=b-A\\hat{x}$. It has the nice property of preserving the sparsity of $A$ because $A^‚ä§A$ destroys sparsity. Sparse matrices have efficient QR algorithms.\n",
        "\n",
        "- **Matrix least square**: $\\n{AX-B}^2$ where $B‚àà‚Ñù^{m√ók}$ and $X‚àà‚Ñù^{n√ók}$. This problem can be decomposed to $k$ columns $\\n{AX-B}^2=\\sum_{i=1}^k\\n{Ax_i-b_i}^2$ with $\\hat{x}_j=A^‚Ä†b_j$. Here $\\hat{X}=A^‚Ä†B$."
      ],
      "metadata": {
        "id": "gRTEnlspqWuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 13: Least squares data fitting**: Given features $x=(x_1,...,x_N)$ and responses $y=(y_1,...,y_N)$ where $x_i‚àà‚Ñù^n,\\ y_i‚àà‚Ñù$ we want to model $y_i‚âà\\hat{f}(x_i)$ where $\\hat{f}:‚Ñù^n‚Üí‚Ñù,\\ \\red{\\hat{f}(x)=Œ∏_1f_1(x)+...+Œ∏_pf_p(x)}$ with basis functions $f_i:‚Ñù^n‚Üí‚Ñù$ is **linear in parameters $Œ∏‚àà‚Ñù^p$**.\n",
        "Model produces prediction $\\hat{y}_i=\\hat{f}(x_i)$ and error residual $r_i=y_i-\\hat{y}_i$. We choose $Œ∏$ to minimize $\\t{rms}(r)$, which is equivalent to $\\BC\\t{minimize}_Œ∏&\\n{r}^2=\\n{y-\\hat{y}}^2\\EC$.\n",
        "\n",
        "- **Design matrix** $A‚àà‚Ñù^{N√óp}$ where $\\red{A_{ij}=f_j(x_i)}$ encapsulates the $p$ basis functions evaluated at $N$ feature variables.\n",
        "Then $\\hat{y}=AŒ∏$ and\n",
        "$\\BC\\t{minimize}_Œ∏&\\n{r}^2=\\n{y-AŒ∏}^2\\EC$ gives $\\blue{\\hat{Œ∏}=A^‚Ä†y}$.\n",
        "\n",
        "  - **Residual sum of squares** $\\red{\\t{RSS}=\\n{r}^2}$. **Mean square error** $\\red{\\t{MSE}=\\/{\\n{r}^2}{N}}$.\n",
        "  **RMS error** $\\red{\\sqrt{\\/{\\n{r}^2}{N}}}$.\n",
        "\n",
        "  - Columns of $A$ are linearly independent. No basis function $f_i$ is redundant.\n",
        "\n",
        "- **Fitting a constant**: $\\red{\\hat{f}(x_i)=Œ∏_1}$, then $f_1(x_i)=1$, and $A=\\v{1}=\\BM1\\\\1\\\\\\vdots\\\\1\\EM$.\n",
        "Solving $\\BC\\t{minimize}_{Œ∏_1}&\\n{AŒ∏_1-y}^2\\EC$\n",
        "we have $\\hat{Œ∏}_1=A^‚Ä†y$\n",
        "$=(A^‚ä§A)^{-1}A^‚ä§y$\n",
        "$=\\/{\\v{1}^‚ä§y}{\\v{1}^‚ä§\\v{1}}$\n",
        "$=\\/{\\sum_iy_i}{N}$\n",
        "$=\\bar{y}$\n",
        "and $\\hat{y}=\\bar{y}\\v{1}$.\n",
        "\n",
        "  - $r=y-\\bar{y}\\v{1}$.\n",
        "  Then $\\t{MSE}=\\/{1}{N}\\n{y-\\bar{y}\\v{1}}^2$\n",
        "  $=\\/{1}{N}\\sum_i(y_i-\\bar{y})^2$\n",
        "  $‚âà\\t{std}(y)^2$.\n",
        "\n",
        "- **Linear regression**: Let feature vector $x_i^‚ä§‚àà‚Ñù^{1√ón}$ be the $i$-th row of feature matrix $X‚àà‚Ñù^{N√ón}$. We have $f_1(x_i)=1$ and $f_j(x_i)=x_{i,j-1}\\ ‚àÄj=2,...,n+1$. Then $\\hat{y}_i=x_i^‚ä§Œ∏_{2:n+1}+Œ∏_1$ and $\\hat{y}=AŒ∏$ where $A=\\BM\\v{1}&X\\EM‚àà‚Ñù^{N√ó(n+1)}$.\n",
        "\n",
        "  - Any complex linear in parameters model can be converted to $\\tilde{x}_i=\\BM f_2(x_i)\\\\\\vdots\\\\f_p(x_i)\\EM$. For example **polynomial fit** $\\hat{f}(x_i)=Œ∏_1+Œ∏_2x_i+...+Œ∏_px_i^{p-1}$ uses $\\tilde{x_i}=\\BM x_i\\\\\\vdots\\\\x_i^{p-1}\\EM$ and $A=\\BM1&x_1&...&x_1^{p-1}\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\1&x_N&...&x_N^{p-1}\\EM=\\BM1&\\tilde{x}_{1,1}&...&\\tilde{x}_{1,n}\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\1&\\tilde{x}_{N,1}&...&\\tilde{x}_{N,n}\\EM$.\n",
        "\n",
        "  - **AR time series**: Given time series $z_1,...,z_M,z_{M+1},...,z_T$ use $\\red{\\hat{z}_{t+1}=Œ∏_1z_t+...+Œ∏_Mz_{t-M+1}}$ to construct $\\hat{y}=(\\hat{z}_{M+1},...,\\hat{z}_T)=AŒ∏$ where $A=\\BM z_M&z_{M-1}&...&z_1\\\\z_{M+1}&z_M&...&z_2\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\z_{T-1}&z_{T-2}&...&z_{T-M}\\EM$ and compare with $y=(z_{M+1},...,z_T)$.\n",
        "\n",
        "**Simple linear regression $x_i‚àà‚Ñù$**: $\\red{\\hat{f}(x_i)=Œ∏_1+Œ∏_2x_i}$, then $f_1(x_i)=1$, $f_2(x_i)=x_i$, and $A=\\BM\\v{1}&x\\EM=\\BM1&x_1\\\\1&x_2\\\\\\vdots&\\vdots\\\\1&x_N\\EM$. Solving $\\BC\\t{minimize}_Œ∏&\\n{AŒ∏-y}^2\\EC$ we have $\\hat{Œ∏}=A^‚Ä†y$ where $A^‚ä§A=\\BM N&\\v{1}^‚ä§x\\\\\\v{1}^‚ä§x&x^‚ä§x\\EM$ and\n",
        "$A^‚ä§y=\\BM\\v{1}^‚ä§y\\\\x^‚ä§y\\EM$.\n",
        "\n",
        "- $\\BM\\hat{Œ∏}_1\\\\\\hat{Œ∏}_2\\EM=\\/{1}{Nx^‚ä§x-(\\v{1}^‚ä§x)^2}\\BM x^‚ä§x&-\\v{1}^‚ä§x\\\\-\\v{1}^‚ä§x&N\\EM\\BM\\v{1}^‚ä§y\\\\x^‚ä§y\\EM$\n",
        "$=\\/{1}{Nx^‚ä§x-N^2\\bar{x}^2}\\BM x^‚ä§x&-N\\bar{x}\\\\-N\\bar{x}&N\\EM\\BM N\\bar{y}\\\\x^‚ä§y\\EM$\n",
        "\n",
        "  - Sample covariance: $(n-1)C_{xy}=\\blue{\\sum_i(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_ix_iy_i-n\\bar{x}\\bar{y}}$.\n",
        "  \n",
        "  - $\\hat{Œ∏}_2=\\/{Nx^‚ä§y-N^2\\bar{x}\\bar{y}}{Nx^‚ä§x-N^2\\bar{x}^2}$\n",
        "  $=\\/{\\sum_i(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_i(x_i-\\bar{x})^2}$\n",
        "  $=\\/{\\t{cov}(x,y)}{\\t{std}(x)^2}$\n",
        "  $‚áí\\blue{\\hat{Œ∏}_2=\\/{\\t{std}(y)}{\\t{std}(x)}\\t{corr}(x,y)}$\n",
        "\n",
        "  - $\\hat{Œ∏}_1$ ensures $\\green{\\bar{y}=\\hat{Œ∏}_1+\\hat{Œ∏}_2\\bar{x}}$ the fitted line goes through $(\\bar{x},\\bar{y})$.\n",
        "  Therefore $\\hat{f}(x)=\\hat{Œ∏}_1+\\hat{Œ∏}_2x$\n",
        "  $=\\bar{y}-\\hat{Œ∏}_2\\bar{x}+\\hat{Œ∏}_2x$\n",
        "  $‚áí\\red{\\hat{y}_i=\\bar{y}+\\hat{Œ∏}_2(x_i-\\bar{x})}$\n",
        "\n",
        "  - $\\hat{y}=\\bar{y}+\\hat{Œ∏}_2(x-\\bar{x})‚áí\\/{\\hat{y}-\\bar{y}}{\\t{std}(y)}=\\t{corr}(x,y)\\/{x-\\bar{x}}{\\t{std}(x)}‚áíZ_{\\hat{y}}=œÅ_{xy}Z_{x}$\n",
        "\n",
        "- **Asset $Œ±,Œ≤$**: Let $r^\\t{rf}$ be risk-free rate, $Œº^\\t{mkt}$ be average index return. Then for index return $x$, we have predicted asset return $\\hat{f}(x)=(r^\\t{rf}+Œ±)+Œ≤(x-Œº^\\t{mkt})$.\n",
        "\n",
        "- **Linear trend vs periodic**: $y‚âà\\hat{y}^\\t{lin}+\\hat{y}^\\t{seas}$ where $\\hat{y}^\\t{lin}=Œ∏_1\\BM1\\\\2\\\\\\vdots\\\\N\\EM$.\n",
        "Use $A=\\BM1&1&0&...&0\\\\2&0&1&...&0\\\\\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\P&0&0&...&1\\\\P+1&1&0&...&0\\\\\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\2P&0&0&...&1\\\\\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\EM‚àà‚Ñù^{N√ó(P+1)}$ and $Œ∏‚àà‚Ñù^{P+1}$ where $P=12$ months.\n",
        "Given $y$ (no $x$), monthly pattern is captured in $Œ∏_{2:P+1}$ as intercepts in $\\hat{Œ∏}=A^‚Ä†y$, as rows of $A$ are $a_i^‚ä§=\\BM i&(\\t{one hot encoder for }i\\%(P+1))\\EM$.\n",
        "\n",
        "**Validation**: validate model predictive ability. Commonly using 80%-20% training-test set split. We should choose the model with the smallest RMS prediction error on the test set.\n",
        "\n",
        "- **Cross validation**: Divide the dataset into 5 folds. Repeat fitting 5 times, each time rotating which fold is the test set. You end up with 5 different models and 5 separate error assessments. Evaluate whether model parameters and test errors found in each round are similar. Consolidate individual test errors into RMS cross-validation error $\\sqrt{(\\sum_{i=1}^5e_i^2)/5}$. To get final parameters either retrain on all 5 folds or average the 5 rounds.\n",
        "\n",
        "- **AR validation**: splitting data into training-test sets randomly leads to chronologically mixed testing. Use training set up to some point in time and leave a gap between training and test sets to ensure complete separation."
      ],
      "metadata": {
        "id": "rErxM4-DRAm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 15: Multi-objective least squares**: $k$ objectives\n",
        "$\\red{\\BC\\t{minimize}_x&J_1=\\n{A_1x-b_1}^2,\\ ...,\\ J_k=\\n{A_kx-b_k}^2\\EC}$ where $A_i‚àà‚Ñù^{m_i√ón},\\ b_i‚àà‚Ñù^{m_i}$.\n",
        "Weighed objective $J=\\sum_{i=1}^kŒª_i\\n{A_ix-b_i}^2=\\n{\\BM\\sqrt{Œª_1}(A_1x-b_1)\\\\\\vdots\\\\\\sqrt{Œª_k}(A_kx-b_k)\\EM}^2$\n",
        "$=\\n{\\BM\\sqrt{Œª_1}A_1\\\\\\vdots\\\\\\sqrt{Œª_k}A_k\\EM x-\\BM\\sqrt{Œª_1}b_1\\\\\\vdots\\\\\\sqrt{Œª_k}b_k\\EM}^2$\n",
        "$‚áí\\red{J=\\n{\\tilde{A}x-\\tilde{b}}^2}$\n",
        "where $\\red{\\tilde{A}=\\BM\\sqrt{Œª_1}A_1\\\\\\vdots\\\\\\sqrt{Œª_k}A_k\\EM}‚àà‚Ñù^{\\sum_{i=1}^km_i√ón}$ and $\\red{\\tilde{b}=\\BM\\sqrt{Œª_1}b_1\\\\\\vdots\\\\\\sqrt{Œª_k}b_k\\EM}‚àà‚Ñù^{\\sum_{i=1}^km_i}$.\n",
        "Provided columns of $\\tilde{A}$ are linearly independent, then $\\hat{x}=(\\tilde{A}^‚ä§\\tilde{A})^{-1}\\tilde{A}^‚ä§\\tilde{b}=\\blue{(\\sum_{i=1}^kŒª_iA_i^‚ä§A_i)^{-1}(\\sum_{i=1}^kŒª_iA_i^‚ä§b_i)}$\n",
        "\n",
        "- $‚àÉA_i:\\rk(A_i)=n‚áí\\rk(\\tilde{A})=n$.\n",
        "If one submatrix has linearly independent columns then so does $\\tilde{A}$.\n",
        "\n",
        "- **Estimation and inversion**: given $y‚àà‚Ñù^m$ and $A‚àà‚Ñù^{m√ón}$ we want to estimate $x‚àà‚Ñù^n$ in $y=Ax+v$ with measurement noise $v‚àà‚Ñù^m$.\n",
        "If $v=0$ and columns of $A$ are independent then $x=A^‚Ä†y$ gives an exact result.\n",
        "In practice we assume $v=y-A\\hat{x}$ is small therefore $J_1=\\n{Ax-y}^2$.\n",
        "\n",
        "  - $\\n{x}^2$ corresponds to prior assumption that $x$ is likely to be small than large.\n",
        "\n",
        "  - $\\n{x-x^\\t{des}}^2$ assumes some prior knowledge in desired result $x^\\t{des}$.\n",
        "\n",
        "  - $\\n{Dx}^2$ uses a difference matrix to force $x_{i+1}$ to be closer to $x_i$, often when $x$ represents time series.\n",
        "\n",
        "- **Tikhonov regularization**: $\\red{\\BC\\t{minimize}&\\n{Ax-y}^2+Œª\\n{x}^2\\EC}$ gives $\\tilde{A}=\\BM A\\\\\\sqrt{Œª}I\\EM$ and $\\blue{\\hat{x}=(A^‚ä§A+ŒªI)^{-1}A^‚ä§b}$.\n",
        "\n",
        "  - $\\tilde{A}$ always has linearly independent columns without any assumption about $A$.\n",
        "\n",
        "- **Periodic signals**: Given noisy $y‚àà‚Ñù^T$ based on a signal that repeats every $P$ periods (assuming $T\\%P=0$), we want to find $x‚àà‚Ñù^P$ that represents one ideal cycle.\n",
        "$A=\\BM I_P\\\\\\vdots\\\\I_P\\EM$ will extract seasonal component of $y$ via $\\hat{y}=\\BM \\hat{x}\\\\\\vdots\\\\\\hat{x}\\EM$ through $\\BC\\t{minimize}&\\n{Ax-y}^2\\EC$ by averaging corresponding entries of $y$ for each $x_i$.\n",
        "\n",
        "  - Wrap-around difference matrix: $x_1‚âàx_2,\\ ...,\\ x_P‚âàx_1$ uses $D^\\t{circ}$ with pattern $[0,-1,1]$ to apply smoothing.\n",
        "  $\\BC\\t{minimize}&\\n{Ax-y}^2+Œª\\n{D^\\t{circ}x}^2\\EC$.\n",
        "  The value of $Œª$ can be chosen via validation.\n",
        "\n",
        "**Regularized data fitting**: $\\hat{f}(x)=\\sum_{i=1}^pŒ∏_if_i(x)$. When $Œ∏_i=\\/{‚àÇ\\hat{f}}{‚àÇf_i}$ is large, the prediction is sensitive to small perturbation in input, except $Œ∏_i$ if $f_i(x)=1$ such as intercept $Œ∏_1$.\n",
        "**Ridge regression**:\n",
        "$\\red{\\BC\\t{minimize}&\\n{y-AŒ∏}^2+Œª\\n{Œ∏_{2:p}}^2\\EC}$.\n",
        "\n",
        "- **Regularization path** describes how $Œ∏$ change with $Œª$ selection. $Œª$ is selected through cross-validation. Training set error worsens with higher $Œª$. Test set error typically first decreases with reduced over-fitting before increasing with over-bias.\n",
        "\n",
        "  1. Minimize for many different choices of $Œª$ increasing in logarithmic steps.\n",
        "  \n",
        "  2. **Pick the $Œª$ that results in minimum test set RMS error**.\n",
        "\n",
        "  - Traditionally avoiding overfitting required ensuring the number of features was no more than 10-20% compared to the number of data points. Regularization allows solving problems with more features than data points.\n",
        "\n",
        "- $\\tilde{A}=\\BM A\\\\\\sqrt{Œª}B\\EM$ where $B‚àà‚Ñù^{(p-1)√óp}$ is truncated $I_p$ with top row shaved off always has linearly independent columns.\n",
        "\n",
        "**Computation**: $\\hat{x}=(\\sum_{i=1}^kŒª_iA_i^‚ä§A_i)^{-1}(\\sum_{i=1}^kŒª_iA_i^‚ä§b_i)$.\n",
        "Assume $L$ choices of $Œª$. Then total complexity is $\\m{O}(Lmn^2)$ flops.\n",
        "\n",
        "- **Gram caching**: $G_i=A_i^‚ä§A_i$ and $h_i=A_i^‚ä§b_i$ can be cached for subsequent choices of $Œª$.\n",
        "Then total complexity $\\m{O}((m+kL+2Ln)n^2)$.\n",
        "\n",
        "- **Kernel trick**: Let $A$ be wide $m‚â™n$. Then $J=\\n{Ax-b}^2+Œª\\n{x-x^\\t{des}}^2$ and $\\hat{x}=(A^‚ä§A+ŒªI)^{-1}(A^‚ä§b+Œªx^\\t{des})$\n",
        "$=x^\\t{des}+A^‚ä§(AA^‚ä§+ŒªI)^{-1}(b-Ax^\\t{des})$\n",
        "for total complexity $\\m{O}((2Lm+n)m^2)$ flops.\n",
        "\n",
        "  - **Push-through matrix**: $A^‚ä§(AA^‚ä§+ŒªI_m)=A^‚ä§AA^‚ä§+ŒªA^‚ä§$\n",
        "  $=(A^‚ä§A+ŒªI_n)A^‚ä§$\n",
        "  $‚áí\\green{A^‚ä§(AA^‚ä§+ŒªI_m)=(A^‚ä§A+ŒªI_n)A^‚ä§}$\n",
        "  $‚áí(A^‚ä§A+ŒªI)^{-1}A^‚ä§(AA^‚ä§+ŒªI)=(A^‚ä§A+ŒªI)^{-1}(A^‚ä§A+ŒªI)A^‚ä§=A^‚ä§$\n",
        "  $‚áí(A^‚ä§A+ŒªI)^{-1}A^‚ä§(AA^‚ä§+ŒªI)(AA^‚ä§+ŒªI)^{-1}=A^‚ä§(AA^‚ä§+ŒªI)^{-1}$\n",
        "  $‚áí\\blue{(A^‚ä§A+ŒªI_n)^{-1}A^‚ä§=A^‚ä§(AA^‚ä§+ŒªI_m)^{-1}}$\n",
        "\n",
        "  - $A^‚ä§b+Œªx^\\t{des}=A^‚ä§b+Œªx^\\t{des}+A^‚ä§Ax^\\t{des}-A^‚ä§Ax^\\t{des}$\n",
        "  $=(A^‚ä§A+ŒªI)x^\\t{des}+A^‚ä§(b-Ax^\\t{des})$.\n",
        "  Therefore\n",
        "  $\\hat{x}=(A^‚ä§A+ŒªI)^{-1}(A^‚ä§b+Œªx^\\t{des})$\n",
        "  $=(A^‚ä§A+ŒªI)^{-1}((A^‚ä§A+ŒªI)x^\\t{des}+A^‚ä§(b-Ax^\\t{des}))$\n",
        "  $=(A^‚ä§A+ŒªI)^{-1}(A^‚ä§A+ŒªI)x^\\t{des}+(A^‚ä§A+ŒªI)^{-1}A^‚ä§(b-Ax^\\t{des})$\n",
        "  $=x^\\t{des}+A^‚ä§(AA^‚ä§+ŒªI)^{-1}(b-Ax^\\t{des})$"
      ],
      "metadata": {
        "id": "oR7MteymlnMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 16: Constrained least squares**: $\\red{\\BC\\t{minimize}&\\n{Ax-b}^2&A‚àà‚Ñù^{m√ón}\\\\\n",
        "\\t{subject to}&Cx=d&C‚àà‚Ñù^{p√ón}\n",
        "\\EC}‚áí\\BC\\t{minimize}&\\n{Ax-b}^2+Œª\\n{Cx-d}^2\\EC$\n",
        "\n",
        "- **KKT equations**: Lagrange $L(x,z)=\\n{Ax-b}^2+z^‚ä§(Cx-d)$\n",
        "$‚áí‚àá_xL=2(Ax-b)^‚ä§A+z^‚ä§C=0$\n",
        "$‚áí2A^‚ä§Ax+C^‚ä§z=2A^‚ä§b$.\n",
        "Therefore $\\blue{\\BM2A^‚ä§A&C^‚ä§\\\\C&0\\EM\n",
        "\\BM\\hat{x}\\\\\\hat{z}\\EM=\n",
        "\\BM2A^‚ä§b\\\\d\\EM}$.\n",
        "\n",
        "  - KKT matrix $\\BM2A^‚ä§A&C^‚ä§\\\\C&0\\EM‚àà‚Ñù^{(n+p)√ó(n+p)}$ is invertible iff $C$ has linearly independent rows ($m‚â§n$) and $\\BM A\\\\C\\EM$ has linearly independent columns (just $A$ having linearly independent columns is sufficient).\n"
      ],
      "metadata": {
        "id": "zSKy7RK-x56U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Statistical Estimation"
      ],
      "metadata": {
        "id": "c07ISyoYwIsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximum likelihood (ML) estimation**: We have a family of distributions $p_x(y)$ on observed sample $y‚àà‚Ñù^m$ indexed by parameters $x‚àà‚Ñù^n$.\n",
        "When we consider $p_x(y)$ as a function of $x$ it is the likelihood function, and $l(x)=\\ln p_x(y)$ is the log-likelihood function.\n",
        "$\\hat{x}_\\t{ml}=\\arg\\max_xl(x)$\n",
        "is a convex optimization problem if $p_x(y)$ is log-concave.\n",
        "\n",
        "- Using measurement model $y=Ax+v$ where $A‚àà‚Ñù^{m√ón}$ where $v‚àà‚Ñù^m,\\ v_i‚àºp(z)$ is IID measurement noise.\n",
        "Then $p_x(y)=\\prod_{i=1}^mp(y_i-a_i^‚ä§x)$\n",
        "$‚áí\\red{l(x)=\\sum_{i=1}^m\\ln p(y_i-a_i^‚ä§x)}$\n",
        "$‚áí\\BC\\t{maximize}&\\sum_{i=1}^m\\ln p(y_i-a_i^‚ä§x)\\EC$\n",
        "\n",
        "  - **Gaussian noise**: $\\blue{v_i‚àºp(z)=\\/{1}{\\sqrt{2œÄ}œÉ}e^{-z^2/2œÉ^2}}$ then\n",
        "  $l(x)=-\\/{m}{2}\\ln(2œÄœÉ^2)-\\/{1}{2œÉ^2}\\sum_i(y_i-a_i^‚ä§x)^2$\n",
        "  $‚áí\\blue{l(x)=-\\/{m}{2}\\ln(2œÄœÉ^2)-\\/{1}{2œÉ^2}\\n{y-Ax}_2^2}$.\n",
        "  Noise density for $‚Ñì_2$ norm approximation.\n",
        "\n",
        "  - **Laplacian noise**: $\\blue{v_i‚àºp(z)=\\/{1}{2a}e^{-|z|/a}}$ then\n",
        "  $l(x)=-m\\ln(2a)-\\/{1}{a}\\sum_i|y_i-a_i^‚ä§x|$\n",
        "  $‚áí\\blue{l(x)=-m\\ln(2a)-\\/{1}{a}\\n{y_i-a_i^‚ä§x}_1}$.\n",
        "  Noise density for $‚Ñì_1$ norm approximation.\n",
        "\n",
        "  - **Uniform noise**: $p(z)=\\/{1}{2a}I_{|z|‚â§a}$ then\n",
        "  $p_x(y)=\\/{1}{(2a)^m}I_{\\max_i|y_i-a_i^‚ä§x|‚â§a}$\n",
        "  $=\\/{1}{(2a)^m}I_{\\n{Ax-y}_‚àû‚â§a}$.\n",
        "  Here $\\hat{x}$ is any $x$ satisfying $\\n{Ax-y}_‚àû‚â§a$\n",
        "\n",
        "- Penalty optimization problem $\\BC\\t{minimize}&\\sum_iœï(b_i-a_i^‚ä§x)\\EC$ can be interpreted as MLE\n",
        "$l(x)=\\sum_iœï(b_i-a_i^‚ä§x)$\n",
        "$‚áí\\red{p(z)=\\/{e^{-œï(z)}}{‚à´e^{-œï(u)}\\ du}}$.\n",
        "Large outlier residuals have tail densities.\n",
        "\n",
        "  - $\\BC\\t{minimize}&\\sum_iœï(z_i)\\EC$\n",
        "  $‚áí\\BC\\t{maximize}&\\sum_i(-œï(z_i))\\EC$\n",
        "  $‚áí\\BC\\t{maximize}&\\sum_i\\/{1}{C}e^{-œï(z_i)}\\EC$\n",
        "  $‚áí\\BC\\t{maximize}&\\sum_ip(z_i)\\EC$\n",
        "\n",
        "  - $‚Ñì_1$-norm corresponds to Laplacian noise, which have fat tails and assigns higher densities to outliers.\n",
        "  The $‚Ñì_1$ ML model is not as surprised and does not shift the entire fit to accommodate them - more robust.\n",
        "\n",
        "  - $p(z)=\\/{e^{-œï(z)}}{‚à´e^{-œï(u)}\\ du}$ converts any penalty (e.g., Huber) minimization into a MLE problem.\n",
        "  Huber norm approximation would transform into a noise density whose peak is Gaussian and whose tails are exponential.\n",
        "\n",
        "- **Poisson fitting**: $P(y_i=k)=\\/{e^{-Œº_i}Œº_i^k}{k!}$ where $Œº_i=a^‚ä§u_i+b$ for parameters $a‚àà‚Ñù^n,\\ b‚àà‚Ñù$.\n",
        "Given observed sample response and feature vectors $y‚àà‚Ñù^m,\\ u‚àà‚Ñù^m,\\ u_i\\in‚Ñù^n$, find MLE parameters.\n",
        "$l(a,b)=\\sum_{i=1}^m[-(a^‚ä§u_i+b)+y_i\\ln(a^‚ä§u_i+b)]$.\n",
        "Then\n",
        "$\\blue{\\BC\\t{maximize}_{(a,b)}&\\sum_{i=1}^m[y_i\\ln(a^‚ä§u_i+b)-(a^‚ä§u_i+b)]\\EC}$.\n",
        "\n",
        "- **Logistic regression**: Let $y_i‚àà\\{0,1\\}$ where latent $\\E[y_i]=p_i$ is a function of features $u_i‚àà‚Ñù^n$ with logistic sigmoid $p_i=\\/{\\e{a^‚ä§u_i+b}}{1+\\e{a^‚ä§u_i+b}}$.\n",
        "Likelihood\n",
        "$p_{a,b}(y)=\\prod_{i=1}^mp_i^{y_i}(1-p_i)^{1-y_i}$\n",
        "$‚áíl(a,b)=\\sum_{i=1}^m[y_i\\ln p_i+(1-y_i)\\ln(1-p_i)]$\n",
        "$=\\sum_{i=1}^m[y_i(a^‚ä§u_i+b)-y_i\\ln(1+\\e{a^‚ä§u_i+b})-(1-y_i)\\ln(1+\\e{a^‚ä§u_i+b})]$\n",
        "$=\\sum_{i=1}^my_i(a^‚ä§u_i+b)-\\sum_{i=1}^m\\ln(1+\\e{a^‚ä§u_i+b})$.\n",
        "Given $y‚àà‚Ñù^m,\\ u‚àà‚Ñù^m,\\ u_i‚àà‚Ñù^n$, finding MLE parameters $a,b$ is a convex problem\n",
        "$\\blue{\\BC\\t{maximize}_{(a,b)}\\sum_{i=1}^my_i(a^‚ä§u_i+b)-\\sum_{i=1}^m\\ln(1+\\e{a^‚ä§u_i+b})\\EC}$\n",
        "\n",
        "- **Gaussian covariance estimation**: Given $y_i‚àà‚Ñù^n,\\ y_i‚àº\\Normal(0,R)$, we want MLE covariance matrix $R=\\E[y_iy_i^‚ä§]$.\n",
        "Likelihood $p_R(y_i)=\\/{1}{(2œÄ)^{n/2}\\det(R)^{1/2}}\\e{\\/{-1}{2}y_i^‚ä§R^{-1}y_i}$\n",
        "$‚áíp_R(y)=\\/{1}{(2œÄ)^{Nn/2}\\det(R)^{N/2}}\\e{\\/{-1}{2}\\sum_{i=1}^Ny_i^‚ä§R^{-1}y_i}$\n",
        "$‚áíl(R)=-\\/{N}{2}\\ln\\det(R)-\\/{1}{2}\\sum_{i=1}^Ny_i^‚ä§R^{-1}y_i$\n",
        "$=-\\/{N}{2}\\ln\\det(R)-\\/{1}{2}\\purple{\\tr(\\sum_{i=1}^Ny_i^‚ä§R^{-1}y_i)}$\n",
        "$=-\\/{N}{2}\\ln\\det(R)-\\/{1}{2}\\tr(R^{-1}\\sum_{i=1}^Ny_iy_i^‚ä§)$\n",
        "which is not concave.\n",
        "Let information matrix $S=R^{-1}$ and $Y=\\sum_{i=1}^Ny_iy_i^‚ä§$, then\n",
        "$l(S)=\\/{N}{2}\\ln\\det(S)-\\/{1}{2}\\tr(SY)$\n",
        "is concave.\n",
        "\n",
        "  - For unconstrained problem $\\blue{\\BC\\t{maximize}_S&\\ln\\det(S)-\\tr(SY)\\EC}$, we have analytical solution\n",
        "  $‚àá_S(\\ln\\det(S)-\\tr(SY))$\n",
        "  $=\\/{\\det(S)S^{-1}}{\\det(S)}-Y$\n",
        "  $=R-Y=0$\n",
        "  $‚áí\\blue{\\hat{R}=\\sum_{i=1}^Ny_iy_i^‚ä§}$.\n",
        "\n",
        "  - Matrix bound $\\BC\n",
        "  \\t{maximize}_S&\\ln\\det(S)-\\tr(SY)\\\\\n",
        "  \\t{subject to}&U^{-1}‚âºS‚âºL^{-1}\n",
        "  \\EC$.\n",
        "  Condition number $\\BC\n",
        "  \\t{maximize}_{(S,u)}&\\ln\\det(S)-\\tr(SY)\\\\\n",
        "  \\t{subject to}&uI‚âºS‚âºŒ∫_\\max uI\n",
        "  \\EC$\n",
        "\n",
        "**Maximum a posteriori (MAP) estimation**: Bayesian MLE. Assume parameter $x$ and observed samples $y$ have joint density $p(x,y)$.\n",
        "Prior $p_x(x)=‚à´p(x,y)\\ dy$.\n",
        "Evidence $p_y(y)=‚à´p(x,y)\\ dx$.\n",
        "Likelihood $p_{y|x}(x,y)=\\/{p(x,y)}{p_x(x)}$.\n",
        "Then posterior $p_{x|y}(x,y)=\\/{p(x,y)}{p_y(y)}=p_{y|x}\\/{p_x(x)}{p_y(y)}$.\n",
        "Then MAP estimate is $\\red{\\hat{x}=\\arg\\max_xp_{x|y}(x,y)}$. Because denominator $p_y(y)$ does not depend on $x$, we have $\\blue{\\hat{x}=\\arg\\max_xp(x,y)=\\arg\\max_x[\\ln p_{y|x}(x,y)+\\ln p_x(x)]}$.\n",
        "\n",
        "- **Linear measurements with noise**: Suppose $y_i=a_i^‚ä§x+v_i$ where $v_i‚àºp_v$ IID. Given $y‚àà‚Ñù^m$, finding MAP $\\hat{x}‚àà‚Ñù^n$ using $p(x,y)=p_x(x)\\prod_{i=1}^mp_v(y_i-a_i^‚ä§x)$ is a convex problem\n",
        "$\\BC\\t{maximize}&\\ln p_x(x)+\\sum_{i=1}^m\\ln p_v(y_i-a_i^‚ä§x)\\EC$\n",
        "\n",
        "  - **Deterministic setup**: Suppose $v_i=0$. Then\n",
        "  $p(x,y)=p_x(x)$\n",
        "  $=\\sum_i\\ln p(x_i)$\n",
        "  $=\\sum_i\\ln\\/{e^{-œï(x_i)}}{‚à´e^{œï(u)}\\ du}$\n",
        "  $=\\sum_i(-œï(x_i))$.\n",
        "  $\\BC\\t{maximize}&\\sum_ip(x_i)\\\\\n",
        "  \\t{subject to}&Ax=b\\EC‚áí\\BC\\t{minimize}&\\sum_{i=1}^nœï(x_i)\\\\\\t{subject to}&Ax=b\\EC$."
      ],
      "metadata": {
        "id": "j28wm5aOwOg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nonparametric distribution estimation** estimates the probability distribution $\\red{\\{p‚àà‚Ñù^n|p‚âΩ0,\\v{1}^‚ä§p=1\\}}$ itself. Let $X$ take values from $\\{Œ±_1,...,Œ±_n\\}‚äÜ‚Ñù$ such that $P(X=Œ±_i)=p_i$.\n",
        "\n",
        "- Prior constraints/information $p‚àà\\m{P}$\n",
        "\n",
        "  - **Moments**: LOTUS $\\E[f(X)]=\\sum_ip_if(Œ±_i)$ implies all moments are a linear function of $p$.\n",
        "\n",
        "  - **Variance** is concave $\\Var(X)=\\E[X^2]-(\\E[X])^2=\\sum_iŒ±_i^2p_i-(\\sum_iŒ±_ip_i)^2$.\n",
        "  Lower bound on variance $\\Var(X)‚â•œÉ_\\min^2$ is a sub-level set, which is convex. Upperbound constraint is **NOT** convex.\n",
        "\n",
        "  - **Probability** $P(X‚ààC)=c^‚ä§p,\\ c_i=I_{Œ±_i‚ààC}$ is a linear function of $p$.\n",
        "\n",
        "  - **Conditional probability** $P(X‚ààA|X‚ààB)=\\/{P(X‚ààA‚à©B)}{P(X‚ààB)}=\\/{c^‚ä§p}{d^‚ä§p},\\ c_i=I_{Œ±_i‚ààA‚à©B},\\ d_i=I_{Œ±_i‚ààB}$ is linear fractional form, which we can convert to linear constraint\n",
        "  $l‚â§P(X‚ààA|X‚ààB)‚â§u‚áíld^‚ä§p‚â§c^‚ä§p‚â§ud^‚ä§p$\n",
        "\n",
        "  - **Entropy** $-\\sum_ip_i\\ln(p_i)$ is concave function of $p$. We can only impose lowerbound convex constraint.\n",
        "\n",
        "  - **Kullback-Leibler (KL) Divergence** wrt reference distribution $q$ is $\\sum_ip_i\\ln(\\/{p_i}{q_i})$.\n",
        "  It is convex function of $p$ (and also $q$).\n",
        "  We can impose upperbound convex constraint.\n",
        "\n",
        "- **Bounding expectations**: $\\BC\n",
        "\\t{min/max}_p&\\sum_if(Œ±_i)p_i\\\\\n",
        "\\t{subject to}&p‚àà\\m{P}\n",
        "\\EC$ is often LP.\n",
        "\n",
        "- **MLE for $\\hat{p}$**: We observe $x_1,...,x_N‚àºX$ such that $Œ±_i$ appears $k_i$ times satisfying $\\sum_{i=1}^nk_i=N$.\n",
        "The density is $p(x_1,...,x_N)=\\prod_{i=1}^np_i^{k_i}$.\n",
        "Then $\\BC\n",
        "\\t{maximize}_p&\\sum_{i=1}^nk_i\\ln(p_i)\\\\\n",
        "\\t{subject to}&p‚àà\\m{P}\n",
        "\\EC$.\n",
        "\n",
        "- **Max entropy**: $\\BC\\t{minimize}&\\sum_ip_i\\ln(p_i)\\\\\n",
        "\\t{subject to}&p‚àà\\m{P}\n",
        "\\EC$\n",
        "\n",
        "- **Minimum KL divergence**: $\\BC\\t{minimize}&\\sum_ip_i\\ln(p_i/q_i)\\\\\n",
        "\\t{subject to}&p‚àà\\m{P}\n",
        "\\EC$"
      ],
      "metadata": {
        "id": "5kXFaJrtAbgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal detector design**: Suppose $X$ takes values in $\\{1,...,n\\}$, depends on parameter $Œ∏‚àà\\{1,...,m\\}$, and has distribution $\\blue{P‚àà‚Ñù^{n√óm},\\ p_{kj}=P(X=k|Œ∏=j)}$.\n",
        "Estimating $Œ∏$ based on observed sample of $X$ is hypothesis testing, also called detection because parameters are used to indicate abnormal context.\n",
        "\n",
        "- Deterministic detector $\\hat{Œ∏}=\\arg\\max_jp_{kj}$ or when the entries of $P$ are $\\{0,1\\}$.\n",
        "\n",
        "- **Randomized detector** $\\blue{T‚àà‚Ñù^{m√ón},\\ T_{ik}=P(\\hat{Œ∏}=i|X=k)}$.\n",
        "Every column $t_k$ is a distribution $t_k‚âΩ0,\\v{1}^‚ä§t_k=1$.\n",
        "\n",
        "- **Detection probability matrix** $\\blue{D=TP‚àà‚Ñù^{m√óm},\\ D_{ij}=(TP)_{ij}=P(\\hat{Œ∏}=i|Œ∏=j)}$.\n",
        "Off-diagonal entries are errors.\n",
        "Columns of $D$ are distributions $D_j‚âΩ0,\\ \\v{1}^‚ä§D_j=1$.\n",
        "\n",
        "  - Detection probabilities $P^d‚àà‚Ñù^m,\\ P_j^d=D_{jj}$.\n",
        "  Error probabilities $P^e=\\v{1}-P^d,\\ P_j^e=\\sum_{i\\neq j}D_{ij}$.\n",
        "\n",
        "  - $D$ is linear in detector $T$. Many objectives and constraints are convex in $D$ and therefore also convex in $T$.\n",
        "\n",
        "- **Scalarization**: We want to minimize off-diagonal entries of $D$\n",
        "$\\BC\\t{minimize wrt }‚Ñù_+^{m√óm}&D_{ij},\\ i\\neq j\\\\\n",
        "\\t{subject to}&t_k‚âΩ0,\\v{1}^‚ä§t_k=1\n",
        "\\EC$\n",
        "scalarized by loss matrix $W‚àà‚Ñù^{m√óm},\\ W_{ii}=0,\\ W_{ij}>0$ as weight.\n",
        "Then objective is $\\sum_{ij}W_{ij}D_{ij}=\\tr(W^‚ä§D)$\n",
        "$=\\tr(W^‚ä§TP)$\n",
        "$=\\tr(PW^‚ä§T)$\n",
        "$=\\sum_{k=1}^nc_k^‚ä§t_k$\n",
        "where $c_k$ is the $k$-th column of $WP^‚ä§‚àà‚Ñù^{m√ón}$.\n",
        "\n",
        "  - Upon observing $X=k$, $\\BC\n",
        "  \\t{minimize}_{t_k}&c_k^‚ä§t_k\\\\\n",
        "  \\t{subject to}&t_k‚âΩ0,\\v{1}^‚ä§t_k=1\n",
        "  \\EC$,\n",
        "  where $\\inf\\{c_k^‚ä§t_k|t_k‚âΩ0,\\v{1}^‚ä§t_k=1\\}=\\min_jc_{kj}$ by variational characterization. Then $\\blue{t_k^*=e_\\hat{Œ∏},\\ \\hat{Œ∏}=\\arg\\min_j(WP^‚ä§)_{jk}}$ is a deterministic detector for a given $W$.\n",
        "\n",
        "**Binary hypothesis testing**: $Œ∏=\\BC\n",
        "1&X‚àºp‚àà‚Ñù^n&\\t{normal - test negative}\\\\\n",
        "2&X‚àºq‚àà‚Ñù^n&\\t{abnormal - test positive}\n",
        "\\EC$.\n",
        "$P=\\BM p&q\\EM$.\n",
        "Let error detection probabilities be\n",
        "$P_\\t{fp}=P(\\hat{Œ∏}=2|Œ∏=1)$ and\n",
        "$P_\\t{fn}=P(\\hat{Œ∏}=1|Œ∏=2)$.\n",
        "Then $D=TP=\\BM Tp&Tq\\EM=\\BM 1-P_\\t{fp}&P_\\t{fn}\\\\\n",
        "P_\\t{fp}&1-P_\\t{fn}\\EM$.\n",
        "\n",
        "- Scalarization: $\\BC\\t{minimize}_T\\t{ wrt }‚Ñù_+^2&(P_\\t{fp},P_\\t{fn})=((Tp)_2,(Tq)_1)\\\\\n",
        "\\t{subject to}&t_k‚âΩ0,\\v{1}^‚ä§t_k=1\n",
        "\\EC$.\n",
        "Let $W=\\BM 0&Œª\\\\1&0\\EM$ then\n",
        "$\\BC\\t{minimize}_T&(Tp)_2+Œª(Tq)_1\\\\\n",
        "\\t{subject to}&t_k‚âΩ0,\\v{1}^‚ä§t_k=1\n",
        "\\EC$ and\n",
        "$WP^‚ä§=\\BM 0&Œª\\\\1&0\\EM\\BM p_1&p_2\\\\q_1&q_2\\EM$\n",
        "$=\\BMŒªq_1&Œªq_2\\\\p_1&p_2\\EM$.\n",
        "\n",
        "- Given $X=k$, we have $\\hat{Œ∏}=\\arg\\min_j(Œªq_k,p_k)$\n",
        "and $t_k^*=e_\\hat{Œ∏}=\\BC (1,0)&q_k/p_k‚â§1/Œª\\\\(0,1)&q_k/p_k>1/Œª\\EC$.\n",
        "Test $T$ is deterministic LRT."
      ],
      "metadata": {
        "id": "YmRMsCrB8BcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment design**: estimating $x‚àà‚Ñù^n$ from $y=Ax+w‚àà‚Ñù^m,\\ w‚àº\\Normal(0,I)$ gives $\\hat{x}=(A^‚ä§A)^{-1}A^‚ä§y$.\n",
        "\n",
        "- **Estimation error** is $e=\\hat{x}-x$\n",
        "$=(A^‚ä§A)^{-1}A^‚ä§y-x$\n",
        "$=(A^‚ä§A)^{-1}A^‚ä§(Ax+w)-x$\n",
        "$‚áí\\red{e=(A^‚ä§A)^{-1}A^‚ä§w}$\n",
        "$‚àº\\Normal(0,E)$\n",
        "where variance $E=\\E[ee^‚ä§]$\n",
        "$=(A^‚ä§A)^{-1}A^‚ä§\\E[ww^‚ä§]A(A^‚ä§A)^{-1}$\n",
        "$‚áí\\blue{E=(A^‚ä§A)^{-1}}$\n",
        "defines an $Œ±$-level confidence ellipsoid\n",
        "$\\m{E}=\\{x|(x-\\hat{x})^‚ä§E^{-1}(x-\\hat{x})‚â§Œ≤\\}$\n",
        "where $Œ≤$ is a function of $Œ±$ and $n$.\n",
        "\n",
        "  - Ellipsoid $x^‚ä§Ax‚â§1$ has volume $‚àù\\/{1}{\\sqrt{\\det(A)}}$\n",
        "\n",
        "- Rows $a_i^‚ä§‚àà‚Ñù^n$ of **design matrix** $A‚àà‚Ñù^{m√ón}$ are chosen during planning phase to minimize the confidence ellipsoid.\n",
        "$v_1,...,v_p$ are possible vectors to choose from $\\sum_{j=1}^pm_j=m$ representing measurements and experiments.\n",
        "Error matrix\n",
        "$E=(A^‚ä§A)^{-1}$\n",
        "$=(\\sum_{i=1}^ma_ia_i^‚ä§)^{-1}$\n",
        "$=(\\sum_{j=1}^pm_jv_jv_j^‚ä§)^{-1}$.\n",
        "The goal of experiment design is to minimize variance of $q^‚ä§x$ through $E^*‚âºE$ so that $q^‚ä§E^*q‚â§q^‚ä§Eq\\ ‚àÄq$\n",
        "\n",
        "  - Integer program $\\BC\\t{minimize}_{m_j}\\t{ wrt }\\v{S}_+^n&E=(\\sum_{j=1}^pm_jv_jv_j^‚ä§)^{-1}\\\\\n",
        "  \\t{subject to}&m_j‚â•0,\\ m_j‚àà‚Ñ§,\\ \\sum_jm_j=m\n",
        "  \\EC$ is hard.\n",
        "\n",
        "  - Relaxed design: Let $Œª_j=\\/{m_j}{m}$ then\n",
        "  $E=\\/{1}{m}(\\sum_{j=1}^pŒª_jv_jv_j^‚ä§)^{-1}$.\n",
        "  $\\red{\\BC\\t{minimize}_{Œª}\\t{ wrt }\\v{S}_+^n&E=\\/{1}{m}(\\sum_{j=1}^pŒª_jv_jv_j^‚ä§)^{-1}\\\\\n",
        "  \\t{subject to}&Œª‚âΩ0,\\v{1}^‚ä§Œª=1\n",
        "  \\EC}$\n",
        "  is a convex optimization problem.\n",
        "  Rounding errors $|\\/{\\t{round}(mŒª_i)}{m}-Œª_i|‚â§\\/{1}{2m}$\n",
        "\n",
        "- **Scalarization**: the $\\v{S}_+^n$ matrix objective is scalarized.\n",
        "\n",
        "  - **D-optimal**: minimize $\\det(E)$ to minimize the volume of the confidence ellipsoid.\n",
        "  $\\BC\\t{minimize}_Œª&\\ln\\det(\\sum_{j=1}^pŒª_jv_jv_j^‚ä§)^{-1}\\\\\n",
        "  \\t{subject to}&Œª‚âΩ0,\\v{1}^‚ä§Œª=1\n",
        "  \\EC$ is convex because $\\ln\\det(X^{-1})=-\\ln\\det(X)$.\n",
        "\n",
        "  - **E-optimal** minimizes the worst-case spectral norm long-axis diameter of the ellipsoid $\\n{E}_2$. **A-optimal** minimizes the average total error $\\tr(E)$.\n",
        "\n",
        "- D-optimal dual: $\\blue{\\BC\\t{minimize}_x&\\ln\\det(\\sum_ix_iv_iv_i^‚ä§)\\\\\n",
        "\\t{subject to}&x‚âΩ0,\\v{1}^‚ä§x=1\n",
        "\\EC}$. $g(Œª,ŒΩ)=\\inf_x(-\\ln\\det(\\sum_ix_iv_iv_i^‚ä§)-Œª^‚ä§x+ŒΩ(\\v{1}^‚ä§x-1))$\n",
        "$=-ŒΩ+\\inf_x(-\\ln\\det(\\sum_ix_iv_iv_i^‚ä§)+(ŒΩ\\v{1}-Œª)^‚ä§x)$\n",
        "$=-ŒΩ-\\sup_x(-(ŒΩ\\v{1}-Œª)^‚ä§x+\\ln\\det(\\sum_ix_iv_iv_i^‚ä§))$.\n",
        "  \n",
        "  - Let $X=\\sum_ix_iv_iv_i^‚ä§$.\n",
        "  Then $‚àÇ_{x_i}\\ln\\det(X)$\n",
        "  $=X^{-‚ä§}‚àÇ_{x_i}X$\n",
        "  $=\\tr[X^{-1}(v_iv_i^‚ä§)]$\n",
        "  $=\\tr[v_i^‚ä§X^{-1}v_i]$.\n",
        "  Then $0=\\tr[v_i^‚ä§X^{-1}v_i]-(ŒΩ-Œª_i)$\n",
        "  $‚áí\\green{(ŒΩ-Œª_i)^*=v_i^‚ä§X^{-1}v_i}$\n",
        "  $‚áíŒΩ-v_i^‚ä§X^{-1}v_i‚â•0$\n",
        "  $‚áí\\green{v_i^‚ä§X^{-1}v_i‚â§ŒΩ}$.\n",
        "\n",
        "  - $g(Œª,ŒΩ)=-ŒΩ-\\sup_x(-\\sum_i(ŒΩ-Œª_i)x_i-\\ln\\det(X))$\n",
        "  $=-ŒΩ+\\sum_iv_i^‚ä§X^{-1}v_ix_i-\\ln\\det(X)$\n",
        "  $=-ŒΩ+\\tr(\\sum_iv_i^‚ä§X^{-1}v_ix_i)-\\ln\\det(X)$\n",
        "  $=-ŒΩ+\\tr(X^{-1}\\sum_ix_iv_iv_i^‚ä§)-\\ln\\det(X)$\n",
        "  $=-ŒΩ+\\tr(X^{-1}X)-\\ln\\det(X)$\n",
        "  $=-ŒΩ+n-\\ln\\det(X)$.\n",
        "\n",
        "  - $\\BC\\t{maximize}&-ŒΩ+n-\\ln\\det(X)\\\\\n",
        "  \\t{subject to}&v_i^‚ä§X^{-1}v_i‚â§ŒΩ\n",
        "  \\EC$.\n",
        "  Let $X^{-1}=ŒΩW,\\ W‚àà\\v{S}_{++}^n$\n",
        "  $‚áí\\BC\\t{maximize}&-ŒΩ+n+\\ln\\det(ŒΩW)\\\\\n",
        "  \\t{subject to}&v_i^‚ä§ŒΩWv_i‚â§ŒΩ\n",
        "  \\EC$\n",
        "  $‚áí\\BC\\t{maximize}&-ŒΩ+n+\\ln\\det(W)+n\\ln(ŒΩ)\\\\\n",
        "  \\t{subject to}&v_i^‚ä§Wv_i‚â§1\n",
        "  \\EC$.\n",
        "  $‚àÇ_ŒΩ(-ŒΩ+n+n\\ln(ŒΩ))=-1+\\/{n}{ŒΩ}=0$\n",
        "  $‚áíŒΩ^*=n$\n",
        "  $‚áí\\BC\\t{maximize}&-n+n+\\ln\\det(W)+n\\ln(n)\\\\\n",
        "  \\t{subject to}&v_i^‚ä§Wv_i‚â§1\n",
        "  \\EC$\n",
        "  $‚áí\\blue{\\BC\\t{maximize}_W&\\ln\\det(W)+n\\ln(n)\\\\\n",
        "  \\t{subject to}&v_i^‚ä§Wv_i‚â§1\n",
        "  \\EC}$"
      ],
      "metadata": {
        "id": "3GeQp9gJDVNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Geometric Problems"
      ],
      "metadata": {
        "id": "dyxTr_Y-wB37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Projection and distance**: Point $x_0$ vs set $C$\n",
        "\n",
        "- Distance $\\red{\\t{dist}(x_0,C)=\\inf\\{\\n{x_0-x}|x‚ààC\\}}$.\n",
        "\n",
        "- $P_C(x_0)‚ààC$ is projection of $x_0$ on $C$.\n",
        "Then $\\n{x_0-P_C(x_0)}=\\t{dist}(x_0,C)$.\n",
        "\n",
        "  - $\\red{P_C(x_0)=\\arg\\min\\{\\n{x_0-x}|x‚ààC\\}}$.\n",
        "\n",
        "  - $\\blue{\\BC\n",
        "  \\t{minimize}_x&\\n{x-x_0}\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0,\\ Ax=b\n",
        "  \\EC}$ finds $P_C(x_0)=x^*$\n",
        "\n",
        "- Hyperplane $C=\\{x|a^‚ä§x=b\\}=c_0+a^\\perp$ where $a^‚ä§c_0=b$.\n",
        "\n",
        "  - $\\blue{\\t{dist}(x_0,C)=\\/{|b-a^‚ä§x_0|}{\\n{a}_2}}$. Proof: Project the vector $c_0-x_0$ onto $a$ we have $(\\/{a^‚ä§(c_0-x_0)}{\\n{a}_2})\\/{a}{\\n{a}_2}$\n",
        "  \n",
        "  - $P_C(x_0)=x_0+(b-a^‚ä§x_0)\\/{a}{\\n{a}_2^2}$\n",
        "\n",
        "- **Separating hyperplane** between $x_0$ and $C$ is defined by normal $a=P_C(x_0)-x_0$ and midpoint $\\/{1}{2}(x_0+P_C(x_0))$. Then original distance is the projection magnitude of midpoint onto normal $b=(P_C(x_0)-x_0)^‚ä§\\/{1}{2}(x_0+P_C(x_0))$.\n",
        "Therefore $\\{x|(P_C(x_0)-x_0)^‚ä§(x-\\/{1}{2}(x_0+P_C(x_0)))=0\\}$\n",
        "\n",
        "- **Indicator and support**: $\\red{S_C(x)=\\sup_{y‚ààC}x^‚ä§y}$ and $\\red{I_C=\\BC0&x‚ààC\\\\‚àû&\\t{otherwise}\\EC}$.\n",
        "Then\n",
        "$\\BC\n",
        "\\t{minimize}_x&\\n{x-x_0}\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0,\\ Ax=b\n",
        "\\EC‚áí\\BC\n",
        "\\t{minimize}&\\n{x-x_0}\\\\\n",
        "\\t{subject to}&I_C(x)‚â§0\n",
        "\\EC‚áí\\BC\n",
        "\\t{minimize}&\\n{y}\\\\\n",
        "\\t{subject to}&I_C(x)‚â§0,\\ y=x-x_0\n",
        "\\EC$.\n",
        "\n",
        "  - $g(ŒΩ,Œª)=\\inf_{y,x}(\\n{y}+ŒªI_C(x)+ŒΩ^‚ä§(y-x+x_0))$\n",
        "  $=ŒΩ^‚ä§x_0+\\inf_x(ŒªI_C(x)-ŒΩ^‚ä§x)+\\inf_y(\\n{y}+ŒΩ^‚ä§y)$\n",
        "  $=ŒΩ^‚ä§x_0-\\sup_x(ŒΩ^‚ä§x-I_C(x))-\\sup_y(-ŒΩ^‚ä§y-\\n{y})$\n",
        "  $=\\BC ŒΩ^‚ä§x_0-S_C(ŒΩ)&\\n{ŒΩ}_*‚â§1,\\ Œª‚â•0\\\\\n",
        "  -‚àû&\\t{otherwise}\\EC$.\n",
        "  $\\blue{\\BC\\t{maximize}&ŒΩ^‚ä§x_0-S_C(ŒΩ)\\\\\n",
        "  \\t{subject to}&\\n{ŒΩ}_*‚â§1\\EC}$.\n",
        "\n",
        "**Distance between sets**: $\\t{dist}(C,D)=\\inf\\{\\n{x-y}|x‚ààC,y‚ààD\\}$.\n",
        "\n",
        "- $\\BC\n",
        "\\t{minimize}&\\n{x-y}\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0,\\ g_i(y)‚â§0\n",
        "\\EC‚áí\\BC\n",
        "\\t{minimize}&\\n{w}\\\\\n",
        "\\t{subject to}&I_C(x)‚â§0,\\ I_D(y)‚â§0,\\ x-y=w\n",
        "\\EC$.\n",
        "\n",
        "  - Dual $\\BC\\t{maximize}&-S_C(-ŒΩ)-S_D(ŒΩ)\\\\\n",
        "  \\t{subject to}&\\n{ŒΩ}_*‚â§1\n",
        "  \\EC$."
      ],
      "metadata": {
        "id": "Ed75Nutw3E4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lowner-John ellipsoid** $\\m{E}_\\t{lj}$ is the minimum volume ellipsoid that contains a set $C$.\n",
        "\n",
        "- The volume of ellipsoid $\\m{E}=\\{v|\\n{Av+b}_2‚â§1\\}$ is proportional to $\\det(A^{-1})$. The problem $\\BC\n",
        "\\t{minimize}_{(A,b)}&\\ln\\det(A^{-1})\\\\\n",
        "\\t{subject to}&\\sup_{v‚ààC}\\n{Av+b}_2‚â§1\n",
        "\\EC$ finds the smallest such ellipsoid that covers set $C$, and is convex in $A,b$. However its evaluation is computationally tractable only for certain special sets $C$ such as finite sets $\\BC\n",
        "\\t{minimize}_{(A,b)}&\\ln\\det(A^{-1})\\\\\n",
        "\\t{subject to}&\\n{Av_i+b}_2‚â§1,\\ (v_1,...,v_m)‚ààC\n",
        "\\EC$\n",
        "\n",
        "  - We can assume $A‚àà\\v{S}_{++}^n$ without loss of generality. Suppose $A=UŒ£V^‚ä§$ is invertible but not symmetric. $Q=VU^‚ä§$ is orthogonal:\n",
        "  $\\n{Av+b}_2‚â§1$\n",
        "  $‚áí\\n{Q(Av+b)}_2‚â§1$\n",
        "  where\n",
        "  $QA=VŒ£V^‚ä§‚àà\\v{S}_{++}^n$.\n",
        "\n",
        "**Maximum volume inscribed ellipsoid**: $C$ is assumed bounded and have nonempty interior with indicator $I_C(x)$ returning $(0,‚àû)$.\n",
        "Ellipsoid $\\m{E}=\\{Bu+d|\\n{u}_2‚â§1\\}$ is a unit ball stretched by $B‚àà\\v{S}_{++}^n$ and shifted by $d‚àà‚Ñù^n$.\n",
        "\n",
        "- The problem $\\BC\\t{maximize}_{(B,d)}&\\ln\\det(B)\\\\\n",
        "\\t{subject to}&\\sup_{\\n{u}_2‚â§1}I_C(Bu+d)‚â§0\n",
        "\\EC$ is convex."
      ],
      "metadata": {
        "id": "8ET-sR7YwHEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification**: Supervised learning scenario given $\\{x_1,...,x_N\\},\\ f(x_i)>0$ and $\\{y_1,...,y_M\\},\\ f(y_i)< 0$. We want to find discriminator $\\{x|f(x)=0\\}$ that separates/discriminates the two sets of points.\n",
        "\n",
        "- **Linear discrimination** separating hyperplane $\\red{\\m{H}=\\{z|a^‚ä§z-b,a‚àà‚Ñù^n,b‚àà‚Ñù\\}}$ satisfies\n",
        "$\\BC a^‚ä§x_i-b>0\\\\a^‚ä§y_i-b< 0\\EC$.\n",
        "\n",
        "  - **Linear alternatives**: primal $\\blue{\\BC b-a^‚ä§x_i< 0\\\\\n",
        "  a^‚ä§y_i-b< 0\n",
        "  \\EC}$\n",
        "  has dual\n",
        "  $g(Œª,\\tilde{Œª})=\\inf_{a,b}(\\sum_iŒª_i(b-a^‚ä§x_i)+\\sum_i\\tilde{Œª}_i(a^‚ä§y_i-b))$\n",
        "  $=\\inf_{a,b}(b(\\v{1}^‚ä§Œª-\\v{1}^‚ä§\\tilde{Œª})+a^‚ä§(\\sum_i\\tilde{Œª}_iy_i-\\sum_iŒª_ix_i))$\n",
        "  $=\\BC0&\\v{1}^‚ä§Œª=\\v{1}^‚ä§\\tilde{Œª},\\ \\sum_i\\tilde{Œª}_iy_i=\\sum_iŒª_ix_i\\\\\n",
        "  -‚àû&\\t{otherwise}\n",
        "  \\EC$.\n",
        "  $\\BC1.\\ g(Œª,\\tilde{Œª})=\\sum_i\\purple{Œª_i}(\\green{b-a^‚ä§x_i})+\\sum_i\\purple{\\tilde{Œª}_i}(\\green{a^‚ä§y_i-b})=0\\\\\n",
        "  2.\\ \\purple{Œª_i}‚â•0,\\ \\purple{\\tilde{Œª}_i}‚â•0,\\ \\green{b-a^‚ä§x_i}< 0,\\ \\green{a^‚ä§y_i-b}< 0\n",
        "  \\EC$\n",
        "  are contradictory if $(Œª,\\tilde{Œª})\\neq 0$.\n",
        "  Therefore the primal conditions are infeasible if\n",
        "  $\\blue{Œª‚âΩ0,\\ \\tilde{Œª}‚âΩ0,\\ \\v{1}^‚ä§Œª=\\v{1}^‚ä§\\tilde{Œª},\\ \\sum_i\\tilde{Œª}_iy_i=\\sum_iŒª_ix_i,\\ (Œª,\\tilde{Œª})\\neq 0}$ are true.\n",
        "\n",
        "  - Conditions 1, 2, 3, and 5 together say $S=\\v{1}^‚ä§Œª=\\v{1}^‚ä§\\tilde{Œª}‚â•0$. Let $p_i=\\/{Œª_i}{S},\\ q_i=\\/{\\tilde{Œª}_i}{S}$.\n",
        "  Then condition 4 says $\\sum_ip_ix_i=\\sum_iq_iy_i$:\n",
        "  convex hull of $\\{x_i\\}$ coincides with convex hull of $\\{y_i\\}$. Geometrically this is a point that lives in both convex hulls when the alternative conditions are true.\n",
        "\n",
        "- **Linear discrimination with slab margin** $\\BC\\t{maximize}_{(t,a,b)}&t\\\\\n",
        "\\t{subject to}&a^‚ä§x_i-b‚â•t\\\\\n",
        "&a^‚ä§y_i-b‚â§-t\\\\\n",
        "&\\n{a}_2‚â§1\\EC‚áí\\BC\n",
        "\\t{minimize}_{(a,b)}&\\n{a}_2\\\\\n",
        "\\t{subject to}&a^‚ä§x_i-b‚â•1\\\\\n",
        "&a^‚ä§y_i-b‚â§-1\\EC$\n",
        "\n",
        "  - $\\n{a}_2$ and $t$ scale together.\n",
        "  The value $a^‚ä§x_i-b$ represents distance from $x_i$ to the hyperplane $\\m{H}=\\{z|a^‚ä§z=b\\}$ along vector $a$.\n",
        "  The thickness of the slab is $2t^*$.\n",
        "\n",
        "  - $g(u,v,Œª)=\\inf_{t,a,b}(-t+\\sum_iu_i(t+b-a^‚ä§x_i)+\\sum_iv_i(a^‚ä§y_i-b+t)+Œª(\\n{a}_2-1))$\n",
        "  $=-Œª+\\inf_tt(-1+\\v{1}^‚ä§u+\\v{1}^‚ä§v)+\\inf_a(Œª\\n{a}_2+a^‚ä§(\\sum_iv_iy_i-\\sum_iu_ix_i))+\\inf_bb(\\v{1}^‚ä§u-\\v{1}^‚ä§v)$\n",
        "  $-Œª-\\sup_a(a^‚ä§(\\sum_iu_ix_i-\\sum_iv_iy_i)-Œª\\n{a}_2)$\n",
        "  $=\\BC-Œª&\\v{1}^‚ä§u=\\v{1}^‚ä§v=\\/{1}{2},\\ \\n{\\sum_iu_ix_i-\\sum_iv_iy_i}_2‚â§1\\\\\n",
        "  -‚àû&\\t{otherwise}\n",
        "  \\EC$.\n",
        "  Dual problem\n",
        "  $\\BC\n",
        "  \\t{maximize}&-\\n{\\sum_iu_ix_i-\\sum_iv_iy_i}_2\\\\\n",
        "  \\t{subject to}&u‚âΩ0,\\ v‚âΩ0,\\ \\v{1}^‚ä§u=\\v{1}^‚ä§v=\\/{1}{2}\n",
        "  \\EC‚áí\\BC\n",
        "  \\t{minimize}&\\n{\\sum_iu_ix_i-\\sum_iv_iy_i}_2\\\\\n",
        "  \\t{subject to}&u‚âΩ0,\\ v‚âΩ0,\\ \\v{1}^‚ä§u=\\v{1}^‚ä§v=\\/{1}{2}\n",
        "  \\EC$\n",
        "  is the minimum distance between two convex hulls\n",
        "  $\\sum_iu_ix_i$ and $\\sum_iv_iy_i$.\n",
        "\n",
        "**Support vector classifier**: $\\{x_1,...,x_N\\}$ and $\\{y_1,...,y_M\\}$ cannot neatly separate. We use slack variables $\\BC u_1,...,u_N‚â•0\\\\v_1,...,v_N‚â•0\\EC$ to allow constraint violations $\\red{\\BC\n",
        "a^‚ä§x_i-b‚â•1-u_i\\\\a^‚ä§y_i-b‚â§-(1-v_i)\n",
        "\\EC}$ such that $\\BC\n",
        "u_i=0&a^‚ä§x_i-b‚â•1\\\\\n",
        "0< u_i‚â§1&a^‚ä§x_i-b‚â•0\\\\\n",
        "u_i>1&a^‚ä§x_i-b< 0\n",
        "\\EC$.\n",
        "\n",
        "- **SVC linear discrimination**: To minimize errors $\\BC\\t{minimize}_{(u,v,a,b)}&\\v{1}^‚ä§u+\\v{1}^‚ä§v\\\\\n",
        "\\t{subject to}&a^‚ä§x_i-b‚â•1-u_i,\\ a^‚ä§y_i-b‚â§-(1-v_i)\\\\\n",
        "&u‚âΩ0,\\ v‚âΩ0\\EC$.\n",
        "To maximize slab and minimize errors $\\red{\\BC\\t{minimize}_{(u,v,a,b)}&\\n{a}_2+Œ≥(\\v{1}^‚ä§u+\\v{1}^‚ä§v)\\\\\n",
        "\\t{subject to}&a^‚ä§x_i-b‚â•1-u_i,\\ a^‚ä§y_i-b‚â§-(1-v_i)\\\\\n",
        "&u‚âΩ0,\\ v‚âΩ0\\EC}$.\n",
        "\n",
        "- **Logistic linear discrimination**: $z_i‚àà\\{0,1\\}$. Given features $\\{x_1,...,x_N\\}$ and $\\{y_1,...,y_M\\}$. Find $a‚àà‚Ñù^n,b‚àà‚Ñù$.\n",
        "\n",
        "  - **Logistic regression**: Let $u_i‚àà‚Ñù^n$ be feature vector with $P(z_i=1)=\\/{\\e{a^‚ä§u_i-b}}{1+\\e{a^‚ä§u_i-b}}$.\n",
        "  $p_{a,b}(x)=\\prod_i(\\/{\\e{a^‚ä§u_i-b}}{1+\\e{a^‚ä§u_i-b}})^{z_i}(\\/{1}{1+\\e{a^‚ä§u_i-b}})^{1-z_i}$\n",
        "  $‚áíl(a,b)=\\sum_i[z_i\\ln(\\/{\\e{a^‚ä§u_i-b}}{1+\\e{a^‚ä§u_i-b}})+(1-z_i)\\ln(\\/{1}{1+\\e{a^‚ä§u_i-b}})]$\n",
        "  $‚áí\\blue{l(a,b)=\\sum_i[z_i(a^‚ä§u_i-b)-\\ln(1+\\e{a^‚ä§u_i-b})]}$\n",
        "\n",
        "  - $\\BC\n",
        "  x_1,...,x_N&z_i=1&P(z_i=1)=\\/{\\e{a^‚ä§x_i-b}}{1+\\e{a^‚ä§x_i-b}}\\\\\n",
        "  y_1,...,y_M&z_i=0&P(z_i=0)=\\/{1}{1+\\e{a^‚ä§y_i-b}}\n",
        "  \\EC$.\n",
        "  Then $p_{a,b}(x,y)=\\prod_{i=1}^N(\\/{\\e{a^‚ä§x_i-b}}{1+\\e{a^‚ä§x_i-b}})\\prod_{i=1}^M(\\/{1}{1+\\e{a^‚ä§y_i-b}})$\n",
        "  $‚áí\\blue{l(a,b)=\\sum_{i=1}^N(a^‚ä§x_i-b)-\\sum_{i=1}^N\\ln(1+\\e{a^‚ä§x_i-b})-\\sum_{i=1}^M\\ln(1+\\e{a^‚ä§y_i-b})}$.\n",
        "  $\\BC\\t{minimize}_{(a,b)}&-l(a,b)\\EC$.\n",
        "\n",
        "- **Nonlinear discrimination**: use separation $\\BC\n",
        "x_i^‚ä§Px_i+q^‚ä§x_i+r>0\\\\\n",
        "y_i^‚ä§Py_i+q^‚ä§y_i+r< 0\n",
        "\\EC$\n",
        "with margin $\\BC\n",
        "x_i^‚ä§Px_i+q^‚ä§x_i+r>1\\\\\n",
        "y_i^‚ä§Py_i+q^‚ä§y_i+r< -1\n",
        "\\EC$\n",
        "\n",
        "  - **Ellipsoidal separation**: $P‚â∫0$ is ellipsoid that is positive inside and negative outside.\n",
        "  $x_i^‚ä§Px_i+q^‚ä§x_i+r>0$ is homogeneous in $(P,q,r)$: using $Œ±(P,q,r)$ maintains the inequality.\n",
        "  $P‚â∫0$ defines open sets that are not numerically stable with flops. $P‚âº-I$ defines closed sets that scales infinitesimally close to $P‚â∫0$.\n",
        "  $\\BC\n",
        "  \\t{find}&P,q,r\\\\\n",
        "  \\t{subject to}&x_i^‚ä§Px_i+q^‚ä§x_i+r>1\\\\\n",
        "  &y_i^‚ä§Py_i+q^‚ä§y_i+r< -1 \\\\\n",
        "  &P‚âº-I\n",
        "  \\EC$"
      ],
      "metadata": {
        "id": "1jCTWjS-jMY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Unconstrained Minimization"
      ],
      "metadata": {
        "id": "0L_sA77WDvLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimization problem** $\\BC\\t{minimize}&f(x)‚ààC^2,\\ \\t{convex}\\EC$.\n",
        "Global minimum necessary and sufficient condition $\\blue{‚àáf(x^*)=0}$ (system of $n$ equations and $n$ variables).\n",
        "Few cases are solved analytically, most require iterative minimizing sequence $x^{(0)},x^{(1)},...‚àà\\dom(f)$ until $f(x^{(k)})-p^*‚â§œµ$.\n",
        "Sublevel set $\\red{S=\\{x‚àà\\dom(f)|f(x)‚â§f(x^{(0)})\\}}$ is closed.\n",
        "\n",
        "- Examples:\n",
        "\n",
        "  - **Quadratic minimization**: $\\BC\n",
        "  \\t{minimize}&\\/{1}{2}x^‚ä§Px+q^‚ä§x+r\n",
        "  \\EC$ has optimality $\\blue{Px^*=-q}$, which has analytic solution.\n",
        "  If $P‚âª0$ then $x^*=-P^{-1}q$ is a unique solution.\n",
        "  Otherwise any non-unique solution is optimal.\n",
        "  Least squares $x^‚ä§(A^‚ä§A)x-2(A^‚ä§b)^‚ä§x+b^‚ä§b$ gives $P=A^‚ä§A$ and $q=-A^‚ä§b$.\n",
        "\n",
        "  - **Geometric programming**: $\\BC\n",
        "  \\t{minimize}&\\ln(\\sum_i\\e{a_i^‚ä§x+b_i})\n",
        "  \\EC$ has optimality $\\/{\\sum_i\\e{a_i^‚ä§x^*+b_i}a_i}{\\sum_j\\e{a_j^‚ä§x^*+b_j}}=0$, which requires iterative solution with any initial guess $x^{(0)}‚àà‚Ñù^n$.\n",
        "\n",
        "  - **Analytic center of log barrier**: $\\BC\n",
        "  \\t{minimize}&f(x)=-\\sum_i\\ln(b_i-a_i^‚ä§x)\n",
        "  \\EC$ on $\\{x|a_i^‚ä§x< b_i, ‚àÄi\\}$\n",
        "  requires iterative solution with $x^{(0)}‚àà\\dom(f)$.\n",
        "\n",
        "  - **Analytic center of LMI**: $\\BC\n",
        "  \\t{minimize}&f(x)=\\ln\\det(F(x)^{-1})\n",
        "  \\EC$ on $\\{x|F(x)‚âª0\\}$ where affine $F:‚Ñù^n‚Üí\\v{S}^p,\\ F(x)=F_0+x_1F_1+...+x_nF_n$.\n",
        "\n",
        "- **Strong convexity**: assume $\\red{‚àá^2f(x)‚âΩmI},\\ m>0,\\ ‚àÄx‚ààS$ as lowerbound on eigenvalue of Hessian.\n",
        "\n",
        "  - **Lowerbound on $p^*$**: Taylor's\n",
        "  $f(y)=f(x)+‚àáf(x)^‚ä§(y-x)+\\/{1}{2}(y-x)^‚ä§‚àá^2f(z)(y-x)$ for some $z‚àà[x,y]$.\n",
        "  Apply strong convexity\n",
        "  $\\green{f(y)‚â•f(x)+‚àáf(x)^‚ä§(y-x)+\\/{m}{2}\\n{y-x}_2^2}$.\n",
        "  When $m=0$ we get 1st-order definition of convexity.\n",
        "  Minimize RHS wrt $y$:\n",
        "  $‚àáf(x)^‚ä§+m(y-x)^‚ä§=0‚áí\\tilde{y}=x-\\/{1}{m}‚àáf(x)$.\n",
        "  Substitute back\n",
        "  $‚àáf(x)^‚ä§(\\tilde{y}-x)+\\/{m}{2}\\n{\\tilde{y}-x}_2^2$\n",
        "  $=-\\/{1}{m}‚àáf(x)^‚ä§‚àáf(x)+\\/{m}{2}\\n{-\\/{1}{m}‚àáf(x)}_2^2$\n",
        "  $=-\\/{1}{2m}\\n{‚àáf(x)}_2^2$\n",
        "  $‚áíf(y)‚â•f(x)-\\/{1}{2m}\\n{‚àáf(x)}_2^2$\n",
        "  $‚áí\\blue{p^*‚â•f(x)-\\/{1}{2m}\\n{‚àáf(x)}_2^2}$.\n",
        "\n",
        "  - **Suboptimality**: $f(x)-p^*‚â§\\/{1}{2m}\\n{‚àáf(x)}_2^2‚â§\\purple{œµ}$\n",
        "  $‚áí\\blue{\\n{‚àáf(x)}_2‚â§\\sqrt{2m\\purple{œµ}}}$.\n",
        "  Using Cauchy-Schwarz\n",
        "  $f(x^*)‚â•f(x)+‚àáf(x)^‚ä§(x^*-x)+\\/{m}{2}\\n{x^*-x}_2^2$\n",
        "  $‚áíp^*‚â•p^*-\\n{‚àáf(x)}_2\\n{x^*-x}_2+\\/{m}{2}\\n{x^*-x}_2^2$\n",
        "  $‚áí\\blue{\\n{x^*-x}_2‚â§\\/{2}{m}\\n{‚àáf(x)}_2}$\n",
        "\n",
        "  - **Upperbound on Hessian**: $‚àÉM>0:\\ \\red{‚àá^2f(x)‚âºMI}$.\n",
        "  Then\n",
        "  $\\green{f(y)‚â§f(x)+‚àáf(x)^‚ä§(y-x)+\\/{M}{2}\\n{y-x}_2^2}$ and\n",
        "  $\\blue{p^*‚â§f(x)-\\/{1}{2M}\\n{‚àáf(x)}_2^2}$.\n",
        "\n",
        "- **Condition number** is the ratio of largest to smallest eigenvalue\n",
        "$\\blue{\\t{cond}(‚àá^2f(x))=\\/{M}{m}}$.\n",
        "**Width** of convex set $C$ along direction $q,\\ \\n{q}_2=1$ is\n",
        "$W_q(C)=\\sup_{z‚ààC}\\{q^‚ä§z\\}-\\inf{z‚ààC}\\{q^‚ä§z\\}$\n",
        "and is the distance between parallel supporting hyperplanes. Then\n",
        "$\\red{Œ∫=\\t{cond}(C)=\\/{W_\\max(C)^2}{W_\\min(C)^2}}$.\n",
        "\n",
        "  - **Sublevel condition number** of $C_Œ±=\\{x|f(x)‚â§Œ±\\},\\ p^*< Œ±‚â§f(x^{(0)})$.\n",
        "  Upper and lower bounds at optimum\n",
        "  $\\green{p^*+\\/{m}{2}\\n{y-x^*}_2^2‚â§f(y)‚â§p^*+\\/{M}{2}\\n{y-x^*}_2^2}$\n",
        "  $‚áí\\BC\n",
        "  f(y)‚â§p^*+\\/{M}{2}\\n{y-x^*}_2^2‚â§Œ±‚áí\\n{y-x^*}_2^2‚â§\\/{2(Œ±-p^*)}{M}\\\\\n",
        "  p^*+\\/{m}{2}\\n{y-x^*}_2^2‚â§f(y)‚â§Œ±‚áí\\n{y-x^*}_2^2‚â§\\/{2(Œ±-p^*)}{m}\n",
        "  \\EC$.\n",
        "  \n",
        "  - $B_\\t{inner}‚äÜC_Œ±‚äÜB_\\t{outer}$ where\n",
        "  $B_\\t{inner}=\\{x|\\n{x-x^*}_2^2‚â§\\/{2(Œ±-p^*)}{M}\\}$ and\n",
        "  $B_\\t{outer}=\\{x|\\n{x-x^*}_2^2‚â§\\/{2(Œ±-p^*)}{m}\\}$.\n",
        "  That is, $C_Œ±$ is a general ellipsoid sandwiched between 2 balls with $W_\\min(C_Œ±)‚â•W(B_\\t{inner})=\\sqrt{\\/{2(Œ±-p^*)}{M}}$ and $W_\\max(C_Œ±)‚â§W(B_\\t{outer})=\\sqrt{\\/{2(Œ±-p^*)}{m}}$.\n",
        "  Therefore condition number $\\blue{\\t{cond}(C_Œ±)‚â§\\/{M}{m}}$.\n"
      ],
      "metadata": {
        "id": "HDAeIwYRD36b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Descent method** produces a minimizing sequence $\\red{x^{(k+1)}=x^{(k)}+t^{(k)}Œîx^{(k)}}$ (or $x^+=x+tŒîx$) where $Œîx^{(k)}‚àà‚Ñù^n$ is step direction, $t^{(k)}>0$ is step size.\n",
        "Taylor's $f(x+tŒîx)‚âàf(x)+t‚àáf(x)^‚ä§Œîx$.\n",
        "Each step satisfies $\\red{f(x+tŒîx)< f(x)}$, implying negative drop $\\red{‚àáf(x)^‚ä§Œîx< 0}$ and convex optimality criterion\n",
        "$‚àáf(x^*)^‚ä§Œîx‚â•0‚áîf(x^*+Œîx)‚â•f(x^*)$.\n",
        "\n",
        "- $\\BC\n",
        "1. &\\t{determine }Œîx\\\\\n",
        "2. &\\t{line search }t>0\\\\\n",
        "3. &\\t{update }x^+=x+tŒîx\n",
        "\\EC$\n",
        "\n",
        "  - **Exact line search**: $\\blue{t_\\t{exact}=\\arg\\min_{s‚â•0}f(x+sŒîx)}$.\n",
        "  \n",
        "- **Backtracking line search**: At every step,\n",
        "$\\BC\n",
        "\\t{let }t=1, \\red{Œ±‚àà(0,1/2), Œ≤‚àà(0,1)}\\\\\n",
        "\\t{while }\\blue{f(x+tŒîx)>f(x)+Œ±t‚àáf(x)^‚ä§Œîx}\\\\\n",
        "\\quad t:=Œ≤t\n",
        "\\EC$\n",
        "\n",
        "  - Taylor's $f(x+tŒîx)‚âàf(x)+t‚àáf(x)^‚ä§Œîx$ when $t$ is small enough.\n",
        "  $‚àáf(x)^‚ä§Œîx< 0$ in descent therefore $t‚àáf(x)^‚ä§Œîx< Œ±t‚àáf(x)^‚ä§Œîx$.\n",
        "  When $t$ is small enough the while loop is guaranteed to exit.\n",
        "\n",
        "  - Backtracking exit condition: $\\red{f(x+tŒîx)‚â§f(x)+Œ±t‚àáf(x)^‚ä§Œîx}$\n",
        "\n",
        "  - Typical $Œ±=0.1$, $Œ≤=0.8$."
      ],
      "metadata": {
        "id": "a83Zl13ZBYVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient descent**: $\\red{Œîx=-‚àáf(x)}$.\n",
        "The drop is $\\blue{‚àáf(x)^‚ä§Œîx=-\\n{‚àáf(x)}_2^2}$ with exit condition $\\n{‚àáf(x)}_2‚â§Œ∑,\\ Œ∑>0$.\n",
        "\n",
        "- **Exact line search**: Let $\\tilde{f}(t)=f(x+tŒîx)=f(x-t‚àáf(x))$ and $(y-x)‚ÜíŒîx‚Üít‚àáf(x)$.\n",
        "Now we apply assumptions of strong convexity to $f$ on $S$.\n",
        "\n",
        "  - **Strong convexity**: From upperbound condition\n",
        "  $\\tilde{f}(t)‚â§f(x)-t\\n{‚àáf(x)}_2^2+\\/{Mt^2}{2}\\n{‚àáf(x)}_2^2$.\n",
        "  Minimize RHS wrt $t$ we have\n",
        "  $\\green{t_\\t{exact}=\\/{1}{M}}$ or\n",
        "  $f(x^+)=\\tilde{f}(t_\\t{exact})‚â§f(x)-\\/{1}{2M}\\n{‚àáf(x)}_2^2$\n",
        "  $‚áí\\/{1}{2M}\\n{‚àáf(x)}_2^2‚â§f(x)-f(x^+)$.\n",
        "  From lowerbound Hessian and suboptimality\n",
        "  $f(x)-p^*‚â§\\/{1}{2m}\\n{‚àáf(x)}_2^2$.\n",
        "  Substitute out $\\n{‚àáf(x)}_2^2$ and rearrange\n",
        "  $\\/{m}{M}(f(x)-p^*)‚â§f(x)-f(x^+)$\n",
        "  $‚áíf(x^+)-f(x)‚â§-\\/{m}{M}(f(x)-p^*)$\n",
        "  $‚áíf(x^+)-p^*‚â§(1-\\/{m}{M})(f(x)-p^*)$\n",
        "  $‚áí\\blue{f(x^{(k)})-p^*‚â§\\purple{(1-\\/{m}{M})^k}(f(x^{(0)})-p^*)}$\n",
        "\n",
        "  - **Iterations**: $f(x^{(k)})-p^*‚â§(1-\\/{m}{M})^k(f(x^{(0)})-p^*)‚â§œµ$\n",
        "  $‚áík\\ln(1-\\/{m}{M})‚â§\\ln(\\/{œµ}{f(x^{(0)})-p^*})$\n",
        "  $‚áík‚â•\\/{\\ln((f(x^{(0)})-p^*)/œµ)}{\\ln(1/(1-m/M))}$\n",
        "  $‚âà\\/{M}{m}\\ln(\\/{f(x^{(0)})-p^*}{œµ})$\n",
        "\n",
        "- **Backtracking line search**: Exit condition\n",
        "$\\tilde{f}(t)=f(x-t‚àáf(x))‚â§f(x)-Œ±t\\n{‚àáf(x)}_2^2$\n",
        "\n",
        "  - **Exit condition**: Substituting $0‚â§t‚â§\\/{1}{M}‚áíM‚â§\\/{1}{t}‚áí-t+\\/{Mt^2}{2}‚â§-\\/{t}{2}$ in strong convexity upperbound\n",
        "  $\\tilde{f}(t)‚â§f(x)-t\\n{‚àáf(x)}_2^2+\\/{Mt^2}{2}\\n{‚àáf(x)}_2^2$\n",
        "  $‚â§f(x)-\\/{t}{2}\\n{‚àáf(x)}_2^2$\n",
        "  $‚â§f(x)-Œ±t\\n{‚àáf(x)}_2^2$.\n",
        "  Backtraking exit condition is met when\n",
        "  $\\blue{t‚â§\\/{1}{M}}$.\n",
        "  Final step size $t$ upon exiting backtracking is $\\blue{\\/{Œ≤}{M}‚â§t‚â§1}$.\n",
        "\n",
        "  - **Upon exiting backtracking**: $f(x^+)‚â§f(x)-Œ±t\\n{‚àáf(x)}_2^2$ the worst case (smallest drop in objective) is\n",
        "  $\\BC\n",
        "  \\t{if }Œ≤/M< 1&Œ±(Œ≤/M)\\n{‚àáf(x)}_2^2\\\\\n",
        "  \\t{if }Œ≤/M‚â•1&Œ±\\n{‚àáf(x)}_2^2\n",
        "  \\EC\n",
        "  =\\min(Œ±,\\/{Œ±Œ≤}{M})\\n{‚àáf(x)}_2^2$.\n",
        "  From lowerbound Hessian and suboptimality\n",
        "  $f(x)-p^*‚â§\\/{1}{2m}\\n{‚àáf(x)}_2^2$\n",
        "  we have\n",
        "  $f(x^+)-p^*‚â§(1-2m\\min(Œ±,\\/{Œ±Œ≤}{M}))(f(x)-p^*)$\n",
        "  $‚áí\\blue{f(x^{(k)})-p^*‚â§\\purple{(1-2m\\min(Œ±,\\/{Œ±Œ≤}{M}))^k}(f(x^{(0)})-p^*)}$"
      ],
      "metadata": {
        "id": "kUxlKkPO0BEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steepest descent**: Taylor's $f(x+v)‚âàf(x)+‚àáf(x)^‚ä§v$ for small step $v$.\n",
        "How to choose $v$ that makes $‚àáf(x)^‚ä§v$ as negative as possible?\n",
        "Let $\\red{Œîx_\\t{nsd}=\\arg\\min_{\\n{v}=1}‚àáf(x)^‚ä§v}$ and $\\red{Œîx_\\t{sd}=\\n{‚àáf(x)}_*Œîx_\\t{nsd}}$ be normalized and unnormalized steepest descent direction vectors.\n",
        "\n",
        "- Conjugate norm\n",
        "$\\n{‚àáf(x)}_*$\n",
        "$=\\max_{\\n{v}=1}‚àáf(x)^‚ä§v$\n",
        "$=-\\min_{\\n{v}=1}‚àáf(x)^‚ä§v$\n",
        "$=-‚àáf(x)^‚ä§Œîx_\\t{nsd}$.\n",
        "Therefore\n",
        "$‚àáf(x)^‚ä§Œîx_\\t{sd}=\\n{‚àáf(x)}_*‚àáf(x)^‚ä§Œîx_\\t{nsd}$\n",
        "$=-(‚àáf(x)^‚ä§Œîx_\\t{nsd})^2$\n",
        "$‚áí\\blue{‚àáf(x)^‚ä§Œîx_\\t{sd}=-\\n{‚àáf(x)}_*^2}$.\n",
        "\n",
        "- $\\BC\n",
        "1. &\\t{compute }Œîx_\\t{sd} \\\\\n",
        "2. &\\t{line search }t>0\\\\\n",
        "3. &\\t{update }x^+=x+tŒîx_\\t{sd}\n",
        "\\EC$\n",
        "\n",
        "- Let $P‚àà\\v{S}_{++}^n$. Define $P$-quadratic norm $\\red{\\n{z}_P=(z^‚ä§Pz)^{1/2}=\\n{P^{1/2}z}_2}$, which satisfy three legal requirements of a norm ($\\n{0}=0$, $\\n{Œ±x}=Œ±\\n{x}$, and $\\n{a+b}‚â§\\n{a}+\\n{b}$). **It is the $P$-inner product $‚ü®z,z‚ü©_P=z^‚ä§Pz$**.\n",
        "\n",
        "  - Let $\\bar{v}=P^{1/2}v$.\n",
        "  Dual norm of $P$-quadratic norm is $\\n{z}_*=\\max_{\\n{v}_P‚â§1}z^‚ä§v$\n",
        "  $=\\max_{\\n{\\bar{v}}_2‚â§1}z^‚ä§P^{-1/2}\\bar{v}$\n",
        "  $=\\max_{\\n{\\bar{v}}_2‚â§1}(P^{-1/2}z)^‚ä§\\bar{v}$\n",
        "  $‚áí\\red{\\n{z}_*=\\n{P^{-1/2}z}_2}$.\n",
        "\n",
        "  - $Œîx_\\t{nsd}=\\arg\\min_{\\n{v}_P‚â§1}‚àáf(x)^‚ä§v$.\n",
        "  Transform\n",
        "  $\\min_{\\n{v}_P‚â§1}‚àáf(x)^‚ä§v$\n",
        "  $=\\min_{\\n{\\bar{v}}_2‚â§1}‚àáf(x)^‚ä§P^{-1/2}\\bar{v}$\n",
        "  $=\\min_{\\n{\\bar{v}}_2‚â§1}(P^{-1/2}‚àáf(x))^‚ä§\\bar{v}$\n",
        "  $‚áí\\bar{v}^*=\\/{-P^{-1/2}‚àáf(x)}{\\n{P^{-1/2}‚àáf(x)}_2}$\n",
        "  the dot product is minimized when two vectors point in opposite directions.\n",
        "  Transform back\n",
        "  $Œîx_\\t{nsd}=P^{-1/2}\\bar{v}^*$\n",
        "  $=\\/{-P^{-1}‚àáf(x)}{\\n{P^{-1/2}‚àáf(x)}_2}$\n",
        "  and\n",
        "  $Œîx_\\t{sd}=\\n{‚àáf(x)}_*Œîx_\\t{nsd}$\n",
        "  $=\\n{P^{-1/2}‚àáf(x)}_2Œîx_\\t{nsd}$\n",
        "  $‚áí\\red{Œîx_\\t{sd}=-P^{-1}‚àáf(x)}$.\n",
        "\n",
        "  - $f$ is transformed: $f(x)=f(P^{-1/2}\\bar{x})=\\bar{f}(\\bar{x})$ where $\\BC\n",
        "  ‚àá\\bar{f}(\\bar{x})=P^{-1/2}‚àáf(x)\\\\\n",
        "  ‚àá^2\\bar{f}(\\bar{x})=P^{-1/2}‚àá^2f(x)P^{-1/2}\n",
        "  \\EC$.\n",
        "  Quick sanity check $Œî\\bar{x}=-‚àá\\bar{f}(\\bar{x})‚áíŒîx=P^{-1/2}Œî\\bar{x}=-P^{-1}‚àáf(x)$.\n",
        "\n",
        "- Under $\\n{‚ãÖ}_2=\\n{‚ãÖ}_{P=I}$ we have $Œîx_\\t{sd}=-‚àáf(x)$. The $P$-quadratic norm is elliptical version of circular $‚Ñì_2$ norm. Gradient descent is special case of steepest descent under Euclidean norm.\n",
        "\n",
        "  - $Œîx_\\t{sd}=-‚àáf(x)$ assumes a perfect round-shape valley, which is why the path of descent zig-zags for a second-order Taylor approximation of $f(x)$ with elliptical Hessian $‚àá^2f(x)=P$.\n",
        "\n",
        "  - $Œîx_\\t{sd}=-P^{-1}‚àáf(x)$ changes basis of gradient along $P$, which aligns path of descent along eigenvectors of $P$. High eigenvalues mean steep curvatures and narrow walls. Low eigenvalues mean flat valleys."
      ],
      "metadata": {
        "id": "szCcJlWozkTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Newton's method**: descent that uses $g=‚àáf(x)$ and $H=‚àá^2f(x)$ to repeatedly build quadratic approximations.\n",
        "\n",
        "- **Newton step** $\\red{Œîx_\\t{nt}=-‚àá^2f(x)^{-1}‚àáf(x)}=-H^{-1}g$ is the step that minimizes second-order Taylor's approximation of $f$ centered at $x$. It satisfies $‚àáf(x)^‚ä§Œîx_\\t{nt}=-g^‚ä§H^{-1}g< 0$ if $H‚âª0$, which is true under strong convexity.\n",
        "\n",
        "  - **Minimizing step**: Taylor's $\\green{\\hat{f}(x+v)=f(x)+‚àáf(x)^‚ä§v+\\/{1}{2}v^‚ä§‚àá^2f(x)v}$\n",
        "  is convex quadratic function of step $v$.\n",
        "  Minimize\n",
        "  $‚àá_v\\hat{f}(x+v)=g^‚ä§+v^‚ä§H=0$\n",
        "  $‚áív^*=-H^{-1}g=Œîx_\\t{nt}$\n",
        "  $‚áí\\red{Œîx_\\t{nt}=\\arg\\inf_v\\hat{f}(x+v)}$\n",
        "  is the step that minimizes 2nd-order Taylor's approximation of $f(x)$ at $x$.\n",
        "  If $f$ is quadratic, then $Œîx_\\t{nt}$ minimizes $f$ in one step.\n",
        "  $\\hat{f}(x+v)$ is a very good approximation of $f(x^*)$ in the neighborhood of $x^*$.\n",
        "\n",
        "  - **Minimizing point**: Taylor's linear approximation $f(x+v)‚âàf(x)+‚àáf(x)^‚ä§v$.\n",
        "  Minimize wrt $x$ we have\n",
        "  $‚àá_xf(x+v)=g+H^‚ä§v=0$\n",
        "  $\\dg{‚áí}v^*=-H^{-1}g=Œîx_\\t{nt}$.\n",
        "\n",
        "  - **Steepest descent**: $Œîx_\\t{nt}=-P^{-1}‚àáf(x)$ using $\\blue{\\n{u}_{P=‚àá^2f(x)}=(u^‚ä§‚àá^2f(x)u)^{1/2}=\\n{‚àá^2f(x)^{1/2}u}_2}$.\n",
        "  The transformed function has Hessian $‚àá^2\\bar{f}(\\bar{x})=H^{-1/2}HH^{-1/2}=I$, which has condition number $\\t{cond}(‚àá^2\\bar{f}(\\bar{x}))=1$.\n",
        "\n",
        "  - **Affine invariance**: Let $B‚àà‚Ñù^{n√ón}$ be nonsingular basis change matrix such that $x=By$ and $\\bar{f}(y)=f(By)$. Then $\\red{\\BC\n",
        "  ‚àá\\bar{f}(y)=B^‚ä§‚àáf(x)\\\\\n",
        "  ‚àá^2\\bar{f}(y)=B^‚ä§‚àá^2f(x)B\n",
        "  \\EC}$.\n",
        "  Then Newton step of $\\bar{f}$ in $y$-space is\n",
        "  $Œîy_\\t{nt}=-(B^‚ä§HB)^{-1}(B^‚ä§g)$\n",
        "  $=-B^{-1}H^{-1}B^{-‚ä§}B^‚ä§g$\n",
        "  $=-B^{-1}H^{-1}g$\n",
        "  $\\dg{‚áí}Œîy_\\t{nt}=-B^{-1}Œîx_\\t{nt}$\n",
        "  and $x+Œîx_\\t{nt}=B(y+Œîy_\\t{nt})$.\n",
        "  Running Newton's method on affine pre-composition yields the same result.\n",
        "\n",
        "- **Newton decrement** at point $x$ is\n",
        "$\\red{Œª(x)^2=‚àáf(x)^‚ä§‚àá^2f(x)^{-1}‚àáf(x)}=g^‚ä§H^{-1}g$ is part of exit criterion.\n",
        "\n",
        "  - **Newton step norm**:\n",
        "  $Œª(x)^2=g^‚ä§H^{-1}g$\n",
        "  $=g^‚ä§H^{-1}HH^{-1}g$\n",
        "  $‚áí\\blue{Œª(x)^2=Œîx_\\t{nt}‚àá^2f(x)Œîx_\\t{nt}=\\n{Œîx_\\t{nt}}_{‚àá^2f(x)}^2}$\n",
        "\n",
        "  - **Line search**:\n",
        "  $\\blue{Œª(x)^2=-‚àáf(x)^‚ä§Œîx_\\t{nt}}$.\n",
        "  Backtracking exit condition is\n",
        "  $\\blue{f(x+tŒîx_\\t{nt})‚â§f(x)-Œ±tŒª^2(x)}$,\n",
        "  which satisfies\n",
        "  $\\/{d}{dt}f(x+tŒîx_\\t{nt})|_{t=0}=‚àáf(x+tŒîx_\\t{nt})|_{t=0}^‚ä§Œîx_\\t{nt}$\n",
        "  $=‚àáf(x)^‚ä§Œîx_\\t{nt}$\n",
        "  $=-Œª(x)^2$.\n",
        "\n",
        "  - **Suboptimality**: $f(x)-\\inf_v\\hat{f}(x+v)=f(x)-\\hat{f}(x+Œîx_\\t{nt})$\n",
        "  $=-g^‚ä§Œîx_\\t{nt}-\\/{1}{2}Œîx_\\t{nt}^‚ä§HŒîx_\\t{nt}$\n",
        "  $=-g^‚ä§Œîx_\\t{nt}+\\/{1}{2}g^‚ä§Œîx_\\t{nt}$\n",
        "  $=-\\/{1}{2}g^‚ä§Œîx_\\t{nt}$\n",
        "  $‚áí\\red{f(x)-\\hat{f}(x+Œîx_\\t{nt})=\\/{Œª(x)^2}{2}}$\n",
        "  $‚áí\\blue{f(x)-p^*‚âà\\/{1}{2}Œª(x)^2}$\n",
        "  is the gap between $f(x)$ and second-order approximated bottom.\n",
        "\n",
        "- $\\BC\n",
        "1. &\\t{compute }Œîx_\\t{nt},\\ Œª^2 \\\\\n",
        "2. &\\t{exit if }Œª^2/2‚â§œµ\\\\\n",
        "3. &\\t{backtracking line search }t>0\\\\\n",
        "3. &\\t{update }x^+=x+tŒîx_\\t{nt}\n",
        "\\EC$\n"
      ],
      "metadata": {
        "id": "UsjScUMVH1JR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self-concordance**: $\\red{\\abs{f'''(x)}‚â§kf''(x)^{3/2}},\\ f:‚Ñù‚Üí‚Ñù$, usually $k=2$ for convenience.\n",
        "Affine and quadratic functions have $f'''(x)=0$ and are automatically self-concordant.\n",
        "Function is invariant to affine pre-composition.\n",
        "$f:‚Ñù^n‚Üí‚Ñù$ is self-concordant if $\\tilde{f}(t)=f(x+tv)$ is self-concordant function of $t,\\ ‚àÄx‚àà\\dom(f),\\ ‚àÄv‚àà‚Ñù^n$.\n",
        "\n",
        "- Self-concordance is preserved under:\n",
        "\n",
        "  - **Affine post-composition**:\n",
        "  $\\blue{\\tilde{f}(x)=Œ±f(x)}$ and $\\blue{\\tilde{f}(x)=f_1(x)+f_2(x)}$ preserve self-concordance.\n",
        "  Proof:\n",
        "  $\\abs{f_1'''(x)+f_2'''(x)}‚â§\\abs{f_1'''(x)}+\\abs{f_2'''(x)}$\n",
        "  $‚â§2(f_1''(x)^{3/2}+f_2''(x)^{3/2})$\n",
        "  $‚â§2(f_1''(x)+f_2''(x))^{3/2}$\n",
        "  using inequality\n",
        "  $u^{3/2}+v^{3/2}‚â§(u+v)^{3/2}$.\n",
        "\n",
        "  - **Affine pre-composition** (affine invariance):\n",
        "  $\\blue{\\tilde{f}(y)=f(ay+b)}$,\n",
        "  $\\tilde{f}''(y)=a^2f''(ay+b)$,\n",
        "  $\\tilde{f}'''(y)=a^3f'''(ay+b)$,\n",
        "  then\n",
        "  $\\abs{\\tilde{f}'''(y)}=\\abs{a^3f'''(ay+b)}‚â§2(a^2f''(ay+b))^{3/2}=2\\tilde{f}''(y)^{3/2}$\n",
        "  $‚áí\\abs{f'''(ay+b)}‚â§2f''(ay+b)^{3/2}$\n",
        "\n",
        "  - **Logarithmic composition**: let $g$ convex on $‚Ñù_{++}$ satisfy $\\blue{\\abs{g'''(x)}‚â§3\\/{g''(x)}{x}},\\ ‚àÄx$ then $\\blue{f(x)=-\\ln(-g(x))-\\ln(x)}$ on $\\{x|x>0, g(x)< 0\\}$ is self-concordant.\n",
        "  E.g.,\n",
        "  $g(x)=-x^p,\\ p‚àà(0,1]$,\n",
        "  $g(x)=-\\ln(x)$,\n",
        "  $g(x)=x\\ln(x)$,\n",
        "  $g(x)=x^p,\\ p‚àà[-1,0]$,\n",
        "  $g(x)=\\/{(ax+b)^2}{x}$.\n",
        "\n",
        "- **Bounds on Hessian**: Let $f(t)$ be self-concordant. Then $\\red{\\/{f''(0)}{(1+tf''(0)^{1/2})^2}‚â§f''(t)‚â§\\/{f''(0)}{(1-tf''(0)^{1/2})^2}}$.\n",
        "\n",
        "  - Proof:\n",
        "  $\\/{d}{dt}(f''(t)^{-1/2})=\\/{-1}{2}f''(t)^{-3/2}f'''(t)$\n",
        "  $=\\/{-f'''(t)}{2f''(t)^{3/2}}$\n",
        "  $\\dg{‚áí}\\abs{\\/{d}{dt}(f''(t)^{-1/2})}‚â§1$\n",
        "  $\\dg{‚áí}-1‚â§\\/{d}{dt}(f''(t)^{-1/2})‚â§1$\n",
        "  $\\dg{‚áí}-t‚â§‚à´_0^t[\\/{d}{ds}(f''(s)^{-1/2})]\\ ds‚â§t$\n",
        "  $\\dg{‚áí}-t‚â§f''(t)^{-1/2}-f''(0)^{-1/2}‚â§t$\n",
        "  $\\dg{‚áí}\\/{1}{(f''(0)^{-1/2}+t)^2}‚â§f''(t)‚â§\\/{1}{(f''(0)^{-1/2}-t)^2}$\n",
        "\n",
        "  - $\\green{Œª(x)=\\sup_{v\\neq0}\\/{-v^‚ä§‚àáf(x)}{(v^‚ä§‚àá^2f(x)v)^{1/2}}}$.\n",
        "  Proof:\n",
        "  By Cauchy-Schwarz $-v^‚ä§g=-(H^{1/2}v)^‚ä§(H^{-1/2}g)$\n",
        "  $‚â§\\n{H^{1/2}v}_2\\n{H^{-1/2}g}_2$\n",
        "  $=(v^‚ä§Hv)^{1/2}(g^‚ä§H^{-1}g)^{1/2}$\n",
        "  $\\dg{‚áí}(g^‚ä§H^{-1}g)^{1/2}‚â•\\/{-v^‚ä§g}{(v^‚ä§Hv)^{1/2}}$\n",
        "\n",
        "  - Let $\\tilde{f}(t)=f(x+tv)$ be a self-concordant slice. Then\n",
        "  $\\tilde{f}(0)=f(x)$,\n",
        "  $\\tilde{f}'(0)=v^‚ä§g$,\n",
        "  $\\tilde{f}''(0)=v^‚ä§Hv$, and\n",
        "  $\\purple{\\tilde{f}'(0)\\tilde{f}''(0)^{-1/2}}=\\/{v^‚ä§g}{(v^‚ä§Hv)^{1/2}}‚â•-Œª(x)$.\n",
        "\n",
        "  - $\\green{\\tilde{f}(t)‚â•\\tilde{f}(0)+t\\tilde{f}'(0)+t\\tilde{f}''(0)^{1/2}-\\ln(1+t\\tilde{f}''(0)^{1/2})}$.\n",
        "  Proof:\n",
        "  Integrate earlier lowerbound twice\n",
        "  $‚à´_0^t\\tilde{f}''(s)\\ ds‚â•‚à´_0^t\\/{\\tilde{f}''(0)}{(1+s\\tilde{f}''(0)^{1/2})^2}\\ ds$\n",
        "  $\\dg{‚áí}\\tilde{f}'(t)-\\tilde{f}'(0)‚â•[\\/{-\\tilde{f}''(0)^{1/2}}{1+s\\tilde{f}''(0)^{1/2}}]_0^t$\n",
        "  $=\\tilde{f}''(0)^{1/2}-\\/{\\tilde{f}''(0)^{1/2}}{1+t\\tilde{f}''(0)^{1/2}}$\n",
        "  $\\dg{‚áí}‚à´_0^t\\tilde{f}'(s)\\ ds‚â•‚à´_0^t[\\tilde{f}'(0)+\\tilde{f}''(0)^{1/2}-\\/{\\tilde{f}''(0)^{1/2}}{1+s\\tilde{f}''(0)^{1/2}}]\\ ds$\n",
        "\n",
        "  - Minimize RHS\n",
        "  $t^*=\\arg\\min_t(\\tilde{f}(0)+t(\\tilde{f}'(0)+\\tilde{f}''(0)^{1/2})-\\ln(1+t\\tilde{f}''(0)^{1/2}))$\n",
        "  $=\\/{-\\tilde{f}'(0)}{\\tilde{f}''(0)+\\tilde{f}''(0)^{1/2}\\tilde{f}'(0)}$.\n",
        "  Substitute back\n",
        "  $\\inf_t\\tilde{f}(t)‚â•\\tilde{f}(0)-\\purple{\\tilde{f}'(0)\\tilde{f}''(0)^{-1/2}}+\\ln(1+\\purple{\\tilde{f}'(0)\\tilde{f}''(0)^{-1/2}})$\n",
        "  $\\dg{‚áí} p^*=\\inf_t\\tilde{f}(t)‚â•f(x)+Œª(x)+\\ln(1-Œª(x))$.\n",
        "  Second term\n",
        "  $Œª(x)+\\ln(1-Œª(x))‚âà-\\/{Œª^2}{2}‚â•-Œª^2$ for $Œª‚â§0.68$. Therefore\n",
        "  $\\blue{p^*‚â•f(x)-Œª(x)^2,\\ Œª(x)‚â§0.68}$.\n",
        "\n",
        "- **Bounds on Hessian (matrix)**:\n",
        "$\\red{\\/{‚àá^2f(x)}{(1+tŒª(x))^2}‚âº‚àá^2f(x+tv)‚âº\\/{‚àá^2f(x)}{(1-tŒª(x))^2}}$ or $\\/{H}{(1+tŒª)^2}‚âºH(t)‚âº\\/{H}{(1-tŒª)^2}$.\n",
        "\n",
        "  - Proof: Let $v=Œîx_\\t{nt}$, then\n",
        "  $\\tilde{f}''(0)=v^‚ä§Hv=Œª(x)^2$.\n",
        "  Starting with 1D\n",
        "  $\\/{f''(0)}{(1+tf''(0)^{1/2})^2}‚â§f''(t)‚â§\\/{f''(0)}{(1-tf''(0)^{1/2})^2}$\n",
        "  $\\dg{‚áí}\\/{v^‚ä§‚àá^2f(x)v}{(1+tŒª(x))^2}‚â§v^‚ä§‚àá^2f(x+tv)v‚â§\\/{v^‚ä§‚àá^2f(x)v}{(1-tŒª(x))^2}$\n",
        "  $\\dg{‚áí}\\/{‚àá^2f(x)}{(1+tŒª(x))^2}‚âº‚àá^2f(x+tv)‚âº\\/{‚àá^2f(x)}{(1-tŒª(x))^2}$\n",
        "\n",
        "  - $\\blue{\\n{(H(t)-H)v}_{H^{-1}}‚â§(\\/{1}{(1-tŒª)^2}-1)^2\\n{v}_H}$.\n",
        "  Proof:\n",
        "  $(\\/{1}{(1+tŒª)^2}-1)H‚âºH(t)-H‚âº(\\/{1}{(1-tŒª)^2}-1)H$\n",
        "  $\\dg{‚áí}-cH‚âºŒîH‚âºcH$\n",
        "  $\\dg{‚áí}-cI‚âºH^{-1/2}ŒîHH^{-1/2}‚âºcI$\n",
        "  $\\dg{‚áí}(H^{-1/2}ŒîHH^{-1/2})(H^{-1/2}ŒîHH^{-1/2})‚âºc^2I$\n",
        "  $\\dg{‚áí}H^{-1/2}ŒîHH^{-1}ŒîHH^{-1/2}‚âºc^2I$\n",
        "  $\\dg{‚áí}H^{1/2}H^{-1/2}ŒîHH^{-1}ŒîHH^{-1/2}H^{1/2}‚âºc^2H$\n",
        "  $\\dg{‚áí}ŒîHH^{-1}ŒîH‚âºc^2H$\n",
        "  $\\dg{‚áí}v^‚ä§ŒîHH^{-1}ŒîHv‚â§c^2v^‚ä§Hv$\n",
        "  $\\dg{‚áí}\\n{ŒîHv}_{H^{-1}}‚â§c^2\\n{v}_H$\n",
        "  \n",
        "- 9.3: **Negative log**:\n",
        "$\\blue{f(x)=-\\ln(x)}$, then $f''(x)=\\/{1}{x^2}$, $f'''(x)=\\/{-2}{x^3}$. Then $\\/{f'''(x)}{2f''(x)^{3/2}}=\\/{2/x^3}{2(1/x^2)^{3/2}}=1$.\n",
        "Self-concordant\n",
        "\n",
        "  - Logarithmic commposition: $f(x)=x\\ln(x)-\\ln(x)$, then $f'(x)=1+\\ln(x)-\\/{1}{x}$, $f''(x)=\\/{x+1}{x^2}$, and $f'''(x)=-\\/{x+2}{x^3}$.\n",
        "  Then $\\/{\\abs{f'''(x)}}{2f''(x)^{3/2}}=\\/{x+2}{2(x+1)^{3/2}}‚â§1$ on $x‚àà‚Ñù_+$.\n",
        "  Self-concordant.\n",
        "\n",
        "- 9.4 **Log barrier**:\n",
        "$\\blue{f(x)=-\\sum_i\\ln(b_i-a_i^‚ä§x)}$ is sum of negative logs pre-composed with affine.\n",
        "Self-concordant.\n",
        "\n",
        "- 9.5 **Log-determinant**:\n",
        "$\\blue{f(X)=-\\ln\\det(X)}$ on $\\v{S}_{++}^n$.\n",
        "Let $\\tilde{f}(t)=f(X+tV)$\n",
        "$=-\\ln\\det(X^{1/2}(I+tX^{-1/2}VX^{-1/2})X^{1/2})$\n",
        "$=-\\/{1}{2}\\ln\\det(X)-\\ln\\det(I+tX^{-1/2}VX^{-1/2})-\\/{1}{2}\\ln\\det(X)$\n",
        "$=-\\ln\\det(X)-\\sum_i\\ln(1+tŒª_i)$\n",
        "is a sum of negative logs pre-composed with affine function of $t$.\n",
        "Self-concordant.\n",
        "\n",
        "- 9.6: **Log of concave quadratic**: $f(x)=-\\ln(x^‚ä§Px+q^‚ä§x+r),\\ P‚àà\\v{S}_{-}^n$ on $\\{x|x^‚ä§Px+q^‚ä§x+r>0\\}$.\n",
        "General case reduces to\n",
        "$f(x)=-\\ln(px^2+qx+r),\\ p< 0$.\n",
        "Let $a,b$ be roots of the quadratic, then\n",
        "$f(x)=-\\ln(-p(x-a)(b-x))$\n",
        "$=-\\ln(-p)-\\ln(x-a)-\\ln(b-x)$\n",
        "is a sum of negative logs.\n",
        "Self-concordant."
      ],
      "metadata": {
        "id": "FGkwzKhJ7gF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Newton's Method convergence**:\n",
        "\n",
        "- **Classical**: Assume strong convexity\n",
        "$‚àá^2f(x)‚âΩmI,\\ ‚àá^2f(x)‚âºMI$\n",
        "and Lipschitz constraint on rate of curvature change\n",
        "$\\n{‚àá^2f(x)-‚àá^2f(y)}_2‚â§L\\n{x-y}_2$.\n",
        "Newton's Method has 2 stages:\n",
        "\n",
        "  - **Damped phase** is when the gradient is relatively large\n",
        "  $\\n{‚àáf(x^{(k)})}_2‚â•Œ∑,\\ 0< Œ∑‚â§\\/{m^2}{L}$,\n",
        "  and backtracking is used to choose step $t< 1$.\n",
        "  The drop is guaranteed to be\n",
        "  $f(x^{(k+1)})-f(x^{(k)})‚â§-Œ≥$.\n",
        "  Number of steps\n",
        "  $k‚â§\\/{f(x^{(0})-p^*}{Œ≥}$,\n",
        "  where\n",
        "  $Œ≥=Œ±Œ≤Œ∑^2\\/{m}{M^2}$ is derived from backtracking line search and curvature bounds.\n",
        "  \n",
        "  - **Quadratic convergence** begins when $x‚âàx^*$. Gradient falls below threashold\n",
        "  $\\n{‚àáf(x^{(l)})}_2< Œ∑,\\ ‚àÄl‚â•k$\n",
        "  and collapses rapidly\n",
        "  $\\/{L}{2m^2}\\n{‚àáf(x^{(l+1)})}_2‚â§(\\/{L}{2m^2}\\n{‚àáf(x^{(l)})}_2)^2$\n",
        "  $\\dg{‚áí}\\/{L}{2m^2}\\n{‚àáf(x^{(l)})}_2‚â§(\\/{1}{2})^{2^{l-k}}$\n",
        "  $\\dg{‚áí}f(x^{(l)})-p^*‚â§\\/{2m^3}{L^2}(\\/{1}{2})^{2^{l-k+1}}$.\n",
        "  During this phase $t=1$ and convergence occurs at quadratic speed.\n",
        "  Number of steps\n",
        "  $l-k‚â§\\log_2(\\log_2(\\/{3m^3}{L^2œµ}))$.\n",
        "  6 iterations give an accuracy of $œµ‚âà5√ó10^{-20}\\/{2m^3}{L^2}$.\n",
        "  Therefore $l-k‚â§6$.\n",
        "  \n",
        "  - In practice we don't know $m,M,L$.\n",
        "  Furthermore these constants are not affine invariant.\n",
        "  \n",
        "- **Newton's Method for self-concordant $f$**: Drop strong convexity and Lipschitz.\n",
        "Use $Œª(x),Œ±‚àà(0,\\/{1}{2}),Œ≤‚àà(0,1)$.\n",
        "$\\BC\n",
        "\\t{Damped}\n",
        "&Œª(x^{(k)})>Œ∑&f(x^{(k+1)})-f(x^{(k)})‚â§-Œ≥&t‚â•\\/{Œ≤}{1+Œª(x)}&k‚â§\\/{f(x^{(0)})-p^*}{Œ≥}‚âà375(f(x^{(0)})-p^*)\\t{ for }œµ=1e-10\\\\\n",
        "\\t{Quadratic}\n",
        "&Œª(x^{(l)})‚â§Œ∑&2Œª(x^{(l+1)})‚â§(2Œª(x^{(l)}))^2&t=1&l-k‚â§\\log_2(\\log_2(\\/{1}{œµ}))‚âà6\\t{ for }œµ=1e-10\n",
        "\\EC$\n",
        "\n",
        "  - Let $\\tilde{f}(t)=f(x+tŒîx_\\t{nt})$, then $\\tilde{f}'(0)=g^‚ä§Œîx=-Œª(x)^2$ and $\\tilde{f}''(0)=Œîx^‚ä§HŒîx=Œª(x)^2$.\n",
        "  Backtracking exit condition $f(x+tŒîx_\\t{nt})‚â§f(x)-Œ±tŒª(x)^2$\n",
        "  $\\dg{‚áí}\\green{\\tilde{f}(t)‚â§\\tilde{f}(0)-Œ±tŒª(x)^2}$\n",
        "\n",
        "  - $\\green{\\tilde{f}(t)‚â§\\tilde{f}(0)-tŒª(x)^2-tŒª(x)-\\ln(1-tŒª(x))}$.\n",
        "  Proof:\n",
        "  $\\tilde{f}''(t)‚â§\\/{\\tilde{f}''(0)}{(1-t\\tilde{f}''(0)^{1/2})^2}$ integrate twice.\n",
        "\n",
        "  - $\\green{t‚â•\\/{Œ≤}{1+Œª(x)}}$ exits backtracking.\n",
        "  Proof:\n",
        "  Let $\\hat{t}=\\/{1}{1+Œª(x)}$.\n",
        "  Then\n",
        "  $\\tilde{f}(\\hat{t})‚â§\\tilde{f}(0)-\\/{Œª(x)^2+Œª(x)}{1+Œª(x)}-\\ln(1-\\/{Œª(x)}{1+Œª(x)})$\n",
        "  $=\\tilde{f}(0)\\purple{-Œª(x)+\\ln(1+Œª(x))}$.\n",
        "  Using known inequality\n",
        "  $-x+\\ln(1+x)‚â§-\\/{x^2}{2(1+x)},\\ ‚àÄx$\n",
        "  $\\dg{‚áí}\\purple{-Œª(x)+\\ln(1+Œª(x))}‚â§-\\/{Œª(x)^2}{2(1+Œª(x))}$\n",
        "  $\\dg{‚áí}\\tilde{f}(\\hat{t})‚â§\\tilde{f}(0)-\\/{Œª(x)^2}{2(1+Œª(x))}$\n",
        "  $‚â§\\tilde{f}(0)-Œ±\\hat{t}Œª(x)^2$.\n",
        "  Therefore $\\hat{t}=\\/{1}{1+Œª(x)}$ meets backtracking exit condition and $t‚â•\\/{Œ≤}{1+Œª(x)}$ exits damped phase backtracking.\n",
        "\n",
        "  - $\\red{Œ≥=Œ±Œ≤\\/{Œ∑^2}{1+Œ∑}}$ is damped phase minimum drop.\n",
        "  Proof:\n",
        "  Substitute smallest possible $t‚â•\\/{Œ≤}{1+Œª(x)}$ and $Œª(x)>Œ∑$ into backtracking exit\n",
        "  $\\tilde{f}(t)-\\tilde{f}(0)‚â§-Œ±tŒª(x)^2$\n",
        "  $‚â§-Œ±Œ≤\\/{Œª(x)^2}{1+Œª(x)}$\n",
        "  $‚â§-Œ±Œ≤\\/{Œ∑^2}{1+Œ∑}=-Œ≥$.\n",
        "\n",
        "  - $\\green{Œª(x)‚â§\\/{1-2Œ±}{2}}$ when $t=1$.\n",
        "  Proof:\n",
        "  $\\tilde{f}(1)‚â§\\tilde{f}(0)-Œª(x)^2-Œª(x)-\\ln(1-Œª(x))$.\n",
        "  Using known Taylor bound inequality and to exit backtracking with $t=1$ we have\n",
        "  $-x-\\ln(1-x)‚â§\\/{1}{2}x^2+x^3,\\ x‚â§0.81$\n",
        "  $\\dg{‚áí}\\tilde{f}(1)‚â§\\tilde{f}(0)-Œª(x)^2+\\/{1}{2}Œª(x)^2+Œª(x)^3$\n",
        "  $‚â§\\tilde{f}(0)-Œ±Œª(x)^2$\n",
        "  $\\dg{‚áí}-\\/{1}{2}Œª(x)^2+Œª(x)^3‚â§-Œ±Œª(x)^2$.\n",
        "\n",
        "  - $\\green{Œª(x^+)‚â§\\/{Œª(x)^2}{(1-Œª(x))^2}}$.\n",
        "  Proof:\n",
        "  Let $g(1)=‚àáf(x^+)$, $H(1)=‚àá^2f(x^+)$, $v=Œîx_\\t{nt}=-H^{-1}g$.\n",
        "  From self-concordance\n",
        "  $\\n{(H(t)-H)v}_{H^{-1}}‚â§(\\/{1}{(1+tŒª)^2}-1)^2\\n{v}_H$.\n",
        "  Then from the Fundamental Theorem of Calculus\n",
        "  $g(1)=g+‚à´_0^1H(t)v\\ dt$\n",
        "  $=‚à´_0^1(H(t)-H)v\\ dt$\n",
        "  $\\dg{‚áí}\\n{g(1)}_{H^{-1}}=‚à´_0^1\\n{(H(t)-H)v}_{H^{-1}}\\ dt$\n",
        "  $‚â§‚à´_0^1(\\/{1}{(1+tŒª)^2}-1)^2\\n{v}_H\\ dt$\n",
        "  $=\\n{v}_H[\\/{1}{Œª(1-tŒª)}-t]_0^1$\n",
        "  $\\dg{‚áí}\\purple{\\n{g(1)}_{H^{-1}}}=\\/{Œª^2}{1-Œª}$.\n",
        "  By definition\n",
        "  $Œª(x^+)=g(1)^‚ä§H(1)^{-1}g(1)$\n",
        "  $=\\purple{\\n{g(1)}_{H(1)^{-1}}}$.\n",
        "  From self-condordance\n",
        "  $H(1)‚âΩ\\/{H}{(1+Œª)^2}$\n",
        "  $\\dg{‚áí} H(1)^{-1}‚âº(1+Œª)^2H^{-1}$\n",
        "  $\\dg{‚áí}g(1)^‚ä§H(1)^{-1}g(1)‚â§(1+Œª)^2g(1)^‚ä§H^{-1}g(1)$\n",
        "  $\\dg{‚áí}\\purple{\\n{g(1)}_{H(1)^{-1}}}‚â§(1+Œª)\\purple{\\n{g(1)}_{H^{-1}}}$\n",
        "  $=\\/{Œª^2(1+Œª)}{1-Œª}$\n",
        "  $=\\/{Œª^2(1-Œª^2)}{(1-Œª)^2}$\n",
        "  $‚â§\\/{Œª^2}{(1-Œª)^2}$\n",
        "\n",
        "  - $\\red{2Œª(x^+)‚â§(2Œª(x))^2},\\ Œª(x)‚â§\\/{1}{4}$.\n",
        "  Proof:\n",
        "  $Œª(x^+)‚â§\\/{Œª(x)^2}{(1-Œª(x))^2}‚â§2Œª(x)^2$ if $Œª(x)‚â§\\/{1}{4}$.\n",
        "\n",
        "  - $\\red{Œ∑=\\/{1-2Œ±}{4}}$.\n",
        "  Proof:\n",
        "  By definition $Œª(x)‚â§Œ∑$ to enter quadratic phase.\n",
        "  Given $Œª(x)‚â§\\/{1-2Œ±}{2}$ to exit backtracking with $t=1$\n",
        "  and $Œª(x)‚â§\\/{1}{4}$, then $Œ∑$ just needs to be smaller than both of those."
      ],
      "metadata": {
        "id": "7ZVW1Rsk3B8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Equality Constrained Minimization"
      ],
      "metadata": {
        "id": "f60C_3zKfpNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equality constraint problem** $\\BC\\t{minimize}&f(x)&f‚ààC^2\\t{ is convex}\\\\\n",
        "\\t{subject to}&Ax=b&\\rk(A‚àà‚Ñù^{p√ón})=p< n\n",
        "\\EC$ has KKT $\\red{\\BC\n",
        "Ax^*=b\\\\\n",
        "‚àáf(x^*)+A^‚ä§ŒΩ^*=0\n",
        "\\EC}$.\n",
        "\n",
        "- **Approaches**: To solve the problem, (1) Eliminate equality constraint and treat as unconstrained problem; (2) solve the dual as unconstrained problem; (3) extend Newton's method to handle equality.\n",
        "\n",
        "- **Equality constrained quadratic**: $\\BC\\t{minimize}&f(x)=\\/{1}{2}x^‚ä§Px+q^‚ä§x+r\\\\\n",
        "\\t{subject to}&Ax=b\n",
        "\\EC$ with KKT $\\BC\n",
        "Ax^*=b\\\\\n",
        "Px^*+q+AŒΩ^*=0\n",
        "\\EC$\n",
        "rewritten as **KKT system**\n",
        "$\\red{\\BM P&A^‚ä§\\\\A&0\\EM\\BM x^*\\\\ŒΩ^*\\EM=\\BM-q\\\\b\\EM}$ has unique solution if **KKT matrix** $\\BM P&A^‚ä§\\\\A&0\\EM‚àà‚Ñù^{(n+p)√ó(n+p)}$ is nonsingular.\n",
        "\n",
        "  - **Fredholm alternative** shows through feasibility problem that the KKT system has no solution (not feasible) iff $‚àÉv‚àà‚Ñù^n,\\ w‚àà‚Ñù^p:\\ \\blue{Pv+A^‚ä§w=0,\\ Av=0,\\ -q^‚ä§v+b^‚ä§w>0}$.\n",
        "  The KKT system has no solution if either $Ax=b$ is not feasible $b‚àâ\\span(A)$ or problem is unbounded below.\n",
        "\n",
        "  - **Unbounded scenario**: Assume Fredholm conditions and $b‚àà\\span(A)$.\n",
        "  Let $x=\\hat{x}+tv,\\ A\\hat{x}=b$.\n",
        "  Then\n",
        "  $f(\\hat{x}+tv)=\\/{1}{2}(\\hat{x}+tv)^‚ä§P(\\hat{x}+tv)+q^‚ä§(\\hat{x}+tv)+r$\n",
        "  $=f(\\hat{x})+t(v^‚ä§P\\hat{x}+q^‚ä§v)+\\/{1}{2}t^2v^‚ä§Pv$\n",
        "  $‚áí\\BC\n",
        "  \\/{1}{2}t^2v^‚ä§Pv=-\\/{1}{2}t^2v^‚ä§A^‚ä§w=-\\/{1}{2}t^2(Av)^‚ä§w=0\\\\\n",
        "  v^‚ä§P\\hat{x}+q^‚ä§v=-w^‚ä§A\\hat{x}+q^‚ä§v=\\purple{-b^‚ä§w+q^‚ä§v}< 0\n",
        "  \\EC$\n",
        "  $‚áíf(\\hat{x}+tv)=f(\\hat{x})+t(\\purple{-b^‚ä§w+q^‚ä§v})$\n",
        "\n",
        "  - **Nonsingular KKT matrix**: If $P‚àà\\v{S}_{++}^n$ then it must be nonsingular. If $P‚àà\\v{S}_+^n$ then it must be positive definite on the nullspace of $A$. I.e., $Ax=0,x\\neq0‚áíx^‚ä§Px>0$ or $\\null(P)‚à©\\null(A)=\\{0\\}$.\n",
        "\n",
        "- **Equality constraint elimination**: $\\{x|Ax=b\\}=\\{Fz+\\hat{x}|z‚àà‚Ñù^{n-p},\\span(F)=\\null(A)\\}$.\n",
        "Then reduce the problem\n",
        "$\\BC\\t{minimize}&f(x)\\\\\n",
        "\\t{subject to}&Ax=b\n",
        "\\EC‚áí\\BC\\t{minimize}&\\tilde{f}(z)=f(Fz+\\hat{x})\\EC,\\ x^*=Fz^*+\\hat{x},\\ ŒΩ^*=-(AA^‚ä§)A‚àáf(x^*)$\n",
        "\n",
        "  - **Elimination matrix**: Given suitable $F$, any other suitable $\\tilde{F}=FT$ for some nonsingular $T$.\n",
        "\n",
        "- **Equality constrained problem dual**: $g(ŒΩ)=-b^‚ä§ŒΩ+\\inf_x(f(x)+(A^‚ä§ŒΩ)^‚ä§x)$\n",
        "$=-b^‚ä§ŒΩ-f^*(-A^‚ä§ŒΩ)$.\n"
      ],
      "metadata": {
        "id": "RW6BOIkEftvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feasible Start Newton's Method**: Starting $x‚àà\\m{F}$, $\\BC\n",
        "1. &\\t{compute }Œîx_\\t{nt},\\ Œª^2 \\\\\n",
        "2. &\\t{exit if }Œª^2/2‚â§œµ\\\\\n",
        "3. &\\t{backtracking line search }t>0\\\\\n",
        "4. &\\t{update }x^+=x+tŒîx_\\t{nt}\n",
        "\\EC$\n",
        "\n",
        "- **Newton's step**: Let $Ax=b$, then\n",
        "$\\BC\\t{minimize}_v&\\red{\\hat{f}(x+v)=f(x)+‚àáf(x)^‚ä§v+\\/{1}{2}v^‚ä§‚àá^2f(x)v}\\\\\n",
        "\\t{subject to}&A(x+v)=b‚áíAv=0\n",
        "\\EC,\\ \\red{Œîx_\\t{nt}=v^*}$ is equality constrained quadratic problem, which has KKT system\n",
        "$\\red{\\BM ‚àá^2f(x)&A^‚ä§\\\\A&0\\EM\\BMŒîx_\\t{nt}\\\\ŒΩ\\EM=\\BM-‚àáf(x)\\\\0\\EM}$\n",
        "\n",
        "  - **$Œîx_\\t{nt}$ is calculated through the block system** and is defined to minimize $\\hat{f}(x+v)$ while obeying $Ax=b$.\n",
        "\n",
        "  - **Linearized gradient**:\n",
        "  Searching for $Œîx_\\t{nt}$ where $x^*‚âàx+Œîx_\\t{nt}$ and Taylor's linear gradient approximation $‚àáf(x+Œîx_\\t{nt})‚âà‚àáf(x)+‚àá^2f(x)Œîx_\\t{nt}$ satisfy KKT conditions arrives at the same block system\n",
        "  $\\BC\n",
        "  Ax^*=b&\\dg{‚áí}AŒîx_\\t{nt}=0\\\\\n",
        "  ‚àáf(x^*)+A^‚ä§ŒΩ^*=0&\\dg{‚áí}‚àáf(x)+‚àá^2f(x)Œîx_\\t{nt}+A^‚ä§ŒΩ^*=0\n",
        "  \\EC\\dg{‚áí}\\BM ‚àá^2f(x)&A^‚ä§\\\\A&0\\EM\\BMŒîx_\\t{nt}\\\\ŒΩ\\EM=\\BM-‚àáf(x)\\\\0\\EM$\n",
        "\n",
        "- **Newton decrement**: $\\red{Œª(x)^2=Œîx_\\t{nt}^‚ä§‚àá^2f(x)Œîx_\\t{nt}^2}$.\n",
        "From KKT system\n",
        "$Hv+A^‚ä§ŒΩ=-g$\n",
        "$\\dg{‚áí}v^‚ä§Hv+(Av)^‚ä§ŒΩ=-v^‚ä§g$\n",
        "$\\dg{‚áí}\\blue{Œª(x)^2=-‚àáf(x)^‚ä§Œîx_\\t{nt}}$\n",
        "\n",
        "  - **Suboptimality**:\n",
        "  $f(x)-\\inf_v\\{\\hat{f}(x+v)|A(x+v)=b\\}$\n",
        "  $=f(x)-\\hat{f}(x+Œîx_\\t{nt})$\n",
        "  $=\\blue{\\/{1}{2}Œª(x)^2}$.\n",
        "  Proof: by Taylor's\n",
        "  $\\hat{f}(x+Œîx_\\t{nt})=f(x)+‚àáf(x)^‚ä§Œîx_\\t{nt}+\\/{1}{2}Œîx_\\t{nt}^‚ä§‚àá^2f(x)Œîx_\\t{nt}$\n",
        "  $=f(x)-Œª(x)^2+\\/{1}{2}Œª(x)^2$\n",
        "  $‚áí\\blue{\\hat{f}(x+Œîx_\\t{nt})=f(x)-\\/{1}{2}Œª(x)^2}$.\n",
        "\n",
        "  - **Line search**: $\\/{d}{dt}f(x+tŒîx_\\t{nt})|_{t=0}=‚àáf(x)^‚ä§Œîx_\\t{nt}=-Œª(x)^2$ and $AŒîx_\\t{nt}=0$ guarantees Newton's step is always a feasible descent direction.\n",
        "\n",
        "  - **Affine invariance**: Let $\\bar{f}(y)=f(By),\\ B‚àà‚Ñù^{n√ón}$ nonsingular.\n",
        "  Then $\\BC\n",
        "  ‚àá\\bar{f}(y)=B^‚ä§‚àáf(By)\\\\\n",
        "  ‚àá^2\\bar{f}(y)=B^‚ä§‚àá^2f(By)B\n",
        "  \\EC$.\n",
        "  Then $\\BM B^‚ä§‚àá^2f(By)B&(AB)^‚ä§\\\\AB&0\\EM\\BMŒîy_\\t{nt}\\\\ŒΩ\\EM=\\BM-B^‚ä§‚àáf(x)\\\\0\\EM$\n",
        "  $‚áíBŒîy_\\t{nt}=Œîx_\\t{nt}$.\n",
        "\n",
        "- **Equivalence with elimination** $\\BC\\t{minimize}&\\tilde{f}(z)=f(Fz+\\hat{x})\\EC$ where $Œîz_\\t{nt},\\tilde{Œª}(z)^2$ are Newton step and decrement.\n",
        "\n",
        "  - $\\green{FŒîz_\\t{nt}=Œîx_\\t{nt}}$.\n",
        "  Proof:\n",
        "  By affine invariance,\n",
        "  $‚àá\\tilde{f}(z)=F^‚ä§‚àáf(x)$ and\n",
        "  $‚àá^2\\tilde{f}(z)=F^‚ä§‚àá^2f(x)F$.\n",
        "  Then $Œîz_\\t{nt}=-H^{-1}g=-(F^‚ä§‚àá^2f(x)F)^{-1}F^‚ä§‚àáf(x)$\n",
        "  $=-F^{-1}‚àá^2f(x)^{-1}‚àáf(x)$\n",
        "  $\\dg{‚áí}FŒîz_\\t{nt}=-FF^{-1}‚àá^2f(x)^{-1}‚àáf(x)$\n",
        "  $=‚àá^2f(x)^{-1}‚àáf(x)$\n",
        "  $=Œîx_\\t{nt}$.\n",
        "\n",
        "  - $\\green{\\tilde{Œª}(z)^2=Œª(x)^2}$.\n",
        "  Proof:\n",
        "  $\\tilde{Œª}(z)^2=-‚àá\\tilde{f}(z)^‚ä§Œîz_\\t{nt}$\n",
        "  $=-‚àáf(x)^‚ä§FF^{-1}Œîx_\\t{nt}$\n",
        "  $=Œª(x)^2$."
      ],
      "metadata": {
        "id": "F_Z9Jrz91bVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infeasible Start Newton's Method**: Starting $x‚àà\\dom(f)$, $\\BC\n",
        "1. &\\t{compute }Œîx_\\t{nt},\\ ŒîŒΩ_\\t{nt} \\\\\n",
        "2. &\\t{backtracking line search }t=1\\\\\n",
        "&\\t{while }\\n{r(x+tŒîx_\\t{nt},ŒΩ+tŒîŒΩ_\\t{nt})}_2>(1-Œ±t)\\n{r(x,ŒΩ)}_2\\\\\n",
        "&\\quad t=Œ≤t\\\\\n",
        "3. &\\t{update }x^+=x+tŒîx_\\t{nt},\\ ŒΩ^+=ŒΩ+tŒîŒΩ_\\t{nt}\n",
        "\\EC$\n",
        "\n",
        "- **Newton's step**: Searching for $Œîx$ where $x^*‚âàx+Œîx$ and Taylor's 1st order gradient approximation $‚àáf(x+Œîx)‚âà‚àáf(x)+‚àá^2f(x)Œîx$ satisfy KKT gives block system and $Œîx_\\t{nt}$\n",
        "$\\BC\n",
        "Ax^*=b&\\dg{‚áí}A(x+Œîx)=b\\dg{‚áí}AŒîx=b-Ax\\\\\n",
        "‚àáf(x^*)+A^‚ä§ŒΩ^*=0&\\dg{‚áí}‚àáf(x)+‚àá^2f(x)Œîx+A^‚ä§ŒΩ^*=0\n",
        "\\EC$\n",
        "$\\dg{‚áí}\\red{\\BM ‚àá^2f(x)&A^‚ä§\\\\A&0\\EM\\BMŒîx_\\t{nt}\\\\ŒΩ^*\\EM=\\BM-‚àáf(x)\\\\\\purple{b-Ax}\\EM}$.\n",
        "\n",
        "- **Residuals**: $\\BC\n",
        "Ax^*-b=0&\\dg{‚áí}r_\\t{prim}(x,ŒΩ)=Ax-b\\\\\n",
        "‚àáf(x^*)+A^‚ä§ŒΩ=0&\\dg{‚áí}r_\\t{dual}(x,ŒΩ)=‚àáf(x)+A^‚ä§ŒΩ\\\\\n",
        "\\EC\\dg{‚áí}r(y)=(r_\\t{dual}(x,ŒΩ),r_\\t{prim}(x,ŒΩ)),\\ y=(x,ŒΩ)$\n",
        "sets up a residual function with optimality condition $r(y^*)=0,\\ y^*=(x^*,ŒΩ^*)$.\n",
        "Now we setup $y^*‚âày+Œîy$ and Taylor's 1st order $\\hat{r}(y+Œîy)=r(y)+Dr(y)Œîy$ with Jacobian\n",
        "$Dr(y)=\\BM‚àá_xr_\\t{dual}&‚àá_ŒΩr_\\t{dual}\\\\\n",
        "‚àá_xr_\\t{prim}&‚àá_ŒΩr_\\t{prim}\\EM=\\BM‚àá^2f(x)&A^‚ä§\\\\\n",
        "A&0\\EM$.\n",
        "\n",
        "  - **Zero residual**: Find $Œîy$ such that $\\hat{r}(y+Œîy)=0$\n",
        "  $\\dg{‚áí}\\green{Dr(y)Œîy=-r(y)}$\n",
        "  $\\dg{‚áí}\\BM‚àá^2f(x)&A^‚ä§\\\\A&0\\EM\\BMŒîx\\\\ŒîŒΩ\\EM=\\BM-‚àáf(x)-A^‚ä§ŒΩ\\\\b-Ax\\EM$.\n",
        "  The top row\n",
        "  $‚àá^2f(x)Œîx+A^‚ä§ŒîŒΩ=-‚àáf(x)-A^‚ä§ŒΩ$\n",
        "  $\\dg{‚áí}‚àá^2f(x)Œîx+A^‚ä§(ŒΩ+ŒîŒΩ)=\\purple{-‚àáf(x)}$\n",
        "  $\\dg{‚áí}\\BM‚àá^2f(x)&A^‚ä§\\\\A&0\\EM\\BMŒîx\\\\ŒΩ^*\\EM=\\BM-‚àáf(x)\\\\b-Ax\\EM$\n",
        "  arrives at the same block system and same Newton's step $Œîx_\\t{nt}$.\n",
        "\n",
        "- **Newton decrement**: $\\/{d}{dt}f(x+tŒîx)|_{t=0}=‚àáf(x)^‚ä§Œîx$\n",
        "$=-Œîx^‚ä§(\\purple{-‚àáf(x)})$\n",
        "$=-Œîx^‚ä§(‚àá^2f(x)Œîx+A^‚ä§ŒΩ^*)$\n",
        "$=-Œª(x)^2-(AŒîx)^‚ä§ŒΩ^*$\n",
        "$=-Œª(x)^2+(Ax-b)^‚ä§ŒΩ^*$\n",
        "is not necessarily negative as algorithm may have to go uphills to reach feasible set.\n",
        "Therefore $Œª(x)^2$ is **not used** in infeasible start.\n",
        "  \n",
        "  - **Norm residual**: $\\/{d}{dt}\\n{r(y+tŒîy)}_2^2|_{t=0}=2r(y+tŒîy)^‚ä§Dr(y+tŒîy)Œîy|_{t=0}$\n",
        "  $=2r(y)^‚ä§Dr(y)Œîy$\n",
        "  $=-2r(y)^‚ä§r(y)$\n",
        "  $\\dg{‚áí}\\blue{\\/{d}{dt}\\n{r(y+tŒîy)}_2^2|_{t=0}=-2\\n{r(y)}_2^2< 0}$\n",
        "  is used instead of Newton's decrement.\n",
        "\n",
        "- **Feasibility after $t=1$**: the bottom row says $AŒîx_\\t{nt}=b-Ax‚áíA(x+Œîx_\\t{nt})=b$.\n",
        "Newton's step $Œîx_\\t{nt}$ is designed to reach feasibility $r_\\t{prim}^+=0$ after one full step.\n",
        "If $t‚àà[0,1]$, then $\\blue{r_\\t{prim}^+=(1-t)r_\\t{prim}}$.\n",
        "\n",
        "- **Comparison with feasible start**: Depending $\\dom(f)$, it may be easier to run a \"Phase I\" method to find a point $x‚àà\\m{F}$ and then use feasible start. A popular option is to run infeasible start Newton's until feasibility is reached, then run feasible start; but this relies on knowing $Ax=b$ is feasible, which a Phase I method would find out.\n"
      ],
      "metadata": {
        "id": "Rh6FsvaoxozI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Interior-Point Methods"
      ],
      "metadata": {
        "id": "cJO5Xcni9KEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inequality constraint**: $\\red{\\BC\\t{minimize}&f_0(x)&f_0,f_i‚ààC^2\\t{ convex, satisfying Slater's}\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0,\\ Ax=b&i=1,...,m,\\ \\rk(A‚àà‚Ñù^{p√ón})=p< n\\EC}$\n",
        "$‚áí\\BC\\t{minimize}&f_0(x)+\\sum_i\\green{I_-(f_i(x))}\\\\\n",
        "\\t{subject to}&Ax=b\n",
        "\\EC$. However $\\green{I_-(u)=\\BC0&\\t{if }u‚â§0\\\\‚àû&\\t{otherwise}\\EC}$ is not differentiable for Newton's method.\n",
        "\n",
        "- **Logarithmic barrier**: $\\red{\\hat{I}_-(u)=-\\/{1}{t}\\ln(-u),\\ \\dom(\\hat{I})=-‚Ñù_{++}}$ where higher $t>0$ tunes the barrier closer to an infinity wall is convex and differentiable.\n",
        "The problem becomes $\\BC\n",
        "\\t{minimize}&f_0-\\/{1}{t}\\sum_i\\ln(-f_i(x))\\\\\n",
        "\\t{subject to}&Ax=b\n",
        "\\EC$\n",
        "\n",
        "  - **Barrier function** $\\red{œï(x)=-\\sum_i\\ln(-f_i(x)),\\ \\dom(œï)=\\{x‚àà‚Ñù^n|f_i(x)\\purple{<} 0\\ ‚àÄi\\}}$\n",
        "  has gradient and Hessian\n",
        "  $\\blue{‚àáœï(x)=-\\sum_i\\/{‚àáf_i(x)}{f_i(x)}}$ and\n",
        "  $\\blue{‚àá^2œï(x)=\\sum_i\\/{‚àáf_i(x)‚àáf_i(x)^‚ä§}{f_i(x)^2}-\\sum_i\\/{‚àá^2f_i(x)}{f_i(x)}}$\n",
        "\n",
        "- **Central path**: Equivalent problem $\\BC\n",
        "\\t{minimize}&tf_0(x)+œï(x)\\\\\n",
        "\\t{subject to}&Ax=b\n",
        "\\EC$ gives optimal $x^*(t)$ as a function of $t$ in $\\m{F}$ called the central path associated with the original problem.\n",
        "\n",
        "  - Point $x^*(t)$ lies on the central path iff $\\BC x^*(t)‚àà\\m{F}:\\ Ax^*(t)=b,\\ f_i(x^*(t))< 0\\ ‚àÄi\\\\‚àÉ\\hat{ŒΩ}‚àà‚Ñù^p:\\ 0=t‚àáf_0(x^*(t))+‚àáœï(x^*(t))+A^‚ä§\\hat{ŒΩ}\\EC$\n",
        "\n",
        "  - 11.8: $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&Ax‚â§b\n",
        "  \\EC‚áí\\BC\\t{minimize}&tc^‚ä§x-\\sum_i\\ln(a_i^‚ä§x-b_i)\\EC$.\n",
        "  Then $x^*(t)$ must satisfy\n",
        "  $\\BC\n",
        "  a_i^‚ä§x^*(t)< b_i\\\\\n",
        "  tc=‚àáœï(x^*(t))\n",
        "  \\EC$.\n",
        "  The second condition $tc=‚àáœï(x^*(t))$ says the hyperplane $\\{cx=cx^*(t)\\}=x^*(t)+c^\\perp$ must be tangential to the level set of all the log barriers (the inner dashlines on Figure 11.2)."
      ],
      "metadata": {
        "id": "CpzYpu8S9PTg"
      }
    }
  ]
}