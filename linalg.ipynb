{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TGLs8-tCVnHg",
        "079tQIsukGEt",
        "7Qn60SWsSQD9",
        "vB6PP0enPpMx",
        "gsx_1C1PpsRx",
        "duhBEROv2rcb",
        "t96JCZrR4Ory",
        "mHFseZC0wEXH",
        "w_2sEW1Guy5-",
        "SnCzrLICYbWf",
        "3XRLm8J7Zd27",
        "lCXdVkbcqRO3"
      ],
      "authorship_tag": "ABX9TyPY/bxgEGDQqgWLj28ydveV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/realdope/math/blob/main/linalg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\require{color}\n",
        "\\newcommand{\\arr}[1]{\\overset{\\small #1}{‚Üí}}\n",
        "\\newcommand{\\larr}[1]{\\overset{\\small #1}{‚ü∂}}\n",
        "\\newcommand{\\imply}[1]{\\overset{\\small #1}{‚áí}}\n",
        "\\newcommand{\\red}[1]{\\color{OrangeRed}{#1}}\n",
        "\\newcommand{\\blue}[1]{\\color{RoyalBlue}{#1}}\n",
        "\\newcommand{\\green}[1]{\\color{ForestGreen}{#1}}\n",
        "\\newcommand{\\purple}[1]{\\color{DarkMagenta}{#1}}\n",
        "\\newcommand{\\lgray}[1]{\\color{lightgray}{#1}}\n",
        "\\newcommand{\\gray}[1]{\\color{gray}{#1}}\n",
        "\\newcommand{\\dgray}[1]{\\color{darkgray}{#1}}\n",
        "\\newcommand{\\E}{\\text{ùîº}}\n",
        "\\newcommand{\\V}{\\text{ùïç}}\n",
        "\\newcommand{\\Var}{\\small\\text{Var}\\normalsize}\n",
        "\\newcommand{\\Cov}{\\small\\text{Cov}\\normalsize}\n",
        "\\newcommand{\\Corr}{\\small\\text{Corr}\\normalsize}\n",
        "\\newcommand{\\Bias}{\\small\\text{Bias}\\normalsize}\n",
        "\\newcommand{\\ob}[2]{\\overbrace{#1}^{\\small#2}}\n",
        "\\newcommand{\\ub}[2]{\\underbrace{#1}_{\\small#2}}\n",
        "\\newcommand{\\m}[1]{\\mathcal{#1}}\n",
        "\\newcommand{\\M}[1]{\\mathscr{#1}}\n",
        "\\newcommand{\\e}[1]{\\small{\\exp}\\normalsize\\left\\{#1\\right\\}}\n",
        "\\newcommand{\\t}[1]{\\text{#1}}\n",
        "\\newcommand{\\/}[2]{\\frac{#1}{#2}}\n",
        "\\newcommand{\\v}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\n}[1]{\\lVert#1\\rVert}\n",
        "\\newcommand{\\vn}[1]{\\lVert\\mathbf{#1}\\rVert}\n",
        "\\newcommand{\\Im}{\\small\\text{Im}\\normalsize}\n",
        "\\newcommand{\\dom}{\\small\\text{dom}\\normalsize}\n",
        "\\newcommand{\\epi}{\\small\\text{epi}\\normalsize}\n",
        "\\newcommand{\\null}{\\small\\text{null}\\normalsize}\n",
        "\\newcommand{\\span}{\\small\\text{span}\\normalsize}\n",
        "\\newcommand{\\rk}{\\small\\text{rk}\\normalsize}\n",
        "\\newcommand{\\det}{\\small\\text{det}\\normalsize}\n",
        "\\newcommand{\\ln}{\\small\\text{ln}\\normalsize}\n",
        "\\newcommand{\\dim}{\\small\\text{dim}\\normalsize}\n",
        "\\newcommand{\\tr}{\\small\\text{tr}\\normalsize}\n",
        "\\newcommand{\\diag}{\\small\\text{diag}\\normalsize}\n",
        "\\newcommand{\\BPM}{\\small\\begin{pmatrix}}\n",
        "\\newcommand{\\EPM}{\\end{pmatrix}\\normalsize}\n",
        "\\newcommand{\\BVM}{\\begin{vmatrix}}\n",
        "\\newcommand{\\EVM}{\\end{vmatrix}\\normalsize}\n",
        "\\newcommand{\\BSM}{\\small\\begin{bmatrix}}\n",
        "\\newcommand{\\ESM}{\\end{bmatrix}\\normalsize}\n",
        "\\newcommand{\\BM}{\\small\\left[\\begin{smallmatrix}}\n",
        "\\newcommand{\\EM}{\\end{smallmatrix}\\right]\\normalsize}\n",
        "\\newcommand{\\BC}{\\small\\begin{cases}}\n",
        "\\newcommand{\\EC}{\\end{cases}\\normalsize}\n",
        "\\newcommand{\\Normal}{\\mathcal{N}}\n",
        "\\newcommand{\\liml}{\\lim\\limits}\n",
        "\\newcommand{\\suml}{\\sum\\limits}\n",
        "\\newcommand{\\prodl}{\\prod\\limits}\n",
        "$$\n",
        "# Colley: Vectors"
      ],
      "metadata": {
        "id": "TGLs8-tCVnHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectors**:\n",
        "\n",
        "- $\\v{a}$ and $\\v{b}$ are equal if $a_i=b_i$ for all $i$.\n",
        "\n",
        "- $\\v{c}=\\v{a}+\\v{b}$ is the sum such that $c_i=a_i+b_i$.\n",
        "  - $\\v{a}+\\v{b}=\\v{b}+\\v{a}$ (commutativity)\n",
        "  - $\\v{a}+(\\v{b}+\\v{c})=(\\v{a}+\\v{b})+\\v{c}$ (associativity)\n",
        "  - $\\v{0}$ is zero vector such that $\\v{a}+\\v{0}=\\v{a}$.\n",
        "\n",
        "- $\\v{b}=k\\v{a}$ is scalar multiplication such that $b_i=ka_i$.\n",
        "  - $(k+l)\\v{a}=k\\v{a}+l\\v{a}$ (distributivity)\n",
        "  - $k(\\v{a}+\\v{b})=k\\v{a}+k\\v{b}$ (distributivity)\n",
        "  - $k(l\\v{a})=(kl)\\v{a}=l(k\\v{a})$.\n",
        "\n",
        "- for points $P_1$ and $P_2$, $\\v{c}=\\overrightarrow{P_1P_2}$ is displacement vector such that $c_i=b_i-a_i$.\n",
        "\n",
        "- $\\v{i}=(1,0)$ and $\\v{j}=(0,1)$ are standard basis vectors.\n",
        "\n",
        "**Lines**: $x$ and $y$ are described as functions of parameter $t$. E.g., $\\BC\n",
        "x=2\\cos t\\\\\n",
        "y=2\\sin t\\EC$ where $0‚â§t< 2œÄ$ describes a circle.\n",
        "\n",
        "- Vector parametric equation: A line going through point $\\v{a}$ parallel to direction $\\v{b}$ is described by $\\v{r}(t)=\\v{a}+t\\v{b}$.\n",
        "\n",
        "- Examples:\n",
        "  - Find where line $\\BC\n",
        "  x=t+5\\\\\n",
        "  y=-2t-4\\\\\n",
        "  z=3t+7\n",
        "  \\EC$ intersects plane $3x+2y-7z=2$.\n",
        "  Solve $3(t+5)+2(-2t-4)-7(3t+7)=2$.\n",
        "  - Determine whether $\\BC\n",
        "  x=t+1\\\\\n",
        "  y=5t+6\\\\\n",
        "  z=-2t\\EC$ intersects with $\\BC\n",
        "  x=3t-3\\\\\n",
        "  y=t\\\\\n",
        "  z=t+1\\EC$.\n",
        "  Solve $\\BC\n",
        "  t_1+1=3t_2-3\\\\\n",
        "  5t_1+6=t_2\\\\\n",
        "  -2t_1=t_2+1\\EC$."
      ],
      "metadata": {
        "id": "oPWgarkPFNNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dot product (Inner product)**: The inner product or dot product $\\v{a}‚ãÖ\\v{b}=\\sum_ia_ib_i$ produces a scalar real number.\n",
        "\n",
        "- Properties\n",
        "  - $\\v{a}^‚ä§\\v{a}‚â•0$, and $\\v{a}^‚ä§\\v{a}=0$ iff $\\v{a}=\\v{0}$.\n",
        "  - $\\v{a}^‚ä§\\v{b}=\\v{b}^‚ä§\\v{a}$.\n",
        "  - $\\v{a}^‚ä§(\\v{b}+\\v{c})=\\v{a}^‚ä§\\v{b}+\\v{a}^‚ä§\\v{c}$\n",
        "  - $(k\\v{a})^‚ä§\\v{b}=k(\\v{a}^‚ä§\\v{b})=\\v{a}^‚ä§(k\\v{b})$\n",
        "\n",
        "- **Norm** or **magnitude** $\\vn{a}_2=\\sqrt{\\sum_ia_i^2}=\\sqrt{\\v{a}‚ãÖ\\v{a}}$\n",
        "  - $\\v{a}‚ãÖ\\v{a}=\\vn{a}_2^2$\n",
        "  - $\\n{k\\v{a}}_2=|k|\\vn{a}_2$\n",
        "  - $\\/{\\v{a}}{\\vn{a}_2}$ is a unit vector in the direction of non-zero $\\v{a}$.\n",
        "  \n",
        "- $\\red{\\v{a}^‚ä§\\v{b}=\\vn{a}_2\\vn{b}_2\\cos Œ∏}$\n",
        "  - Proof: the law of cosines for triangle with sides $\\v{a}$, $\\v{b}$, and $\\v{c}$ states\n",
        "  $\\vn{c}^2=\\vn{a}^2+\\vn{b}^2-2\\vn{a}\\vn{b}\\cos Œ∏_{a,b}$.\n",
        "  Let $\\v{c}=\\v{b}-\\v{a}$ then\n",
        "  $2\\vn{a}_2\\vn{b}_2\\cos Œ∏=\\vn{a}_2^2+\\vn{b}_2^2-\\n{\\v{b}-\\v{a}}_2^2$\n",
        "  $=\\v{a}‚ãÖ\\v{a}+\\v{b}‚ãÖ\\v{b}-(\\v{b}-\\v{a})‚ãÖ(\\v{b}-\\v{a})$\n",
        "  $=(\\v{a}‚ãÖ\\v{a}+\\v{b}‚ãÖ\\v{b}-\\v{b}‚ãÖ\\v{b}-\\v{a}‚ãÖ\\v{a})+2\\v{a}‚ãÖ\\v{b}$.\n",
        "  - $\\red{\\cosŒ∏=\\/{\\v{a}^‚ä§\\v{b}}{\\vn{a}_2\\vn{b}_2}}$ is the angle between $\\v{a}$ and $\\v{b}$.\n",
        "  - $0=\\cos(\\/{œÄ}{2})$. Therefore $\\v{a}\\perp\\v{b}‚áí\\v{a}‚ãÖ\\v{b}=0$.\n",
        "\n",
        "- $\\red{\\t{proj}_{\\v{a}}\\v{b}=\\/{\\v{a}^‚ä§\\v{b}}{\\vn{a}_2^2}\\v{a}=\\/{\\v{aa}^‚ä§}{\\vn{a}_2^2}\\v{b}}$ is the projection of $\\v{b}$ onto $\\v{a}$.\n",
        "\n",
        "  - Proof: $\\t{proj}_{\\v{a}}\\v{b}$\n",
        "  $=(\\vn{b}_2\\cos Œ∏)\\/{\\v{a}}{\\vn{a}_2}$\n",
        "  $=\\/{\\vn{a}_2\\vn{b}_2\\cos Œ∏}{\\vn{a}_2}\\/{\\v{a}}{\\vn{a}_2}$\n",
        "  $=\\/{\\v{a}‚ãÖ\\v{b}}{\\vn{a}_2}\\/{\\v{a}}{\\vn{a}_2}$\n",
        "\n",
        "  - **$\\v{u}^‚ä§\\v{b}=\\v{b}^‚ä§\\v{u}$ is the magnitude of the projection of $\\v{b}$ onto unit-vector $\\v{u}$**\n",
        "\n",
        "- **Cauchy-Schwarz inequality**: $\\red{|\\v{x}‚ãÖ\\v{y}|^2‚â§\\vn{x}_2^2\\vn{y}_2^2}$\n",
        "\n",
        "  - $\\red{(\\v{x}^‚ä§\\v{y})^‚ä§(\\v{x}^‚ä§\\v{y})‚â§(\\v{x}^‚ä§\\v{x})(\\v{y}^‚ä§\\v{y})}$\n",
        "\n",
        "  - $\\red{(\\sum_ix_iy_i)^2‚â§(\\sum_ix_i^2)(\\sum_iy_i^2)}$\n",
        "\n",
        "- **Triangle (Minkowski) inequality**: $\\n{\\v{x}+\\v{y}}_2‚â§\\vn{x}_2+\\vn{y}_2$\n",
        "\n",
        "- Algorithm for finding orthogonal vector $\\v{v}$ of $\\v{u}$: Pick a component, say $u_1$, and sum all other components $s=u_2+...u_k$. Then $\\v{v}=(-s,u_1,...,u_1)$.\n",
        "\n",
        "**Cross product**: $\\v{a}√ó\\v{b}$ gives a vector such that $\\n{\\v{a}√ó\\v{b}}_2=\\vn{a}_2\\vn{b}_2\\sin Œ∏$ is the area of the parallelogram formed by $\\v{a}$ and $\\v{b}$. On the right hand where $\\v{a}$ takes the index finger, $\\v{b}$ middle finger, then $\\v{a}√ó\\v{b}$ points in the direction of the thumb.\n",
        "\n",
        "- $\\v{a}√ó\\v{b}=\\BVM\n",
        "\\v{i} & \\v{j} & \\v{k} \\\\\n",
        "a_1 & a_2 & a_3 \\\\\n",
        "b_1 & b_2 & b_3\n",
        "\\EVM$"
      ],
      "metadata": {
        "id": "RPgk8id2oYlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plane** in $‚Ñù^3$ is uniquely defined by (1) a point $P_0=(x_0,y_0,z_0)$ and (2) a normal $\\v{n}=A\\v{i}+B\\v{j}+C\\v{k}$ such that $\\overrightarrow{P_0P}‚ãÖ\\v{n}=0$ for all points $P$ on the plane. This plane is defined by equation $A(x-x_0)+B(y-y_0)+C(z-z_0)=0$ (Colley 1.5)\n",
        "\n",
        "- Examples:\n",
        "  - Given plane $7x+2y-3z=1$, the normal is $\\v{n}=k(7\\v{i}+2\\v{j}-3\\v{k}),\\ k>0$.\n",
        "  - Given 3 points $P_0(1,2,0)$, $P_1(3,1,2)$, and $P_2(0,1,1)$, the normal is $\\v{n}=k(\\overrightarrow{P_0P_1}√ó\\overrightarrow{P_0,P_2}),\\ k>0$.\n",
        "\n",
        "- Vector parametric equation: A plane through point $\\v{a}$ and parallel to nonzero nonparallel vectors $\\v{b}$ and $\\v{c}$ is described by $\\v{x}(s,t)=s\\v{b}+t\\v{c}+\\v{a}$."
      ],
      "metadata": {
        "id": "vHkA6RTA9cdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix multiplication**: $C_{m√óp}=A_{m√ón}B_{n√óp}$, element $c_{ij}$ is dot product of $i$th row of $A$ and $j$th column of $B$: $c_{ij}=\\sum\\limits_{k=1}^na_{ik}b_{kj}$.\n",
        "- By universal convention, $\\v{x}$, $\\v{a}$, $(\\v{x}-\\v{a})$, and all data vectors are $n√ó1$ column matrices. Hence $\\v{x}‚ãÖ\\v{x}=\\v{x}^T\\v{x}$.\n",
        "- Properties:\n",
        "  - $A(BC)=(AB)C$\n",
        "  - $k(AB)=(kA)B=A(kB)$\n",
        "  - $A(B+C)=AB+AC$\n",
        "  - $(A+B)C=AC+BC$\n",
        "  - $AB=AIB=(AC)(C^TB)$ If rows of $C$ are orthogonal\n",
        "  - $AB=AIB=(AC^T)(CB)$ If columns of $C$ are orthogonal."
      ],
      "metadata": {
        "id": "pfs4EVnatVH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colley: Vector Differentiation"
      ],
      "metadata": {
        "id": "079tQIsukGEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7 Indeterminate forms** in limit calculation:\n",
        "- $\\lim\\limits_{x‚Üía}\\/{f(x)}{g(x)}=\\/{0}{0}$ or $\\/{‚àû}{‚àû}$: use L'Hopital's rule: $\\lim\\limits_{x‚Üía}\\/{f(x)}{g(x)}=\\lim\\limits_{x‚Üía}\\/{f'(x)}{g'(x)}$.\n",
        "- $\\lim\\limits_{x‚Üía}f(x)g(x)=0‚ãÖ‚àû$: transform $\\lim\\limits_{x‚Üía}f(x)g(x)=\\lim\\limits_{x‚Üía}\\/{f(x)}{1/g(x)}$\n",
        "- $\\lim\\limits_{x‚Üía}f(x)-g(x)=‚àû-‚àû$: Use first-order Taylor series expansion to reveal the leftover part.\n",
        "- $1^‚àû$, $0^0$, $‚àû^0$: take $\\ln$ then analyze limit of product."
      ],
      "metadata": {
        "id": "e7BnJ5xjBV-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topology**: $\\n{\\v{x}-\\v{a}}=r$ is a sphere centered at $\\v{a}$ with radius $r$. $\\n{\\v{x}-\\v{a}}‚â§r$ is a closed ball. $\\n{\\v{x}-\\v{a}}< r$ is an open ball.\n",
        "  - Set $X‚äÜ‚Ñù^n$ is **open in $‚Ñù^n$** if for every point $\\v{x}‚ààX$, there is some open ball centered at $\\v{x}$ that lies entirely within $X$.\n",
        "  - A **neighborhood** of point $\\v{x}‚ààX$ is an open set in $X$ containing $\\v{x}$.\n",
        "  - Point $\\v{x}‚àà‚Ñù^n$ is **in the boundary of $X‚äÜ‚Ñù^n$** if every open ball at $\\v{x}$ no matter how small contains some points in $X$ and some points not in $X$.\n",
        "  - Set $X‚äÜ‚Ñù^n$ is **closed in $‚Ñù^n$** if it contains all its boundary points.\n",
        "\n",
        "**Function**: For $f:X‚ÜíY$ where $X$ and $Y$ are domain and codomain, $\\t{range} f=\\{y‚ààY|y=f(x)\\t{ for some }x‚ààX\\}$. $f$ is **onto** if $\\t{range} f=Y$: every element in $Y$ has an element in $X$ assigned to it. A function is **one-to-one** if $x_1\\neq x_2‚áî f(x_1)\\neq f(x_2)$: no two elements in $X$ are assigned to a same element in $Y$.\n",
        "\n",
        "- For function $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$, then $\\v{f}(\\v{x})=(f_1(x_1,...,x_n),...,f_m(x_1,...,x_n))$\n",
        "\n",
        "**Limit**: $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{L}$ means we can make $\\n{\\v{f}(\\v{x})-\\v{L}}$ arbitrarily small by keeping $\\n{\\v{x}-\\v{a}}$ sufficiently small. (Colley D2.1)\n",
        "\n",
        "- $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{L}$ means that given any $œµ>0$ you can find a $Œ¥>0$ such that if $\\v{x}‚ààX$ and $0< \\n{\\v{x}-\\v{a}}< Œ¥$, then $\\n{\\v{f}(\\v{x})-\\v{L}}< œµ$. (Colley D2.2) Given any $œµ>0$ you can find a $Œ¥>0$ such that if points $\\v{x}‚ààX$ are inside an open ball centered at $\\v{a}$ with radius $Œ¥$ then points $\\v{f}(\\v{x})$ are inside an openball centered at $\\v{L}$ with radius $œµ$.\n",
        "\n",
        "- If a limit exists, it is unique. That is, If $\\lim_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{L}$ and $\\lim_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{M}$, then $\\v{L}=\\v{M}$. (Colley T2.4)\n",
        "  - Proof:\n",
        "  For any $œµ>0$ we can find $Œ¥_1>0$ such that if $\\n{\\v{x}-\\v{a}}< Œ¥_1$ then\n",
        "  $\\n{\\v{f}(\\v{x})-\\v{L}}< œµ/2$.\n",
        "  Similarly we can find $Œ¥_2>0$ such that if $\\n{\\v{x}-\\v{a}}< Œ¥_2$ then\n",
        "  $\\n{\\v{f}(\\v{x})-\\v{M}}< œµ/2$.\n",
        "  Let $\\n{\\v{x}-\\v{a}}<\\min(Œ¥_1,Œ¥_2)$ then\n",
        "  $\\n{\\v{L}-\\v{M}}$\n",
        "  $=\\n{\\v{L}-\\v{f}(\\v{x})+\\v{f}(\\v{x})-\\v{M}}$\n",
        "  $‚â§\\n{\\v{L}-\\v{f}(\\v{x})}+\\n{\\v{f}(\\v{x})-\\v{M}}$\n",
        "  $< œµ$.\n",
        "  I.e., $\\lim_{\\v{x}‚Üí\\v{a}}\\v{L}=\\v{M}$.\n",
        "\n",
        "- Let $\\v{F},\\v{G}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ be vector functions, $f,g:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be scalar functions, and $k‚àà‚Ñù$. (Colley T2.5)\n",
        "  - If $\\lim_{\\v{x}‚Üí\\v{a}}\\v{F}(\\v{x})=\\v{L}$ and $\\lim_{\\v{x}‚Üí\\v{a}}\\v{G}(\\v{x})=\\v{M}$, then $\\lim_{\\v{x}‚Üí\\v{a}}\\v{F}(\\v{x})+\\v{G}(\\v{x})=\\v{L}+\\v{M}$.\n",
        "  - If $\\lim_{\\v{x}‚Üí\\v{a}}\\v{F}(\\v{x})=\\v{L}$ then $\\lim_{\\v{x}‚Üí\\v{a}}k\\v{F}(\\v{x})=k\\v{L}$.\n",
        "  - If $\\lim_{\\v{x}‚Üí\\v{a}}f(\\v{x})=L$ and $\\lim_{\\v{x}‚Üí\\v{a}}g(\\v{x})=M$ then $\\lim_{\\v{x}‚Üí\\v{a}}f(\\v{x})g(\\v{x})=LM$.\n",
        "  - If $\\lim_{\\v{x}‚Üí\\v{a}}f(\\v{x})=L$, $g(\\v{x})\\neq 0$ for $\\v{x}‚ààX$ and $\\lim_{\\v{x}‚Üí\\v{a}}g(\\v{x})=M$, then $\\lim_{\\v{x}‚Üí\\v{a}}f(\\v{x})/g(\\v{x})=L/M$.\n",
        "\n",
        "- Let $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is a vector-valued function. Then $\\lim_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{L}$ where $\\v{L}=(L_1,...,L_m)$ iff $\\lim_{\\v{x}‚Üí\\v{a}}f_i(\\v{x})=L_i$ for $i=1..m$. (Colley T2.6)\n",
        "\n",
        "**Continuous**: $f:X‚äÜ‚Ñù‚Üí‚Ñù$ is continuous if its graph can be drawn without taking the pen off the paper.\n",
        "\n",
        "- Let $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ and let $\\v{a}‚ààX$. Then $\\v{f}$ is **continuous at $\\v{a}$** if $\\lim_{\\v{x}‚Üí\\v{a}}\\v{f}(\\v{x})=\\v{f}(\\v{a})$. If $\\v{f}$ is continuous at all points in $X$ then $\\v{f}$ is continuous. (Colley D2.7)\n",
        "\n",
        "- Let $\\v{F},\\v{G}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ be vector functions, $f,g:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be scalar functions, and $k‚àà‚Ñù$.\n",
        "  - If $\\v{F},\\v{G}$ are continuous at $\\v{a}‚ààX$ then $\\v{F}+\\v{G}$ is continuous at $\\v{a}$.\n",
        "  - If $\\v{F}$ is contniuous at $\\v{a}‚ààX$ then $k\\v{F}$ is continuous at $\\v{a}$.\n",
        "  - If $f,g$ are continuous at $\\v{a}‚ààX$ then $f(\\v{x}g(\\v{x})$ and $f(\\v{x})/g(\\v{x})$ are continuous at $\\v{a}$.\n",
        "  - $\\v{F}$ is continuous at $\\v{a}‚ààX$ iff $F_i,\\ i=1..m$ are all continuous at $\\v{a}$.\n",
        "\n",
        "- If $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$, $\\t{range} \\v{f}‚äÜY$, and $g:Y‚äÜ‚Ñù^m‚Üí‚Ñù^p$ are continuous functions, then $g(f):X‚äÜ‚Ñù^n‚Üí‚Ñù^p$ is defined and also continuous. (Colley T2.8)"
      ],
      "metadata": {
        "id": "5uqUaHl7Be89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partial**: Suppose $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ and let $\\v{x}=(x_1,...,x_n)$. Then a **partial function with respect to $x_i$** $F(x_i)=f(a_1,...,x_i,...,a_n)$ is obtained by holding all variables constant except $x_i$. (Colley D3.1)\n",
        "\n",
        "- **Partial derivative of $f$ with respect to $x_i$** is the derivative of the partial function with respect to $x_i$: $\\/{‚àÇf}{‚àÇx_i}=\\lim\\limits_{h‚Üí0}\\/{f(x_1,...,x_i+h,...,x_n)-f(x_1,...,x_n)}{h}$. Notation $\\/{‚àÇf}{‚àÇx_i}$, $D_{x_i}f$, or $f_{x_i}$. (Colley D3.2)\n",
        "\n",
        "- **Gradient** $‚àáf(\\v{x})=(\\/{‚àÇf}{‚àÇx_1},...,\\/{‚àÇf}{‚àÇx_n})$,\n",
        "$‚àáf(\\v{a})=(\\/{‚àÇf}{‚àÇx_1}(\\v{a}),...,\\/{‚àÇf}{‚àÇx_n}(\\v{a}))$.\n",
        "  - $Df(\\v{a})=\\BSM\n",
        "  \\/{‚àÇf}{‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇf}{‚àÇx_n}(\\v{a})\n",
        "  \\ESM$\n",
        "  is row matrix:\n",
        "  $‚àáf(\\v{a})‚ãÖ(\\v{x}-\\v{a})=Df(\\v{a})(\\v{x}-\\v{a})=f(\\v{a})+\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})(x_i-a_i)$.\n",
        "  - Let $X$ be open in $‚Ñù^n$ and let $\\v{f}:X‚Üí‚Ñù^m$, then $D\\v{f}(\\v{x})=\\BSM\n",
        "  \\/{‚àÇf_1}{‚àÇx_1} & ... & \\/{‚àÇf_1}{‚àÇx_n} \\\\\n",
        "  \\vdots & \\ddots & \\vdots \\\\\n",
        "  \\/{‚àÇf_m}{‚àÇx_1} & ... & \\/{‚àÇf_m}{‚àÇx_n}\n",
        "  \\ESM$, and $D\\v{f}(\\v{a})(\\v{x}-\\v{a})$ is $m√ó1$.\n",
        "\n",
        "**Differentiable** ($‚Ñù‚Üí‚Ñù$): the derivative of $f:X‚äÜ‚Ñù‚Üí‚Ñù$ at $a‚ààX$ is $f'(a)=\\lim\\limits_{h‚Üí0}\\/{f(a+h)-f(a)}{h}$ or $f'(a)=\\lim\\limits_{x‚Üía}\\/{f(x)-f(a)}{x-a}$. If $f'(a)$ exists then $f$ is differentiable at $a$.\n",
        "\n",
        "- If $f:X‚äÜ‚Ñù‚Üí‚Ñù$ is differentiable at $a‚ààX$ then tangent line at point $(a,f(a))$ described by $h(x)=f(a)+f'(a)(x-a)$ (i.e., $y=mx+b$ shifted to the right by $a$) is a good linear approximation of $f(x)$ at $a$.\n",
        "\n",
        "- For $f:X‚äÜ‚Ñù^2‚Üí‚Ñù$ the tangent plane at point $(a,b,f(a,b))$ is described by a pair of parametric lines along $x$ and $y$ $\\BC\n",
        "\\v{l}_x(t)=(a,b,f(a,b))+t(1,0,\\/{‚àÇf}{‚àÇx}(a,b)) \\\\\n",
        "\\v{l}_y(t)=(a,b,f(a,b))+t(1,0,\\/{‚àÇf}{‚àÇy}(a,b))\n",
        "\\EC$.\n",
        "The normal of the plane is $\\v{n}=\\/{d\\v{l}_x}{dt}√ó\\/{d\\v{l}_y}{dt}$\n",
        "$=\\BVM\n",
        "\\v{i} & \\v{j} & \\v{k} \\\\\n",
        "1 & 0 & \\/{‚àÇf}{‚àÇx}(a,b) \\\\\n",
        "0 & 1 & \\/{‚àÇf}{‚àÇy}(a,b)\n",
        "\\EVM$\n",
        "$=-\\/{‚àÇf}{‚àÇx}(a,b)\\v{i}-\\/{‚àÇf}{‚àÇy}(a,b)\\v{j}+\\v{k}$.\n",
        "Therefore\n",
        "$-\\/{‚àÇf}{‚àÇx}(a,b)(x-a)-\\/{‚àÇf}{‚àÇy}(a,b)(y-b)+(z-f(a,b))=0$\n",
        "$‚áíz=f(a,b)+\\/{‚àÇf}{‚àÇx}(a,b)(x-a)+\\/{‚àÇf}{‚àÇy}(a,b)(y-b)$\n",
        "describes the tangent plane (Colley T3.3)\n",
        "\n",
        "**Differentiable** ($‚Ñù^2‚Üí‚Ñù$): Let $X$ be open in $‚Ñù^2$. $f:X‚äÜ‚Ñù^2‚Üí‚Ñù$ is differentiable at $(a,b)‚ààX$ if $\\/{‚àÇf}{‚àÇx}(a,b)$ and $\\/{‚àÇf}{‚àÇy}(a,b)$ exist and if tangent plane $h(x,y)=f(a,b)+\\/{‚àÇf}{‚àÇx}(a,b)(x-a)+\\/{‚àÇf}{‚àÇy}(a,b)(y-b)$ is a good linear approximation to $f$ near $(a,b)$ that is $\\lim\\limits_{(x,y)‚Üí(a,b)}\\/{f(x,y)-h(x,y)}{\\n{(x,y)-(a,b)}}=0$. If $f$ is differentiable at all points of its domain, then $f$ is differentiable. (Colley D3.4)\n",
        "\n",
        "- Suppose $X$ is open in $‚Ñù^2$. If $f:X‚Üí‚Ñù$ has continuous partial derivatives in a neighborhood of $(a,b)$ in $X$ then $f$ is differentiable at $(a,b)$. (Colley T3.5)\n",
        "\n",
        "  - Mean Value Theorem: if $F$ is continuous on $[a,b]$ and differentiable on $(a,b)$, then there exists $c‚àà(a,b)$ such that $F(b)-F(a)=F'(c)(b-a)$\n",
        "\n",
        "  - Proof: Show $\\lim\\limits_{(x,y)‚Üí(a,b)}\\/{f(x,y)-h(x,y)}{\\n{(x,y)-(a,b)}}=0$.\n",
        "  $f(x,y)-f(a,b)$\n",
        "  $=[f(x,y)-f(a,y)]+[f(a,y)-f(a,b)]$\n",
        "  $=\\/{‚àÇf}{‚àÇx}(c_1,y)(x-a)$\n",
        "  $+\\/{‚àÇf}{‚àÇy}(a,c_2)(y-b)$.\n",
        "  Numerator\n",
        "  $|f(x,y)-h(x,y)|$\n",
        "  $=|\\/{‚àÇf}{‚àÇx}(c_1,y)(x-a)$\n",
        "  $-\\/{‚àÇf}{‚àÇx}(a,b)(x-a)$\n",
        "  $+\\/{‚àÇf}{‚àÇy}(a,c_2)(y-b)$\n",
        "  $-\\/{‚àÇf}{‚àÇy}(a,b)(y-b)|$\n",
        "  $‚â§|\\/{‚àÇf}{‚àÇx}(c_1,y)(x-a)$\n",
        "  $-\\/{‚àÇf}{‚àÇx}(a,b)(x-a)|$\n",
        "  $+|\\/{‚àÇf}{‚àÇy}(a,c_2)(y-b)$\n",
        "  $-\\/{‚àÇf}{‚àÇy}(a,b)(y-b)|$\n",
        "  $=|\\/{‚àÇf}{‚àÇx}(c_1,y)-\\/{‚àÇf}{‚àÇx}(a,b)||x-a|$\n",
        "  $+|\\/{‚àÇf}{‚àÇy}(a,c_2)-\\/{‚àÇf}{‚àÇy}(a,b)||y-b|$\n",
        "  $‚â§|\\/{‚àÇf}{‚àÇx}(c_1,y)-\\/{‚àÇf}{‚àÇx}(a,b)|\\n{(x,y)-(a,b)}$\n",
        "  $+|\\/{‚àÇf}{‚àÇy}(a,c_2)-\\/{‚àÇf}{‚àÇy}(a,b)|\\n{(x,y)-(a,b)}$.\n",
        "  $\\lim\\limits_{(x,y)‚Üí(a,b)}\\/{f(x,y)-h(x,y)}{\\n{(x,y)-(a,b)}}$\n",
        "  $‚â§\\lim\\limits_{(x,y)‚Üí(a,b)}\\ob{|\\/{‚àÇf}{‚àÇx}(c_1,y)-\\/{‚àÇf}{‚àÇx}(a,b)|}{\\t{by continuity: }(c_1,y)=(a,b)}$\n",
        "  $+\\lim\\limits_{(x,y)‚Üí(a,b)}\\ob{|\\/{‚àÇf}{‚àÇy}(a,c_2)-\\/{‚àÇf}{‚àÇy}(a,b)|}{\\t{by continuity: }c_2=b}=0$.\n",
        "  Therefore continuous partials ‚áí differentiable.\n",
        "\n",
        "- If $f:X‚äÜ‚Ñù^2‚Üí‚Ñù$ is differentiable at $(a,b)$ then it is continuous at $(a,b)$. (Colley T3.6)\n",
        "\n",
        "  - Proof: See Colley T3.9\n",
        "\n",
        "- **Differentiable** ($‚Ñù^n‚Üí‚Ñù$): Let $X$ be open in $‚Ñù^n$, $f:X‚Üí‚Ñù$, and $\\v{a}=(a_1,...,a_n)‚ààX$. Then $f$ is **differentiable at $\\v{a}$** if all partial derivatives $\\/{‚àÇf}{‚àÇx_i}(\\v{a}),\\ i=1..n$ exist and $h:‚Ñù^n‚Üí‚Ñù$ defined by $h(\\v{x})=f(\\v{a})+\\/{‚àÇf}{‚àÇx_1}(\\v{a})(x_1-a_1)+...+\\/{‚àÇf}{‚àÇx_n}(\\v{a})(x_n-a_n)$ is a good linear approximation to $f$ near $\\v{a}$, meaning $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{f(\\v{x})-h(\\v{x})}{\\n{\\v{x}-\\v{a}}}=0$. (Colley D3.7)\n",
        "\n",
        "**Differentiable** ($‚Ñù^n‚Üí‚Ñù^m$): Let $X$ be open in $‚Ñù^n$, let $\\v{f}:X‚Üí‚Ñù^m$, and let $\\v{a}‚ààX$. Then $\\v{f}$ is **differentiable at $\\v{a}$** if $D\\v{f}(\\v{a})$ exists and if the tangent hyperplane $\\v{h}:‚Ñù^n‚Üí‚Ñù^m$ defined by $\\v{h}(\\v{x})=\\v{f}(\\v{a})+D\\v{f}(\\v{a})(\\v{x}-\\v{a})$ is a good linear approximation to $\\v{f}$ near $\\v{a}$. That is, we must have $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{\\n{\\v{f}(\\v{x})-\\v{h}(\\v{x})}}{\\n{\\v{x}-\\v{a}}}=0$. (Colley D3.8)\n",
        "\n",
        "- If $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is differentiable at $\\v{a}$ then it is continuous at $\\v{a}$ (Colley T3.9)\n",
        "\n",
        "  - Proof: Show $\\lim_{\\v{x}‚Üí\\v{a}}\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})}=0$, which defines continuity at $\\v{a}$.\n",
        "  $\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})}$\n",
        "  $=\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})+[D\\v{f}(\\v{a})(\\v{x}-\\v{a})-D\\v{f}(\\v{a})(\\v{x}-\\v{a})]}$\n",
        "  $‚â§\\ob{\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})-D\\v{f}(\\v{a})(\\v{x}-\\v{a})}}{=\\n{\\v{f}(\\v{x})-\\v{h}(\\v{x})}‚â§\\n{\\v{x}-\\v{a}}\\t{ because differentiable}}+\\ob{\\n{D\\v{f}(\\v{a})(\\v{x}-\\v{a})}}{‚â§|D\\v{f}(\\v{a})|‚ãÖ\\n{\\v{x}-\\v{a}}\\t{ triangle}}$\n",
        "  $‚â§(1+|D\\v{f}(\\v{a})|)\\n{\\v{x}-\\v{a}}$\n",
        "  $‚Üí0$. Therefore differentiable ‚áí continuous.\n",
        "  $|D\\v{f}(\\v{a})|=\\left(\\sum_{i,j}b_{i,j}^2\\right)^{1/2}$.\n",
        "\n",
        "- If $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is such that for all $i=1..m$ and $j=1..n$ all $\\/{‚àÇf_i}{‚àÇx_j}$ exist and are continuous in a neighborhood of $\\v{a}‚ààX$ then $\\v{f}$ is differentiable at $\\v{a}$. (Colley T3.10)\n",
        "\n",
        "  - Proof: Extrapolate $f:X‚äÜ‚Ñù^2‚Üí‚Ñù$ (Colley T3.5) to $m$ functions using Colley T3.11.\n",
        "\n",
        "- A function $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is differentiable at $\\v{a}‚ààX$ iff for $i=1..m$ all $f_i:X‚äÜ‚Ñù^n‚Üí‚Ñù$ are differentiable at $\\v{a}$. (Colley T3.11)\n",
        "\n",
        "  - Proof: Show $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})-D\\v{f}(\\v{a})(\\v{x}-\\v{a})}}{\\n{\\v{x}-\\v{a}}}=0$. Let $G_i=f_i(\\v{x})-f_i(\\v{a})-Df_i(\\v{a})(\\v{x}-\\v{a})$.\n",
        "  Then\n",
        "  $\\/{\\n{\\v{f}(\\v{x})-\\v{f}(\\v{a})-D\\v{f}(\\v{a})(\\v{x}-\\v{a})}}{\\n{\\v{x}-\\v{a}}}$\n",
        "  $=\\/{\\n{(G_1,...,G_m)}}{\\n{\\v{x}-\\v{a}}}$\n",
        "  $=\\/{(G_1^2+...+G_m^2)^{1/2}}{\\n{\\v{x}-\\v{a}}}$\n",
        "  $‚â§\\/{|G_1|}{\\n{\\v{x}-\\v{a}}}+...+\\/{|G_m|}{\\n{\\v{x}-\\v{a}}}$\n",
        "  $‚Üí0$. Therefore $f_i$ differentiable ‚áí $\\v{f}$ differentiable.\n",
        "  $\\/{(G_1^2+...+G_m^2)^{1/2}}{\\n{\\v{x}-\\v{a}}}$\n",
        "  $‚â•\\/{|G_i|}{\\n{\\v{x}-\\v{a}}}$.\n",
        "  Therefore $\\v{f}$ differentiable ‚áí $f_i$ differentiable.\n"
      ],
      "metadata": {
        "id": "fBuEZmk_C8Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Derivative Summary**:\n",
        "\n",
        "- For differentiable $f:X‚äÜ‚Ñù‚Üí‚Ñù$, the derivative $f'(a)$ is the number such that $h(x)=f(a)+f'(a)(x-a)$ is a good linear approximation to $f(x)$ near $a$.\n",
        "\n",
        "- For differentiable $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$, the derivative $‚àáf(\\v{a})$ is the vector such that $h(\\v{x})=f(\\v{a})+‚àáf(\\v{a})‚ãÖ(\\v{x}-\\v{a})$ is a good linear approximation to $f(\\v{x})$ near $\\v{a}$.\n",
        "\n",
        "- For differentiable $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$, the derivative $D\\v{f}(\\v{a})$ is the matrix such that $\\v{h}(\\v{x})=\\v{f}(\\v{a})+D\\v{f}(\\v{a})(\\v{x}-\\v{a})$ is a good linear approximation to $\\v{f}(\\v{x})$ near $\\v{a}$.\n",
        "\n",
        "- Differentiation is linear: let $\\v{f},\\v{g}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ be differentiable at $\\v{a}‚ààX$ then (Colley 4.1)\n",
        "\n",
        "  - $\\v{h}=\\v{f}+\\v{g}$ is differentiable at $\\v{a}$:\n",
        "  $D\\v{h}(\\v{a})=D(\\v{f}+\\v{g})(\\v{a})=D\\v{f}(\\v{a})+D\\v{g}(\\v{a})$\n",
        "\n",
        "  - $\\v{k}=c\\v{f}$ is differentiable at $\\v{a}$:\n",
        "  $D\\v{k}(\\v{a})=D(c\\v{f})(\\v{a})=cD\\v{f}(\\v{a})$\n",
        "\n",
        "**$k$th order partial derivative** for $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is $\\/{‚àÇ^kf}{‚àÇx_{i_1}...‚àÇx_{i_k}}=\\/{‚àÇ}{‚àÇx_{i_k}}...\\/{‚àÇ}{‚àÇx_{i_1}}f(x_1,...,x_n)$\n",
        "\n",
        "- Suppose $X$ is open in $‚Ñù^n$ and $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ has at least $k$ order continuous partial derivatives. Then $f$ is said to be **of class $C^k$**. $\\v{f}:X‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is of class $C^k$ iff all its component functions are of class $C^k$. (Colley D4.4)\n",
        "\n",
        "- Suppose $X$ is open in $‚Ñù^n$ and $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is of class $C^k$. Then $\\/{‚àÇ^kf}{‚àÇx_{i_1}...‚àÇx_{i_k}}=\\/{‚àÇ^kf}{‚àÇx_{j_1}...‚àÇx_{j_k}}$ where $j_1,...,j_k$ is any permutation of $i_1,...,i_k$. (Colley T4.5)"
      ],
      "metadata": {
        "id": "UmF16mmYCJRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chain rule** ($‚Ñù‚àò‚Ñù‚Üí‚Ñù$): Suppose $X$ and $T$ are open subsets of $‚Ñù$ and $f:X‚äÜ‚Ñù‚Üí‚Ñù$ and $x:T‚äÜ‚Ñù‚Üí‚Ñù$ are defined so that $f‚àòx:T‚Üí‚Ñù$ exists, $x$ is differentiable at $t_0‚ààT$, and $f$ is differentiable at $x_0=x(t_0)$, then $h=f‚àòx$ is differentiable at $t_0$ and $h'(t_0)=f'(x_0)x'(t_0)$ or $\\/{dh}{dt}(t_0)=\\/{df}{dx}(x_0)\\/{dx}{dt}(t_0)$. (Colley T5.1)\n",
        "\n",
        "- **Chain rule** ($‚Ñù‚àò‚Ñù^2‚Üí‚Ñù$): Suppose $\\v{x}:T‚äÜ‚Ñù‚Üí‚Ñù^2$ is differentiable at $t_0‚ààT$ and range $\\v{x}‚äÜX$. Suppose $f:X‚äÜ‚Ñù^2‚Üí‚Ñù$ is $C^1$ and differentiable at $\\v{x}_0=\\v{x}(t_0)=(x_0,y_0)‚ààX$ where $T$ and $X$ are open in $‚Ñù$ and $‚Ñù^2$ respectively. Then $h=f‚àò\\v{x}:T‚Üí‚Ñù$ is differentiable at $t_0$ and $\\/{dh}{dt}(t_0)=\\/{‚àÇf}{‚àÇx}(\\v{x}_0)\\/{dx}{dt}(t_0)+\\/{‚àÇf}{‚àÇy}(\\v{x}_0)\\/{dy}{dt}(t_0)$. (Colley 5.2)\n",
        "\n",
        "  - Proof: Let $h=f‚àò\\v{x}$, $x=x(t)$, $y=y(t)$, $x_0=x(t_0)$ and $y_0=y(t_0)$. Then\n",
        "  $\\/{dh}{dt}(t_0)=\\lim\\limits_{t‚Üít_0}\\/{h(t)-h(t_0)}{t-t_0}$\n",
        "  $=\\lim\\limits_{t‚Üít_0}\\/{f(x,y)-f(x_0,y_0)}{t-t_0}$\n",
        "  $=\\lim\\limits_{t‚Üít_0}\\/{f(x,y)-f(x_0,y)+f(x_0,y)-f(x_0,y_0)}{t-t_0}$\n",
        "  $=\\lim\\limits_{t‚Üít_0}\\/{f(x,y)-f(x_0,y)}{t-t_0}$\n",
        "  $+\\lim\\limits_{t‚Üít_0}\\/{f(x_0,y)-f(x_0,y_0)}{t-t_0}$\n",
        "  $=\\lim\\limits_{t‚Üít_0}\\ob{\\/{‚àÇf}{‚àÇx}(c_1,y)\\/{x-x_0}{t-t_0}}{\\t{mean value theorem}}$\n",
        "  $+\\lim\\limits_{t‚Üít_0}\\ob{\\/{‚àÇf}{‚àÇy}(x_0,c_2)\\/{y-y_0}{t-t_0}}{\\t{mean value theorem}}$\n",
        "  $=\\ob{\\/{‚àÇf}{‚àÇx}(x_0,y_0)}{(c_1,y)‚Üí(x_0,y_0)}\\/{dx}{dt}(t_0)$\n",
        "  $+\\ob{\\/{‚àÇf}{‚àÇy}(x_0,y_0)}{c_2‚Üíy_0}\\/{dy}{dt}(t_0)$\n",
        "\n",
        "  - The requirement that $f$ be of class $C^1$ is redundant because $f$ is differentiable at $\\v{x}_0$.\n",
        "\n",
        "  - Example 2: $f(x,y)=(x+y^2)/(2x^2+1)$, and $\\v{x}(t)=(2t, t+1)$ gives parametric equations for a line.\n",
        "  $f(\\v{x}(t))=\\/{t^2+4t+1}{8t^2+1}$\n",
        "  $‚áí\\/{df}{dt}=\\/{2(2-7t-16t^2)}{(8t^2+1)^2}$.\n",
        "  Alternatively,\n",
        "  $\\/{‚àÇf}{‚àÇx}=\\/{2x^2+1-4x(x+y^2)}{(2x^2+1)^2}$\n",
        "  $=\\/{1-2x^2-4xy^2}{(2x^2+1)^2}$,\n",
        "  $\\/{‚àÇf}{‚àÇy}=\\/{2y}{2x^2+1}$,\n",
        "  $(\\/{dx}{dt},\\/{dy}{dt})=(2,1)$. Therefore\n",
        "  $\\/{df}{dt}=\\/{‚àÇf}{‚àÇx}\\/{dx}{dt}+\\/{‚àÇf}{‚àÇy}\\/{dy}{dt}$\n",
        "  $=\\/{2(2-7t-16t^2)}{(8t^2+1)^2}$.\n",
        "\n",
        "- **Chain rule** ($‚Ñù‚àò‚Ñù^n‚Üí‚Ñù$): Suppose $\\v{x}:T‚äÜ‚Ñù‚Üí‚Ñù^n$ is differentiable at $t_0‚ààT$ and range $\\v{x}‚äÜX$. Suppose $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is differentiable at $\\v{x}_0=\\v{x}(t_0)‚ààX$ where $T$ and $X$ are open in $‚Ñù$ and $‚Ñù^n$ respectively. Let $h=f‚àò\\v{x}:T‚Üí‚Ñù$, then\n",
        "$\\/{dh}{dt}(t_0)=Df(\\v{x}_0)D\\v{x}(t_0)=‚àáf(\\v{x}_0)‚ãÖ\\/{d\\v{x}}{dt}|_{t=t_0}=\n",
        "\\BSM\n",
        "\\/{‚àÇf}{‚àÇx_1}(\\v{x}_0) &\n",
        "... &\n",
        "\\/{‚àÇf}{‚àÇx_n}(\\v{x}_0)\n",
        "\\ESM\\BSM\n",
        "\\/{dx_1}{dt}(t_0) \\\\\n",
        "\\vdots \\\\\n",
        "\\/{dx_n}{dt}(t_0)\n",
        "\\ESM$\n",
        "\n",
        "- **Chain rule** ($‚Ñù^2‚àò‚Ñù^3‚Üí‚Ñù$): Suppose $\\v{x}:T‚äÜ‚Ñù^2‚Üí‚Ñù^3$ is differentiable at $\\v{t}_0=(s_0,t_0)‚ààT$ and range $\\v{x}‚äÜX$. Suppose $f:X‚äÜ‚Ñù^3‚Üí‚Ñù$ is differentiable at $\\v{x}_0=\\v{x}(\\v{t}_0)=(x_0,y_0,z_0)‚ààX$ where $T$ and $X$ are open in $‚Ñù^2$ and $‚Ñù^3$ respectively. Then for $h=f‚àò\\v{x}:T‚Üí‚Ñù$,\n",
        "$\\BC\n",
        "\\/{‚àÇh}{‚àÇs}=\\/{‚àÇf}{‚àÇx}\\/{‚àÇx}{‚àÇs}+\\/{‚àÇf}{‚àÇy}\\/{‚àÇy}{‚àÇs}+\\/{‚àÇf}{‚àÇz}\\/{‚àÇz}{‚àÇs} \\\\\n",
        "\\/{‚àÇh}{‚àÇt}=\\/{‚àÇf}{‚àÇx}\\/{‚àÇx}{‚àÇt}+\\/{‚àÇf}{‚àÇy}\\/{‚àÇy}{‚àÇt}+\\/{‚àÇf}{‚àÇz}\\/{‚àÇz}{‚àÇt}\n",
        "\\EC$\n",
        "\n",
        "- **Chain rule** ($‚Ñù^n‚àò‚Ñù^m‚Üí‚Ñù^p$): Suppose $\\v{x}:T‚äÜ‚Ñù^n‚Üí‚Ñù^m$ is differentiable at $\\v{t}_0‚ààT$ and range $\\v{x}‚äÜX$. Suppose $\\v{f}:X‚äÜ‚Ñù^m‚Üí‚Ñù^p$ is differentiable at $\\v{x}_0=\\v{x}(\\v{t})$ where $T$ and $X$ are open in $‚Ñù^n$ and $‚Ñù^m$ respectively. Then $\\v{h}=\\v{f}‚àò\\v{x}$ is differentiable at $\\v{t}_0$ and $D\\v{h}(\\v{t}_0)=D\\v{f}(\\v{x}_0)D\\v{x}(\\v{t}_0)$\n",
        "where\n",
        "$\\/{‚àÇh_{i=1,...,p}}{‚àÇt_{j=1,...,n}}=\\sum\\limits_{k=1}^m\\/{‚àÇf_i}{‚àÇx_k}\\/{‚àÇx_k}{‚àÇt_j}$. (Colley T5.3)\n",
        "\n",
        "  - Example 4: $\\v{f}(x_1,x_2,x_3)=(x_1-x_2, x_1x_2x_3)$ and $\\v{x}(t_1,t_2)=(t_1t_2,t_1^2,t_2^2)$.\n",
        "  Then\n",
        "  $\\v{f}‚àò\\v{x}=(t_1t_2-t_1^2,t_1^3t_2^3)$ and\n",
        "  $D(\\v{f}‚àò\\v{x})=\\BSM\n",
        "  t_2-2t_1 & t_1 \\\\\n",
        "  3t_1^2t_2^3 & 3t_1^3t_2^2\n",
        "  \\ESM$.\n",
        "  Alternatively,\n",
        "  $D\\v{f}(\\v{x})=\\BSM\n",
        "  1 & -1 & 0 \\\\\n",
        "  x_2x_3 & x_1x_3 & x_1x_2\n",
        "  \\ESM$ and\n",
        "  $D\\v{x}(\\v{t})=\\BSM\n",
        "  t_2 & t_1 \\\\\n",
        "  2t_1 & 0 \\\\\n",
        "  0 & 2t_2\n",
        "  \\ESM$ then\n",
        "  $D\\v{f}(\\v{x})D\\v{x}(\\v{t})=\\BSM\n",
        "  t2-2t_1 & t_1 \\\\\n",
        "  x_2x_3t_2+x_1x_32t_1 & x_2x_3t_1 + x_1x_22t_2\n",
        "  \\ESM$\n",
        "  $=\\BSM\n",
        "  t_2-2t_1 & t_1\\\\\n",
        "  t_1^2t_2^3+2t_1^2t_2^3 & t_1^3t_2^2+2t_1^3t_2^2\n",
        "  \\ESM$\n",
        "\n",
        "  - Example 6: $\\BC x=r\\cosŒ∏ \\\\ y=r\\sinŒ∏\\EC$. For $w=f(x,y)=g(r,Œ∏)$ how are derivatives of $(r,Œ∏)$ related to $(x,y)$?\n",
        "  $Dg(r,Œ∏)=Df(x,y)D\\v{x}(r,Œ∏)$\n",
        "  $‚áí\\BSM\n",
        "  \\/{‚àÇg}{‚àÇr} & \\/{‚àÇg}{‚àÇŒ∏}\n",
        "  \\ESM=\\BSM\n",
        "  \\/{‚àÇf}{‚àÇx} & \\/{‚àÇf}{‚àÇy}\n",
        "  \\ESM\\BSM\n",
        "  \\cosŒ∏ & -r\\sinŒ∏\\\\\n",
        "  \\sinŒ∏ & r\\cosŒ∏\n",
        "  \\ESM$.\n",
        "  $\\BSM\n",
        "  \\/{‚àÇf}{‚àÇx} & \\/{‚àÇf}{‚àÇy}\n",
        "  \\ESM=\\BSM\n",
        "  \\/{‚àÇg}{‚àÇr} & \\/{‚àÇg}{‚àÇŒ∏}\n",
        "  \\ESM\\/{1}{r}\\BSM\n",
        "  r\\cosŒ∏ & -\\sinŒ∏\\\\\n",
        "  r\\sinŒ∏ & \\cosŒ∏\n",
        "  \\ESM$."
      ],
      "metadata": {
        "id": "EWviZ9flILsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Directional derivative**: Let $X$ be open in $‚Ñù^n$, $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ and $\\v{a}‚ààX$. If $\\v{v}‚àà‚Ñù^n$ is a unit direction vector, then the **directional derivative of $f$ at $\\v{a}$ in the direction of $\\v{v}$** is $D_\\v{v}f(\\v{a})=\\lim\\limits_{h‚Üí0}\\/{f(\\v{a}+h\\v{v})-f(\\v{a})}{h}$ (Colley D6.1)\n",
        "\n",
        "- Let $X‚äÜ‚Ñù^n$ be open and $f:X‚Üí‚Ñù$ is differentiable at $\\v{a}‚ààX$. Then the directional derivative $D_\\v{v}f(\\v{a})$ exists for all direction vectors $\\v{v}‚àà‚Ñù^n$ and $D_\\v{v}f(\\v{a})=‚àáf(\\v{a})‚ãÖ\\v{v}$. (Colley T6.2)\n",
        "\n",
        "  - Proof: Let $F(t)=f(\\v{a}+t\\v{v})$ then $D_\\v{v}f(\\v{a})=\\lim\\limits_{h‚Üí0}\\/{F(h)-F(0)}{h}=F'(0)$.\n",
        "  If we let $\\v{x}=\\v{a}+t\\v{v}$ then\n",
        "  $D_\\v{v}f(\\v{a})=\\/{d}{dt}f(\\v{a}+t\\v{v})|_{t=0}$\n",
        "  $=Df(\\v{x})D\\v{x}(t)|_{t=0}$\n",
        "  $=Df(\\v{x})\\v{v}|_{t=0}$\n",
        "  $=Df(\\v{a})\\v{v}$\n",
        "  $=‚àáf(\\v{a})‚ãÖ\\v{v}$.\n",
        "\n",
        "  - Example 2: $f(x,y)=x^2-3xy+2x-5y$ at $(0,0)$ with $\\v{v}=v\\v{i}+w\\v{j}$.\n",
        "  Then $D_\\v{v}f(0,0)=\\BSM\n",
        "  2 & -5\n",
        "  \\ESM\\BSM\n",
        "  v \\\\ w\n",
        "  \\ESM$\n",
        "  $=2v-5w$\n",
        "\n",
        "- $D_\\v{u}f(\\v{a})$ is maximized with respect to $\\v{u}$ when $\\v{u}$ points in the same direction as $‚àáf(\\v{a})$ and is minimized when $\\v{u}$ points in the opposite direction. The maximum and minimum values of $D_\\v{u}f(\\v{a})$ are $¬±\\n{‚àáf(\\v{a})}$. (Colley T6.3)\n",
        "\n",
        "  - Proof: Let $Œ∏$ be the angle between $‚àáf(\\v{a})$ and $\\v{u}$. Then $D_\\v{u}f(\\v{a})=\\n{‚àáf(\\v{a})}\\vn{u}\\cosŒ∏$\n",
        "  $=\\n{‚àáf(\\v{a})}\\cosŒ∏$. I.e., $-\\n{‚àáf(\\v{a})}‚â§D_\\v{u}f(\\v{a})‚â§\\n{‚àáf(\\v{a})}$\n",
        "\n",
        "  - If $f(x,y)$ is the height of a hill above the $(x,y)$ plane, then at position $(a,b)$ on the side of the hill $‚àáf(a,b)$ points toward the $(x,y)$ direction with the greatest uphill slope $\\n{‚àáf(a,b)}$.\n"
      ],
      "metadata": {
        "id": "bXxULTmKVYd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient and level set**: Let $X‚äÜ‚Ñù^n$ be open and $f:X‚Üí‚Ñù$ be $C^1$. If $\\v{x}_0$ is on the level set $S=\\{\\v{x}‚ààX|f(\\v{x})=c\\}$, then $‚àáf(\\v{x}_0)$ is perpendicular to $S$. (Colley T6.4)\n",
        "\n",
        "- If $f(x,y)$ is the height of a hill, then the level set is the topographical contour of the mountain at height $c$.\n",
        "\n",
        "- Proof: Let $\\v{x}:T‚äÜ‚Ñù‚ÜíS‚äÇ‚Ñù^n$ be the parametric contour contained in $S$ passing through $\\v{x}_0=\\v{x}(t_0)$ for some $t_0‚ààT$.\n",
        "  - $f(\\v{x}(t))=c$ therefore $\\/{d}{dt}f(\\v{x}(t))|_{t=t_0}=0$.\n",
        "  - Chain rule $\\/{d}{dt}f(\\v{x}(t))=‚àáf(\\v{x}(t))‚ãÖ\\/{d\\v{x}}{dt}$ where $\\/{d\\v{x}}{dt}$ is the slope of the parametric curve in $S$.\n",
        "  - $0=\\/{d}{dt}f(\\v{x}(t))|_{t=t_0}=‚àáf(\\v{x}_0)‚ãÖ\\/{d\\v{x}}{dt}(t_0)$\n",
        "  \n",
        "  As shown, the slope of $S$ at $\\v{x}_0$ is perpendicular to $‚àáf(\\v{x}_0)$.\n",
        "\n",
        "- For spheres $f(x,y)=x^2+y^2=c$ the normal at $\\v{x}_0$ is $‚àáf(\\v{x}_0)$.\n",
        "\n",
        "- For general $z=f(x,y)$ functions, the normal at $(a,b)$ can be determined either using the parametric lines $\\BC\n",
        "\\v{l}_x(t)=(a,b,f(a,b))+t(1,0,\\/{‚àÇf}{‚àÇx}(a,b)) \\\\\n",
        "\\v{l}_y(t)=(a,b,f(a,b))+t(1,0,\\/{‚àÇf}{‚àÇy}(a,b))\n",
        "\\EC$ or by applying the level set method to $F(x,y,z)=f(x,y)-z=0$ to arrive at T3.3 $z=f(a,b)+\\/{‚àÇf}{‚àÇx}(a,b)(x-a)+\\/{‚àÇf}{‚àÇy}(a,b)(y-b)$\n",
        "$=f(\\v{a})+Df(\\v{a})(\\v{x}-\\v{a})$\n",
        "\n",
        "**Implicit function theorem**: Let $F:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be $C^1$ and let $\\v{a}‚ààS=\\{\\v{x}‚àà‚Ñù^n|F(\\v{x})=c\\}$. If $\\/{‚àÇF}{‚àÇx_n}(\\v{a})\\neq0$ then there is a neighborhood $U$ of $(a_1,...,a_{n-1})$ in $‚Ñù^{n-1}$, neighborhood $V$ of $x_n$ in $‚Ñù$, and a function $f:U‚äÜ‚Ñù^{n-1}‚ÜíV$ of $C^1$ such that if $(x_1,...,x_{n-1})‚ààU$ and $x_n‚ààV$ satisfy $F(x_1,...,x_n)=c$ then $x_n=f(x_1,...,x_{n-1})$. (Colley T6.5)\n",
        "\n",
        "- If $\\v{a}$ is a point on $F(\\v{x})=c$ and $‚àáF(\\v{a})\\neq0$, then we can take any $x_i$ such that $\\/{‚àÇF}{‚àÇ_{x_i}}(\\v{a})\\neq0$ and express $x_i$ as a differentiable function of the other components of $\\v{x}$ in the neighborhood of $\\v{a}$.\n",
        "\n",
        "**General implicit function theorem**: Generally, we have system of $m$ equations $\\BC\n",
        "F_1(x_1,...,x_n,y_1,...,y_m)=c_1 \\\\\n",
        "\\vdots \\\\\n",
        "F_m(x_1,...,x_n,y_1,...,y_m)=c_m\n",
        "\\EC$ or $\\v{F}(\\v{x},\\v{y})=\\v{c}$ where we can solve for $\\v{y}=(y_1,...,y_m)$ in terms of $\\v{x}=(x_1,...,x_n)$.\n",
        "  \n",
        "- Suppose $A$ is open in $‚Ñù^{n+m}$, $\\v{F}:A‚Üí‚Ñù^m$ is $C^1$, and $(\\v{a},\\v{b})=(a_1,...,a_n,b_1,...,b_m)‚ààA$ satisfies $\\v{F}(\\v{a},\\v{b})=\\v{c}$. If Jacobian\n",
        "$\\left|\\/{‚àÇ(F_1,...,F_m)}{‚àÇ(y_1,...,y_m)}\\right|_{(\\v{x},\\v{y})=(\\v{a},\\v{b})}$\n",
        "$=\\det\\BSM\n",
        "\\/{‚àÇF_1}{‚àÇy_1}(\\v{a},\\v{b}) & ... & \\/{‚àÇF_1}{‚àÇy_m}(\\v{a},\\v{b}) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\/{‚àÇF_m}{‚àÇy_1}(\\v{a},\\v{b}) & ... & \\/{‚àÇF_m}{‚àÇy_m}(\\v{a},\\v{b})\n",
        "\\ESM \\neq0$, then there is a neighborhood $U$ of $\\v{a}$ in $‚Ñù^n$ and $\\v{f}:U‚Üí‚Ñù^m$ of $C^1$ such that $\\v{f}(\\v{a})=\\v{b}$ and $\\v{F}(\\v{x},\\v{f}(\\v{x}))=\\v{c}$ for all $\\v{x}‚ààU$ (Colley T6.6)\n",
        "\n",
        "  - T6.6 $\\v{f}(\\v{a})=\\v{b}$ corresponds to T6.5 $f:U‚ÜíV$ where $U$ and $V$ are neighborhoods of $(a_1,...,a_{n-1})$ and $a_n$. T6.6 $\\v{F}(\\v{x},\\v{f}(\\v{x}))=\\v{c}$ corresponds to T6.5 $F(x_1,...,x_n)=c$ and $x_n=f(x_1,...,x_{n-1})$.\n",
        "\n",
        "**Inverse function theorem**: Starting with $\\BC\n",
        "y_1=f_1(x_1,...,x_n)\\\\\n",
        "\\vdots\\\\\n",
        "y_n=f_1(x_1,...,x_n)\n",
        "\\EC$ or $\\v{y}=\\v{f}(\\v{x})$ we want to solve for $\\v{x}=\\v{g}(\\v{y})$ by setting $\\v{F}(\\v{x},\\v{y})=\\v{f}(\\v{x})-\\v{y}=\\v{0}$ and then solve for $\\v{x}$ in terms of $\\v{y}$ near $(\\v{a},\\v{b})$.\n",
        "\n",
        "- Suppose $\\v{f}=(f_1,...,f_n)$ is $C^1$ on open set $A‚äÜ‚Ñù^n$. If Jacobian $\\left|\\/{‚àÇ(f_1,...,f_n)}{‚àÇ(x_1,...,x_n)}\\right|_{\\v{x}=\\v{a}}\\neq0$ then there is an open set $U‚äÜ‚Ñù^n$ containing $\\v{a}$ such that $\\v{f}$ is one-to-one on $U$, the set $V=\\v{f}(U)$ is also open, and there is a uniquely determined inverse function $\\v{g}:V‚ÜíU$ of $C^1$. (Colley T6.7)\n",
        "\n",
        "  - If $\\v{F}(\\v{x},\\v{y})=\\v{f}(\\v{x})-\\v{y}$, then $\\/{‚àÇ\\v{F}}{‚àÇ\\v{x}}(\\v{a},\\v{b})=\\/{‚àÇ\\v{f}}{‚àÇ\\v{x}}(\\v{a})$. Therefore T6.7 and T6.6 Jacobian requirements are equivalent: $\\left|\\/{‚àÇ(f_1,...,f_n)}{‚àÇ(x_1,...,x_n)}\\right|_{\\v{x}=\\v{a}}=\\left|\\/{‚àÇ(F_1,...,F_m)}{‚àÇ(x_1,...,x_m)}\\right|_{(\\v{x},\\v{y})=(\\v{a},\\v{b})}$. The phrase \"open set $U‚äÜ‚Ñù^n$ containing $\\v{a}$\" is equivalent to \"neighborhood $U$ of $\\v{a}$ in $‚Ñù^n$\". The phrase \"the set $V=\\v{f}(U)$ is also open\" is equivalent to \"neighborhood $V$ of $\\v{b}$ in $‚Ñù^n$\".\n",
        "\n",
        "  - If the Jacobian requirement is satisfied then $\\v{y}=\\v{f}(\\v{x})$ may be solved uniquely as $\\v{x}=\\v{g}(\\v{y})$ near $(\\v{a},\\v{b})$."
      ],
      "metadata": {
        "id": "y41S1oms2Fmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colley: Maxima and Minima"
      ],
      "metadata": {
        "id": "7Qn60SWsSQD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taylor's theorem** ($‚Ñù‚Üí‚Ñù$): Let $X$ be open in $‚Ñù$ and $f:X‚äÜ‚Ñù‚Üí‚Ñù$ be $C^k$. Given $k$th order Taylor polynomial of $f$ at $a‚ààX$:\n",
        "$p_k(x)=f(a)+f'(a)(x-a)+...+\\/{f^{(k)}(a)}{k!}(x-a)^k$\n",
        "then\n",
        "$R_k(x,a)=f(x)-p_k(x)$\n",
        "satisfies\n",
        "$\\lim\\limits_{x‚Üía}\\/{R_k(x,a)}{(x-a)^k}=0$.\n",
        "(Colley T1.1)\n",
        "\n",
        "- Proof (Taylor polynomial):\n",
        "By fundamental theorem of calculus\n",
        "$f(x)=f(a)+‚à´_a^xf'(t)\\ dt$\n",
        "$=f(a)-‚à´_a^xf'(t)\\ (-dt)$\n",
        "$=f(a)-\\ob{[f'(t)(x-t)]_{t=a}^x+‚à´_a^x(x-t)f''(t)\\ dt}{u=f'(t),\\ v=x-t,\\ dv=-dt,\\ du=f''(t)\\ dt}$\n",
        "$=f(a)+f'(t)(x-t)+‚à´_a^x(x-t)f''(t)\\ dt$.\n",
        "\n",
        "- Proof (remainder convergence):\n",
        "$R_k(x,a)=‚à´_a^x\\/{f^{(k+1)}(t)}{k!}(x-t)^k\\ dt$.\n",
        "Because $a‚â§t‚â§x$, $|x-t|‚â§|x-a|$. Also assume $f$ is $C^{k+1}$ then $f^{(k+1)}(t)$ is bounded.\n",
        "$\\lim\\limits_{x‚Üía}|\\/{R_k(x,a)}{(x-a)^k}|$\n",
        "$=\\lim\\limits_{x‚Üía}|‚à´_a^x\\/{f^{(k+1)}(t)}{k!}\\/{(x-t)^k}{(x-a)^k}\\ dt|$\n",
        "$‚â§\\lim\\limits_{x‚Üía}‚à´_a^x\\/{M}{k!}\\/{|x-a|^k}{(x-a)^k}\\ dt$\n",
        "$=\\lim\\limits_{x‚Üía}\\/{M}{k!}|x-a|$.\n",
        "\n",
        "- Suppose $f:X‚äÜ‚Ñù‚Üí‚Ñù$ differentiable at point $a‚ààX$. The tangent line\n",
        "$p_1(x)=f(a)+f'(a)(x-a)$\n",
        "is the best linear approximation:\n",
        "$p_1(a)=f(a)$ and $p_1'(a)=f'(a)$,\n",
        "while remainder $R_1(x,a)=f(x)-p_1(x)$\n",
        "satisfies $\\lim\\limits_{x‚Üía}\\/{R_1(x,a)}{x-a}=0$.\n",
        "The tangent line is a good approximation over very small neighborhood of $a$. If $f$ is $C^2$ then parabolic approximation may be used\n",
        "$p_2(x)=f(a)+f'(a)(x-a)+\\/{f''(a)}{2}(x-a)^2$\n",
        "so that\n",
        "$p_2(a)=f(a)$,\n",
        "$p_2'(a)=f'(a)$, and\n",
        "$p_2''(a)=f''(a)$,\n",
        "while remainder $R_2(x,a)=f(x)-p_2(x)$\n",
        "satisfies $\\lim\\limits_{x‚Üía}\\/{R_2(x,a)}{(x-a)^2}=0$.\n",
        "\n",
        "- **Lagrange's remainder**: If $f$ is $C^{k+1}$ then there exists $a‚â§z‚â§x$ such that $R_k(x,a)=\\/{f^{(k+1)}(z)}{(k+1)!}(x-a)^{k+1}$. (Colley P1.2)\n",
        "\n",
        "  - Mean value theorem: for continuous $g,h$ where $h$ does not change sign on $[a,b]$ then there is $z‚àà[a,b]$ such that $‚à´_a^bg(t)h(t)\\ dt=g(z)‚à´_a^bh(t)\\ dt$.\n",
        "\n",
        "  - Proof: $R_k(x,a)=‚à´_a^x\\/{f^{(k+1)}(t)}{k!}(x-t)^k\\ dt$\n",
        "  $=\\/{f^{(k+1)}(z)}{k!}‚à´_a^x(x-t)^k\\ dt$\n",
        "  $=\\/{f^{(k+1)}(z)}{(k+1)!}(x-a)^{k+1}$.\n",
        "\n",
        "  - Example 3: $f(x)=\\cos x$ at $x=\\/{œÄ}{2}$. Taylor series $p_5(x)=-(x-\\/{œÄ}{2})+\\/{1}{3!}(x-\\/{œÄ}{2})^3-\\/{1}{5!}(x-\\/{œÄ}{2})^5$ and $R_5(x,\\/{œÄ}{2})=-\\/{\\cos z}{6!}(x-\\/{œÄ}{2})^6$ where $z‚àà[\\/{œÄ}{2},x]$.\n",
        "  Then $|R_5(x,\\/{œÄ}{2})|‚â§|\\/{1}{6!}(x-\\/{œÄ}{2})^6|$.\n",
        "  Therefore\n",
        "  $|R_5(0,\\/{œÄ}{2})|=|R_5(œÄ,\\/{œÄ}{2})|$\n",
        "  $‚â§|\\/{1}{6!}(\\/{œÄ}{2})^6|=0.02$ on $[0,œÄ]$.\n",
        "\n",
        "**First-order Taylor's Series** ($‚Ñù^n‚Üí‚Ñù$): Let $X$ be open in $‚Ñù^n$ and $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is differentiable at $\\v{a}‚ààX$. Let $p_1(\\v{x})=f(\\v{a})+Df(\\v{a})(\\v{x}-\\v{a})$. Then\n",
        "$f(\\v{x})=p_1(\\v{x})+R_1(\\v{x},\\v{a})$ where\n",
        "$\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{R_1(\\v{x},\\v{a})}{\\n{\\v{x}-\\v{a}}}=0$. (Colley T1.3)\n",
        "\n",
        "- $p_1(\\v{x})=f(\\v{a})+\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})(x_i-a_i)$ is also the tangent hyperplane of $f(\\v{x})$ at $\\v{a}$, providing a good linear approximation of $f$ near $\\v{a}$. First order Taylor's series is just linear approximation satisfying\n",
        "$p_1(\\v{a})=f(\\v{a})$ and\n",
        "$\\/{‚àÇp_1}{‚àÇx_i}(\\v{a})=\\/{‚àÇf}{‚àÇx_i}(\\v{a})$.\n",
        "\n",
        "- **Differential**: Let $\\v{h}=\\v{x}-\\v{a}$ then\n",
        "$p_1(\\v{x})=f(\\v{a})+\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})h_i$.\n",
        "The incremental change is $Œîf=f(\\v{a}+\\v{h})-f(\\v{a})$.\n",
        "The total differential is $df(\\v{a},\\v{h})=\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})h_i$.\n",
        "For $\\v{h}‚âà\\v{0}$, $Œîf=df$. (Colley D1.4)\n",
        "\n",
        "  - $Œîx_i$ and $dx_i$ are often used instead of $h_i$: $df(\\v{a},\\v{dx})=\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})dx_i$.\n",
        "\n",
        "**Second-order Taylor's Series** ($‚Ñù^n‚Üí‚Ñù$): Let $X$ be open in $‚Ñù^n$ and $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is $C^2$. Let\n",
        "$\\v{h}=\\v{x}-\\v{a}$ and\n",
        "$p_2(\\v{x})=f(\\v{a})+Df(\\v{a})\\v{h}+\\/{1}{2}\\v{h}^THf(\\v{a})\\v{h}$ then\n",
        "$f(\\v{x})=p_2(\\v{x})+R_2(\\v{x},\\v{a})$ where\n",
        "$\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{|R_2(\\v{x},\\v{a})|}{\\n{\\v{x}-\\v{a}}^2}=0$. (Colley T1.5)\n",
        "\n",
        "- $p_2(\\v{x})=f(\\v{a})+\\sum\\limits_{i=1}^n\\/{‚àÇf}{‚àÇx_i}(\\v{a})(x_i-a_i)+\\/{1}{2}\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n\\/{‚àÇf}{‚àÇx_i‚àÇx_j}(\\v{a})(x_i-a_i)(x_j-a_j)$\n",
        "satisfies\n",
        "$p_2(\\v{a})=f(\\v{a})$,\n",
        "$\\/{‚àÇp_2}{‚àÇx_i}(\\v{a})=\\/{‚àÇf}{‚àÇx_i}(\\v{a})$, and\n",
        "$\\/{‚àÇ^2p_2}{‚àÇx_i‚àÇx_j}(\\v{a})=\\/{‚àÇ^2f}{‚àÇx_i‚àÇx_j}(\\v{a})$.\n",
        "\n",
        "- **Hessian** of $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ is the symmetric matrix\n",
        "$Hf(\\v{a})=‚àá_\\v{x}^2f(\\v{a})=\\BSM\n",
        "\\/{‚àÇ^2f}{‚àÇx_1‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇ^2f}{‚àÇx_1‚àÇx_n}(\\v{a})\\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\/{‚àÇ^2f}{‚àÇx_n‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇ^2f}{‚àÇx_n‚àÇx_n}(\\v{a})\n",
        "\\ESM$\n",
        "\n",
        "**Remainders**: In $‚Ñù‚Üí‚Ñù$, let $h=x-a$ then\n",
        "$R_k(x,a)=‚à´_a^x\\/{f^{(k+1)}(s)}{k!}(x-s)^k\\ ds$\n",
        "$=‚à´_0^1\\/{f^{(k+1)}(a+th)}{k!}h^{k+1}(1-t)^k\\ dt$.\n",
        "$R_1(x,a)=‚à´_a^xf''(t)(x-t)\\ dt$\n",
        "$=‚à´_0^1(1-t)f''(a+th)h^2\\ dt$\n",
        "\n",
        "- Proof: Let $s=a+(x-a)t$ then $t=\\/{s-a}{x-a}$ and $ds=(x-a)\\ dt$.\n",
        "Integrand\n",
        "$(x-s)^k\\ ds$\n",
        "$=[x-a-(x-a)t]^k(x-a)\\ dt$\n",
        "$=[(x-a)(1-t)]^k(x-a)\\ dt$\n",
        "$=(x-a)^{k+1}(1-t)^k\\ dt$.\n",
        "Therefore\n",
        "$R_k(x,a)=‚à´_0^1\\/{f^{(k+1)}(a+t(x-a))}{k!}(x-a)^{k+1}(1-t)^k\\ dt$\n",
        "$=‚à´_0^1\\/{f^{(k+1)}(a+th)}{k!}h^{k+1}(1-t)^k\\ dt$.\n",
        "\n",
        "- Let $\\v{h}=\\v{x}-\\v{a}$, then $R_1(\\v{x},\\v{a})=‚à´_0^1(1-t)\\v{h}^THf(\\v{a}+t\\v{h})\\v{h}\\ dt$\n",
        "$=\\sum\\limits_{i,j=1}^n‚à´_0^1(1-t)\\/{‚àÇ^2f}{‚àÇx_i‚àÇx_j}(\\v{a}+t\\v{h})h_ih_j\\ dt$.\n",
        "\n",
        "- $R_2(\\v{x},\\v{a})=\\sum\\limits_{i,j,k=1}^n‚à´_0^1\\/{(1-t)^2}{2}\\/{‚àÇ^3f}{‚àÇx_i‚àÇx_j‚àÇx_k}(\\v{a}+t\\v{h})h_ih_jh_k\\ dt$\n",
        "\n",
        "- **Lagrange's remainder**: $R_k(x,a)=\\/{f^{(k+1)}(z)}{(k+1)!}(x-a)^{k+1}$\n",
        "\n",
        "  - $R_1(\\v{x},\\v{a})=\\/{1}{2}\\sum\\limits_{i,j=1}^n\\/{‚àÇ^2f}{‚àÇx_i‚àÇx_j}(\\v{z})h_ih_j$ for suitable $\\v{z}$ on the line segment joining $\\v{a}$ and $\\v{x}=\\v{a}+\\v{h}$.\n",
        "\n",
        "  - $R_2(\\v{x},\\v{a})=\\/{1}{3!}\\sum\\limits_{i,j,k=1}^n\\/{‚àÇ^3f}{‚àÇx_i‚àÇx_j‚àÇx_k}(\\v{z})h_ih_jh_k$ for suitable $\\v{z}$ on the line segment joining $\\v{a}$ and $\\v{x}=\\v{a}+\\v{h}$."
      ],
      "metadata": {
        "id": "Kp7XyEue8B1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f$ has **local minimum** at $\\v{a}‚ààX$ if there some neighborhood $U$ of $\\v{a}$ such that $f(\\v{x})‚â•f(\\v{a})$ for all $\\v{x}‚ààU$. $f$ has **local maximum** at $\\v{a}‚ààX$ if $f(\\v{x})‚â§f(\\v{a})$ for all $\\v{x}‚ààU$. (Colley D2.1) **Critical point** is a point $\\v{a}‚ààX$ where $Df(\\v{a})$ is either 0 or undefined.\n",
        "\n",
        "- Let $X$ be open in $‚Ñù^n$ and $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be $C^1$. If $f$ has local extremum at $\\v{a}‚ààX$ then $Df(\\v{a})=\\v{0}$. (Colley T2.2)\n",
        "\n",
        "**Second derivative test (Hessian test)**: Let $X$ be open in $‚Ñù^n$, $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be $C^2$, and $\\v{a}‚ààX$ be a critical point. Find $Hf(\\v{a})$,\n",
        "$d_1=\\BVM \\/{‚àÇ^2f}{‚àÇx_1‚àÇx_1}(\\v{a}) \\EVM$,\n",
        "$d_2=\\BVM\n",
        "\\/{‚àÇ^2f}{‚àÇx_1‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_1‚àÇx_2}(\\v{a})\\\\\n",
        "\\/{‚àÇ^2f}{‚àÇx_2‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_2‚àÇx_2}(\\v{a})\n",
        "\\EVM$,\n",
        "$d_3=\\BVM\n",
        "\\/{‚àÇ^2f}{‚àÇx_1‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_1‚àÇx_2}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_1‚àÇx_3}(\\v{a})\\\\\n",
        "\\/{‚àÇ^2f}{‚àÇx_2‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_2‚àÇx_2}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_2‚àÇx_3}(\\v{a})\\\\\n",
        "\\/{‚àÇ^2f}{‚àÇx_3‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_3‚àÇx_2}(\\v{a}) & \\/{‚àÇ^2f}{‚àÇx_3‚àÇx_3}(\\v{a})\\\\\n",
        "\\EVM$, ... (sequence of principal minors)\n",
        "1. If $d_k>0$ for all $k$ then $Hf(\\v{a})$ is positive definite and $\\v{a}$ is a local minimum.\n",
        "2. If $\\BC\n",
        "d_k>0 &k\\t{ even}\\\\\n",
        "d_k< 0&k\\t{ odd}\n",
        "\\EC$ then $Hf(\\v{a})$ is negative definite and $\\v{a}$ is a local maximum.\n",
        "3. If $\\det Hf(\\v{a})\\neq0$ but $Hf(\\v{a})$ is indefinite (neither positive nor negative definite) then $\\v{a}$ is a saddle point.\n",
        "4. If $\\det Hf(\\v{a})=0$ then $\\v{a}$ is degenerate. (Colley T2.3)\n",
        "\n",
        "- Example 3: $f(x,y)=x^2+xy+y^2+2x-2y+5$ has $\\BC\n",
        "\\/{‚àÇf}{‚àÇx}=2x+y+2=0\\\\\n",
        "\\/{‚àÇf}{‚àÇy}=x+2y-2=0\n",
        "\\EC$\n",
        "so $(-2,2)$ is a critical point.\n",
        "$Œîf(\\v{a})=f(\\v{a}+\\v{h})-f(\\v{a})$\n",
        "$=f(-2+h,2+k)-f(-2,2)$\n",
        "$=h^2+hk+k^2$.\n",
        "If $Œîf‚â§0$ for all small $\\v{h}$ then $(-2,2)$ is a local maximum. If $Œîf‚â•0$ then $(-2,2)$ is a local minimum.\n",
        "$Œîf=h^2+hk+k^2$\n",
        "$=(h+\\/{1}{2}k)^2+\\/{3}{4}k^2$\n",
        "$‚â•0$ for all $\\v{h}$ therefore $(-2,2)$ is a local minimum.\n",
        "- Example 4: $f(x,y)=x^2+xy+y^2+2x-2y+5$ has $Hf(\\v{a})=\\BSM\n",
        "2 & 1 \\\\\n",
        "1 & 2\n",
        "\\ESM$.\n",
        "Then $d_1=\\BVM 2 \\EVM$ and\n",
        "$d_2=\\BVM\n",
        "2 & 1 \\\\\n",
        "1 & 2\n",
        "\\EVM=3$.\n",
        "The Hessian is positive definite therefore $(-2,2)$ is a local minimum.\n",
        "\n",
        "- Suppose $f(\\v{x})$ is $C^2$ and $\\v{a}$ is critical point of $f$. Then by second-order Taylor series\n",
        "$Œîf=f(\\v{a}+\\v{h})-f(\\v{a})$\n",
        "$‚âà\\ob{p_2(\\v{a}+\\v{h})}{p_2(\\v{x})}-f(\\v{a})$\n",
        "$=\\ob{Df(\\v{a})\\v{h}}{Df(\\v{a})=\\v{0}}+\\/{1}{2}\\v{h}^THf(\\v{a})\\v{h}$.\n",
        "Therefore\n",
        "$Œîf‚âà\\/{1}{2}\\v{h}^THf(\\v{a})\\v{h}$.\n",
        "\n",
        "  - **Quadratic form** $Q(\\v{h})=\\v{h}^TB\\v{h}=\\sum\\limits_{i,j=1}^nb_{ij}h_ih_j$ where $B$ is a symmetric matrix $B^T=B$ or $b_{ij}=b_{ji}$ is said to be **positive definite** if $Q(\\v{h})>0$ for all $\\v{h}\\neq\\v{0}$ and **negative definite** if $Q(\\v{h})< 0$ for all $\\v{h}\\neq\\v{0}$.\n",
        "\n",
        "  - Let $B_k,\\ k=1,...,n$ be the upper-left-most $k√ók$ submatrix of symmetric matrix $B$ such that\n",
        "  $B_1=\\BSM b_{11} \\ESM$,\n",
        "  $B_2=\\BSM\n",
        "  b_{11} & b_{12}\\\\\n",
        "  b_{21} & b_{22}\n",
        "  \\ESM$,\n",
        "  $B_3=\\BSM\n",
        "  b_{11} & b_{12} & b_{13}\\\\\n",
        "  b_{21} & b_{22} & b_{23}\\\\\n",
        "  b_{31} & b_{32} & b_{33}\\\\\n",
        "  \\ESM$, ...\n",
        "  If $(\\det B_1, \\det B_2, ..., \\det B_n)$ consists entirely of positive numbers then $B$ and $Q$ are positive definite. If $\\BC\n",
        "  \\det B_k< 0 &k\\t{ odd} \\\\\n",
        "  \\det B_k> 0 &k\\t{ even}\n",
        "  \\EC$ then $B$ and $Q$ are negative definite.\n",
        "\n",
        "- **Extreme value theorem**: Subset $X‚äÜ‚Ñù^n$ is **compact** if it is both closed and bounded (Colley D2.4) If $X$ is compact and $f|_X:‚Ñù^n‚Üí‚Ñù$ is continuous, then there must exist $\\v{a}_\\max$ and $\\v{a}_\\min$ in $X$ such that for all $\\v{x}‚ààX$, $f(\\v{a}_\\min)‚â§f(\\v{x})‚â§f(\\v{a}_\\max)$. (Colley T2.5)\n",
        "\n",
        "  - $f|_X$ is the restriction of $f$ to compact subset $X$.\n",
        "\n",
        "- Proof: If $B$ is symmetric matrix associated with a positive definite $Q$, then there exists $M>0$ such that $Q(\\v{h})‚â•M\\vn{h}^2$ for all $\\v{h}‚àà‚Ñù^n$.\n",
        "  1. If $\\v{h}=\\v{0}$ then $Q(\\v{h})=0$. Therefore $M>0$ exists.\n",
        "  2. If $\\vn{h}=1$ then $S=\\{\\v{h}‚àà‚Ñù^n:\\vn{h}=1\\}$ is a compact set and $Q|_S:S‚äÜ‚Ñù^n‚Üí‚Ñù$ must have a global minimum $M$ such that $Q(\\v{h})‚â•M$ for all $\\v{h}‚ààS$. Because $Q$ is positive definite, $Q(\\v{h})>0$ for all $\\v{h}\\neq\\v{0}$ and therefore $M>0$ exists.\n",
        "  3. for nonzero $\\v{h}‚àà‚Ñù^n$, then $\\/{\\v{h}}{\\vn{h}}$ is a unit vector such that $Q(\\vn{h}\\/{\\v{h}}{\\vn{h}})‚â•\\vn{h}^2M$.\n",
        "\n",
        "  Now,\n",
        "  $Œîf=\\/{1}{2}\\v{h}^THf(\\v{a})\\v{h}+R_2(\\v{x},\\v{a})$\n",
        "  where\n",
        "  $\\lim\\limits_{\\v{x}‚Üí\\v{a}}\\/{|R_2(\\v{x},\\v{a})|}{\\n{\\v{x}-\\v{a}}^2}=0$.\n",
        "\n",
        "  1. **Positive definite** Suppose $Hf(\\v{a})$ is positive definite. There must exist $M>0$ such that\n",
        "  $\\/{1}{2}\\v{h}^THf(\\v{a})\\v{h}‚â•M\\vn{h}^2$.\n",
        "  $\\lim\\limits_{\\v{h}‚Üí\\v{0}}\\/{|R_2(\\v{x},\\v{a})|}{\\vn{h}^2}=0$\n",
        "  therefore there must exist $\\v{h}$ small enough such that\n",
        "  $\\/{|R_2(\\v{x},\\v{a})|}{\\vn{h}^2}< M‚áí|R_2(\\v{x},\\v{a})|< M\\vn{h}^2$.\n",
        "  Under such conditions $0< Œîf< ‚àû$ and $\\v{a}$ is a local minimum.\n",
        "\n",
        "  2. **Negative definite** Suppose $Hf(\\v{a})$ is negative definite. Let $g=-f$ then $Hg(\\v{a})$ is positive definite. Therefore $\\v{a}$ is a local minimum of $g$ and a local maximum of $f$.\n",
        "\n"
      ],
      "metadata": {
        "id": "aKF7qZ6edtwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constrained optimization**: Let $X$ be open in $‚Ñù^n$ and $f,g:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be $C^1$. Let $S=\\{\\v{x}‚ààX|g(\\v{x})=c\\}$ denote the level set of $g$ at height $c$. Then if $f|_S$ has an extremum at $\\v{x}_0‚ààS$ such that $‚àág(\\v{x}_0)\\neq\\v{0}$, there must some **Lagrange multiplier** $Œª‚àà‚Ñù$ such that $‚àáf(\\v{x}_0)=Œª‚àág(\\v{x}_0)$. (Colley T3.1)\n",
        "\n",
        "- Solve $\\BC\\t{extremum}&f(\\v{x})\\\\\\t{subject to}&g(\\v{x})=c\\EC$ with $\\red{\\BC ‚àáf(\\v{x})=Œª‚àág(\\v{x})\\\\g(\\v{x})=c\\EC}$ equivalent to Lagrangian $\\m{L}(\\v{x},Œª)=f(\\v{x})-Œª(g(\\v{x})-c)$\n",
        "\n",
        "- Intuition: level set $S=\\{\\v{x}‚ààX|g(\\v{x})=c\\}$ creates a restricted domain for $\\v{x}$ satisfying $g$ constraint. This restricted domain looks like a contour on the domain, and acts as a path. We're interested in extremum for $f|_S$, which is surface $f$ limited to a path/contour in the domain. We walk along the domain path until we touch the extrema of $f|_S$, which have flat slope and are tangential to the level set of $f$.\n",
        "\n",
        "- Proof: $‚àág(\\v{x}_0)$ and $‚àáf(\\v{x}_0)$ are perpendicular to level sets of $g$ and $f$ (Ch2T6.4). $‚àáf(\\v{x}_0)=Œª‚àág(\\v{x}_0)$ says for $\\v{x}_0$ to be a constrained extremum, the level sets of $f$ and $g$ are tangent at $\\v{x}_0$. Suppose $\\v{x}_0$ is an extremum of $f$ restricted to a parametric curve $\\v{x}:T‚äÜ‚Ñù‚ÜíS‚äÇ‚Ñù^n$ on hyperplane $S=\\{\\v{x}|g(\\v{x})=c\\}$ with $\\v{x}(t_0)=\\v{x}_0$ for some $t_0‚ààT$.\n",
        "\n",
        "  - $\\v{x}_0$ is an extremum of $f$ therefore $\\/{d}{dt}f(\\v{x}(t))|_{t=t_0}=0$.\n",
        "\n",
        "  - Chain rule: $\\/{d}{dt}f(\\v{x}(t))=‚àáf(\\v{x}(t))‚ãÖ\\/{d\\v{x}}{dt}$ where $\\/{d\\v{x}}{dt}(t)$ is the slope of the parametric curve on $S$.\n",
        "\n",
        "  - $0=\\/{d}{dt}f(\\v{x}(t))|_{t=t_0}=‚àáf(\\v{x}_0)‚ãÖ\\/{d\\v{x}}{dt}(t_0)$.\n",
        "\n",
        "  As shown, $‚àáf(\\v{x}_0)$ and $‚àág(\\v{x}_0)$ are both perpendicular to $S$. Therefore  $‚àáf(\\v{x}_0)=Œª‚àág(\\v{x}_0)$.\n",
        "\n",
        "**Multiple constraint Lagrange**: Let $X$ be open in $‚Ñù^n$ and $f,g_1,...,g_k:X‚äÜ‚Ñù^n‚Üí‚Ñù$ be $C^1$ where $k< n$. Let $S=\\{\\v{x}‚ààX|g_1(\\v{x})=c_1,...,g_2(\\v{x})=c_k\\}$. If $f|_S$ has extremum at $\\v{x}_0‚ààX$ where $‚àág_1(\\v{x}_0),...,‚àág_k(\\v{x}_0)$ are linearly independent vectors, then there must exist $Œª_1,...,Œª_k‚àà‚Ñù$ such that $‚àáf(\\v{x}_0)=Œª_1‚àág_1(\\v{x}_0)+...+Œª_k‚àág_k(\\v{x}_0)$. (Colley T3.2)\n",
        "\n",
        "- $S$ is the intersection of $S_1,...,S_k$ where $S_j=\\{\\v{x}‚àà‚Ñù^n|g_j(\\v{x})=c_j\\}$. Any vector tangent to $S$ at $\\v{x}_0$ must be perpendicular to each $‚àág_j(\\v{x}_0)$.\n",
        "\n",
        "**Second derivative test (bordered Hessian test)**: We seek extrema of $f:X‚äÜ‚Ñù^n‚Üí‚Ñù$ subject to constraints $g_1(\\v{x})=c_1,...,g_k(\\v{x})=c_k,\\ k< n$ which are all $C^2$. Then $(Œª;\\v{a})=(Œª_1,...,Œª_k;a_1,...,a_n)$ is an unconstrained critical point to Lagrangian function $L(l_1,...,l_k;x_1,...,x_n)=f(\\v{x})-\\sum\\limits_{i=1}^kl_i(g_i(\\v{x})-c_i)$.\n",
        "The implicit function theorem requires that $D\\v{g}(\\v{a})=\\BSM\n",
        "\\/{‚àÇg_1}{‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇg_1}{‚àÇ\\blue{x_n}}(\\v{a}) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\/{‚àÇg_k}{‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇg_k}{‚àÇ\\blue{x_n}}(\\v{a})\n",
        "\\ESM$ satisfies $\\det\\BVM\n",
        "\\/{‚àÇg_1}{‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇg_1}{‚àÇ\\blue{x_k}}(\\v{a}) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\/{‚àÇg_k}{‚àÇx_1}(\\v{a}) & ... & \\/{‚àÇg_k}{‚àÇ\\blue{x_k}}(\\v{a})\n",
        "\\EVM\\neq0$ for any $k$ of the $n$ columns, which is true if $‚àág_{1,...,k}(\\v{a})$ are linearly independent.\n",
        "$HL(Œª;\\v{a})=\\BSM\n",
        "0 & -(‚àág)^T \\\\\n",
        "‚àág & ‚àá_\\v{x}^2L\n",
        "\\ESM=\\BSM\n",
        "0 & ... & 0 & -\\/{‚àÇg_1}{‚àÇx_1}(\\v{a}) & ... & -\\/{‚àÇg_1}{‚àÇx_n}(\\v{a}) \\\\\n",
        "\\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & ... & 0 & -\\/{‚àÇg_k}{‚àÇx_1}(\\v{a}) & ... & -\\/{‚àÇg_k}{‚àÇx_n}(\\v{a}) \\\\\n",
        "-\\/{‚àÇg_1}{‚àÇx_1}(\\v{a}) & ... & -\\/{‚àÇg_k}{‚àÇx_1}(\\v{a}) & \\/{‚àÇ^2L}{‚àÇx_1‚àÇx_1}(Œª;\\v{a}) & ... & \\/{‚àÇ^2L}{‚àÇx_1‚àÇx_n}(Œª;\\v{a}) \\\\\n",
        "\\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "-\\/{‚àÇg_1}{‚àÇx_n}(\\v{a}) & ... & -\\/{‚àÇg_k}{‚àÇx_n}(\\v{a}) & \\/{‚àÇ^2L}{‚àÇx_n‚àÇx_1}(Œª;\\v{a}) & ... & \\/{‚àÇ^2L}{‚àÇx_n‚àÇx_n}(Œª;\\v{a})\n",
        "\\ESM$ is a $(n+k)√ó(n+k)$ symmetric matrix, from which we find the sequence of minors $s_1,...,s_{n-k}=(-1)^kd_{2k+1},\\ ...,\\ (-1)^kd_{k+n}$ and conclude positive definite or negative definite: $s_j>0$ for all $j$ then positive definite, $s_1< 0$ followed by alternating signs then negative definite.\n",
        "\n",
        "- Implicit function theorem requirement needs any combination of $k$ out of $n$ columns to produce a non-zero determinant. If it fails, the \"border\" rows and columns lose rank and $\\det HL(Œª;\\v{a})=0$. The critical point will be degenerate.\n",
        "\n",
        "- Let $f,g:U‚äÜ‚Ñù^2‚Üí‚Ñù$ be $C^2$. Let $\\v{a}$ be critical point of $f|_S$ constrained by $S=\\{\\v{x}‚ààU|g(\\v{x})=c\\}$. Then $L(\\v{x})=f(\\v{x})-Œªg(\\v{x})$. Then the **bordered Hessian** $HL(\\v{a})=\\BSM\n",
        "0 & -\\/{‚àÇg}{‚àÇx}(\\v{a}) & -\\/{‚àÇg}{‚àÇy}(\\v{a}) \\\\\n",
        "-\\/{‚àÇg}{‚àÇx}(\\v{a}) & \\/{‚àÇ^2g}{‚àÇx^2}(\\v{a}) & \\/{‚àÇ^2g}{‚àÇx‚àÇy}(\\v{a}) \\\\\n",
        "-\\/{‚àÇg}{‚àÇy}(\\v{a}) & \\/{‚àÇ^2g}{‚àÇy‚àÇx}(\\v{a}) & \\/{‚àÇ^2g}{‚àÇy^2}(\\v{a})\n",
        "\\ESM$, then use $s_1=(-1)^kd_{2k+1}=\\det HL(\\v{a})$ to determine its positive/negative definiteness. (Marsden 3.4 T10)\n",
        "\n",
        "- Example 1: An open box is to have a fixed volume of 4. What dimensions should the box have to minimize the material used?\n",
        "$A(x,y,z)=2xy+2yz+xz$\n",
        "with constraint\n",
        "$V(x,y,z)=xyz=4$.\n",
        "Create new area function with two variables\n",
        "$a(x,y)=A(x,y,\\/{4}{xy})$\n",
        "$=2xy+\\/{8}{x}+\\/{4}{y}$.\n",
        "$\\BC\n",
        "\\/{‚àÇa}{‚àÇx}=2y-\\/{8}{x^2}=0 \\\\\n",
        "\\/{‚àÇa}{‚àÇy}=2x-\\/{4}{y^2}=0\n",
        "\\EC$\n",
        "$‚áíx(1-\\/{1}{8}x^3)=0$\n",
        "$‚áíx=0,2$.\n",
        "Constrained critical point of $A$ is $(2,1,2)$.\n",
        "Hessian $Ha=\\BSM\n",
        "\\/{16}{x^3} & 2 \\\\ 2 & \\/{8}{y^3}\n",
        "\\ESM$\n",
        "so $Ha(2,1)=\\BSM\n",
        "2 & 2 \\\\ 2 & 8\n",
        "\\ESM$ is positive definite so $(2,1)$ is a local minimum of $a$.\n",
        "  - The creation of $a(x,y)$ applies implicit function theorem.\n",
        "- Example 2: $A(x,y,z)=2xy+2yz+xz$ constrained by $V(x,y,z)=xyz=4$. Then $\\BC\n",
        "2y+z=Œªyz\\\\\n",
        "2x+z=Œªxz\\\\\n",
        "2y+x=Œªxy\\\\\n",
        "xyz=4\n",
        "\\EC$\n",
        "using Lagrangian setup has one solution $(2,1,2)$.\n",
        "\n",
        "- Example 6: $A(x,y,z)=2xy+2yz+xz$ constrained by $V(x,y,z)=xyz=4$. $f|_g$ has critical point $\\v{a}=(2,1,2)$.\n",
        "$L(x,y,z)=2xy-2yz+xz-Œªxyz$.\n",
        "$HL(\\v{a})=\\BSM\n",
        "0 & -yz & -xz & -xy \\\\\n",
        "-yz & 0 & 2-Œªz & 1-Œªy \\\\\n",
        "-xz & 2-Œªz & 0 & 2-Œªx \\\\\n",
        "-xy & 1-Œªy & 2-Œªx & 0\n",
        "\\ESM$ gives minors\n",
        "$s_1=-d_{3}=32$ and $s_2=-d_{4}=48$ therefore $\\v{a}$ is a local minimum."
      ],
      "metadata": {
        "id": "hEjiFlHfeuv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Linear Independence, Basis, and Transformations"
      ],
      "metadata": {
        "id": "vB6PP0enPpMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector** is a $‚Ñù^{m√ó1}$ single column matrix. **Matrix** $\\v{A}‚àà‚Ñù^{m√ón}$ has $m$ rows and $n$ columns and $\\v{B}‚àà‚Ñù^{n√ók}$, then matrix multiplication is a matrix of dot products\n",
        "$\\v{AB}‚àà‚Ñù^{m√ók}=\\BSM\n",
        "\\sum_{i=1}^na_{1i}b_{i1} & \\sum_{i=1}^na_{1i}b_{i2} & ... & \\sum_{i=1}^na_{1i}b_{ik}\\\\\n",
        "\\sum_{i=1}^na_{2i}b_{i1} & \\sum_{i=1}^na_{2i}b_{i2} & ... & \\sum_{i=1}^na_{2i}b_{ik}\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\sum_{i=1}^na_{mi}b_{i1} & \\sum_{i=1}^na_{mi}b_{i2} & ... & \\sum_{i=1}^na_{mi}b_{ik}\n",
        "\\ESM$\n",
        "\n",
        "- Properties\n",
        "\n",
        "  - $(\\v{AB})\\v{C}=\\v{A}(\\v{BC})$\n",
        "  - $(\\v{A}+\\v{B})\\v{C}=\\v{AC}+\\v{BC}$\n",
        "  and\n",
        "  $A(\\v{C}+\\v{D})=\\v{AC}+\\v{AD}$\n",
        "  - $\\v{I}_m\\v{A}=\\v{A}\\v{I}_n=\\v{A}$\n",
        "\n",
        "- Transpose and inverse: If $\\v{A}‚àà‚Ñù^{2√ó2}$, then $\\v{A}^{-1}=\\/{1}{\\det\\v{A}}\\BM a_{22} & -a_{12} \\\\ -a_{21} & a_{11} \\EM$.\n",
        "\n",
        "  - If $\\v{A}‚àà‚Ñù^{m√óm}$ then: $\\v{A}\\v{A}^{-1}=\\v{I}_m=\\v{A}^{-1}\\v{A}$\n",
        "  - $(\\v{A}^{-1})^{-1}=\\v{A}$\n",
        "  - $(\\v{AB})^{-1}=\\v{B}^{-1}\\v{A}^{-1}$\n",
        "  - **Symmetric matrix**: $\\v{A}^‚ä§=\\v{A}$: sums of symmetric matrices are also symmetric. Inverse of symmetric matrices are also symmetric.\n",
        "  - $(\\v{A}^‚ä§)^‚ä§=\\v{A}$\n",
        "  - $(\\v{AB})^‚ä§=\\v{B}^‚ä§\\v{A}^‚ä§$\n",
        "  - $(\\v{A}+\\v{B})^‚ä§=\\v{A}^‚ä§+\\v{B}^‚ä§$\n",
        "\n",
        "**System of equations**\n",
        "$\\BC\n",
        "2x_1+3x_2+5x_3=1\\\\\n",
        "4x_1-2x_2-7x_3=8\\\\\n",
        "9x_1+5x_2-3x_3=2\n",
        "\\EC‚üπ\\BSM 2 & 3 & 5 \\\\ 4 & -2 & -7 \\\\ 9 & 5 & -3\\ESM\n",
        "\\BSM x_1 \\\\ x_2 \\\\ x_3\\ESM=\n",
        "x_1\\BSM 2\\\\4\\\\9\\ESM+x_2\\BSM 3\\\\-2\\\\5\\ESM+x_3\\BSM 5\\\\-7\\\\-3\\ESM=\n",
        "\\BSM 1 \\\\ 8 \\\\ 2\\ESM$\n",
        "\n",
        "- **Gaussian elimination**: row echelon form by elementary transformations (1) exchange 2 rows, (2) multiplication of a row with constant, (3) addition of two rows. The computations required is $O(n^3)$.\n",
        "\n",
        "  - **Row echelon form**: staircase structure where each nonzero row has its **pivot** is to the right of the pivot on the row above it. The pivots are basic variables, the others are free variables.\n",
        "\n",
        "  - **Reduced row echelon form** is when every pivot is 1, and it is the only nonzero entry in its column.\n",
        "\n",
        "  - **Minus-1 trick** for determining null space: Augument $\\t{rref}(\\v{A})$ by inserting a new row with a $-1$ pivot for each free variable. We end up with a square matrix, and the $-1$ columns are the bases of the null space.\n",
        "\n",
        "- **Square Inverse**: $[A|I_n]‚Üí[I_n|A^{-1}]$ using RREF.\n",
        "\n",
        "- Inverse solution: $\\v{x}=\\v{A}^{-1}\\v{b}$ requires $\\v{A}$ to be invertible and square. If $\\v{A}$ is tall rectangular we can use **Moore-Penrose pseudoinverse** $\\v{x}=(\\v{A}^‚ä§\\v{A})^{-1}\\v{A}^‚ä§\\v{b}$, which forces $\\v{A}$ into a square shape. However $(\\v{A}^‚ä§\\v{A})^{-1}$ is very computationally expensive and prone to compounded rounding errors. This finds the least square solution $\\v{x}$.\n",
        "\n",
        "- Iterative solution: Let $\\v{x}^*$ be a solution, and let $\\v{x}^{(k+1)}=\\v{C}\\v{x}^{(k)}+\\v{d}$, and iteratively reduce $\\n{\\v{x}^{(k+1)}-\\v{x}^*}$.\n",
        "\n",
        "- 2.3.1: $\\BM1&0&8&-4\\\\0&1&2&12\\EM\\v{x}=\\BM 42\\\\8\\EM$\n",
        "$\\BC x_1+8x_3-4x_4=42 \\\\ x_2+2x_3+12x_4=8\\EC$.\n",
        "  \n",
        "  - $\\v{x}_0=\\BM42\\\\8\\\\0\\\\0\\EM$ is one solution by setting non-pivot free variables to 0. The full solution set is $\\v{x}_0+\\null(\\v{A})$, where\n",
        "  $\\null(\\v{A})=\\{\\v{x}:\\v{Ax}=\\v{0}\\}$.\n",
        "  We obtain the kernel by expressing non-pivot columns as a combination of pivot columns. $\\v{a}_1=\\BM1\\\\0\\EM$ and $\\v{a}_2=\\BM0\\\\1\\EM$ are pivot columns. $\\v{a}_3=\\BM8\\\\2\\EM$ and $\\v{a}_4=\\BM-4\\\\12\\EM$ are non-pivot.\n",
        "  \n",
        "  - $8\\v{a}_1+2\\v{a}_2-\\v{a}_3+0\\v{a}_4=\\v{0}$, or\n",
        "  $\\BM1&0&8&-4\\\\0&1&2&12\\EM\\BM8\\\\2\\\\-1\\\\0\\EM=\\BM0\\\\0\\EM$ and\n",
        "  $\\BM8\\\\2\\\\-1\\\\0\\EM‚àà\\null(\\v{A})$.\n",
        "  \n",
        "  - Similarly,\n",
        "  $-4\\v{a}_1+12\\v{a}_2+0\\v{a}_3-\\v{a}_4=\\v{0}$, and\n",
        "  $\\BM-4\\\\12\\\\0\\\\-1\\EM‚àà\\null(\\v{A})$.\n",
        "  \n",
        "  - Therefore\n",
        "  $\\v{x}=\\BM 42\\\\8\\\\0\\\\0 \\EM+\n",
        "  Œª_1\\BM 8\\\\2\\\\-1\\\\0 \\EM+\n",
        "  Œª_2\\BM -4\\\\12\\\\0\\\\-1 \\EM$\n",
        "  for all $Œª_1,Œª_2‚àà‚Ñù$.\n",
        "\n",
        "  - (minus-1 trick): $\\BM 1 & 0 & \\green{8} & \\blue{-4}\\\\0 & 1 & \\green{2} & \\blue{12}\\\\\\dgray{0} & \\dgray{0} & \\green{-1} & \\blue{0}\\\\\\dgray{0} & \\dgray{0} & \\green{0} & \\blue{-1}\\EM$\n",
        "\n",
        "- 2.6: $\\BM 1&-2&1&-1&1\\\\0&0&1&-1&3\\\\0&0&0&1&-2 \\EM \\v{x}=\\BM 0\\\\-2\\\\1 \\EM$\n",
        "$‚áí\\BM \\green{1}&-2&0&0&-2&\\green{2}\\\\0&0&\\green{1}&0&1&\\green{-1}\\\\0&0&0&\\green{1}&-2&\\green{1} \\EM$.\n",
        "One solution is $\\v{x}_0=\\BM2\\\\0\\\\-1\\\\1\\\\0\\EM$.\n",
        "Using minus-1 trick $\\BM 1&\\green{-2}&0&0&\\blue{-2}\\\\\\dgray{0}&\\green{-1}&\\dgray{0}&\\dgray{0}&\\blue{0}\\\\0&\\green{0}&1&0&\\blue{1}\\\\0&\\green{0}&0&1&\\blue{-2}\\\\\\dgray{0}&\\green{0}&\\dgray{0}&\\dgray{0}&\\blue{-1} \\EM$,\n",
        "$\\null(\\v{A})=\\span(\\BM-2\\\\-1\\\\0\\\\0\\\\0\\EM,\\BM-2\\\\0\\\\1\\\\-2\\\\-1\\EM)$.\n",
        "The full solution is $\\BM2\\\\0\\\\-1\\\\1\\\\0\\EM+Œª_1\\BM-2\\\\-1\\\\0\\\\0\\\\0\\EM+Œª_2\\BM-2\\\\0\\\\1\\\\-2\\\\-1\\EM$."
      ],
      "metadata": {
        "id": "EMCRVv2VPnJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group**: $(\\m{G},‚äó)$ is called a **group** for set $\\m{G}$ and operation $‚äó:\\m{G}√ó\\m{G}‚Üí\\m{G}$ if the following 4 properties hold:\n",
        "$\\BC\n",
        "1.& \\t{Closure} & ‚àÄx,y‚àà\\m{G}:x‚äó\\v{y}‚àà\\m{G}\\\\\n",
        "2.& \\t{Associativity} & ‚àÄx,y,z‚àà\\m{G}:(x‚äóy)‚äóz=x‚äó(y‚äóz)\\\\\n",
        "3.& \\t{Neutral element} & ‚àÉe‚àà\\m{G}‚àÄx‚àà\\m{G}:x‚äóe=e‚äóx=x\\\\\n",
        "4.& \\t{Inverse element} & ‚àÄx‚àà\\m{G}‚àÉy‚àà\\m{G}:x‚äóy=y‚äóx=e\\\\\n",
        "*& \\t{Abelian group} & ‚àÄx,y‚àà\\m{G}:x‚äóy=y‚äóx\n",
        "\\EC$\n",
        "\n",
        "- The neutral element rule says there is only 1 element $e‚àà\\m{G}$ that all choices of $x$ work with. The inverse rule says every element $x$ has a corresponding $y$ (which may be different for every choice of $x$).\n",
        "\n",
        "**Vector space** $V=(\\m{V},+,‚ãÖ)$ is a set $\\m{V}$ with $\\BC\n",
        "\\t{Inner operation} &+:\\m{V}√ó\\m{V}‚Üí\\m{V}\\\\\n",
        "\\t{Outer operation} &‚ãÖ:‚Ñù√ó\\m{V}‚Üí\\m{V}\n",
        "\\EC$ satisfying $\\BC\n",
        "1.& (\\m{V},+)\\t{ is Abelian} & ‚àÄ\\v{x},\\v{y}‚àà\\m{V}:\\v{x}+\\v{y}=\\v{y}+\\v{x}\\\\\n",
        "2.& \\t{Distributivity} & ‚àÄŒª‚àà‚Ñù,\\v{x},\\v{y}‚àà\\m{V}:Œª(\\v{x}+\\v{y})=Œª\\v{x}+Œª\\v{y}\\\\\n",
        "& & ‚àÄŒª,œà‚àà‚Ñù,\\v{x}‚àà\\m{V}:(Œª+œà)\\v{x}=Œª\\v{x}+œà\\v{x}\\\\\n",
        "3.& \\t{Associativity} & ‚àÄŒª,œà‚àà‚Ñù,\\v{x}‚àà\\m{V}:Œª(œà\\v{x})=œà(Œª\\v{x})\\\\\n",
        "4.& \\t{Neutral element}&‚àÄ\\v{x}‚àà\\m{V}:1\\v{x}=\\v{x}\n",
        "\\EC$\n",
        "\n",
        "- Vector space/subspace is defined by closure under linear operations (vector addition and scalar multiple).\n",
        "\n",
        "- **Subspace**: Let $V=(\\m{V},+,‚ãÖ)$ be a vector space and let $\\m{U}‚äÜ\\m{V},\\m{U}\\neq‚àÖ$. Then $U=(\\m{U},+,‚ãÖ)‚äÜV$ is called a vector subspace of $V$ if $U$ is a vector space with $+$ and $‚ãÖ$ restricted to $\\m{U}√ó\\m{U}$ and $‚Ñù√ó\\m{U}$.\n",
        "\n",
        "  - To prove $(\\m{U},+,‚ãÖ)‚äÜV$ is a subspace, need to show closure $\\BC\n",
        "  1.& \\v{0}‚àà\\m{U}\\\\\n",
        "  2.& ‚àÄŒª‚àà‚Ñù‚àÄ\\v{x}‚àà\\m{U}:Œª\\v{x}‚àà\\m{U}\\\\\n",
        "  3.& ‚àÄ\\v{x},\\v{y}‚àà\\m{U}:\\v{x}+\\v{y}‚àà\\m{U}\n",
        "  \\EC$.\n",
        "\n",
        "**Affine subspace**: Let $V$ be a vector space, $\\v{x}_0‚ààV$ be **support point** and $U‚äÜV$ be **direction space**, then $L=\\v{x}_0+U:=\\{\\v{x}_0+\\v{u}:\\v{u}‚ààU\\}$\n",
        "$=\\{\\v{v}‚ààV|‚àÉ\\v{u}‚ààU:\\v{v}=\\v{x}_0+\\v{u}\\}‚äÜV$ is called **affine subspace / hyperplane** of $V$.\n",
        "\n",
        "- Affine subspaces are not required to pass through $\\v{0}$.\n",
        "\n",
        "- Consider two affine spaces $L=\\v{x}_0+U$ and $\\tilde{L}=\\tilde{\\v{x}}_0+\\tilde{U}$. Then $L‚äÜ\\tilde{L}$ iff $U‚äÜ\\tilde{U}$ and $\\v{x}_0-\\tilde{\\v{x}}_0‚àà\\tilde{U}$.\n",
        "\n",
        "- If $B=(\\v{b}_1,...,\\v{b}_k)$ is an ordered basis of $U$, then every $\\v{x}‚ààL$ is parametrically described by $\\v{x}=\\v{x}_0+Œª_1\\v{b}_1+...+Œª_k\\v{b}_k$.\n",
        "\n",
        "- For vector spaces $V,W$ and linear mapping $Œ¶:V‚ÜíW$, and $\\v{a}‚ààW$, then $œï:V‚ÜíW,œï(\\v{x})=\\v{a}+Œ¶(\\v{x})$ is called an **affine mapping** from $V$ to $W$ and $\\v{a}$ is called the **translation vector** of $œï$.\n",
        "\n",
        "  - $œï:V‚ÜíW,œï=œÑ‚àòŒ¶$ is the composition of a linear mapping $Œ¶:V‚ÜíW$ and a translation $œÑ:W‚ÜíW$."
      ],
      "metadata": {
        "id": "z7GOqQ7CL6V_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear independence**: Consider vector space $V$ and $\\v{x}_{1..k}‚ààV$ where $k‚àà‚Ñï$ and $Œª_{1..k}‚àà‚Ñù$. Every $\\v{v}‚ààV$ of the form $\\v{v}=Œª_1\\v{x}_1+...+Œª_k\\v{x}_k=\\sum_{i=1}^kŒª_i\\v{x}_i‚ààV$ is called a **linear combination** of $\\v{x}_1,...,\\v{x}_k$. Furthermore $\\v{x}_1,...,\\v{x}_k$ are called **linearly independent** if $\\sum_{i=1}^kŒª_i\\v{x}_i=\\v{0}‚áíŒª_1=...=Œª_k=0$.\n",
        "\n",
        "- Let $\\v{B}‚àà‚Ñù^{m√ón}$ and $\\v{Œª}‚àà‚Ñù^n$, then $\\v{BŒª}=Œª_1\\BM B_{11}\\\\\\vdots\\\\B_{m1}\\EM+Œª_2\\BM B_{12}\\\\\\vdots\\\\B_{m2}\\EM...+Œª_n\\BM B_{1n}\\\\\\vdots\\\\B_{mn}\\EM$ is a linear combination of $\\v{B}$ columns. Columns of $\\v{B}=[\\v{x}_1,...,\\v{x}_k]$ are called linearly independent iff $\\sum_{i=1}^kŒª_i\\v{x}_i=\\v{B}\\v{Œª}=\\v{0}$ implies $\\v{Œª}=\\v{0}$.\n",
        "\n",
        "- **Linear dependence**: If $\\v{0}$ is one of the vectors then they're dependent. If $\\v{x}_i$ is a linear combination of the others, then they're dependent. For $m>k$, $m$ linear combinations of $k$ vectors are guaranteed to be dependent.\n",
        "\n",
        "- **Row echelon form**: Vectors are independent if $\\t{ref}([\\v{x}_1,...,\\v{x}_k])$ has a pivot in every column. A non-pivot column is a linear combination of the pivot columns to the left.\n",
        "  - E.g., in $\\BM 1&-1&3 \\\\ 0&1&-2 \\EM$, which can be solved using RREF $\\BM 1&0&1 \\\\ 0&1&-2 \\EM$ resulting in $\\v{c}_3=\\v{c}_1-2\\v{c}_2$.\n",
        "\n",
        "- Suppose $\\v{B}=[\\v{b}_1,...,\\v{b}_k]$ is a matrix whose columns are linearly independent vectors. Define $\\v{x}_1,...,\\v{x}_m$ as linear combinations of $\\v{B}$ columns:\n",
        "$\\v{x}_j=Œª_{1j}\\v{b}_1+...+Œª_{kj}\\v{b}_k$\n",
        "$=\\v{B}\\v{Œª}_j$.\n",
        "Then $\\v{x}_1,...,\\v{x}_m$ are linearly independent iff\n",
        "$\\v{Œª}_1,...,\\v{Œª}_m$ are linearly independent.\n",
        "\n",
        "  - Convention: $\\v{Œª}_{j=1..m}=\\BM Œª_{1j}\\\\\\vdots\\\\Œª_{kj}\\EM$ refers to the $j$-th column vector of matrix $\\v{Œª}‚àà‚Ñù^{k√óm}$.\n",
        "\n",
        "  - Proof: $\\sum_{j=1}^mœà_j\\v{x}_j=\\v{0}$\n",
        "  $‚áí\\sum_{j=1}^mœà_j\\v{B}\\v{Œª}_j=\\v{0}$\n",
        "  $‚áí\\v{B}\\sum_{j=1}^mœà_j\\v{Œª}_j=\\v{0}$\n",
        "  $‚áí\\sum_{j=1}^mœà_j\\v{Œª}_j=\\v{0}$\n",
        "  because $\\v{B}$ has linearly independent columns.\n",
        "  Therefore\n",
        "  $\\v{x}_1,...,\\v{x}_m$ are linearly independent iff\n",
        "  $\\sum_{j=1}^mœà_j\\v{Œª}_j=\\v{0}$\n",
        "  implies $œà_1=...=œà_m=0$ i.e.,\n",
        "  $\\v{Œª}_1,...,\\v{Œª}_m$ are linearly independent.\n",
        "\n",
        "**Generating set and basis**: Consider vector space $V=(\\m{V},+,‚ãÖ)$ and vectors $\\m{A}=\\{\\v{b}_1,...,\\v{b}_k\\}‚äÜ\\m{V}$. If every vector in $\\m{V}$ can be expressed as a linear combination of $\\v{b}_1,...,\\v{b}_k$ then $\\m{A}$ is called the **generating set** of $V$. The set of all linear combinations of vectors in $\\m{A}$ is called the **span** of $\\m{A}$. If vectors in $\\m{A}$ span $V$ and are linearly independent and there exists no smaller set that spans $V$ then the minimal generating set $\\m{A}$ is called the **basis** of $V$.\n",
        "\n",
        "- For $V=(\\m{V},+,‚ãÖ)$ and $\\m{B}‚äÜ\\m{V},\\m{B}\\neq‚àÖ$, the following statements are equivalent:\n",
        "  \n",
        "  - $\\m{B}$ is the minimal generating set and basis of $V$.\n",
        "  \n",
        "  - $\\m{B}$ is a maximal linearly independent set of vectors in $V$; adding another vector will make it dependent.\n",
        "\n",
        "  - Every vector $\\v{x}‚ààV$ is a unique linear combination of vectors in $\\m{B}$. If $\\v{x}=\\sum_iŒª_i\\v{b}_i=\\sum_iœà_i\\v{b}_i$ then $Œª_i=œà_i$.\n",
        "\n",
        "- **Basis** of $U=\\span(\\v{x}_1,...,\\v{x}_m)‚äÜ‚Ñù^n$ are the vectors corresponding to the pivot columns of $\\t{ref}([\\v{x}_1,...,\\v{x}_k])$.\n",
        "\n",
        "  - Ordered basis $B=(\\v{b}_1,...,\\v{b}_n)$, unordered basis $\\m{B}=\\{\\v{b}_1,...,\\v{b}_n\\}$, and basis matrix $\\v{B}=[\\v{b}_1,...,\\v{b}_n]$.\n",
        "\n",
        "- **Dimension** of $V$ is the number of basis vectors that all bases of $V$ possess. Let $U‚äÜV$ be a subspace, then $\\dim(U)‚â§\\dim(V)$. Equality $\\dim(U)=\\dim(V)$ iff $U=V$.\n",
        "\n",
        "  - $\\dim(V)$ is not the number of elements of each basis vector. For example, $V=\\span(\\BM 0\\\\1 \\EM)$ is one-dimensional although the basis vector has 2 elements.\n",
        "\n",
        "- **Rank** of a matrix $\\v{A}‚àà‚Ñù^{m√ón}$, $\\rk(\\v{A})$, is the number of linearly independent columns and rows of $\\v{A}$.\n",
        "\n",
        "  - $\\v{A}‚àà‚Ñù^{m√ón}$ has **full rank** if $\\rk(A)=\\min(m,n)$ is the largest possible for a matrix of the same dimensions.\n",
        "\n",
        "  - $\\rk(\\v{A})=\\rk(\\v{A}^‚ä§)$: the number of linearly independent columns and rows are equal.\n",
        "\n",
        "  - Let $U=\\span(\\v{A})$. Then $\\dim(U)=\\rk(A)$.\n",
        "  \n",
        "  - Matrix $\\v{A}‚àà‚Ñù^{n√ón}$ is invertible iff $\\rk(\\v{A})=n$.\n",
        "\n",
        "  - The system $\\v{Ax}=\\v{b}$ can be solved iff $\\rk(\\v{A})=\\rk(\\v{A}|\\v{b})$, where $\\v{A}|\\v{b}$ denotes augmented system.\n",
        "\n",
        "  - Proof (row rank = column rank): $\\t{rref}(\\v{A})$ and $\\t{rref}(\\v{A}^‚ä§)$ are kind of the same and have same number of pivots.\n",
        "\n",
        "- $\\rk(\\v{uv}^‚ä§)=1$ outer product produces rank-1 matrix.\n",
        "\n",
        "  - Proof: $\\v{uv}^‚ä§=\\v{u}\\BM v_1&...&v_n\\EM=\\BM v_1\\v{u}&...&v_n\\v{u}\\EM$. $\\span(\\BM v_1\\v{u}&...&v_n\\v{u}\\EM)=\\span(\\v{u})$.\n",
        "\n",
        "- 2.14: $x_1=\\BM 1\\\\2\\\\-3\\\\4\\EM$,\n",
        "$x_2=\\BM 1\\\\1\\\\0\\\\2\\EM$,\n",
        "$x_3=\\BM -1\\\\-2\\\\1\\\\1\\EM$.\n",
        "Then\n",
        "$\\BM 1&1&-1\\\\2&1&-2\\\\-3&0&1\\\\4&2&2\\EM\n",
        "=\\BM 1&1&-1\\\\0&1&0\\\\0&3&-2\\\\0&2&-6\\EM\n",
        "=\\BM \\red{1}&1&-1\\\\0&\\red{1}&0\\\\0&0&\\red{2}\\\\0&0&0\\EM$.\n",
        "Therefore independent.\n",
        "\n",
        "- 2.15: Consider linearly independent $\\v{b}_{1..4}‚àà‚Ñù^n$.\n",
        "$\\BC\n",
        "x_1=\\v{b}_1-2\\v{b}_2+\\v{b}_3-\\v{b}_4\\\\\n",
        "x_2=-4\\v{b}_1-2\\v{b}_2+4\\v{b}_4\\\\\n",
        "x_3=2\\v{b}_1+3\\v{b}_2-\\v{b}_3-3\\v{b}_4\\\\\n",
        "x_4=17\\v{b}_1-10\\v{b}_2+11\\v{b}_3+\\v{b}_4\n",
        "\\EC$\n",
        "$‚áí\\BM 1&-4&2&17 \\\\ -2&-2&3&-10 \\\\ 1&0&-1&11 \\\\ -1&4&-3&1 \\EM$\n",
        "$‚áí\\BM 1&-4&2&17\\\\0&-10&7&24\\\\0&-4&3&6\\\\0&0&-1&18 \\EM$\n",
        "$‚áí\\BM 1&-4&2&17\\\\0&-10&7&24\\\\0&0&1&-18\\\\0&0&0&0 \\EM$\n",
        "Therefore $x_1,x_2,x_3,x_4$ are not independent.\n",
        "\n",
        "- 2.17: Find the basis of $U‚äÜ‚Ñù^5$ spanned by $x_1=\\BM 1\\\\2\\\\-1\\\\-1\\\\-1 \\EM$, $x_2=\\BM 2\\\\-1\\\\1\\\\2\\\\-2 \\EM$, $x_3=\\BM 3\\\\-4\\\\3\\\\5\\\\-3 \\EM$, and $x_4=\\BM -1\\\\8\\\\-5\\\\-6\\\\1 \\EM$.\n",
        "$\\BM 1&2&3&-1\\\\2&-1&-4&8\\\\-1&1&3&-5\\\\-1&2&5&-6\\\\-1&-2&-3&1\\EM$\n",
        "$‚Üí\\BM 1&2&3&-1\\\\0&1&2&-2\\\\0&1&2&-2\\\\0&4&8&-7\\\\0&0&0&0\\EM$\n",
        "$‚Üí\\BM \\red{1}&2&3&-1\\\\0&\\red{1}&2&-2\\\\0&0&0&\\red{1}\\\\0&0&0&0\\\\0&0&0&0\\EM$. Therefore $x_1,x_2,x_4$ are a basis of $U$."
      ],
      "metadata": {
        "id": "egNCOD3arpYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Mappings**: For vector spaces $V,W$, mapping $Œ¶:V‚ÜíW$ is called a **linear mapping** (linear transformation or homomorphism) if $‚àÄ\\v{x},\\v{y}‚ààV\\ ‚àÄŒª,œà‚àà‚Ñù:Œ¶(Œª\\v{x}+œà\\v{y})=ŒªŒ¶(\\v{x})+œàŒ¶(\\v{y})$. Furthermore $Œ¶:\\m{V}‚Üí\\m{W}$ is called\n",
        "$\\BC\n",
        "\\t{Injective (one-to-one) if}& ‚àÄ\\v{x},\\v{y}‚àà\\m{V}:Œ¶(\\v{x})=Œ¶(\\v{y})‚áí\\v{x}=\\v{y}\\\\\n",
        "\\t{Surjective (onto) if}& Œ¶(\\m{V})=\\m{W}\\\\\n",
        "\\t{Bijective (invertible) if}& \\t{both injective and surjective}\n",
        "\\EC$\n",
        "\n",
        "- If $Œ¶$ is bijective then $‚àÉŒ®:\\m{W}‚Üí\\m{V}$ such that $Œ®‚àòŒ¶(\\v{x})=\\v{x}$ and denoted $Œ®=Œ¶^{-1}$. Linear bijective $V‚ÜíW$ mappings are called **isomorphisms**, which exist between vector spaces $V$ and $W$ iff $\\dim(V)=\\dim(W)$.\n",
        "\n",
        "  - Vector spaces of the same dimension are called **isomorphic**, and can be transformed into each other.\n",
        "\n",
        "  - Linear $Œ¶:V‚ÜíV$ mappings are called **endomorphic**, or **automorphic** if also bijective.\n",
        "\n",
        "  - Consider $‚Ñù^n$ canonical basis vectors $\\v{e}_1,...,\\v{e}_n$ and $n$-dimensional vector space $V$ with ordered basis $B$, then the mapping $Œ¶:‚Ñù^n‚ÜíV,Œ¶(\\v{e}_i)=\\v{b}_i$ is an isomorphism.\n",
        "\n",
        "- Consider ordered basis $B=(\\v{b}_1,...,\\v{b}_n)$ of vector space $V$ and vector $\\red{\\v{x}=\\v{B}\\v{x}_B}=x_{B1}\\v{b}_1+...+x_{Bn}\\v{b}_n‚ààV$, then $\\v{x}_B=\\BM x_{B1}\\\\\\vdots\\\\x_{Bn}\\EM$ are the **coordinates** of $\\v{x}$ with respect to $B$.\n",
        "\n",
        "  - $\\v{x}=\\v{B}\\v{x}_B$: the basis $\\v{B}$ is the \"unit of measurement\" for the coordinate values.\n",
        "\n",
        "**Transformation matrix**: Consider vector spaces $V,W$ with ordered bases $B=(\\v{b}_1,...,\\v{b}_n)$ and $C=(\\v{c}_1,...,\\v{c}_m)$. Consider linear mapping\n",
        "$Œ¶:V‚ÜíW,Œ¶(\\v{b}_{j=1..n})=Œ±_{1j}\\v{c}_1+...+Œ±_{mj}\\v{c}_m=\\sum_{i=1}^mŒ±_{ij}\\v{c}_i$.\n",
        "Then $\\v{A}_{BC}=[Œ±_{ij}]‚àà‚Ñù^{m√ón}$\n",
        "is the transformation matrix of $Œ¶$ with respect to $B$ and $C$ such that $\\red{Œ¶(\\v{B})=\\v{C}\\v{A}_{BC}}$.\n",
        "\n",
        "- $\\v{A}_{BC}$ takes input coordinate vector in $B$-units, converts into standard Cartesian units, transforms the vector, converts into $C$-units, and then outputs coordinate vector in $C$-units.\n",
        "\n",
        "- Let $\\v{x}_B$ be the coordinate vector of $\\v{x}‚ààV$ with respect to $B$ and let $\\v{y}_C$ be the coordinate vector of $\\v{y}=Œ¶(\\v{x})‚ààW$ with respect to $C$, then $\\red{\\v{C}\\v{y}_C=Œ¶(\\v{B})\\v{x}_B}$ and $\\red{\\v{y}_C=\\v{A}_{BC}\\v{x}_B}$.\n",
        "\n",
        "  - Proof: $Œ¶(\\v{B}\\v{x}_B)$\n",
        "  $=Œ¶(x_{B1}\\v{b}_1+...+x_{Bn}\\v{b}_n)$\n",
        "  $=x_{B1}Œ¶(\\v{b}_1)+...+x_{Bn}Œ¶(\\v{b}_n)$\n",
        "  $=Œ¶(\\v{B})\\v{x}_B$\n",
        "  by property of linear mapping.\n",
        "  $\\v{y}=Œ¶(\\v{x})$\n",
        "  $‚áí\\v{C}\\v{y}_C=Œ¶(\\v{B}\\v{x}_B)$\n",
        "  $=Œ¶(\\v{B})\\v{x}_B$\n",
        "  $=\\v{C}\\v{A}_{BC}\\v{x}_B$\n",
        "  $‚áí\\v{C}^{-1}\\v{C}\\v{y}_C=\\v{C}^{-1}\\v{C}\\v{A}_{BC}\\v{x}_B$\n",
        "  $‚áí\\v{y}_C=\\v{A}_{BC}\\v{x}_B$.\n",
        "\n",
        "- **Standard matrix** $\\v{A}_{std}$ takes coordinate vector $\\v{x}$ of standard basis $\\v{I}_n$ as input and outputs coordinate vector $\\v{y}$ of standard basis $\\v{I}_m$. It corresponds to transformation $Œ¶:‚Ñù^n‚Üí‚Ñù^m$. Then $\\v{y}=\\v{A}\\v{x}$ and $\\red{\\v{A}_{BC}=\\v{C}^{-1}\\v{A}_{std}\\v{B}}$.\n",
        "  \n",
        "  - Intuition: Suppose $Œ¶(\\v{x})=\\BM a_{11}x_1+...+a_{1n}x_n\\\\a_{21}x_1+...+a_{2n}x_n\\\\\\vdots\\\\a_{m1}x_1+...+a_{mn}x_n\\EM$,\n",
        "  $\\v{A}_{std}=\\BM a_{11}&...&a_{1n}\\\\\\vdots&\\ddots&\\vdots\\\\a_{m1}&...&a_{mn}\\EM$,\n",
        "  and $Œ¶(\\v{x})=\\v{A}_{std}\\v{x}$. Then\n",
        "  $Œ¶(\\v{B})=\\v{A}_{std}\\v{B}=\\v{B}'$ transforms the gridlines so that $\\v{x}=\\v{B}\\v{x}_B$ becomes $\\v{x}'=\\v{B}'\\v{x}_B$, which keeps the same coordinates.\n",
        "  $Œ¶(\\v{B})=\\v{B}'=\\v{CA}_{BC}$ expresses the new gridlines in $C$-measurement units.\n",
        "  $\\v{CA}_{BC}=\\v{A}_{std}\\v{B}$\n",
        "  $‚áí\\v{A}_{BC}=\\v{C}^{-1}\\v{A}_{std}\\v{B}$.\n",
        "\n",
        "- Consider $V,W$ with bases $B,C$ and transformation $Œ¶:V‚ÜíW,Œ¶(\\v{b}_j)=Œ±_{1j}\\v{c}_1+...+Œ±_{mj}\\v{c}_m$. Let $\\v{x}=\\BM \\v{b}_1&...&\\v{b}_n\\EM\\v{x}_B$\n",
        "$=\\v{B}\\v{x}_B$,\n",
        "then\n",
        "$\\v{y}=Œ¶(\\v{x})=Œ¶(\\v{B}\\v{x}_B)$\n",
        "$=\\BM Œ¶(\\v{b}_1)&...&Œ¶(\\v{b}_n)\\EM\\v{x}_B$\n",
        "$=\\BM Œ±_{11}\\v{c}_1+...+Œ±_{n1}\\v{c}_n&...&Œ±_{1n}\\v{c}_1+...+Œ±_{nn}\\v{c}_n\\EM\\v{x}_B$\n",
        "$=\\BM\\v{c}_1&...&\\v{c}_n\\EM\\BM Œ±_{11}&...&Œ±_{1n}\\\\\\vdots&\\ddots&\\vdots\\\\Œ±_{n1}&...&Œ±_{nn}\\EM\\v{x}_B$\n",
        "$=\\v{C}\\v{A}_{BC}\\v{x}_B=\\v{C}\\v{y}_C$.\n",
        "Therefore\n",
        "$\\v{C}\\v{y}_C=Œ¶(\\v{B})\\v{x}_B$ and\n",
        "$\\v{y}_C=\\v{A}_{BC}\\v{x}_B$.\n",
        "\n",
        "**Basis Change**: Consider linear mapping $Œ¶:V‚ÜíW$, ordered bases $B$ and $\\tilde{B}$ of $V$ and ordered bases $C$ and $\\tilde{C}$ of $W$ such that $\\tilde{\\v{B}}=\\v{B}\\v{S}$ and $\\tilde{\\v{C}}=\\v{C}\\v{T}$. Let $\\v{A}_{BC}‚àà‚Ñù^{m√ón}$ be transformation matrix with respect to $B$ and $C$, then $\\tilde{\\v{A}}_{BC}$ corresponding to $\\tilde{B}$ and $\\tilde{C}$ is given by $\\red{\\tilde{\\v{A}}_{BC}=\\v{T}^{-1}\\v{A}_{BC}\\v{S}}$, where $\\v{S}=\\v{B}^{-1}\\tilde{\\v{B}}‚àà‚Ñù^{n√ón}$ and $\\v{T}=\\v{C}^{-1}\\tilde{\\v{C}}‚àà‚Ñù^{m√óm}$.\n",
        "\n",
        "- Proof: Let $\\tilde{\\v{B}}=\\v{B}\\v{S}$ and $\\tilde{\\v{C}}=\\v{C}\\v{T}$.\n",
        "Deriving through the new bases:\n",
        "$Œ¶(\\tilde{\\v{B}})=\\tilde{\\v{C}}\\tilde{\\v{A}}_{BC}$\n",
        "$=\\v{C}\\v{T}\\tilde{\\v{A}}_{BC}$.\n",
        "Deriving through the old bases:\n",
        "$Œ¶(\\tilde{\\v{B}})=Œ¶(\\v{BS})$\n",
        "$=Œ¶(\\BM \\v{b}_1 & ... & \\v{b}_n\\EM\\BM s_{11} &...& s_{1n}\\\\\\vdots&\\ddots&\\vdots\\\\ s_{n1} &...& s_{nn}\\EM)$\n",
        "$=\\BM Œ¶(\\v{b}_1) & ... & Œ¶(\\v{b}_n)\\EM\\BM s_{11} &...& s_{1n}\\\\\\vdots&\\ddots&\\vdots\\\\ s_{n1} &...& s_{nn}\\EM$\n",
        "$=Œ¶(\\v{B})\\v{S}=\\v{CA}_{BC}\\v{S}$.\n",
        "Therefore\n",
        "$\\green{\\v{T}\\tilde{\\v{A}}_{BC}=\\v{A}_{BC}\\v{S}}$\n",
        "$‚áí\\tilde{\\v{A}}_{BC}=\\v{T}^{-1}\\v{A}_{BC}\\v{S}$.\n",
        "\n",
        "- Consider vector space $V$ with basis $B$ and linear transformation $Œ¶(\\v{B}):V‚ÜíV,Œ¶(\\v{B})=\\v{BA}_{BB}$. The transformation matrix $\\v{A}_{BB}$ converts input $\\v{x}=\\v{B}\\v{x}_B$ to output $\\v{y}=\\v{B}\\v{y}_B$ by $\\v{y}_B=\\v{A}_{BB}\\v{x}_B$.\n",
        "  \n",
        "  - Consider vector space $W$ with basis $C$. If we want output $\\v{y}=\\v{C}\\v{y}_C$, then $Œ¶(\\v{B}):V‚ÜíW,Œ¶(\\v{B})=\\v{CA}_{BC}$ describes the same transformation but measured with a different output basis. $\\green{\\v{BA}_{BB}=\\v{CA}_{BC}}$\n",
        "  $‚áí\\blue{\\v{A}_{BC}=(\\v{C}^{-1}\\v{B})\\v{A}_{BB}}$, where $\\v{y}_C=(\\v{C}^{-1}\\v{B})\\v{y}_B$.\n",
        "  Therefore $\\v{y}_C=\\v{A}_{BC}\\v{x}_B$.\n",
        "\n",
        "  - If we want input $\\v{x}=\\v{C}\\v{x}_C$, then $Œ¶(\\v{C}):W‚ÜíW,Œ¶(\\v{C})=\\v{CA}_{CC}$ describes the same transformation but measured with both different input and output bases.\n",
        "  $Œ¶(\\v{B})=Œ¶(\\v{CC}^{-1}\\v{B})=Œ¶(\\v{C})(\\v{C}^{-1}\\v{B})$\n",
        "  $=\\green{\\v{CA}_{CC}(\\v{C}^{-1}\\v{B})=\\v{BA}_{BB}}$\n",
        "  $‚áí\\v{A}_{CC}=(\\v{C}^{-1}\\v{B})\\v{A}_{BB}(\\v{C}^{-1}\\v{B})^{-1}$\n",
        "  $‚áí\\blue{\\v{A}_{CC}=(\\v{C}^{-1}\\v{B})\\v{A}_{BB}(\\v{B}^{-1}\\v{C})}$, where\n",
        "  $\\v{x}_B=(\\v{B}^{-1}\\v{C})\\v{x}_C$. Therefore\n",
        "  $\\v{y}_C=\\v{A}_{CC}\\v{x}_C$.\n",
        "\n",
        "- $\\tilde{\\v{B}}\\larr{\\v{S}}\\v{B}\\larr{\\v{A}_{BC}}\\v{C}\\larr{\\v{T}^{-1}}\\tilde{\\v{C}}$.\n",
        "\n",
        "- $\\v{A},\\tilde{\\v{A}}‚àà‚Ñù^{m√ón}$ are **equivalent** if there exist invertible $\\v{S}‚àà‚Ñù^{n√ón},\\v{T}‚àà‚Ñù^{m√óm}$ such that $\\tilde{\\v{A}}=\\v{T}^{-1}\\v{A}\\v{S}$. Two matrices are equivalent if you can apply different basis conversion to input & output of one matrix to get the other.\n",
        "\n",
        "  - $\\rk(\\v{A})=\\rk(\\tilde{\\v{A}})$\n",
        "\n",
        "- $\\v{A},\\tilde{\\v{A}}‚àà‚Ñù^{n√ón}$ are **similar** if there exist invertible $\\v{S}‚àà‚Ñù^{n√ón}$ such that $\\tilde{\\v{A}}=\\v{S}^{-1}\\v{A}\\v{S}$. Two matrices are similar if you can apply the same basis conversion and its inverse to input & output of one to get the other.\n",
        "\n",
        "  - $\\rk(\\v{A})=\\rk(\\tilde{\\v{A}})$, $\\det(\\v{A})=\\det(\\tilde{\\v{A}})$, $\\tr(\\v{A})=\\tr(\\tilde{\\v{A}})$, same eigenvalues, and same characteristic polynomials.\n",
        "\n",
        "- 2.21: Let $\\BC\n",
        "Œ¶(\\v{b}_1)=\\v{c}_1-\\v{c}_2+3\\v{c}_3-\\v{c}_4\\\\\n",
        "Œ¶(\\v{b}_2)=2\\v{c}_1+\\v{c}_2+7\\v{c}_3+2\\v{c}_4\\\\\n",
        "Œ¶(\\v{b}_3)=3\\v{c}_2+\\v{c}_3+4\\v{c}_4\n",
        "\\EC$, then $\\v{A}_{BC}=[\\v{Œ±}_1,\\v{Œ±}_2,\\v{Œ±}_3]=\\BM\n",
        "1&2&0\\\\-1&1&3\\\\3&7&1\\\\-1&2&4\\EM$.\n",
        "\n",
        "- 2.22: Let $\\v{A}_{BB}=\\BM \\cos(œÄ/4)&-\\sin(œÄ/4) \\\\\n",
        "\\sin(œÄ/4)&\\cos(œÄ/4) \\EM$\n",
        "$=\\BM 1/\\sqrt{2}&-1/\\sqrt{2}\\\\1/\\sqrt{2}&1/\\sqrt{2} \\EM$  be an endomorphic transformation in basis $B$, i.e., $Œ¶(\\v{B})=\\v{B}\\v{A}_{BB}$.\n",
        "Let $\\v{B}=\\BM 1&0\\\\0&1\\EM$ and $\\v{x}=\\v{B}\\BM 1\\\\1\\EM$.\n",
        "If we transform $\\v{x}$ in canonical basis, then $\\v{y}_B=\\v{A}_{BB}\\BM 1\\\\1\\EM$\n",
        "$=\\BM 0\\\\\\sqrt{2}\\EM$ with respect to basis $B$.\n",
        "If we transform $\\v{x}$ and express it with respect to basis $\\v{C}=\\BM 1/\\sqrt{2}&-1/\\sqrt{2}\\\\1/\\sqrt{2}&1/\\sqrt{2} \\EM$, then\n",
        "$Œ¶(\\v{B})=\\v{BA}_{BB}=\\v{C}\\v{A}_{BC}$\n",
        "$‚áí\\v{A}_{BC}=\\v{C}^{-1}\\v{B}\\v{A}_{BB}$\n",
        "$=\\v{I}_n$.\n",
        "Then\n",
        "$\\v{y}_C=\\v{A}_{BC}\\v{x}_B$\n",
        "$=\\BM 1\\\\1\\EM$.\n",
        "\n",
        "- 2.23: $\\v{A}_{II}=\\BM 2&1\\\\1&2\\EM$ corresponds to canonical basis in $‚Ñù^2$. Define new basis $\\v{B}=\\BM 1&1\\\\1&-1\\EM$.\n",
        "$\\v{B}^{-1}=\\/{1}{-2}\\BM -1&-1\\\\-1&1\\EM$\n",
        "$=\\BM 1/2&1/2\\\\1/2&-1/2\\EM$.\n",
        "Then $\\v{I}\\v{A}_{II}=\\v{B}\\v{A}_{IB}$\n",
        "$‚áí\\v{A}_{IB}=\\v{B}^{-1}\\v{A}_{II}$.\n",
        "Then $\\v{A}_{BB}=\\v{B}^{-1}\\v{A}_{II}\\v{B}$\n",
        "$=\\BM 3/2&3/2\\\\1/2&-1/2\\EM\\v{B}$\n",
        "$=\\BM 3&0\\\\0&1\\EM$.\n"
      ],
      "metadata": {
        "id": "H9eRDx-zy46D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image and Kernel**: For $Œ¶:V‚äÜ‚Ñù^n‚ÜíW‚äÜ‚Ñù^m,Œ¶(\\v{x})=\\v{Ax}$ where $\\v{A}‚àà‚Ñù^{m√ón}$. The **kernel / null space** and **image / column space** are $\\BC\n",
        "\\blue{\\null(\\v{A}):=\\{\\v{v}‚ààV|\\v{Av}=\\v{0}_W\\}} \\\\\n",
        "\\blue{\\Im(\\v{A}):=\\{\\v{w}‚ààW|\\v{Av}=\\v{w}\\t{ for some }\\v{v}\\}}\n",
        "\\EC$.\n",
        "\n",
        "- $\\null(Œ¶)\\neq‚àÖ$ because $Œ¶(\\v{0}_V)=\\v{0}_W$.\n",
        "\n",
        "- $\\red{\\null(\\v{A})=\\span(\\t{minus-1 columns of rref}(\\v{A}))}‚äÜV$ is called the **null space** of $\\v{A}$, and is the general solution to $\\v{Ax}=\\v{0}$, which captures all possible linear combinations of $V$ that produce $\\v{0}_W$.\n",
        "\n",
        "  - $Œ¶$ is injective (one-to-one) iff $\\null(\\v{A})=\\{\\v{0}\\}$.\n",
        "\n",
        "- $\\red{\\span(\\v{A})=\\span(\\t{pivot columns of }\\v{A})}‚äÜW$ is called **column space** of $\\v{A}$.\n",
        "\n",
        "  - If $\\dim(V)=\\dim(W)$ then $Œ¶$ is injective ‚áî $Œ¶$ is surjective ‚áî $Œ¶$ is bijective.\n",
        "\n",
        "- $\\null(\\v{A}^‚ä§)‚äÜW$ is the **left null space** of $\\v{A}$. $\\red{\\span(\\v{A})^\\perp=\\null(\\v{A}^‚ä§)}$: every vector in that subspace is orthogonal to every vector in $\\span(\\v{A})$, or the subspace that the linear combinations of columns of $\\v{A}$ cannot reach. $\\null(\\v{A}^‚ä§)‚à©\\span(\\v{A})=\\{\\v{0}_W\\}$.\n",
        "\n",
        "  - $\\dim(\\span(\\v{A}))+\\dim(\\null(\\v{A}^‚ä§))=m$\n",
        "\n",
        "- $\\span(\\v{A}^‚ä§)$ is the **row space** of $\\v{A}$.\n",
        "\n",
        "  - $\\red{\\null(\\v{A})^\\perp=\\span(\\v{A}^‚ä§)}$\n",
        "\n",
        "- **Rank-Nullity Theorem**: $\\dim(\\null(\\v{A}))+\\dim(\\span(\\v{A}))=n$.\n",
        "\n",
        "  - $\\rk(\\v{A})=\\dim(\\span(\\v{A}))$\n",
        "\n",
        "  - If $\\rk(\\v{A})=n$ then $\\null(Œ¶)=\\{\\v{0}\\}$.\n",
        "\n",
        "  - The subspace of solutions for $\\v{Ax}=\\v{0}$ has dimension $n-\\rk(\\v{A})$.\n",
        "\n",
        "- Let $\\v{A}‚àà‚Ñù^{m√ón},\\v{F}‚àà‚Ñù^{n√ók}$, and let $\\span(\\v{F})=\\null(\\v{A})$, then\n",
        "\n",
        "  - $\\v{AF}=\\v{0}$\n",
        "\n",
        "  - $\\rk(\\v{A})=n-k$.\n",
        "\n",
        "  - If $\\v{b}‚àà\\span(\\v{A})$, then $\\v{Ax}=\\v{b}$ has solution set $\\{\\v{x}_0+\\null(A)\\}=\\{\\v{x}_0+\\v{Fz}\\ ‚àÄ\\v{z}‚àà‚Ñù^k\\}$. Here $\\v{x}_0‚àà‚Ñù^n$ is one solution e.g., $\\v{x}_0=\\v{A}^‚Ä†\\v{b}$.\n",
        "\n",
        "- 2.25: $\\v{A}=\\BM 1&2&-1&0\\\\1&0&0&1 \\EM$\n",
        "$‚Üí\\BM 1&0&0&1 \\\\ 0&1&-1/2&-1/2 \\\\0&0&\\gray{-1}&0 \\\\ 0&0&0&\\gray{-1}\\EM$.\n",
        "$\\span(\\v{A})=\\span(\\BM 1\\\\1 \\EM,\\BM 2\\\\0 \\EM)$.\n",
        "$\\null(\\v{A})=\\span(\\BM 0\\\\-1/2\\\\-1\\\\0\\EM, \\BM 1\\\\-1/2\\\\0\\\\-1\\EM)$"
      ],
      "metadata": {
        "id": "X6b70vpY81n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2.5: Solve systems of equations\n",
        "\n",
        "  - $\\v{A}=\\BM 1&1&-1&-1\\\\2&5&-7&-5\\\\2&-1&1&3\\\\5&2&-4&2\\EM$ and $\\v{b}=\\BM 1\\\\-2\\\\4\\\\6\\EM$.\n",
        "  $‚áí\\BM 1&1&-1&-1&1\\\\0&-3&5&3&4\\\\0&3&-3&-5&-2\\\\0&3&-1&-7&-1\\EM$\n",
        "  $‚Üí\\BM 1&1&-1&-1&1\\\\0&-3&5&3&4\\\\0&0&2&-2&2\\\\0&0&4&-4&3\\EM$\n",
        "  $‚Üí\\BM 1&1&-1&-1&1\\\\0&-3&5&3&4\\\\0&0&1&-1&1\\\\0&0&0&0&1\\EM$. No solution.\n",
        "\n",
        "  - $\\v{A}=\\BM 1&-1&0&0&1\\\\1&1&0&-3&0\\\\2&-1&0&1&-1\\\\-1&2&0&-2&-1\\EM$, and $\\v{b}=\\BM 3\\\\6\\\\5\\\\-1\\EM$.\n",
        "  $‚áí\\BM 1&0&0&0&-1&3\\\\0&1&0&0&-2&0\\\\0&0&0&1&-1&-1\\\\0&0&0&0&0&0\\EM$.\n",
        "  Then the solution affine subspace after applying minus-1 rule is $\\v{x}=\\BM 3\\\\0\\\\0\\\\-1\\\\0\\EM+Œª_1\\BM 0\\\\0\\\\1\\\\0\\\\0\\EM+Œª_2\\BM 1\\\\2\\\\0\\\\1\\\\1\\EM$\n",
        "\n",
        "- 2.6: $\\v{A}=\\BM0&1&0&0&1&0\\\\0&0&0&1&1&0\\\\0&1&0&0&0&1\\EM$ and $\\v{b}=\\BM2\\\\-1\\\\1\\EM$.\n",
        "$‚áí\\BM0&1&0&0&0&1&1\\\\0&0&0&1&0&1&-2\\\\0&0&0&0&1&-1&1\\EM$.\n",
        "Therefore the affine subspace is $\\v{x}=\\BM0\\\\1\\\\0\\\\-2\\\\1\\\\0\\EM+Œª_1\\BM1\\\\0\\\\0\\\\0\\\\0\\\\0\\EM+Œª_2\\BM0\\\\0\\\\1\\\\0\\\\0\\\\0\\EM+Œª_3\\BM0\\\\1\\\\0\\\\1\\\\-1\\\\-1\\EM$\n",
        "\n",
        "- 2.7 (eigen): Let $\\v{A}=\\BM6&4&3\\\\6&0&9\\\\0&8&0\\EM$. Solve $\\v{Ax}=12\\v{x}$ subject to constraint $\\sum_ix_i=1$.\n",
        "Rewrite problem as $(\\v{A}-12\\v{I})\\v{x}=\\v{0}$.\n",
        "Then\n",
        "$\\BM-6&4&3&0\\\\6&-12&9&0\\\\0&8&-12&0\\EM$\n",
        "$‚Üí\\BM1&0&-3/2&0\\\\0&1&-3/2&0\\\\0&0&0&0\\EM$.\n",
        "Then $\\v{x}=Œª\\BM3/2\\\\3/2\\\\1\\EM$\n",
        "$=\\BM3/8\\\\3/8\\\\1/4\\EM$\n",
        "\n",
        "- 2.8: Solve inverse\n",
        "\n",
        "  - $\\v{A}=\\BM2&3&4\\\\3&4&5\\\\4&5&6\\EM$.\n",
        "  $\\BM2&3&4&1&0&0\\\\3&4&5&0&1&0\\\\4&5&6&0&0&1\\EM‚Üí\n",
        "  \\BM1&0&-1&0&-5&4\\\\0&1&2&0&4&-3\\\\0&0&0&1&-2&1\\EM$.\n",
        "  No inverse because $\\rk(\\v{A})< 3$.\n",
        "\n",
        "  - $\\v{A}=\\BM1&0&1&0\\\\0&1&1&0\\\\1&1&0&1\\\\1&1&1&0\\EM$.\n",
        "  $\\BM1&0&1&0&1&0&0&0\\\\0&1&1&0&0&1&0&0\\\\1&1&0&1&0&0&1&0\\\\1&1&1&0&0&0&0&1\\EM$\n",
        "  $‚Üí‚Äã\\BM1&0&0&0&0&-1&0&1\\\\0&1&0&0&-1&0&0&1\\\\0&0&1&0&1&1&0&-1\\\\0&0&0&1&1&1&1&-2\\EM$\n",
        "\n",
        "- 2.9: Subspace of $‚Ñù^3$?\n",
        "\n",
        "  - $A=\\{(Œª,Œª+Œº^3,Œª-Œº^3)|Œª,Œº‚àà‚Ñù\\}$\n",
        "  $=Œª\\BM1\\\\1\\\\1\\EM+Œº^3\\BM0\\\\1\\\\-1\\EM$\n",
        "  is a linear combination passing through $\\v{0}$.\n",
        "\n",
        "  - $B=\\{(Œª^2,-Œª^2,0)|Œª‚àà‚Ñù\\}$\n",
        "  $=Œª^2\\BM1\\\\-1\\\\0\\EM$\n",
        "  is a line through $\\v{0}$, but $Œª^2>0$ and $-Œª^2\\BM1\\\\-1\\\\0\\EM‚àâB$.\n",
        "  Not subspace.\n",
        "\n",
        "- 2.10:Linearly independent?\n",
        "\n",
        "  - $\\v{x}_1=\\BM2\\\\-1\\\\3\\EM$, $\\v{x}_2=\\BM1\\\\1\\\\-2\\EM$, $\\v{x}_3=\\BM3\\\\-3\\\\8\\EM$.\n",
        "  $\\BM2&1&3\\\\-1&1&-3\\\\3&-2&8\\EM$\n",
        "  $‚Üí\\BM1&0&2\\\\0&1&-1\\\\0&0&0\\EM$.\n",
        "  $\\v{x}_3=2\\v{x}_1-\\v{x}_2$.\n",
        "\n",
        "- 2.11: write $\\v{y}=\\BM1\\\\-2\\\\4\\EM$ as a linear combination of $\\v{x}_1=\\BM1\\\\1\\\\1\\EM$, $\\v{x}_2=\\BM1\\\\2\\\\3\\EM$, and $\\v{x}_3=\\BM2\\\\-1\\\\1\\EM$.\n",
        "$\\BM1&1&2&1\\\\1&2&-1&-2\\\\1&3&1&4\\EM$\n",
        "$‚Üí\\BM1&0&0&-5\\\\0&1&0&12/5\\\\0&0&1&9/5\\EM$.\n",
        "Then $\\v{y}=-5\\v{x}_1+\\/{12}{5}\\v{x}_2+\\/{9}{5}\\v{x}_3$.\n",
        "\n",
        "- 2.12 (intersection of spans): $U_1=\\span(\\BM1\\\\1\\\\-3\\\\1\\EM,\\BM2\\\\-1\\\\0\\\\-1\\EM,\\BM-1\\\\1\\\\-1\\\\1\\EM)$ and $U_2=\\span(\\BM-1\\\\-2\\\\2\\\\1\\EM,\\BM2\\\\-2\\\\0\\\\0\\EM,\\BM-3\\\\6\\\\-2\\\\-1\\EM)$.\n",
        "Determine the basis for $U_1‚à©U_2$.\n",
        "First we simplify them.\n",
        "$U_1$ basis:\n",
        "$\\BM1&2&-1\\\\1&-1&1\\\\-3&0&-1\\\\1&-1&1\\EM$\n",
        "$‚Üí\\BM1&0&1/3\\\\0&1&-2/3\\\\0&0&0\\\\0&0&0\\EM$ and\n",
        "$U_1=\\span(\\BM1\\\\1\\\\-3\\\\1\\EM,\\BM2\\\\-1\\\\0\\\\-1\\EM)$.\n",
        "$U_2$ basis:\n",
        "$\\BM-1&2&-3\\\\-2&-2&6\\\\2&0&-2\\\\1&0&-1\\EM$\n",
        "$‚Üí\\BM1&0&-1\\\\0&1&-2\\\\0&0&0\\\\0&0&0\\EM$ and\n",
        "$U_2=\\span(\\BM-1\\\\-2\\\\2\\\\1\\EM,\\BM2\\\\-2\\\\0\\\\0\\EM)$.\n",
        "Then the intersection $U_1‚à©U_2$ is at\n",
        "$a\\BM1\\\\1\\\\-3\\\\1\\EM+b\\BM2\\\\-1\\\\0\\\\-1\\EM=c\\BM-1\\\\-2\\\\2\\\\1\\EM+d\\BM2\\\\-2\\\\0\\\\0\\EM$.\n",
        "Solving\n",
        "$\\BM1&2&1&-2&0\\\\1&-1&2&2&0\\\\-3&0&-2&0&0\\\\1&-1&-1&0&0\\EM$\n",
        "$‚Üí\\BM1&0&0&-4/9&0\\\\0&1&0&-10/9&0\\\\0&0&1&2/3&0\\\\0&0&0&0&0\\EM$\n",
        "we have\n",
        "$\\BC\n",
        "a-4/9d=0\\\\\n",
        "b-10/9d=0\\\\\n",
        "c+2/3d=0\n",
        "\\EC$.\n",
        "Let $d=9$ then $a=4$, $b=10$, and $c=-6$.\n",
        "Then $U_1‚à©U_2=\\span(a\\BM1\\\\1\\\\-3\\\\1\\EM+b\\BM2\\\\-1\\\\0\\\\-1\\EM)$\n",
        "$=\\span(\\BM24\\\\-6\\\\-12\\\\-6\\EM)$.\n",
        "\n",
        "- 2.13: Let $\\v{A}_1=\\BM1&0&1\\\\1&-2&-1\\\\2&1&3\\\\1&0&1\\EM$\n",
        "$‚Üí\\BM1&0&1\\\\0&1&1\\\\0&0&0\\\\0&0&0\\EM$ and $\\v{A}_2=\\BM3&-3&0\\\\1&2&3\\\\7&-5&2\\\\3&-1&2\\EM$\n",
        "$‚Üí\\BM1&0&1\\\\0&1&1\\\\0&0&0\\\\0&0&0\\EM$. Let $U_1,U_2$ be the solution space of $\\v{A}_1\\v{x}=\\v{0}$ and $\\v{A}_2\\v{x}=\\v{0}$.\n",
        "\n",
        "  - Determine dimension of $U_1$ and $U_2$.\n",
        "  $\\max\\rk(\\v{A}_1)=\\max\\rk(\\v{A}_2)=\\min(m,n)=3$.\n",
        "  $\\rk(\\v{A}_1)=\\rk(\\v{A}_2)=2$. Therefore $\\dim(U_1)=\\dim(U_2)=1$.\n",
        "\n",
        "  - Determine the bases of $U_1$ and $U_2$.\n",
        "  $U_1=U_2=\\span(\\BM1\\\\1\\\\-1\\EM)$.\n",
        "\n",
        "- 2.14: Let $\\v{A}_1=\\BM1&0&1\\\\1&-2&-1\\\\2&1&3\\\\1&0&1\\EM$ $‚Üí\\BM1&0&1\\\\0&1&1\\\\0&0&0\\\\0&0&0\\EM$ and $\\v{A}_2=\\BM3&-3&0\\\\1&2&3\\\\7&-5&2\\\\3&-1&2\\EM$\n",
        "$‚Üí\\BM1&0&1\\\\0&1&1\\\\0&0&0\\\\0&0&0\\EM$. Let $U_1,U_2$ be spanned by columns of $\\v{A}_1,\\v{A}_2$.\n",
        "\n",
        "  - Determine the dimension of $U_1,U_2$.\n",
        "  $\\dim(U_1)=\\dim(U_2)=\\rk(\\v{A}_1)=\\rk(\\v{A}_2)=2$\n",
        "\n",
        "  - Determine the bases of $U_1,U_2$.\n",
        "  $U_1=\\span(\\BM1\\\\1\\\\2\\\\1\\EM,\\BM0\\\\-2\\\\1\\\\0\\EM)$ and\n",
        "  $U_2=\\span(\\BM3\\\\1\\\\7\\\\3\\EM,\\BM-3\\\\2\\\\-5\\\\-1\\EM)$.\n",
        "\n",
        "  - Determine a basis of $U_1‚à©U_2$.\n",
        "  $a\\BM1\\\\1\\\\2\\\\1\\EM+b\\BM0\\\\-2\\\\1\\\\0\\EM=c\\BM3\\\\1\\\\7\\\\3\\EM+d\\BM-3\\\\2\\\\-5\\\\-1\\EM$.\n",
        "  Then $\\BM1&0&3&-3\\\\1&-2&1&2\\\\2&1&7&-5\\\\1&0&3&-1\\EM$\n",
        "  $‚Üí\\BM1&0&3&0\\\\0&1&1&0\\\\0&0&0&1\\\\0&0&0&0\\EM$.\n",
        "  Therefore\n",
        "  $d=0$. Let $c=1$ then $a=3$, and $b=1$,\n",
        "  then $U_1‚à©U_2=\\span(\\BM3\\\\1\\\\7\\\\3\\EM)$.\n",
        "\n",
        "- 2.17: $Œ¶:‚Ñù^3‚Üí‚Ñù^4$. $Œ¶(\\BM x_1\\\\x_2\\\\x_3\\EM)=\\BM\n",
        "3x_1+2x_2+x_3\\\\x_1+x_2+x_3\\\\x_1-3x_2\\\\2x_1+3x_2+x_3\\EM$.\n",
        "\n",
        "  - Find $\\v{A}_Œ¶$.\n",
        "  $\\v{A}_{std}=\\BM3&2&1\\\\1&1&1\\\\1&-3&0\\\\2&3&1\\EM$.\n",
        "  Let $\\v{B}=\\v{I}_3,\\v{C}=\\v{I}_4$ be the basis of $‚Ñù^3,‚Ñù^4$, then $\\v{A}_Œ¶=\\v{A}_{std}$.\n",
        "\n",
        "  - Determine $\\rk(\\v{A}_Œ¶)$.\n",
        "  $\\v{A}_Œ¶‚Üí\\BM1&0&0\\\\0&1&0\\\\0&0&1\\\\0&0&0\\EM$. Therefore $\\rk(\\v{A}_Œ¶)=3$.\n",
        "\n",
        "  - Determine Image and kernel of $Œ¶$.\n",
        "  $\\Im(Œ¶)=\\span(\\BM3\\\\1\\\\1\\\\2\\EM,\\BM2\\\\1\\\\-3\\\\3\\EM,\\BM1\\\\1\\\\0\\\\1\\EM)$.\n",
        "  Then $\\null(Œ¶)=\\{\\v{0}\\}$."
      ],
      "metadata": {
        "id": "qJlgv1OFQxc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Inner Products, Norms, Orthogonal, and Projections"
      ],
      "metadata": {
        "id": "gsx_1C1PpsRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inner product**: Let $V$ be a vector space. Then inner product $‚ü®‚ãÖ,‚ãÖ‚ü©:V√óV‚Üí‚Ñù$ is a positive-definite, symmetric, bilinear mapping on $V$. Furthermore $(V,‚ü®‚ãÖ,‚ãÖ‚ü©)$ is called an inner product space.\n",
        "\n",
        "- **Bilinear mapping**: Recall $Œ¶:V‚ÜíW$ is linear if $‚àÄ\\v{x},\\v{y}‚ààV\\ ‚àÄŒª,œà‚àà‚Ñù:Œ¶(Œª\\v{x}+œà\\v{y})=ŒªŒ¶(\\v{x})+œàŒ¶(\\v{y})$. $Œ©:V√óV‚Üí‚Ñù$ is **bilinear** if $‚àÄ\\v{x},\\v{y},\\v{z}‚ààV\\ ‚àÄŒª,œà‚àà‚Ñù:\\BC\n",
        "Œ©(Œª\\v{x}+œà\\v{y},\\v{z})=ŒªŒ©(\\v{x},\\v{z})+œàŒ©(\\v{y},\\v{z})\\\\\n",
        "Œ©(\\v{x},Œª\\v{y}+œà\\v{z})=ŒªŒ©(\\v{x},\\v{y})+œàŒ©(\\v{x},\\v{z})\n",
        "\\EC$ (covariance rules), and is\n",
        "$\\BC\n",
        "\\t{symmetric}& \\t{if }Œ©(\\v{x},\\v{y})=Œ©(\\v{y},\\v{x})‚àÄ\\v{x},\\v{y}‚ààV\\\\\n",
        "\\t{positive-definite}& \\t{if }‚àÄ\\v{x}‚ààV\\backslash\\{\\v{0}\\}:Œ©(\\v{x},\\v{x})>0\\t{, and }Œ©(\\v{0},\\v{0})=0\n",
        "\\EC$\n",
        "\n",
        "- Matrix $\\v{A}‚àà‚Ñù^{n√ón}$ is **positive-definite** if $\\green{‚àÄ\\v{x}‚ààV\\backslash\\{\\v{0}\\}:\\v{x}^‚ä§\\v{Ax}>0}$ (quadratic form). Matrix $\\v{A}$ is positive-**semi**definite if $‚àÄ\\v{x}‚ààV\\backslash\\{\\v{0}\\}:\\v{x}^‚ä§\\v{Ax}‚â•0$.\n",
        "\n",
        "  - If positive definite then $\\null(\\v{A})=\\{\\v{0}\\}$ because $\\v{x}^‚ä§\\v{Ax}>0‚áî\\v{Ax}>0‚áî\\v{A}$ invertible.\n",
        "\n",
        "  - If positive definite then diagonals are positive $a_{ii}=\\v{e}_i^‚ä§\\v{A}\\v{e}_i>0$, where $\\v{e}_i$ is the $i$-th column of $\\v{I}_n$\n",
        "\n",
        "  - Positive-definite matrix can be non-symmetric, but in application and by convention positive-definite is always symmetric and inherit all properties of symmetry.\n",
        "\n",
        "- For vector space $V$ with basis $B$, it holds that $‚ü®‚ãÖ,‚ãÖ‚ü©:V√óV‚Üí‚Ñù$ is an inner product iff there exists positive-definite matrix $\\v{A}‚àà‚Ñù^{n√ón}$ with $\\red{‚ü®\\v{x},\\v{y}‚ü©=\\v{x}_B^‚ä§\\v{A}\\v{y}_B}$, where $\\v{x}_B,\\v{y}_B$ are coordinates of $\\v{x},\\v{y}‚ààV$ with respect to $B$.\n",
        "\n",
        "  - $\\green{‚ü®\\v{x},\\v{y}‚ü©=‚ü®\\sum_{i=1}^nx_{Bi}\\v{b}_i,\\sum_{j=1}^ny_{Bj}\\v{b}_j‚ü©=\\sum_{i=1}^n\\sum_{j=1}^nx_{Bi}‚ü®\\v{b}_i,\\v{b}_j‚ü©y_{Bj}=\\v{x}_B^‚ä§\\v{A}\\v{y}_B}$\n",
        "\n",
        "  - Proof: $\\blue{A_{ij}=‚ü®\\v{b}_i,\\v{b}_j‚ü©}=‚ü®\\v{b}_j,\\v{b}_i‚ü©=A_{ji}$. Also $\\v{x}_B^‚ä§\\v{A}\\v{y}_B=\\v{y}_B^‚ä§\\v{A}\\v{x}_B$ iff $\\v{A}$ is symmetric.\n",
        "  Furthermore, inner product positive-definite $‚ü®\\v{x},\\v{x}‚ü©>0‚áî\\v{x}_B^‚ä§\\v{A}\\v{x}_B>0‚áî\\v{A}$ is positive-definite.\n",
        "\n",
        "- **Dot product** is inner product for the standard basis $\\v{I}_n$: $\\red{‚ü®\\v{x},\\v{y}‚ü©=\\v{x}^‚ä§\\v{y}}$. I.e., $A_{ij}=‚ü®\\v{b}_i,\\v{b}_j‚ü©=\\v{b}_i^‚ä§\\v{b}_j$.\n",
        "\n",
        "  - $\\green{‚ü®\\v{x},\\v{y}‚ü©=‚ü®\\v{Bx}_B,\\v{By}_B‚ü©=(\\v{Bx}_B)^‚ä§(\\v{By}_B)=\\v{x}_B^‚ä§\\v{B}^‚ä§\\v{B}\\v{y}_B=\\v{x}_B^‚ä§\\v{A}\\v{y}_B}‚áí\\red{\\v{A}=\\v{B}^‚ä§\\v{B}}$.\n",
        "\n",
        "  - Inner product can still be redefined $‚ü®\\v{x},\\v{y}‚ü©_\\v{A}=\\v{x}^‚ä§\\v{A}\\v{y}$ in standard basis instead of using dot product.\n",
        "\n",
        "- 3.4: $\\v{A}_1=\\BM9&6\\\\6&5\\EM$, $\\v{A}_2=\\BM9&6\\\\6&3\\EM$. Test positive-definite.\n",
        "\n",
        "  - $\\v{x}^‚ä§\\v{A}_1\\v{x}=\\BM 9x_1+6x_2&6x_1+5x_2\\EM\\v{x}$\n",
        "  $=9x_1^2+12x_1x_2+5x_2^2$\n",
        "  $=(3x_1+2x_2)^2+x_2^2>0$\n",
        "  by completing square therefore positive-definite.\n",
        "  By principal minors, $d_1=9>0$, $d_2=\\det(\\v{A}_1)=9>0$.\n",
        "\n",
        "  - $\\v{x}^‚ä§\\v{A}_2\\v{x}=9x_1^2+12x_1x_2+3x_2^2$\n",
        "  $=(3x_1+2x_2)^2-x_2^2$ by completing square, therefore not positive-definite.\n",
        "  B principal minors, $d_1=9>0$, $d_2=\\det(\\v{A}_2)=-9< 0$, and therefore not positive-definite."
      ],
      "metadata": {
        "id": "KhjWgRLEqNL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Norm** on a vector space $V$ is function $\\n{‚ãÖ}:\\v{x}‚ààV‚Üí\\vn{x}‚àà‚Ñù$ equal to its length, with properties\n",
        "$\\BC\n",
        "\\t{absolutely homogeneous} &\\n{Œª\\v{x}}=|Œª|\\vn{x}\\\\\n",
        "\\t{triangle inequality} &\\n{\\v{x}+\\v{y}}‚â§\\vn{x}+\\vn{y}\\\\\n",
        "\\t{positive-definite} &\\vn{x}‚â•0\\t{, and }\\vn{x}=0‚áî\\v{x}=0\\\\\n",
        "\\t{Cauchy-Schwarz ($‚Ñì_2$)} &|‚ü®\\v{x},\\v{y}‚ü©|‚â§\\vn{x}\\vn{y}\n",
        "\\EC$\n",
        "\n",
        "- $‚Ñì_1$ **norm** (Manhattan): $\\vn{x}_1=\\sum_{i=1}^n|x_i|$. Not induced by an inner product and has no inner product space.\n",
        "\n",
        "  - $\\vn{x}_1=c$ is a diamond-shape.\n",
        "\n",
        "- $‚Ñì_2$ **norm** (Euclidean): $\\vn{x}_2=\\sqrt{\\sum_{i=1}^nx_i^2}=\\sqrt{\\v{x}^‚ä§\\v{x}}=\\sqrt{‚ü®\\v{x},\\v{x}‚ü©}$. Has inner product space $(V,‚ü®‚ãÖ,‚ãÖ‚ü©)$. We use this.\n",
        "\n",
        "  - $\\vn{x}_2=c$ is a sphere.\n",
        "\n",
        "- **Distance**: On inner product space $(V,‚ü®‚ãÖ,‚ãÖ‚ü©)$, the **metric** $d:V√óV‚Üí‚Ñù,\\blue{d(\\v{x},\\v{y})=\\n{\\v{x}-\\v{y}}}=\\sqrt{‚ü®\\v{x}-\\v{y},\\v{x}-\\v{y}‚ü©}$ is the distance between $\\v{x},\\v{y}‚ààV$, and satisfies $\\BC\n",
        "1.&\\t{positive-definite}&d(\\v{x},\\v{y})‚â•0\\ ‚àÄ\\v{x},\\v{y}‚ààV\\t{ and }d(\\v{x},\\v{y})=0‚áî\\v{x}=\\v{y}\\\\\n",
        "2.&\\t{symmetry}&d(\\v{x},\\v{y})=d(\\v{y},\\v{x})\\ ‚àÄ\\v{x},\\v{y}‚ààV\\\\\n",
        "3.&\\t{triangle inequality}&d(\\v{x},\\v{z})‚â§d(\\v{x},\\v{y})+d(\\v{y},\\v{z})\\ ‚àÄ\\v{x},\\v{y},\\v{c}‚ààV\n",
        "\\EC$\n",
        "\n",
        "  - $d(\\v{x},\\v{y})=\\n{\\v{x}-\\v{y}}=\\sqrt{\\sum_{i=1}^n(x_i-y_i)^2}$ represents the root squares error.\n",
        "\n",
        "- **Angle** between $\\v{x},\\v{y}‚ààV$ is $\\red{\\cosœâ=\\/{‚ü®\\v{x},\\v{y}‚ü©}{\\vn{x}\\vn{y}}=\\/{‚ü®\\v{x},\\v{y}‚ü©}{\\sqrt{‚ü®\\v{x},\\v{x}‚ü©‚ü®\\v{y},\\v{y}‚ü©}}}$, which by Cauchy-Schwarz satisfies $\\blue{-1‚â§\\/{‚ü®\\v{x},\\v{y}‚ü©}{\\vn{x}\\vn{y}}‚â§1}$.\n"
      ],
      "metadata": {
        "id": "WS2OseUG6Sfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Orthogonal**: $\\v{x}\\perp\\v{y}‚áî‚ü®\\v{x},\\v{y}‚ü©=0$. Additionally if $\\vn{x}=\\vn{y}=1$ then $\\v{x},\\v{y}$ are called **orthonormal**.\n",
        "\n",
        "- $\\v{0}$ is orthogonal to every vector.\n",
        "\n",
        "- **Orthogonal/orthonormal matrix** $\\v{A}‚àà‚Ñù^{n√ón}$ satisfies $\\red{\\v{A}^‚ä§\\v{A}=\\v{I}}$ or $\\v{A}^{-1}=\\v{A}^‚ä§$. This implies all columns are orthogonal to each other, and have unit length. **Linear transformation using orthonormal matrix does not change vector norm or relative angle between vectors**. They're rotations ($\\det(\\v{A})=1$) and reflections ($\\det(\\v{A})=-1$).\n",
        "\n",
        "  - Proof: $\\vn{Ax}=\\sqrt{(\\v{Ax})^‚ä§(\\v{Ax})}=\\vn{x}$.\n",
        "  Angle\n",
        "  $\\cosœâ=\\/{‚ü®\\v{Ax},\\v{Ay}‚ü©}{\\vn{Ax}\\vn{Ay}}$\n",
        "  $=\\/{(\\v{Ax})^‚ä§(\\v{Ay})}{\\vn{Ax}\\vn{Ay}}$\n",
        "  $=\\/{‚ü®\\v{x},\\v{y}‚ü©}{\\vn{x}\\vn{y}}$.\n",
        "\n",
        "- **Orthonormal basis**: Consider $n$-dimensional vector space $V$ with basis $B$. If the basis satisfies $\\BC ‚ü®\\v{b}_i,\\v{b}_j‚ü©=0&i\\neq j\\\\‚ü®\\v{b}_i,\\v{b}_i‚ü©=1\\EC$ then it is called orthonormal basis (ONB). ONB matrix is an orthogonal matrix.\n",
        "\n",
        "  - All ONB matrices are rotation or reflection transformations.\n",
        "\n",
        "  - **Gram-Schmidt process**: Given basis $\\tilde{\\v{B}}$. Start with $\\v{u}_1=\\tilde{\\v{b}}_1$, form $\\v{u}_2=\\tilde{\\v{b}}_2-œÄ_{\\v{u_1}}(\\tilde{\\v{b}}_2)$, then $\\v{u}_3=\\tilde{\\v{b}}_3-œÄ_{\\v{u_1}}(\\tilde{\\v{b}}_3)-œÄ_{\\v{u_2}}(\\tilde{\\v{b}}_3)$, etc.\n",
        "  Finally normalize. Streamlined algorithm: construct augmented matrix and find row echelon form $\\t{ref}([\\tilde{\\v{B}}^‚ä§\\tilde{\\v{B}}|\\tilde{\\v{B}}^‚ä§])$.\n",
        "  At the end point LHS is upper-triangular, and RHS rows are the orthogonal basis. Transpose and normalize.\n",
        "\n",
        "- **Orthogonal complement**: Consider vector space $V$ and $U‚äÜV$ where $\\dim(V)=D$ and $\\dim(U)=M$. Then orthogonal complement $U^\\perp‚äÜV$ where $U‚à©U^\\perp=\\{\\v{0}\\}$ and $\\dim(U^\\perp)=D-M$ contains all vectors in $V$ that are orthogonal to every vector in $U$. Any vector $\\v{x}‚ààV$ can be decomposed as $\\v{x}=\\v{B}\\v{x}_B+\\v{B}^\\perp\\v{x}_{B^\\perp}$ for basis $\\v{B}$ of $U$ and $\\v{B}^\\perp$ of $U^\\perp$ where $\\v{B}^\\perp=\\null(\\v{B}^‚ä§)$\n"
      ],
      "metadata": {
        "id": "cb1LwgH3eNtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Orthogonal Projection onto Line**: Let $U‚äÜV$ be a 1-dimensional subspace with basis $\\v{b}$. A linear mapping $œÄ:V‚ÜíU$ is a projection if $\\red{œÄ^2=œÄ‚àòœÄ=œÄ}$. Let $\\v{x}‚ààV$, then $\\BC\n",
        "\\t{projection component}&œÄ_U(\\v{x})‚ààU\\t{ or }œÄ_U(\\v{x})=Œª\\v{b}, Œª‚àà‚Ñù\\\\\n",
        "\\t{orthogonal complement}&(\\v{x}-\\v{a})‚ààU^\\perp\\t{ or }‚ü®\\v{x}-\\v{a},\\v{b}‚ü©=0\\\\\n",
        "\\t{least squares error}&œÄ_U(\\v{x})=\\arg\\min_\\v{a}\\n{\\v{x}-\\v{a}}\n",
        "\\EC$\n",
        "\n",
        "- The orthogonal projection of $\\v{x}$ onto $\\v{b}$ is $\\red{\\/{\\v{b}^‚ä§\\v{x}}{\\vn{b}^2}\\v{b}=\\/{\\v{bb}^‚ä§}{\\v{b}^‚ä§\\v{b}}\\v{x}}$.\n",
        "\n",
        "- $\\blue{œÄ_U(\\v{x})=\\/{\\v{b}\\v{b}^‚ä§}{\\vn{b}^2}\\v{x}}=\\v{P}_œÄ\\v{x}$ where $\\red{\\v{P}_œÄ=\\/{\\v{b}\\v{b}^‚ä§}{\\vn{b}^2}}‚àà‚Ñù^{n√ón}$ is the **projection matrix**.\n",
        "\n",
        "  - Proof: $‚ü®\\v{b},\\v{x}-œÄ_U(\\v{x})‚ü©=‚ü®\\v{b},\\v{x}-Œª\\v{b}‚ü©$\n",
        "  $=‚ü®\\v{b},\\v{x}‚ü©-Œª‚ü®\\v{b},\\v{b}‚ü©=0$\n",
        "  $‚áí\\green{Œª=\\/{‚ü®\\v{b},\\v{x}‚ü©}{\\vn{b}^2}}$\n",
        "  $‚áíœÄ_U(\\v{x})=Œª\\v{b}=\\v{b}Œª=\\/{\\v{b}\\v{b}^‚ä§\\v{x}}{\\vn{b}^2}$\n",
        "\n",
        "  - $\\green{\\n{œÄ_U(\\v{x})}}=\\vn{b}\\/{|\\v{b}^‚ä§\\v{x}|}{\\vn{b}^2}\\/{\\vn{x}}{\\vn{x}}$\n",
        "  $=\\/{\\vn{b}}{\\vn{b}}\\/{|\\v{b}^‚ä§\\v{x}|}{\\vn{b}\\vn{x}}\\vn{x}$\n",
        "  $=\\green{|\\cosœâ|\\vn{x}}$.\n",
        "\n",
        "**Orthogonal Projection onto Subspace**: Let $U‚äÜV$ with $\\dim(U)=m‚â§n$ with basis $\\v{B}‚àà‚Ñù^{n√óm}$. Then $œÄ_U(\\v{x})=\\v{BŒª}$, where $\\v{Œª}‚àà‚Ñù^m$ are the coordinates of the projection.\n",
        "\n",
        "- $\\blue{œÄ_U(\\v{x})=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§\\v{x}}=\\v{P}_œÄ\\v{x}$ where $\\red{\\v{P}_œÄ=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§}‚àà‚Ñù^{n√ón}$ is the projection matrix (hat matrix).\n",
        "\n",
        "  - Proof: $‚ü®\\v{b}_i,\\v{x}-œÄ_U(\\v{x})‚ü©=\\v{b}_i^‚ä§(\\v{x}-\\v{BŒª})=0$\n",
        "  $‚áí\\green{\\v{B}^‚ä§(\\v{x}-\\v{BŒª})=\\v{0}}$\n",
        "  $‚áí\\v{B}^‚ä§\\v{x}-\\v{B}^‚ä§\\v{BŒª}=\\v{0}$\n",
        "  $‚áí\\green{\\v{Œª}=(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§\\v{x}}$.\n",
        "  Then\n",
        "  $œÄ_U(\\v{x})=\\v{BŒª}=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§\\v{x}$\n",
        "\n",
        "  - If $\\v{B}$ is orthonormal basis, then $\\v{P}_œÄ=\\v{BB}^‚ä§$.\n",
        "\n",
        "  - If using $‚ü®\\v{x},\\v{y}‚ü©_\\v{A}=\\v{x}^‚ä§\\v{Ay}$, then\n",
        "  $‚ü®\\v{b}_i,\\v{x}-œÄ_U(\\v{x})‚ü©_\\v{A}=\\v{b}_i^‚ä§\\v{A}(\\v{x}-\\v{BŒª})$\n",
        "  $‚áí\\v{B}^‚ä§\\v{A}(\\v{x}-\\v{BŒª})=\\v{0}$\n",
        "  $‚áí\\v{Œª}=(\\v{B}^‚ä§\\v{AB})^{-1}\\v{B}^‚ä§\\v{A}$\n",
        "  $‚áí\\v{P}_œÄ=\\v{B}(\\v{B}^‚ä§\\v{AB})^{-1}\\v{B}^‚ä§\\v{A}$\n",
        "\n",
        "- **Normal equation** $\\green{\\v{Œª}=(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§\\v{x}}=\\arg\\min_\\v{œà}\\n{\\v{x}-\\v{Bœà}}^2$ is the least squares estimator to the $\\v{BŒª}=\\v{x}$ regression. $\\v{B}^‚ä§\\v{B}‚àà‚Ñù^{m√óm}$ is invertible and positive-definite. $(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§$ is the **pseudoinverse** of $\\v{B}$.\n",
        "\n",
        "  - Proof: We know $\\null(\\v{B})=\\{\\v{0}\\}$. By equivalence,\n",
        "  $\\v{B}^‚ä§\\v{Bx}=\\v{0}$\n",
        "  $‚áî\\v{x}^‚ä§(\\v{B}^‚ä§\\v{Bx})=(\\v{Bx})^‚ä§(\\v{Bx})=\\n{\\v{Bx}}^2=0$\n",
        "  $‚áî\\v{Bx}=\\v{0}$.\n",
        "  Therefore $\\null(\\v{B}^‚ä§\\v{B})=\\null(\\v{B})=\\{\\v{0}\\}$ and $\\v{B}^‚ä§\\v{B}$ is invertible.\n",
        "  $\\BC\n",
        "  \\v{x}^‚ä§(\\v{B}^‚ä§\\v{B})\\v{x}=\\n{\\v{Bx}}^2>0‚áî\\v{Bx}\\neq\\v{0}‚áî\\v{x}\\neq\\v{0}\\\\\n",
        "  \\v{x}^‚ä§(\\v{B}^‚ä§\\v{B})\\v{x}=\\n{\\v{Bx}}^2=0‚áî\\v{Bx}=\\v{0}‚áî\\v{x}=\\v{0}\n",
        "  \\EC$\n",
        "  therefore $\\v{B}^‚ä§\\v{B}$ is positive-definite.\n",
        "\n",
        "  - System of equations: $\\v{Ax}=\\v{b}$ where $\\v{b}$ does not lie in the subspace $\\span(\\v{A})$.\n",
        "\n",
        "- **Projection onto affine subspace** $L=\\v{x}_0+U$ is $œÄ_L(\\v{x})=\\v{x}_0+œÄ_U(\\v{x}-\\v{x}_0)$.\n",
        "\n",
        "- **Idempotent**: $œÄ_U‚àòœÄ_U(\\v{x})=œÄ_U(œÄ_U(\\v{x}))=œÄ_U(\\v{x})$. Projection matrix $\\v{P}_œÄ\\v{P}_œÄ=\\v{P}_œÄ$ is symmetric.\n",
        "\n",
        "  - Proof (idempotent): $\\v{P}_œÄ\\v{P}_œÄ=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}(\\v{B}^‚ä§\\v{B})(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§$\n",
        "  $=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§=\\v{P}_œÄ$.\n",
        "\n",
        "  - Proof (symmetric): $\\v{P}_œÄ^‚ä§=(\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§)^‚ä§$\n",
        "  $=(\\v{B}^‚ä§)^‚ä§((\\v{B}^‚ä§\\v{B})^{-1})^‚ä§(\\v{B})^‚ä§$\n",
        "  $=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§=\\v{P}_œÄ$ because inverse of a symmetric matrix is also symmetric.\n",
        "\n",
        "- **Orthogonal complement**: Let $œÄ:V‚ÜíV$ be a projection and $\\t{id}_V:V‚ÜíV,\\t{id}_V(\\v{x})=\\v{x}$ be identity. Then $\\t{id}_V-œÄ$ is a projection, $\\Im(\\t{id}_V-œÄ)=\\null(œÄ)$, and $\\null(\\t{id}_V-œÄ)=\\Im(œÄ)$.\n",
        "Similarly $\\v{P}_œÄ$ and $\\v{I}-\\v{P}_œÄ$ are projection matrices that project to orthogonal subspaces. Together they decompose $\\v{x}$ into orthogonal components $\\v{x}=\\v{P}_œÄ\\v{x}+(\\v{I}-\\v{P}_œÄ)\\v{x}$. Furthermore $\\blue{\\v{P}_œÄ(\\v{I}-\\v{P}_œÄ)=\\v{0}}$.\n",
        "\n",
        "  - Proof: $(\\t{id}_V-œÄ)‚àò(\\t{id}_V-œÄ)=\\t{id}_V‚àò\\t{id}_V-2œÄ‚àò\\t{id}_V+œÄ‚àòœÄ$\n",
        "  $=\\t{id}_V-2œÄ+œÄ$\n",
        "  $=(\\t{id}_V-œÄ)$ idempotent.\n",
        "  $(\\t{id}_V-œÄ)(\\v{x})=\\green{\\v{x}-œÄ(\\v{x})}$\n",
        "  $‚áí\\BC\n",
        "  œÄ(\\green{\\v{x}-œÄ(\\v{x})})=œÄ(\\v{x})-œÄ(\\v{x})=\\v{0}&\\Im(\\t{id}_V-œÄ)=\\null(œÄ)\\\\\n",
        "  \\green{\\v{x}-œÄ(\\v{x})}=\\v{0}‚áí\\v{x}=œÄ(\\v{x})&\\null(\\t{id}_V-œÄ)=\\Im(œÄ)\n",
        "  \\EC$\n",
        "\n",
        "  - Proof: $(\\v{I}-\\v{P}_œÄ)(\\v{I}-\\v{P}_œÄ)=\\v{I}-2\\v{P}_œÄ+\\v{P}_œÄ\\v{P}_œÄ=\\v{I}-\\v{P}_œÄ$ idempotent.\n",
        "  $\\v{P}_œÄ(\\v{I}-\\v{P}_œÄ)=\\v{P}_œÄ-\\v{P}_œÄ\\v{P}_œÄ$\n",
        "  $=\\v{P}_œÄ-\\v{P}_œÄ=\\v{0}$. Furthermore\n",
        "  $‚ü®\\v{P}_œÄ\\v{x},(\\v{I}-\\v{P}_œÄ)\\v{x}‚ü©$\n",
        "  $=(\\v{P}_œÄ\\v{x})^‚ä§((\\v{I}-\\v{P}_œÄ)\\v{x})$\n",
        "  $=\\v{x}^‚ä§\\v{P}_œÄ^‚ä§(\\v{I}-\\v{P}_œÄ)\\v{x}$\n",
        "  $=\\v{x}^‚ä§\\v{P}_œÄ(\\v{I}-\\v{P}_œÄ)\\v{x}$\n",
        "  $=0$.\n",
        "\n",
        "- $œÄ_U(\\v{x})$ is an eigenvector of $\\v{P}_œÄ$.\n",
        "\n",
        "- 3.10: $\\v{b}=\\BM1\\\\2\\\\2\\EM$. Then $P_œÄ=\\/{\\v{b}\\v{b}^‚ä§}{\\vn{b}^2}$\n",
        "$=\\/{1}{9}\\BM1&2&2\\\\2&4&4\\\\2&4&4\\EM$.\n",
        "$P_œÄP_œÄ=\\/{1}{9^2}\\BM9&18&18\\\\18&36&36\\\\18&36&36\\EM$\n",
        "$=\\/{1}{9}\\BM1&2&2\\\\2&4&4\\\\2&4&4\\EM$.\n",
        "\n",
        "- 3.11: $U=\\span(\\BM1\\\\1\\\\1\\EM,\\BM0\\\\1\\\\2\\EM)$ and $\\v{x}=\\BM6\\\\0\\\\0\\EM$.\n",
        "Then $\\v{P}_œÄ=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§$\n",
        "$=\\/{1}{6}\\BM5&2&-1\\\\2&2&2\\\\-1&2&5\\EM$.\n",
        "Then $\\v{P}_œÄ\\v{x}=\\BM5\\\\2\\\\-1\\EM$"
      ],
      "metadata": {
        "id": "Sft1aiKn0t4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3.3: compute the distance between $\\v{x}=\\BM1\\\\2\\\\3\\EM$ and $\\v{y}=\\BM-1\\\\-1\\\\0\\EM$.\n",
        "$\\v{x}-\\v{y}=\\BM-2\\\\-3\\\\3\\EM$\n",
        "and $d=\\sqrt{‚ü®\\v{x}-\\v{y},\\v{x}-\\v{y}‚ü©}$.\n",
        "\n",
        "  - $‚ü®\\v{x},\\v{y}‚ü©=\\v{x}^‚ä§\\v{y}$. Then\n",
        "  $d(\\v{x},\\v{y})=\\sqrt{22}$.\n",
        "\n",
        "  - $‚ü®\\v{x},\\v{y}‚ü©=\\v{x}^‚ä§\\BM2&1&0\\\\1&3&-1\\\\0&-1&2\\EM\\v{y}$. Then $d(\\v{x},\\v{y})=\\sqrt{83}$.\n",
        "\n",
        "- 3.4: compute the angle between $\\v{x}=\\BM1\\\\2\\EM$ and $\\v{y}=\\BM-1\\\\-1\\EM$.\n",
        "The angle $\\cosœâ=\\/{‚ü®\\v{x},\\v{y}‚ü©}{\\sqrt{‚ü®\\v{x},\\v{x}‚ü©‚ü®\\v{y},\\v{y}‚ü©}}$.\n",
        "\n",
        "  - $‚ü®\\v{x},\\v{y}‚ü©=\\v{x}^‚ä§\\v{y}$. Then $\\cosœâ=\\/{-3}{\\sqrt{10}}$\n",
        "\n",
        "- 3.5: $U=\\span(\\BM0\\\\-1\\\\2\\\\0\\\\2\\EM,\\BM1\\\\-3\\\\1\\\\-1\\\\2\\EM,\\BM-3\\\\4\\\\1\\\\2\\\\1\\EM,\\BM-1\\\\-3\\\\5\\\\0\\\\7\\EM)$. $\\v{x}=\\BM-1\\\\-9\\\\-1\\\\4\\\\1\\EM$.\n",
        "\n",
        "  - Determine $œÄ_U(\\v{x})$.\n",
        "  $\\BM0&1&-3&-1\\\\-1&-3&4&-3\\\\2&1&1&5\\\\0&-1&2&0\\\\2&2&1&7\\EM‚Üí\\BM1&0&0&1\\\\0&1&0&2\\\\0&0&1&1\\\\0&0&0&0\\\\0&0&0&0\\EM$.\n",
        "  $\\v{B}=\\BM0&1&-3\\\\-1&-3&4\\\\2&1&1\\\\0&-1&2\\\\2&2&1\\EM$.\n",
        "  $œÄ_U(\\v{x})=\\v{B}(\\v{B}^‚ä§\\v{B})^{-1}\\v{B}^‚ä§\\v{x}=\\BM1\\\\-5\\\\-1\\\\-2\\\\3\\EM$.\n",
        "\n",
        "  - Determine the distance $d(\\v{x},U)$.\n",
        "  $\\v{x}-œÄ_U(\\v{x})=\\BM2\\\\4\\\\0\\\\-6\\\\2\\EM$ and $d(\\v{x},U)=\\sqrt{‚ü®\\v{x}-œÄ_U(\\v{x}),\\v{x}-œÄ_U(\\v{x})‚ü©}$\n",
        "\n",
        "- 3.6: Consider $‚Ñù^3$ with $‚ü®\\v{x},\\v{y}‚ü©_\\v{A}:=\\v{x}^‚ä§\\BM2&1&0\\\\1&2&-1\\\\0&-1&2\\EM\\v{y}$. Define $(\\v{e}_1,\\v{e}_2,\\v{e}_3)$ as standard basis.\n",
        "\n",
        "  - Let $U=\\span(\\v{e}_1,\\v{e}_3)$. Find $œÄ_U(\\v{e}_2)$.\n",
        "  $\\v{B}=\\BM1&0\\\\0&0\\\\0&1\\EM$.\n",
        "  $œÄ_U(\\v{e}_2)=\\v{B}(\\v{B}^‚ä§\\v{AB})^{-1}\\v{B}^‚ä§\\v{Ax}$\n",
        "  $=\\BM1/2\\\\0\\\\-1/2\\EM$."
      ],
      "metadata": {
        "id": "oENJE81ZVW1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schur Complement**: Given symmetric block matrix $\\v{M}=\\BM \\v{A}&\\v{B}\\\\\\v{B}^‚ä§&\\v{C}\\EM$. If $\\v{C}$ is positive-definite (invertible), then Schur Complement of $\\v{C}$ is $\\v{A}-\\v{BC}^{-1}\\v{B}^‚ä§$. If $\\v{A}$ is invertible, then Schur Complement of $\\v{A}$ is $\\v{C}-\\v{B}^‚ä§\\v{A}^{-1}\\v{B}$.\n",
        "\n",
        "- **Schur Complement Lemma**: $\\v{M}$ is positive-definite iff $\\v{C}$ is positive-definite and $\\v{A}-\\v{BC}^{-1}\\v{B}^‚ä§$ is positive-definite.\n",
        "\n",
        "  - Or $\\v{M}$ is positive-definite iff $\\v{A}$ is positive-definite and $\\v{C}-\\v{B}^‚ä§\\v{A}^{-1}\\v{B}$  is positive-definite.\n",
        "\n",
        "  - $\\v{M}$ is positive-definite iff $\\v{A}$ and $\\v{C}$ are positive-definite. Intuitive check: $\\v{A}$ and $\\v{C}$ are both principal minors of $\\v{M}$.\n",
        "\n",
        "  - If $\\v{C}$ is strictly positive-definite, then $\\v{M}$ is positive-semidefinite iff $\\v{A}-\\v{BC}^{-1}\\v{B}^‚ä§$ is positive-semidefinite.\n",
        "\n",
        "- Used to check if large matrix $\\v{M}$ is positive-definite without calculating REF or eigenvalues.\n",
        "\n",
        "- Proof (block inverse): Define inverse\n",
        "$\\BM\\v{Œ£}_\\v{xx}&\\v{Œ£}_\\v{xy}\\\\\\v{Œ£}_\\v{yx}&\\v{Œ£}_\\v{yy}\\EM\\BM\\v{Œõ}_\\v{xx}&\\v{Œõ}_\\v{xy}\\\\\\v{Œõ}_\\v{yx}&\\v{Œõ}_\\v{yy}\\EM=\\BM\\v{I}&\\v{0}\\\\\\v{0}&\\v{I}\\EM$. Then, $\\BC\n",
        "\\t{bottom-left:}&\n",
        "\\v{Œ£}_\\v{yx}\\v{Œõ}_\\v{xx}+\\v{Œ£}_\\v{yy}\\v{Œõ}_\\v{yx}=\\v{0}&\n",
        "\\v{Œõ}_\\v{yx}=-\\v{Œ£}_\\v{yy}^{-1}\\v{Œ£}_\\v{yx}\\v{Œõ}_\\v{xx}&\n",
        "\\v{Œõ}_\\v{yx}=-\\v{Œõ}_\\v{yy}\\v{Œ£}_\\v{yx}\\v{Œ£}_\\v{xx}^{-1}\\\\\n",
        "\\t{top-right:}&\n",
        "\\v{Œ£}_\\v{xx}\\v{Œõ}_\\v{xy}+\\v{Œ£}_\\v{xy}\\v{Œõ}_\\v{yy}=\\v{0}&\n",
        "\\v{Œõ}_\\v{xy}=-\\v{Œ£}_\\v{xx}^{-1}\\v{Œ£}_\\v{xy}\\v{Œõ}_\\v{yy}&\n",
        "\\v{Œõ}_\\v{xy}=-\\v{Œõ}_\\v{xx}\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}\\\\\n",
        "\\t{top-left:}&\n",
        "\\v{Œ£}_\\v{xx}\\v{Œõ}_\\v{xx}+\\v{Œ£}_\\v{xy}\\v{Œõ}_\\v{yx}=\\v{I}\\\\\n",
        "&(\\v{Œ£}_\\v{xx}-\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}\\v{Œ£}_\\v{yx})\\v{Œõ}_\\v{xx}=\\v{I}&\n",
        "\\v{Œõ}_\\v{xx}=(\\v{Œ£}_\\v{xx}-\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}\\v{Œ£}_\\v{yx})^{-1}\\\\\n",
        "\\t{bottom-right:}&\n",
        "\\v{Œ£}_\\v{yx}\\v{Œõ}_\\v{xy}+\\v{Œ£}_\\v{yy}\\v{Œõ}_\\v{yy}=\\v{I}\\\\\n",
        "&(-\\v{Œ£}_\\v{yx}\\v{Œ£}_\\v{xx}^{-1}\\v{Œ£}_\\v{xy}+\\v{Œ£}_\\v{yy})\\v{Œõ}_\\v{yy}=\\v{I}&\n",
        "\\v{Œõ}_\\v{yy}=(\\v{Œ£}_\\v{yy}-\\v{Œ£}_\\v{yx}\\v{Œ£}_\\v{xx}^{-1}\\v{Œ£}_\\v{xy})^{-1}\n",
        "\\EC$\n",
        "\n",
        "- Proof (minimizing quadratic): B&V 3.15"
      ],
      "metadata": {
        "id": "BGvDwUGS0dA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Principal minors for positive-definite/positive-semidefinite**:\n",
        "\n",
        "- **Principal minor** of $\\v{A}‚àà‚Ñù^{n√ón}$: determinant of the submatrix constructed using rows and columns chosen from subset of $k$ indices from $\\{1,...,n\\}$.\n",
        "\n",
        "  - **Leading principal minor**: determinant of submatrix that includes the first $k$ rows/columns of $\\v{A}$ as a contiguous top-left block.\n",
        "\n",
        "- $\\v{A}$ is positive-definite iff all leading principal minors are positive.\n",
        "\n",
        "- $\\v{A}$ is positive-semidefinite iff all principal minors are positive."
      ],
      "metadata": {
        "id": "mTMRxnl0bm4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Determinants, Eigendecomposition, and SVD"
      ],
      "metadata": {
        "id": "duhBEROv2rcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Determinant**: If columns of $\\v{A}‚àà‚Ñù^{\\red{n√ón}}$ are edges of a parallelepiped extending from origin, then $|\\det(\\v{A})|$ is its volume.\n",
        "\n",
        "- **Laplace expansion**: recursive determinant calculation. For $\\v{A}‚àà‚Ñù^{n√ón}$, pick any $j=1..n$ and go along the $j$-th column $\\det(\\v{A})=\\suml_{k=1}^n(-1)^{k+j}a_{kj}\\det(\\v{A}_{kj})$ or go along the $j$-th row $\\det(\\v{A})=\\suml_{k=1}^n(-1)^{k+j}a_{jk}\\det(\\v{A}_{jk})$, where $\\v{A}_{kj}‚àà‚Ñù^{(n-1)√ó(n-1)}$ is the submatrix obtained by deleting row $k$ and column $j$.\n",
        "\n",
        "  - If $\\v{T}‚àà‚Ñù^{n√ón}$ is a triangular matrix, $\\red{\\det(\\v{T})=\\prod_iT_{ii}}$. Therefore find $\\t{ref}(\\v{A})$ while keeping tally of row swaps $R_i‚ÜîR_j$ ($√ó-1$) and scaling a row $R_i‚ÜêkR_i$ ($√ók$).\n",
        "\n",
        "- Let $\\v{A}‚àà‚Ñù^{n√ón}$ be invertible. Then $\\t{rref}(\\v{A})=\\v{E}_k...\\v{E}_1\\v{A}=\\v{I}_n‚áí\\green{\\v{A}=\\v{E}_1^{-1}...\\v{E}_k^{-1}\\v{I}_n}$ where $\\v{E}_i‚àà‚Ñù^{n√ón}$ is  **elementary matrix** representing a row operation $\\BC\n",
        "R_{i}‚ÜîR_{j}&\\v{E}_\\t{swap}=-1&\\det(\\v{E}_\\t{swap}\\v{B})=-\\det(\\v{B})\\\\\n",
        "R_{i}‚ÜêkR_{i}&\\v{E}_\\t{scale}=k&\\det(\\v{E}_\\t{scale}\\v{B})=k\\det(\\v{B})\\\\\n",
        "R_{i}‚ÜêR_{i}+kR_{j}&\\v{E}_\\t{add}=1&\\det(\\v{E}_\\t{add}\\v{B})=\\det(\\v{B})\n",
        "\\EC$\n",
        "\n",
        "  - Matrix $\\v{A}‚àà‚Ñù^{n√ón}$ is invertible $‚áî\\det(\\v{A})\\neq0‚áî\\rk(\\v{A})=n$. Proof: $\\det(\\v{I}_n)=1$ and $\\v{E}_i\\neq\\v{0}$.\n",
        "\n",
        "  - $\\det(\\v{AB})=\\det(\\v{A})\\det(\\v{B})$. Proof: both $\\v{A}$ and $\\v{B}$ are products of elementary matrices.\n",
        "\n",
        "  - $\\det(Œª\\v{A})=Œª^n\\det(\\v{A})$. Proof: application of $\\v{E}_\\t{scale}$ on $n$ rows.\n",
        "\n",
        "  - $\\det(\\v{A}^‚ä§)=\\det(\\v{A})$. Proof: from Laplace expansion\n",
        "\n",
        "  - $\\det(\\v{A}^{-1})=\\/{1}{\\det(\\v{A})}$. Proof: $\\det(\\v{I}_n)=1$.\n",
        "\n",
        "  - For $Œ¶:V‚ÜíV$, similar matrices $\\v{A}_{BB}=\\v{BA}_{std}\\v{B}^{-1}$ have equal determinant $\\det(\\v{A}_{std})=\\det(\\v{A}_{BB})$.\n",
        "\n",
        "**Trace** of a square matrix $\\v{A}‚àà‚Ñù^{n√ón}$ is $\\red{\\tr(\\v{A})=\\suml_{i=1}^na_{ii}}$ satisfying $\\BC\n",
        "\\tr(\\v{A}+\\v{B})=\\tr(\\v{A})+\\tr(\\v{B}),\\ \\v{A},\\v{B}‚àà‚Ñù^{n√ón}\\\\\n",
        "\\tr(Œ±\\v{A})=Œ±\\tr(\\v{A}),\\ Œ±‚àà‚Ñù,\\v{A}‚àà‚Ñù^{n√ón}\\\\\n",
        "\\tr(\\v{I}_n)=n\\\\\n",
        "\\tr(\\v{AB})=\\tr(\\v{BA}),\\ \\v{A}‚àà‚Ñù^{n√ók},\\v{B}‚àà‚Ñù^{k√ón}\n",
        "\\EC$\n",
        "\n",
        "- Cyclic invariance: $\\tr(\\v{AKL})=\\tr(\\v{KLA})$ for $\\v{A}‚àà‚Ñù^{a√ók}$, $\\v{K}‚àà‚Ñù^{k√ól}$, $\\v{L}‚àà‚Ñù^{l√óa}$.\n",
        "\n",
        "  - If $\\v{x},\\v{y}‚àà‚Ñù^n$ are two vectors, then $\\tr(\\v{x}\\v{y}^‚ä§)=\\tr(\\v{y}^‚ä§\\v{x})=\\v{y}^‚ä§\\v{x}‚àà‚Ñù$.\n",
        "\n",
        "- Trace is a property of the transformation $Œ¶:V‚ÜíV$  and is independent of the basis. Similar matrices $\\v{A}_{BB}=\\v{BA}_{std}\\v{B}^{-1}$ have equal trace.\n",
        "\n",
        "  - Proof: By cyclic property $\\tr(\\v{A}_{BB})=\\tr(\\v{BA}_{std}\\v{B}^{-1})=\\tr(\\v{B}^{-1}\\v{BA}_{std})=\\tr(\\v{A}_{std})$.\n",
        "\n",
        "- **Frobenius inner product** $\\red{‚ü®\\v{A},\\v{B}‚ü©=\\tr(\\v{A}^‚ä§\\v{B})=\\suml_{i=1}^m\\suml_{j=1}^nA_{ij}B_{ij}}‚àà‚Ñù$ where $\\v{A},\\v{B}‚àà‚Ñù^{m√ón}$ is matrix dot product.\n",
        "\n",
        "  - Proof: $(\\v{A}^‚ä§\\v{B})_{uv}=\\suml_{i=1}^mA_{iu}B_{iv}$\n",
        "  $‚áí\\tr(\\v{A}^‚ä§\\v{B})=\\suml_{j=1}^n(\\v{A}^‚ä§\\v{B})_{jj}$\n",
        "  $=\\suml_{j=1}^n\\left(\\suml_{i=1}^mA_{ij}B_{ij}\\right)$\n",
        "\n",
        "  - **Frobenius norm** $\\vn{A}_F=\\sqrt{\\tr(\\v{A}^‚ä§\\v{A})}$\n",
        "\n",
        "  - **Quadratic form**: Let $\\v{M}‚àà‚Ñù^{n√ón}$, then $\\tr(\\v{M}\\v{xx}^‚ä§)=\\tr(\\v{x}^‚ä§\\v{Mx})=\\v{x}^‚ä§\\v{Mx}‚àà‚Ñù$\n",
        "  by cyclic property.\n",
        "\n",
        "- $\\tr(\\v{xx}^‚ä§)=\\vn{x}_2^2$."
      ],
      "metadata": {
        "id": "SdT_KPCn4EtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eigenvalues and eigenvectors**: $\\red{\\v{Ax}=Œª\\v{x}}$ where $\\v{A}‚àà‚Ñù^{\\red{n√ón}}$, $Œª‚àà‚Ñù$, and $\\v{x}‚àà‚Ñù^n\\backslash\\{\\v{0}\\}$. Then $Œª$ is an **eigenvalue** of $\\v{A}$ and $\\v{x}$ is a corresponding **eigenvector**. Imagine a set of arrows pointing outwards from a circle. When transformation $\\v{A}$ is applied, most are knocked off course except eigenvectors that stay in the same directions but change their lengths by factors equal to their eigenvalues. Eigenvectors are the structural axes of the transformation.\n",
        "\n",
        "- If $Œª$ is an eigenvalue, then $\\green{(\\v{A}-Œª\\v{I}_n)\\v{x}=\\v{0}}$ for nontrivial $\\v{x}‚àà\\red{E_Œª=\\null(\\v{A}-Œª\\v{I}_n)\\backslash\\{\\v{0}\\}}$ called the **eigenspace** with respect to $Œª$. It also holds that $\\rk(\\v{A}-Œª\\v{I}_n)< n$, and $\\det(\\v{A}-Œª\\v{I}_n)=0$.\n",
        "\n",
        "  - If $\\v{x}‚ààE_Œª$, then all collinear vectors $c\\v{x}‚ààE_Œª,\\ c‚àà‚Ñù\\backslash\\{0\\}$.\n",
        "\n",
        "  - Two vectors are **collinear** if they're parallel, and **codirected** if they also share the same direction.\n",
        "\n",
        "  - $\\dim(E_Œª)‚â•1$ is called the **geometric multiplicity** of $Œª$.\n",
        "\n",
        "- **Characteristic polynomial** for $\\v{A}‚àà‚Ñù^{n√ón}$ and $Œª‚àà‚Ñù$ is $\\red{p_\\v{A}:=\\det(\\v{A}-Œª\\v{I})}=\\suml_{i=1}^{n-1}c_iŒª^i+(-1)^nŒª^n$, where $c_0=\\det(\\v{A})$ and $c_{n-1}=(-1)^{n-1}\\tr(\\v{A})$.\n",
        "\n",
        "  - $Œª$ is an eigenvalue of $\\v{A}$ iff $Œª$ is a root of $p_\\v{A}$. The number of times $Œª$ appears as a root in $p_\\v{A}$ is called its **algebraic multiplicity**. The set of all eigenvalues is called the **eigenspectrum**. $\\v{A}$ is guaranteed to have $n$ eigenvalues, including repeats and complex numbers.\n",
        "\n",
        "- $\\v{A}$ and $\\v{A}^‚ä§$ share the same eigenvalues.\n",
        "\n",
        "  - Proof: $\\det(\\v{A}^‚ä§-Œª\\v{I})=\\det((\\v{A}-Œª\\v{I})^‚ä§)=\\det(\\v{A}-Œª\\v{I})$.\n",
        "\n",
        "- Similar matrices $\\v{A}_{BB}=\\v{BA}_{std}\\v{B}^{-1}$ share the same eigenvalues.\n",
        "\n",
        "  - Proof: $\\det(\\v{A}_{BB}-Œª\\v{I})=\\det(\\v{BA}_{std}\\v{B}^{-1}-Œª\\v{BIB}^{-1})=\\det(\\v{B}(\\v{A}_{std}-Œª\\v{I})\\v{B}^{-1})=\\det(\\v{A}_{std}-Œª\\v{I})$.\n",
        "\n",
        "- Let $\\v{T}=\\v{A}+c\\v{I}$. Then eigenvalues of $\\v{T}$ are $Œª_i+c$ for each eigenvalue $Œª_i$ of $\\v{A}$.\n",
        "\n",
        "  - Proof: Let $Œª_i$ be an eigenvalue of $\\v{A}$ with eigenvector $\\v{x}_i$. Then\n",
        "  $\\v{T}\\v{x}_i=(\\v{A}+c\\v{I})\\v{x}_i=Œª_i\\v{x}_i+c\\v{x}_i$\n",
        "  $=(Œª_i+c)\\v{x}_i$.\n",
        "  Therefore $Œª_i+c$ is an eigenvalue of $\\v{T}$.\n",
        "\n",
        "**Examples of transformations**:\n",
        "\n",
        "- $\\v{A}_1=\\BM1/2&0\\\\0&2\\EM$ stretch. $\\det(\\v{A}_1)=1$ area is preserved.\n",
        "$\\det(\\v{A}-Œª\\v{I})=(1/2-Œª)(2-Œª)$. $E_{1/2}=\\span(\\BM1\\\\0\\EM)$.\n",
        "$E_2=\\span(\\BM0\\\\1\\EM)$. The horizontal axis is compressed by half, and vertical axis is stretched by 2.\n",
        "\n",
        "- $\\v{A}_2=\\BM1&1/2\\\\0&1\\EM$ horizontal shear. $\\det(\\v{A}_2)=1$ area is preserved.\n",
        "$\\det(\\v{A}-Œª\\v{I})=(1-Œª)^2$. $E_1=\\span(\\BM1\\\\0\\EM)$.\n",
        "Defective. Only vectors along the horizontal axis preserve direction.\n",
        "$\\BM1&1/2\\\\0&1\\EM\\BM0\\\\a\\EM=\\BM a/2\\\\a\\EM$ all vertical components are horizontally displaced by a 1/2 factor.\n",
        "\n",
        "  - $\\v{A}_2=\\BM1&1\\\\0&2\\EM$ shear. $\\det(\\v{A}_2)=2$. $E_1=\\span(\\BM-1\\\\0\\EM)$. $E_2=\\span(\\BM-1\\\\-1\\EM)$.\n",
        "  Non-defective. Then\n",
        "  $\\v{D}=\\BM2&0\\\\0&1\\EM$, $\\v{P}=\\BM-1&-1\\\\-1&0\\EM$. $\\v{P}^{-1}=\\BM0&-1\\\\-1&1\\EM$.\n",
        "  $\\v{PDP}^{-1}=\\BM-2&-1\\\\-2&0\\EM\\BM0&-1\\\\-1&1\\EM$\n",
        "  $=\\BM1&1\\\\0&2\\EM$. Diagonlized, but $\\v{P}$ is not orthogonal.\n",
        "\n",
        "- $\\v{A}_3=\\BM\\cos(œÄ/6)&-sin(œÄ/6)\\\\sin(œÄ/6)&\\cos(œÄ/6)\\EM=\\/{1}{2}\\BM\\sqrt{3}&-1\\\\1&\\sqrt{3}\\EM$. Rotation matrices are orthonormal with $\\det(\\v{A}_3)=1$ preserving area. Reflections are orthonormal with $\\det(\\v{A})=-1$.\n",
        "$\\det(\\v{A}-Œª\\v{I})=(\\sqrt{3}-Œª)^2+1$\n",
        "$‚áíŒª=\\/{\\sqrt{3}}{2}¬±\\/{j}{2}$.\n",
        "\n",
        "- $\\v{A}_4=\\BM1&-1\\\\-1&1\\EM$. $\\det(\\v{A}_4)=0$ area is collapsed.\n",
        "$\\det(\\v{A}-Œª\\v{I})=\\det\\BM1-Œª&-1\\\\-1&1-Œª\\EM$\n",
        "$=\\det\\BM-Œª&-Œª\\\\-1&1-Œª\\EM=\\det\\BM-Œª&0\\\\-1&2-Œª\\EM$\n",
        "$=Œª(2-Œª)$.\n",
        "$E_0=\\span(\\BM1\\\\1\\EM)$. $E_2=\\span(\\BM1\\\\-1\\EM)$.\n",
        "2-dimensional domain is collapsed onto 1-dimensional.\n",
        "\n",
        "- $\\v{A}_5=\\BM1&1/2\\\\1/2&1\\EM$ shear/stretch. $\\det(\\v{A}_5)=3/4$.\n",
        "$\\det(\\v{A}-Œª\\v{I})=\\det\\BM1-Œª&1/2\\\\1/2&1-Œª\\EM$\n",
        "$=(1-Œª)^2-1/4=3/4-2Œª+Œª^2$. $Œª=\\/{2¬±\\sqrt{4-3}}{2}$.\n",
        "$E_{1/2}=\\span(\\BM1\\\\-1\\EM)$.\n",
        "$E_{3/2}=\\span(\\BM1\\\\1\\EM)$.\n",
        "\n",
        "- 4.5: $\\v{A}=\\BM4&2\\\\1&3\\EM$. Calculate eigenvalues and eigenspaces.\n",
        "$\\v{A}-Œª\\v{I}=\\BM4-Œª&2\\\\1&3-Œª\\EM$.\n",
        "$\\det(\\v{A}-Œª\\v{I})=(4-Œª)(3-Œª)-2=10-7Œª+Œª^2$.\n",
        "$Œª=\\/{7¬±\\sqrt{49-40}}{2}=(5,2)$.\n",
        "$\\v{A}-5\\v{I}=\\BM-1&2\\\\1&-2\\EM‚Üí\\BM1&-2\\\\0&0\\EM$.\n",
        "$E_5=\\span(\\BM-2\\\\-1\\EM)$.\n",
        "$\\v{A}-2\\v{I}=\\BM2&2\\\\1&1\\EM‚Üí\\BM1&1\\\\0&0\\EM$.\n",
        "$E_2=\\span(\\BM1\\\\-1\\EM)$.\n",
        "\n",
        "- 4.8: $\\v{A}=\\BM3&2&2\\\\2&3&2\\\\2&2&3\\EM$. $\\det(\\v{A}-Œª\\v{I})=\\det\\BM3-Œª&2&2\\\\2&3-Œª&2\\\\2&2&3-Œª\\EM$\n",
        "$=\\det\\BM7-Œª&7-Œª&7-Œª\\\\2&3-Œª&2\\\\2&2&3-Œª\\EM$\n",
        "$=(7-Œª)\\det\\BM1&1&1\\\\2&3-Œª&2\\\\2&2&3-Œª\\EM$\n",
        "$=(7-Œª)\\det\\BM1&0&0\\\\2&1-Œª&0\\\\2&0&1-Œª\\EM$\n",
        "$=(7-Œª)(1-Œª)^2$.\n",
        "$\\v{A}-7\\v{I}=\\BM-4&2&2\\\\2&-4&2\\\\2&2&-4\\EM‚Üí\\BM1&0&-1\\\\0&1&-1\\\\0&0&0\\EM$.\n",
        "$E_7=\\span(\\BM1\\\\1\\\\1\\EM)$.\n",
        "$\\v{A}-\\v{I}=\\BM2&2&2\\\\2&2&2\\\\2&2&2\\EM‚Üí\\BM1&1&1\\\\0&0&0\\\\0&0&0\\EM$.\n",
        "$E_1=\\span(\\BM1\\\\-1\\\\0\\EM,\\BM1\\\\0\\\\-1\\EM)$.\n",
        "By Gram-Schmidt,\n",
        "$\\v{u}_1=\\BM1\\\\1\\\\1\\EM$, $\\v{u}_2=\\BM1\\\\-1\\\\0\\EM$.\n",
        "$œÄ_{\\v{u}_2}(\\BM1\\\\0\\\\-1\\EM)=\\/{\\v{uu}^‚ä§}{\\vn{u}^2}\\BM1\\\\0\\\\-1\\EM=\\/{1}{2}\\BM1&-1&0\\\\-1&1&0\\\\0&0&0\\EM\\BM1\\\\0\\\\-1\\EM=\\BM1/2\\\\-1/2\\\\0\\EM$.\n",
        "$\\v{u}_3=\\BM1\\\\0\\\\-1\\EM-œÄ_{\\v{u}_2}(\\BM1\\\\0\\\\-1\\EM)$\n",
        "$=\\BM1/2\\\\1/2\\\\-1\\EM$."
      ],
      "metadata": {
        "id": "ZubxYH7_f1Ox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Non-defective matrix**: $\\v{A}‚àà‚Ñù^{n√ón}$ is called **defective** if it has $< n$ linearly independent eigenvectors. A non-defective matrix does not require $n$ distinct eigenvalues if geometric multiplicity is equal to algebraic multiplicity.\n",
        "\n",
        "- Eigenvectors $\\v{x}_1,...,\\v{x}_n$ of $\\v{A}‚àà‚Ñù^{n√ón}$ with distinct $Œª_1,...,Œª_n$ are linearly independent.\n",
        "\n",
        "  - Proof: Given distinct $Œª_1,Œª_2$, then the definition of linearly independent says $c_1\\v{x}_1+c_2\\v{x}_2=\\v{0}‚áíc_1=c_2=0$.\n",
        "\n",
        "  - **Base case**: $c_1\\v{x}_1=\\v{0}‚áíc_1=0$ because $\\v{x}_i\\neq\\v{0}$.\n",
        "\n",
        "  - **Hypothesis $k$**: assume when given $Œª_1,...,Œª_k$ all distinct, $c_1\\v{x}_1+...+c_k\\v{x}_k=\\v{0}‚áíc_1=...=c_k=0$.\n",
        "\n",
        "  - **Prove $k+1$**: $\\BC\n",
        "  \\t{1. need to show}&c_1\\v{x}_1+...+c_{k+1}\\v{x}_{k+1}=\\v{0}‚áíc_1=...=c_{k}=c_{k+1}=0\\\\\n",
        "  \\t{2. $\\v{A}‚ãÖ(1)$}&c_1\\v{Ax}_1...+c_{k+1}\\v{Ax}_{k+1}=c_1Œª_1\\v{x}_1...+\\green{c_{k+1}Œª_{k+1}\\v{x}_{k+1}}=\\v{0}\\\\\n",
        "  \\t{3. $Œª_{k+1}‚ãÖ(1)$}&c_1Œª_{k+1}\\v{x}_1...+\\green{c_{k+1}Œª_{k+1}\\v{x}_{k+1}}=\\v{0}\\\\\n",
        "  \\t{4. $(3)-(2)$}&c_1(Œª_{k+1}-Œª1)\\v{x}_1...+c_k(Œª_{k+1}-Œª_k)\\v{x}_k=\\v{0}‚áíc_1=...=c_{k}=0\\\\\n",
        "  \\t{5. back to $(1)$}&0\\v{x}_1+...+0\\v{x}_k+c_{k+1}\\v{x}_{k+1}=\\v{0}‚áíc_{k+1}=0\n",
        "  \\EC$\n",
        "\n",
        "- Given $\\v{A}‚àà‚Ñù^{m√ón}$ construct $\\v{S}:=\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$. Then $\\v{S}$ is symmetric and $\\blue{\\rk(\\v{A})=\\rk(\\v{S})}$.\n",
        "\n",
        "  - Proof (symmetric): $\\v{S}^‚ä§=(\\v{A}^‚ä§\\v{A})^‚ä§=(\\v{A})^‚ä§(\\v{A}^‚ä§)^‚ä§=\\v{A}^‚ä§\\v{A}=\\v{S}$\n",
        "\n",
        "  - Proof: (1) $\\v{Ax}=\\v{0}‚áí\\v{A}^‚ä§\\v{Ax}=\\v{0}$.\n",
        "  (2) Assume $\\v{A}^‚ä§\\v{Ax}=\\v{0}$.\n",
        "  Then $\\v{x}^‚ä§\\v{A}^‚ä§\\v{Ax}=0$\n",
        "  $‚áí(\\v{Ax})^‚ä§\\v{Ax}=0$\n",
        "  $‚áí\\vn{Ax}_2^2=0$\n",
        "  $‚áí\\v{Ax}=\\v{0}$.\n",
        "  Therefore $\\v{Ax}=\\v{0}‚áî\\v{Sx}=\\v{0}$ and $\\null(\\v{A})=\\null(\\v{S})$.\n",
        "\n",
        "- Given $\\v{S}=\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$. Then $\\v{S}$ is positive-semidefinite. **$\\v{S}$ is full rank $‚áî\\v{S}$ is positive-definite $‚áî\\v{S}$ invertible**.\n",
        "\n",
        "  - Proof (positive-semidefinite): $\\v{x}^‚ä§\\v{Sx}=\\v{x}^‚ä§\\v{A}^‚ä§\\v{A}\\v{x}=(\\v{Ax})^‚ä§\\v{Ax}=\\vn{Ax}^2‚â•0$.\n",
        "\n",
        "  - Proof (positive-definite): $\\v{x}^‚ä§\\v{Sx}=\\vn{Ax}^2=0‚áî\\v{Ax}=\\v{0}$. If $\\rk(\\v{A})=n$ then $\\v{Ax}=\\v{0}‚áî\\v{x}=\\v{0}$.\n",
        "\n",
        "- **Positive-definite matrix ‚áî positive, real eigenvalues**.\n",
        "\n",
        "  - Proof (symmetric ‚áî real eigenvalues): If $z=a+bi$ then conjugate $\\bar{z}=a-bi$ and $|z|^2=z‚ãÖ\\bar{z}$.\n",
        "  Then $‚ü®\\v{x},\\v{x}‚ü©=\\bar{\\v{x}}^‚ä§\\v{x}=:\\v{x}^*\\v{x}$ where $\\v{x}^*$ is called conjugate transpose.\n",
        "  We compare $Œª$ vs $\\bar{Œª}$ conjugate pair:\n",
        "  $\\v{x}^*\\v{Sx}=\\v{x}^*Œª\\v{x}=Œª\\vn{x}^2$.\n",
        "  $(\\v{x}^*\\v{Sx})^*=\\green{\\bar{Œª}}\\vn{x}^2=(\\v{x})^*(\\v{S})^*(\\v{x}^*)^*=\\v{x}^*\\green{\\v{S}^‚ä§}\\v{x}=\\v{x}^*\\green{\\v{S}}\\v{x}=\\green{Œª}\\vn{x}^2$\n",
        "  $‚áíŒª=\\bar{Œª}‚àà‚Ñù$\n",
        "  \n",
        "  - Proof (positive-definite ‚áî positive eigenvalues): $\\v{Sx}=Œª\\v{x}‚áí\\green{\\v{x}^‚ä§\\v{Sx}=Œª\\vn{x}^2}‚áíŒª=\\/{\\v{x}^‚ä§\\v{Sx}}{\\vn{x}^2}$\n",
        "  $‚áí\\blue{\\BC Œª>0&\\t{if $\\v{S}$ is positive-definite}\\\\\n",
        "  Œª‚â•0&\\t{if $\\v{S}$ is positive-semidefinite}\n",
        "  \\EC}$\n",
        "\n",
        "- Given $\\v{S}=\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$. Then the number of nonzero eigenvalues of $\\v{S}$ is equal to $\\rk(\\v{S})$.\n",
        "\n",
        "  - Proof: $\\rk(\\v{S})=\\rk(\\v{D})$ where $\\v{S}=\\v{PDP}^{-1}$. Proof below.\n",
        "\n",
        "- Given $\\v{S}=\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$. Then $\\v{T}:=\\v{S}+c\\v{I}_n$ where $c>0$ is guaranteed to be positive-definite and full rank.\n",
        "\n",
        "  - Proof: Eigenvalues of $\\v{T}$ are $Œª_i+c>0$ for each eigenvalue $Œª_i$ of $\\v{S}$.\n",
        "\n",
        "- **Spectral theorem**: If $\\v{A}‚àà‚Ñù^{n√ón}$ is symmetric, then it is non-defective and there exists an orthonormal basis consisting of eigenvectors of $\\v{A}$, and each eigenvalue is real.\n",
        "\n",
        "  - Proof (symmetric ‚áí orthogonal eigenvectors): Let $Œª_1\\neqŒª_2$. Then $\\v{Ax}_1=Œª_1\\v{x}_1$ and $\\v{Ax}_2=Œª_2\\v{x}_2$.\n",
        "  $\\BC\n",
        "  (\\v{Ax}_1)^‚ä§\\v{x}_2=\\v{x}_1^‚ä§\\v{A}^‚ä§\\v{x}_2=\\v{x}_1^‚ä§\\v{Ax}_2=\\v{x}_1^‚ä§Œª_2\\v{x}_2=Œª_2\\v{x}_1^‚ä§\\v{x}_2\\\\\n",
        "  (\\v{Ax}_1)^‚ä§\\v{x}_2=Œª_1\\v{x}_1^‚ä§\\v{x}_2\n",
        "  \\EC$\n",
        "  $‚áíŒª_1\\v{x}_1^‚ä§\\v{x}_2=Œª_2\\v{x}_1^‚ä§\\v{x}_2‚áí\\v{x}_1^‚ä§\\v{x}_2=0$.\n",
        "\n",
        "  - **Schur's proof** (symmetric ‚áí non-defective): **Base case**: $\\v{A}‚àà‚Ñù^{1√ó1}$ is non-defective.\n",
        "\n",
        "  - **Hypothesis $n-1$**: Assume symmetric $\\v{A}‚àà‚Ñù^{(n-1)√ó(n-1)}$ has $(n-1)$ orthogonal eigenvectors.\n",
        "\n",
        "  - **Prove $n$**: $\\BC\n",
        "  \\t{Pick $\\v{Ax}_1=Œª_1\\v{x}_1$:}& W=\\span(\\v{x}_1)\\\\\n",
        "  \\t{Pick $\\v{w}‚ààW^\\perp$:}&(\\v{Aw})^‚ä§\\v{x}_1=\\v{w}^‚ä§\\v{A}^‚ä§\\v{x}_1=\\v{w}^‚ä§\\v{A}\\v{x}_1=Œª_1\\v{w}^‚ä§\\v{x}_1=0‚áí(\\v{Aw})‚ààW^\\perp \\\\\n",
        "  \\t{Find $\\tilde{\\v{A}}=\\v{A}_{W^\\perp}$:}& \\t{ONB }\\v{B}=\\BM\\v{x}_1&\\v{u}_2&...&\\v{u}_n\\EM\\small‚áí\\v{B}^{-1}(\\v{AB})=\\BM\\v{x}_1^‚ä§\\\\\\v{u}_2^‚ä§\\\\\\vdots\\\\\\v{u}_n^‚ä§\\EM\\BMŒª_1\\v{x}_1&\\v{Au}_2&...&\\v{Au}_n\\EM=\\BMŒª_1&\\v{0}^‚ä§\\\\\\v{0}&\\tilde{\\v{A}}\\EM\n",
        "  \\EC$.\n",
        "  $\\tilde{\\v{A}}\\v{w}=\\v{Aw}\\ ‚àÄ\\v{w}‚ààW^\\perp$ is separated from $\\v{x}_1$ by the $\\v{B}^{-1}\\v{AB}$ basis change.\n",
        "  Because $\\v{B}^{-1}\\v{AB}$ is symmetric then $\\tilde{\\v{A}}‚àà‚Ñù^{(n-1)√ó(n-1)}$ is also symmetric and by hypothesis has $n-1$ orthogonal eigenvectors."
      ],
      "metadata": {
        "id": "zqq5Gh74nKdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eigendecomposition**: Matrix $\\v{A}‚àà‚Ñù^{n√ón}$ is diagonalizable if it is similar to a diagonal matrix $\\v{D}$: there exists $\\v{P}‚àà‚Ñù^{n√ón}$ such that $\\red{\\v{A}=\\v{PDP}^{-1}}$. Non-defective matrix $\\v{A}‚àà‚Ñù^{n√ón}$ can be diagonlized into $\\v{A}=\\v{PDP}^‚ä§$ where $\\v{P}‚àà‚Ñù^{n√ón}$ is the orthonormal basis consisting of eigenvectors of $\\v{A}$ and diagonals of $\\v{D}$ are eigenvalues of $\\v{A}$.\n",
        "\n",
        "- $\\v{AP}=\\v{A}\\BM\\v{p}_1&...&\\v{p}_n\\EM=\\BM\\v{Ap}_1&...&\\v{Ap}_n\\EM$,\n",
        "$\\v{PD}=\\BM\\v{p}_1&...&\\v{p}_n\\EM\\BMŒª_1&...&0\\\\\\vdots&\\ddots&\\vdots\\\\0&...&Œª_n\\EM=\\BMŒª_1\\v{p}_1&...&Œª_n\\v{p}_n\\EM$. Then\n",
        "$\\v{AP}=\\v{PD}‚áí\\v{Ap}_i=Œª_i\\v{p}_i$.\n",
        "\n",
        "- Spectral theorem says symmetric matrix $\\v{S}‚àà‚Ñù^{n√ón}$ is similar to a diagonal matrix of real eigenvalues.\n",
        "\n",
        "- Geometric pipeline: $\\v{A}=\\v{PDP}^{-1}$ stretches and rotates space in the standard grid. A standard coordinate is taken in, rebased along eigenbasis to another coordinate (by rotation or reflection), stretched along eigenvectors by eigenvalues, and rebased back to standard coordinate as output.\n",
        "\n",
        "- $\\red{\\v{A}^k=\\v{PD}^k\\v{P}^‚ä§}$.\n",
        "\n",
        "- $\\red{\\det(\\v{A})=\\det(\\v{D})}$.\n",
        "\n",
        "- $\\red{\\rk(\\v{A})=\\rk(\\v{D})}$. Diagonalizable rank-$r$ matrices have $r$ non-zero eigenvalues.\n",
        "\n",
        "  - Proof: $\\rk(\\v{XY})‚â§\\min(\\rk(\\v{X}),\\rk(\\v{Y}))$. Matrix rank is invariant to multiplication with invertible matrix $\\v{P}$ and $\\v{P}^{-1}$\n",
        "  $\\BC\\small\n",
        "  \\rk(\\v{PD})‚â§\\min(\\rk(\\v{P}),\\rk(\\v{D})) & \\rk(\\v{D})‚â§\\min(\\rk(\\v{P}^{-1}),\\rk(\\v{PD}))&\\rk(\\v{PD})=\\rk(\\v{D}) \\\\\n",
        "  \\rk(\\v{DP}^{-1})‚â§\\min(\\rk(\\v{D}),\\rk(\\v{P}^{-1})) & \\rk(\\v{D})‚â§\\min(\\rk(\\v{DP}^{-1}),\\rk(\\v{P}))&\\rk(\\v{DP}^{-1})=\\rk(\\v{D})\n",
        "  \\EC$.  \n",
        "  $\\v{S}=\\v{PDP}^{-1}‚áí\\rk(\\v{D})=\\rk(\\v{S})$. The rank of a diagonal matrix is the number of non-zero diagonal entries.\n",
        "\n",
        "  - Symmetric positive-semidefinite rank-$r$ matrices have $r$ positive eigenvalues. The rest are zero.\n",
        "\n",
        "- 4.11: $\\v{A}=\\/{1}{2}\\BM5&-2\\\\-2&5\\EM$.\n",
        "$\\det(\\v{A}-Œª\\v{I})=\\/{1}{4}\\det(\\BM5-2Œª&-2\\\\-2&5-2Œª\\EM)$\n",
        "$=\\/{1}{4}\\det\\BM3-2Œª&3-2Œª\\\\-2&5-2Œª\\EM$\n",
        "$=\\/{1}{4}\\det\\BM3-2Œª&0\\\\-2&7-2Œª\\EM$\n",
        "$=(3/2-Œª)(7/2-Œª)$.\n",
        "$\\v{A}-\\/{3}{2}\\v{I}=\\/{1}{2}\\BM2&-2\\\\-2&2\\EM‚Üí\\BM1&-1\\\\0&0\\EM$\n",
        "$‚áíE_{3/2}=\\span(\\BM1\\\\1\\EM)$.\n",
        "$\\v{A}-\\/{7}{2}\\v{I}=\\/{1}{2}\\BM-2&-2\\\\-2&-2\\EM‚Üí\\BM1&1\\\\0&0\\EM$\n",
        "$‚áíE_{7/2}=\\span(\\BM1\\\\-1\\EM)$.\n",
        "Therefore $\\v{P}=\\/{1}{\\sqrt{2}}\\BM1&1\\\\1&-1\\EM$, $\\v{D}=\\/{1}{2}\\BM3&0\\\\0&7\\EM$, and\n",
        "$\\v{PDP}^‚ä§=\\/{1}{4}\\BM3&7\\\\3&-7\\EM\\BM1&1\\\\1&-1\\EM$\n",
        "$=\\/{1}{4}\\BM10&-4\\\\-4&10\\EM=\\/{1}{2}\\BM5&-2\\\\-2&5\\EM=\\v{A}$"
      ],
      "metadata": {
        "id": "0nvT89S1sKvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More on determinants and traces**: Let $\\v{A}$ be symmetrical.\n",
        "\n",
        "- $\\red{\\det(\\v{A})=\\prod\\limits_{i=1}^nŒª_i}$. Determinant is a product of eigenvalues.\n",
        "\n",
        "  - Proof: $\\det(\\v{A}-Œª\\v{I})=\\prod_i(Œª_i-Œª)$\n",
        "  $‚áí\\det(\\v{A})=\\prod_iŒª_i$.\n",
        "\n",
        "- $\\red{\\tr(\\v{A})=\\suml_{i=1}^nŒª_i}$. Trace is a sum of eigenvalues.\n",
        "\n",
        "  - Proof: $\\det(\\v{A}-Œª\\v{I})=(-1)^n\\prod_i(Œª-Œª_i)$. Then $\\BC\n",
        "  \\t{summing polynomial coefficients}& c_{n-1}=(-1)^n\\sum_i(-Œª_i)\n",
        "  =(-1)^{n+1}\\sum_iŒª_i\\\\\n",
        "  \\t{multiplying diagonal of $\\v{A}-Œª\\v{I}$}& c_{n-1}=(-1)^{n-1}\\sum_ia_{ii}\n",
        "  \\EC$\n",
        "\n",
        "- $\\ln(\\det(\\v{A}))=\\tr(\\ln(\\v{A}))$\n",
        "\n",
        "  - Proof: $\\ln(\\det(\\v{A}))=\\ln(\\prod_iŒª_i)=\\sum_i\\ln(Œª_i)=\\tr(\\ln(\\v{A}))$\n",
        "\n",
        "- Quick 2√ó2 eigenvalues: Let $m=\\tr(\\v{A})$ and $p=\\det(\\v{A})$. Then $Œª_{1,2}=m¬±\\sqrt{m^2-p}$.\n",
        "\n",
        "- **Matrix factoring**: $\\BC\n",
        "\\v{X}+t\\v{V}=\\v{X}(I+t\\v{X}^{-1}\\v{V})&\\v{X}^{-1}\\v{V}\\t{ is usually not symmetric}\\\\\n",
        "\\v{X}+t\\v{V}=\\v{X}^{1/2}(I+t\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2})\\v{X}^{1/2}&\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2}\\t{ symmetry guaranteed}\\\\\n",
        "\\EC$\n",
        "\n",
        "  - $\\v{X}^{-1}\\v{V}=\\v{X}^{-1/2}(\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2})\\v{X}^{1/2}$. Therefore $\\v{X}^{-1}\\v{V}$ and $\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2}$ are similar matrices and have the same eigenvalues.\n",
        "\n",
        "  - $\\det(\\v{X}^{1/2}(I+t\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2})\\v{X}^{1/2})=\\det(\\v{X})\\det(I+t\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2})=\\det(\\v{X})\\prod_{i=1}^n(1+tŒª_i)$ where $Œª_i‚àà‚Ñù$ are eigenvalues of symmetric $\\v{X}^{-1/2}\\v{V}\\v{X}^{-1/2}$.\n",
        "\n",
        "  - $\\v{X}^{1/2}$ is in practice calculated as Cholesky factors $\\v{LL}^‚ä§=\\v{X}$.\n",
        "\n",
        "- $\\v{u}$ is an eigenvector of outer product $\\v{uv}^‚ä§$ with eigenvalue $\\v{v}^‚ä§\\v{u}$. The other $(n-1)$ eigenvalues are 0.\n",
        "\n",
        "  - Proof: $\\v{u}(\\v{v}^‚ä§\\v{u})=(\\v{v}^‚ä§\\v{u})\\v{u}$ as $\\v{v}^‚ä§\\v{u}$ is scalar.\n",
        "  Because $\\rk(\\v{uv}^‚ä§)=1$ as outer product, $\\null(\\v{uv}^‚ä§)=n-1$, which means there are $(n-1)$ 0 eigenvalues corresponding to eigenvectors $\\v{x}$ such that $\\v{v}^‚ä§\\v{x}=0$.\n",
        "\n",
        "- **Matrix determinant lemma**: $\\det(\\v{A}+\\v{uv}^‚ä§)=\\det(\\v{A})(1+\\v{v}^‚ä§\\v{A}^{-1}\\v{u})$\n",
        "\n",
        "  - Proof: $\\v{A}+\\v{uv}^‚ä§=\\v{A}(\\v{I}+\\v{A}^{-1}\\v{uv}^‚ä§)$\n",
        "  $=\\v{A}(\\v{I}+\\v{u}'\\v{v}^‚ä§)$ where $\\v{u}'=\\v{A}^{-1}\\v{u}$.\n",
        "  Eigenvalues of $\\v{I}+\\v{u}'\\v{v}^‚ä§$ are $1$ and $1+\\v{v}^‚ä§\\v{u}'$.\n",
        "  Therefore $\\det(\\v{I}+\\v{u}'\\v{v}^‚ä§)=1+\\v{v}^‚ä§\\v{A}^{-1}\\v{u}$.\n",
        "\n",
        "  - $\\det(\\v{I}+t\\v{uu}^‚ä§)=1+t\\vn{u}_2^2$.\n",
        "\n",
        "- Any positive-semidefinite $\\v{Z}‚àà‚Ñù^{n√ón}$ can be decomposed into $\\v{Z}=\\sum_{i=1}^n\\v{v}_i\\v{v}_i^‚ä§$ where $\\v{v}_i‚àà‚Ñù^n$.\n",
        "\n",
        "  - Proof: $\\v{Z}$ is positive-semidefinite if diagonalizable with eigenvalues $Œª_i‚â•0\\ ‚àÄi=1,...,n$.\n",
        "  $\\v{Z}=\\v{PDP}^‚ä§=\\sum_{i=1}^nŒª_i\\v{p}_i\\v{p}_i^‚ä§$\n",
        "  $=\\sum_{i=1}^n\\v{v}_i\\v{v}_i^‚ä§$\n",
        "  where $\\v{v}_i=\\sqrt{Œª_i}\\v{p}_i$"
      ],
      "metadata": {
        "id": "X7sLUKmpq-Iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single Value Decomposition**: Matrix $\\v{A}‚àà‚Ñù^{m√ón}$ of rank $r‚àà[0,\\min(m,n)]$. Then $\\red{\\v{A}=\\v{UŒ£V}^‚ä§}$ where $\\v{U}‚àà‚Ñù^{m√óm},\\v{V}‚àà‚Ñù^{n√ón}$ are orthogonal matrices, and $\\v{Œ£}‚àà‚Ñù^{m√ón}$ is a truncated diagonal matrix such that $Œ£_{ii}=œÉ_i‚â•0$ and $Œ£_{ij}=0$ off diagonal. $\\blue{œÉ_1‚â•...‚â•œÉ_r>0}$ are called singular values, and $\\v{u}_i,\\v{v}_j$ are called left/right singular vectors.\n",
        "\n",
        "- $\\v{A}$ represents $Œ¶:‚Ñù^n‚Üí‚Ñù^m$. A $n$-dimensional coordinate is taken in standard basis, rebased along $\\v{V}$ through rotation or reflection, transformed by $œÉ_{i}$ rescaling or dimension augmentation/reduction via $\\v{Œ£}$, and then released as a $m$-dimensional coordinate in $\\v{U}$ basis.\n",
        "\n",
        "- Let $(\\v{v}_1,...,\\v{v}_n)$ be the eigenbasis of $\\v{A}^‚ä§\\v{A}$, then $(\\v{Av}_1,...,\\v{Av}_n)$ preserves orthogonality.\n",
        "\n",
        "  - Proof: $\\v{S}=\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$ is symmetric positive-semidefinite and non-defective.\n",
        "  $‚ü®\\v{Av}_i,\\v{Av}_j‚ü©=(\\v{Av}_i)^‚ä§(\\v{Av}_j)$\n",
        "  $=\\v{v}_i^‚ä§(\\v{A}^‚ä§\\v{A}\\v{v}_j)$\n",
        "  $=\\v{v}_i^‚ä§(Œª_j\\v{v}_j)$\n",
        "  $=0$\n",
        "\n",
        "- $\\v{A}=\\v{UŒ£V}^‚ä§$ requires $\\red{\\v{Av}_i=œÉ_i\\v{u}_i}$ to hold true for $i=1..r$ where $r=\\rk(\\v{A})‚àà[0,\\min(m,n)]$.\n",
        "\n",
        "  - Example: $\\v{A}‚àà‚Ñù^{3√ó2}$. Then $\\v{AV}=\\v{UŒ£}$\n",
        "  $‚áí\\BM a_{11}&a_{12}\\\\a_{21}&a_{22}\\\\a_{31}&a_{32}\\EM\\BM v_{11}&v_{12}\\\\v_{21}&v_{22}\\EM=\\BM u_{11}&u_{12}&u_{13}\\\\u_{21}&u_{22}&u_{23}\\\\u_{31}&u_{32}&u_{33}\\EM\\BMœÉ_1&0\\\\0&œÉ_2\\\\0&0\\EM$\n",
        "  $‚áí\\BM a_{11}v_{11}+a_{12}v_{21}&a_{11}v_{12}+a_{12}v_{22}\\\\a_{21}v_{11}+a_{22}v_{21}&a_{21}v_{12}+a_{22}v_{22}\\\\a_{31}v_{11}+a_{32}v_{21}&a_{31}v_{12}+a_{32}v_{22}\\EM=\\BM œÉ_1u_{11}&œÉ_2u_{12}\\\\œÉ_1u_{21}&œÉ_2u_{22}\\\\œÉ_1u_{31}&œÉ_2u_{32}\\EM$\n",
        "  $‚áí\\BM \\v{Av}_1&\\v{Av}_2\\EM=\\BM œÉ_1\\v{u}_1&œÉ_2\\v{u}_2\\EM$.\n",
        "\n",
        "- $\\v{A}^‚ä§\\v{A}$ and $\\v{AA}^‚ä§$ have the same non-zero eigenvalues.\n",
        "\n",
        "  - Proof: $(\\v{A}^‚ä§\\v{A})\\v{x}=Œª\\v{x}‚áí\\v{A}(\\v{A}^‚ä§\\v{A}\\v{x})=\\v{A}(Œª\\v{x})‚áí(\\v{AA}^‚ä§)(\\v{Ax})=Œª(\\v{Ax})‚áí(\\v{AA}^‚ä§)\\v{y}=Œª\\v{y}$\n",
        "\n",
        "- The eigenvectors associated with 0-eigenvalues of $\\v{AA}^‚ä§$ are the basis vectors for the left null space of $\\v{A}$.\n",
        "\n",
        "  - Proof: Let $\\v{x}$ be an eigenvector for 0-eigenvalue of $\\v{AA}^‚ä§$, then $\\v{AA}^‚ä§\\v{x}=0\\v{x}=\\v{0}$\n",
        "  $‚áí\\v{x}^‚ä§\\v{AA}^‚ä§\\v{x}=(\\v{A}^‚ä§\\v{x})^‚ä§(\\v{A}^‚ä§\\v{x})$\n",
        "  $=\\n{\\v{A}^‚ä§\\v{x}}^2=0$\n",
        "  $‚áí\\v{A}^‚ä§\\v{x}=\\v{0}$\n",
        "  $‚áí\\v{x}‚àà\\null(\\v{A}^‚ä§)$.\n",
        "\n",
        "- **Full SVD**: Right singular vectors $\\v{V}$ are the orthonormal eigenbasis of $\\v{A}^‚ä§\\v{A}$. The singular values $œÉ_i=\\sqrt{Œª_i}$ are square roots of eigenvalues of $\\v{A}^‚ä§\\v{A}$. Left singular vector $\\v{u}_1,...,\\v{u}_r$ are obtained by $\\red{\\v{u}_i=\\/{1}{œÉ}\\v{Av}_i}$. The leftovers are filled with orthonormal basis of $\\null(\\v{A}^‚ä§)$.\n",
        "\n",
        "  - Proof ($\\v{V},\\v{Œ£}$): Symmetric non-defective $\\v{A}^‚ä§\\v{A}‚àà‚Ñù^{n√ón}$. Then $\\v{A}^‚ä§\\v{A}=\\green{\\v{P}\\BMŒª_1&...&0\\\\\\vdots&\\ddots&\\vdots\\\\0&...&Œª_n\\EM\\v{P}^‚ä§}$ by eigendecomposition.\n",
        "  $\\v{A}^‚ä§\\v{A}=(\\v{UŒ£V}^‚ä§)^‚ä§(\\v{UŒ£V}^‚ä§)$\n",
        "  $=(\\v{V}^‚ä§)^‚ä§\\v{Œ£}^‚ä§(\\v{U}^‚ä§\\v{U})\\v{Œ£V}^‚ä§$\n",
        "  $=\\v{VŒ£}^‚ä§\\v{Œ£V}^‚ä§$\n",
        "  $=\\green{\\v{V}\\BMœÉ_1^2&...&0\\\\\\vdots&\\ddots&\\vdots\\\\0&...&œÉ_n^2\\EM\\v{V}^‚ä§}$. Because $\\v{A}^‚ä§\\v{A}$ is positive-semidefinite, only $Œª_1,...,Œª_r$ (therefore $œÉ_1,...,œÉ_r$) are non-zero.\n",
        "\n",
        "  - Proof ($\\v{U}$):\n",
        "  Symmetric non-defective $\\v{AA}^‚ä§‚àà‚Ñù^{m√óm}$.\n",
        "  $\\v{AA}^‚ä§=(\\v{UŒ£V}^‚ä§)(\\v{UŒ£V}^‚ä§)^‚ä§$\n",
        "  $=\\v{UŒ£V}^‚ä§\\v{VŒ£}^‚ä§\\v{U}^‚ä§$\n",
        "  $=\\v{UŒ£Œ£}^‚ä§\\v{U}^‚ä§$\n",
        "  $=\\green{\\v{U}\\BMœÉ_1^2&...&0\\\\\\vdots&\\ddots&\\vdots\\\\0&...&œÉ_m^2\\EM\\v{U}^‚ä§}$, again only $œÉ_1,...,œÉ_r$ are non-zero.\n",
        "  While $\\v{U}$ is the orthonormal eigenbasis of $\\v{AA}^‚ä§$, it cannot be computed that way independently from $\\v{V}$ because the $¬±$ signs of $\\v{u}_i$ and $\\v{v}_i$ are constrained by the requirement $\\v{Av}_i=œÉ_i\\v{u}_i$ for $i=1,...,r$.\n",
        "  Therefore the first $r$ columns are computed by $\\v{u}_i=\\/{1}{œÉ_i}\\v{Av}_i$ for $œÉ_1,...,œÉ_r$. The remaining $m-r$ columns correspond to zero eigenvalues of $\\v{AA}^‚ä§$, and are filled with those orthonormal eigenvectors, or equivalently with orthonormal basis of $\\null(\\v{A}^‚ä§)$.\n",
        "\n",
        "- **Reduced SVD**: $\\v{A}‚àà‚Ñù^{m√ón}$, then $\\v{U}‚àà‚Ñù^{m√ón}$, $\\v{Œ£}‚àà‚Ñù^{n√ón}$ is a diagonal matrix, and $\\v{V}‚àà‚Ñù^{n√ón}$. There is no need to fill $\\v{U}$ leftover columns with $\\null(\\v{A}^‚ä§)$.\n",
        "\n",
        "- **Reduced rank SVD**: $\\v{A}‚àà‚Ñù^{m√ón}$ is rank-$r$. Then $\\v{U}_r‚àà‚Ñù^{m√ór}$, $\\v{Œ£}_r‚àà‚Ñù^{r√ór}$ is non-zero diagonal, $\\v{V}_r‚àà‚Ñù^{n√ór}$ only takes eigenvectors of non-zero eigenvalues. $\\red{\\v{A}=\\v{U}_r\\v{Œ£}_r\\v{V}_r^‚ä§}=\\suml_{i=1}^rœÉ_i\\v{u}_i\\v{v}_i^‚ä§=\\suml_{i=1}^rœÉ_i\\v{A}_i$ where $\\v{A}_i=\\v{u}_i\\v{v}_i^‚ä§$.\n",
        "\n",
        "**$\\v{A}=\\v{PDP}^{-1}$ vs $\\v{A}=\\v{UŒ£V}^‚ä§$**:\n",
        "  \n",
        "- For symmetric matrices, SVD and eigendecomposition are the same.\n",
        "\n",
        "- SVD always exists for any matrix $‚Ñù^{m√ón}$. Eigendecomposition only exists for non-defective square matrices.\n",
        "\n",
        "- $\\v{P}$ is not necessarily orthogonal, but $\\v{U},\\v{V}$ are always orthonormal.\n",
        "\n",
        "- Both are a pipeline of 3 operations: change coordinates to new basis in domain, independent scaling along new basis, and change basis again. Key difference is SVD domain/codomain can be vector spaces of different dimensions.\n",
        "\n",
        "- $\\v{V}$ are eigenvectors of $\\v{A}^‚ä§\\v{A}$. $\\v{U}$ are eigenvectors of $\\v{AA}^‚ä§$. $\\v{Œ£}$ non-zero diagonals are square roots of non-zero eigenvalues of both $\\v{AA}^‚ä§$ and $\\v{A}^‚ä§\\v{A}$.\n",
        "\n",
        "  - $\\v{Œ£}$ diagonal are all real and non-negative. That is not generally true for $\\v{D}$.\n",
        "\n",
        "- 4.12: $\\v{A}=\\BM1&-0.8\\\\0&1\\\\1&0\\EM$.\n",
        "Then $\\v{S}=\\v{A}^‚ä§\\v{A}=\\BM1&0&1\\\\-0.8&1&0\\EM\\BM1&-0.8\\\\0&1\\\\1&0\\EM=\\BM2&-0.8\\\\-0.8&1.64\\EM$.\n",
        "$\\v{S}-Œª\\v{I}=\\BM2-Œª&-0.8\\\\-0.8&1.64-Œª\\EM$\n",
        "$=2.64-3.64Œª+Œª^2$\n",
        "$‚áíŒª=\\/{3.64¬±\\sqrt{3.64^2-4(2.64)}}{2}=1,2.64$.\n",
        "$\\BM2-1&-0.8\\\\-0.8&1.64-1\\EM‚Üí\\BM1&-0.8\\\\0&0\\EM$.\n",
        "$E_1=\\span(\\BM4\\\\5\\EM)$.\n",
        "$\\BM2-2.64&-0.8\\\\-0.8&1.64-2.64\\EM‚Üí\\BM1&5/4\\\\0&0\\EM$. $E_{2.64}=\\span(\\BM5\\\\-4\\EM)$.\n",
        "Therefore $\\v{Œ£}=\\BM\\sqrt{2.64}&0\\\\0&1\\\\0&0\\EM$,\n",
        "$\\v{V}=\\BM0.78&0.63\\\\-0.63&0.78\\EM$.\n",
        "$\\v{u}_1=\\/{1}{1.62}\\v{Av}_1=\\BM0.79\\\\-0.39\\\\0.48\\EM$.\n",
        "$\\v{u}_2=\\v{Av}_2=\\BM0\\\\0.78\\\\0.63\\EM$.\n",
        "$\\v{A}^‚ä§=\\BM1&0&1\\\\-0.8&1&0\\EM‚Üí\\BM1&0&1\\\\0&1&4/5\\EM$.\n",
        "$\\null(\\v{A}^‚ä§)=\\span(\\BM5\\\\4\\\\-5\\EM)$.\n",
        "$\\v{u}_3=\\BM5\\\\4\\\\-5\\EM-\\v{u}_1\\v{u}_1^‚ä§\\BM5\\\\4\\\\-5\\EM-\\v{u}_2\\v{u}_2^‚ä§\\BM5\\\\4\\\\-5\\EM=\\BM0.62\\\\0.49\\\\-0.61\\EM$.\n",
        "Therefore\n",
        "$\\v{A}=\\BM0.79&0&0.62\\\\-0.39&0.78&0.49\\\\0.48&0.63&-0.61\\EM\\BM\\sqrt{2.64}&0\\\\0&1\\\\0&0\\EM\\BM0.78&-0.63\\\\0.63&0.78\\EM$\n",
        "\n",
        "- 4.13: $\\v{A}=\\BM1&0&1\\\\-2&1&0\\EM$.\n",
        "$\\v{A}^‚ä§\\v{A}=\\BM5&-2&1\\\\-2&1&0\\\\1&0&1\\EM$ with eigenbasis $E_6=\\span(\\BM5\\\\-2\\\\1\\EM)$, $E_1=\\span(\\BM0\\\\1\\\\2\\EM)$, and $E_0=\\span(\\BM-1\\\\-2\\\\1\\EM)$.\n",
        "Therefore $\\v{Œ£}=\\BM\\sqrt{6}&0&0\\\\0&1&0\\EM$,\n",
        "$\\v{V}=\\BM5/\\sqrt{30}&0&-1\\sqrt{6}\\\\-2/\\sqrt{30}&1/\\sqrt{5}&-2/\\sqrt{6}\\\\1/\\sqrt{30}&2/\\sqrt{5}&1/\\sqrt{6}\\EM$.\n",
        "$\\v{u}_1=\\/{1}{\\sqrt{6}}\\v{Av}_1=\\BM1/\\sqrt{5}\\\\-2/\\sqrt{5}\\EM$.\n",
        "$\\v{u}_2=\\v{Av}_2=\\BM2/\\sqrt{5}\\\\1/\\sqrt{5}\\EM$.\n",
        "$\\v{U}=\\/{1}{\\sqrt{5}}\\BM1&2\\\\-2&1\\EM$."
      ],
      "metadata": {
        "id": "f92zhXBq8gLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix approximations**: Let matrix $\\v{A}‚àà‚Ñù^{m√ón}$ have rank-$r$. The **rank-$k$ approximation** is $\\red{\\hat{\\v{A}}(k):=\\suml_{i=1}^kœÉ_i\\v{A}_i}$ such that $\\rk(\\hat{\\v{A}})=k$, and rank-1 matrices $\\v{A}_i:=\\v{u}_i\\v{v}_i^‚ä§$ are outer-products.\n",
        "\n",
        "- **Spectral norm** $\\vn{A}_2:=\\max_\\v{x}\\/{\\vn{Ax}_2}{\\vn{x}_2}$ measures the maximum norm amplification multiple of a vector by $\\v{A}$.\n",
        "\n",
        "  - $\\v{A}=\\sum_{i=1}^rœÉ_i\\v{A}_i$, then $\\vn{A}_2=œÉ_1$ its largest singular value is the greatest scale factor.\n",
        "  \n",
        "  - $\\n{\\hat{\\v{A}}(1)}_2=\\vn{A}_2$\n",
        "\n",
        "- **Eckart-Young theorem**: Let $\\v{A}‚àà‚Ñù^{m√ón}$ have rank-$r$, and let $\\v{B}‚àà‚Ñù^{m√ón}$ have rank-$k$. For $k‚â§r$ with $\\hat{\\v{A}}=\\sum_{i=1}^kœÉ_i\\v{u}_i\\v{v}_i^‚ä§$, it holds that $\\blue{\\hat{\\v{A}}(k)=\\arg\\min_{\\rk(\\v{B})=k}\\n{\\v{A}-\\v{B}}_2}$: $\\hat{\\v{A}}(k)$ is the least spectral norm error rank-$k$ approximation. Furthermore, $\\blue{\\n{\\v{A}-\\hat{\\v{A}}(k)}_2=œÉ_{k+1}}$.\n",
        "\n",
        "  - Proof ($\\n{\\v{A}-\\hat{\\v{A}}(k)}_2=œÉ_{k+1}$): The error matrix $\\v{A}-\\hat{\\v{A}}(k)=\\sum_{i=k+1}^rœÉ_i\\v{u}_i\\v{v}_i^‚ä§$ is also a sum of lower rank matrices. Then spectral norm of the error $\\n{\\v{A}-\\hat{\\v{A}}}_2=œÉ_{k+1}$ is its largest singular value.\n",
        "\n",
        "  - Grassmann formula: Let $V,W‚äÜ‚Ñù^n$. Then $\\dim(V+W)=\\dim(V)+\\dim(W)-\\dim(V‚à©W)$. If $\\dim(V)+\\dim(W)>n$ then $\\dim(V‚à©W)>0$.\n",
        "\n",
        "  - Proof ($\\n{\\v{A}-\\v{B}}_2‚â•œÉ_{k+1}$): To prove this, we only need to find one unit vector $\\v{z}$ that meets the worst-case scenario $\\n{(\\v{A}-\\v{B})\\v{z}}‚â•œÉ_{k+1}$. Then $\\v{Bz}=\\v{0}$ and $\\vn{Az}‚â•œÉ_{k+1}$, and $\\v{z}‚àà\\null(\\v{B})‚à©\\span(\\v{v}_1,...,\\v{v}_{k+1})$. Because $\\dim(\\null(\\v{B}))+\\dim(\\span(\\v{v}_1,...,\\v{v}_{k+1}))=(n-k)+(k+1)=n+1>n$, their intersection is non-empty $\\null(\\v{B})‚à©\\span(\\v{v}_1,...,\\v{v}_{k+1})\\neq‚àÖ$. Therefore unit vector $\\v{z}=c_1\\v{v}_1+...+c_{k+1}\\v{v}_{k+1}$ exists and\n",
        "  $\\n{(\\v{A}-\\v{B})\\v{z}}=\\vn{Az}=\\sqrt{\\sum_{i=1}^{k+1}œÉ_i^2c_i^2\\n{\\v{v}_i}^2}‚â•œÉ_{k+1}^2\\sqrt{\\sum_{i=1}^{k+1}c_i^2\\n{\\v{v}_i}^2}=œÉ_{k+1}$"
      ],
      "metadata": {
        "id": "R-5e1_xuSDMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 4.1: $\\v{A}=\\BM1&3&5\\\\2&4&6\\\\0&2&4\\EM$.\n",
        "$\\det(\\v{A})=2-24+20=-2$.\n",
        "\n",
        "- 4.2: $\\BM2&0&1&2&0\\\\2&-1&0&1&1\\\\0&1&2&1&2\\\\-2&0&2&-1&2\\\\2&0&0&1&1\\EM\\arr{1}$$\\BM2&0&1&2&0\\\\0&1&1&1&-1\\\\0&1&2&1&2\\\\0&0&3&1&2\\\\0&0&1&1&-1\\EM$$‚Üí\\BM2&0&1&2&0\\\\0&1&1&1&-1\\\\0&0&1&0&3\\\\0&0&3&1&2\\\\0&0&1&1&-1\\EM$$\\arr{1}\\BM2&0&1&2&0\\\\0&1&1&1&-1\\\\0&0&1&0&3\\\\0&0&0&-1&7\\\\0&0&0&-1&4\\EM‚Üí$$\\BM2&0&1&2&0\\\\0&1&1&1&-1\\\\0&0&1&0&3\\\\0&0&0&-1&7\\\\0&0&0&0&-3\\EM$. $\\det(\\v{A})=6$.\n",
        "\n",
        "- 4.3: $\\v{A}=\\BM1&0\\\\1&1\\EM‚Üí\\BM1-Œª&0\\\\1&1-Œª\\EM$. $Œª=1‚Üí\\BM0&0\\\\1&0\\EM$. $E_1=\\span(\\BM0\\\\-1\\EM)$.\n",
        "$\\v{B}=\\BM-2&2\\\\2&1\\EM‚Üí\\BM-2-Œª&2\\\\2&1-Œª\\EM$.\n",
        "$(-2-Œª)(1-Œª)-4=-6+Œª+Œª^2‚ÜíŒª=\\/{-1¬±\\sqrt{1+24}}{2}$.\n",
        "$Œª=-3‚Üí\\BM1&2\\\\2&4\\EM$$‚Üí\\BM1&2\\\\0&0\\EM$$‚ÜíE_{-3}=\\span(\\BM2\\\\-1\\EM)$.\n",
        "$Œª=2‚Üí\\BM-4&2\\\\2&-1\\EM$$‚Üí\\BM1&-1/2\\\\0&0\\EM$$‚ÜíE_{2}=\\span(\\BM-1/2\\\\-1\\EM)$\n",
        "\n",
        "- 4.6: $\\v{A}=\\BM2&3&0\\\\1&4&3\\\\0&0&1\\EM$$‚Üí\\BM2-Œª&3&0\\\\1&4-Œª&3\\\\0&0&1-Œª\\EM$.\n",
        "$\\det(\\v{A})=(2-Œª)(4-Œª)(1-Œª)-3(1-Œª)=(1-Œª)(Œª^2-6Œª+5)$\n",
        "$Œª=\\/{6¬±\\sqrt{36-20}}{2}=1,5$.\n",
        "$Œª=1‚Üí\\BM1&3&0\\\\1&3&3\\\\0&0&0\\EM$$‚Üí\\BM1&3&0\\\\0&0&0\\\\0&0&1\\EM$$‚ÜíE_1=\\span(\\BM3\\\\-1\\\\0\\EM)$.\n",
        "$Œª=5‚Üí\\BM-3&3&0\\\\1&-1&3\\\\0&0&-4\\EM$$‚Üí\\BM1&-1&0\\\\0&0&0\\\\0&0&1\\EM$$‚ÜíE_5=\\span(\\BM-1\\\\-1\\\\0\\EM)$.\n",
        "\n",
        "- 4.8: $\\v{A}=\\BM3&2&2\\\\2&3&-2\\EM$. Rank 2.\n",
        "$\\v{A}^‚ä§\\v{A}=\\BM13&12&2\\\\12&13&-2\\\\2&-2&8\\EM$.\n",
        "$E_9=\\span(\\BM1\\\\-1\\\\4\\EM)$.\n",
        "$E_{25}=\\span(\\BM1\\\\1\\\\0\\EM)$.\n",
        "$\\v{V}=\\BM1/\\sqrt{2}&1/\\sqrt{18}\\\\1/\\sqrt{2}&-1/\\sqrt{18}\\\\0&4/\\sqrt{18}\\EM$.\n",
        "$\\v{Œ£}=\\BM5&0\\\\0&3\\EM$.\n",
        "$\\v{u}_1=\\/{1}{5\\sqrt{2}}\\v{A}\\BM1\\\\1\\\\0\\EM=\\BM1/\\sqrt{2}\\\\1/\\sqrt{2}\\EM$.\n",
        "$\\v{u}_2=\\/{1}{3\\sqrt{18}}\\v{A}\\BM1\\\\-1\\\\4\\EM=\\BM1/\\sqrt{2}\\\\-1/\\sqrt{2}\\EM$.\n",
        "$\\v{A}=\\BM1/\\sqrt{2}&1/\\sqrt{2}\\\\1/\\sqrt{2}&-1/\\sqrt{2}\\EM\\BM5&0\\\\0&3\\EM\\BM1/\\sqrt{2}&1/\\sqrt{2}&0\\\\1/\\sqrt{18}&-1/\\sqrt{18}&4/\\sqrt{18}\\EM$"
      ],
      "metadata": {
        "id": "-WPnEzsdiV3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Vector Calculus\n",
        "\n",
        "- Numerator layout $‚àáf(x)(y-x)$"
      ],
      "metadata": {
        "id": "t96JCZrR4Ory"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jacobian gradients**: Differentiating a scalar with respect to a vector $f:‚Ñù^n‚Üí‚Ñù,\\v{x}‚Ü¶f(\\v{x}),\\v{x}‚àà‚Ñù^n$ gives the Jacobian vector $\\red{‚àá_\\v{x}f‚â°\\/{df}{d\\v{x}}‚â°\\/{‚àÇf}{‚àÇ\\v{x}}=\\BSM \\/{‚àÇf(\\v{x})}{‚àÇx_1}&...&\\/{‚àÇf(\\v{x})}{‚àÇx_n}\\ESM}‚àà‚Ñù^{1√ón}$ using **numerator layout**: If the vector in the derivative numerator $\\/{‚àÇ\\blue{\\v{x}}}{‚àÇt}$ then the Jacobian is a column matrix.\n",
        "If the vector is in the denominator $\\/{‚àÇf}{‚àÇ\\blue{\\v{x}}}$ then the Jacobian is a row matrix.\n",
        "In the Jacobian matrix, each column corresponds to a denominator element and each row corresponds to a numerator element.\n",
        "\n",
        "- Suppose $f:‚Ñù^n‚Üí‚Ñù$, then gradient $‚àáf(\\v{x})$ lives in $‚Ñù^n$. Imagine $‚Ñù^n$ is the ground, and $f$ is a surface floating in the air. Then $‚àáf(\\v{x}_0)$ is a vector parallel to the ground radiating from $\\v{x}_0$ toward the direction that results in the steepest ascent on the floating surface. The magnitude of $‚àáf(\\v{x}_0)$ is the rate of change in the steepest ascent at $\\v{x}_0$.\n",
        "\n",
        "  - Level set $f(\\v{x})=c$ is a contour parallel to the ground and also floating in the air through the surface. $‚àáf$ too is parallel to the ground with the level set, but it is perpendicular to the path of the contour.\n",
        "\n",
        "  - The tangent plane at $f(\\v{x}_0)$ has normal $(‚àáf(\\v{x}_0),-1)$.\n",
        "\n",
        "- $\\BC\n",
        "\\t{sum rule:}&\\/{‚àÇ}{‚àÇ\\v{x}}(f(\\v{x})+g(\\v{x}))=\\/{‚àÇf}{‚àÇ\\v{x}}(\\v{x})+\\/{‚àÇg}{‚àÇ\\v{x}}(\\v{x})\\\\\n",
        "\\t{product rule:}&\\/{‚àÇ}{‚àÇ\\v{x}}(f(\\v{x})g(\\v{x}))=(\\/{‚àÇf}{‚àÇ\\v{x}}g)(\\v{x})+(\\/{‚àÇg}{‚àÇ\\v{x}}f)(\\v{x})\\\\\n",
        "\\t{chain rule:}&\\/{‚àÇ}{‚àÇ\\v{x}}(g‚àòf)(\\v{x})=\\/{‚àÇ}{‚àÇ\\v{x}}g(f(\\v{x}))=\\/{‚àÇg}{‚àÇf}\\/{‚àÇf}{‚àÇ\\v{x}}\\\\\n",
        "\\EC$\n",
        "\n",
        "  - $f:‚Ñù^2‚Üí‚Ñù,(x_1(t),x_2(t))‚Ü¶f(\\v{x})$. Then $\\/{df}{dt}=\\/{‚àÇf}{‚àÇ\\v{x}}\\/{‚àÇ\\v{x}}{‚àÇt}=\\BSM\\/{‚àÇf}{‚àÇx_1}&\\/{‚àÇf}{‚àÇx_2}\\ESM\\BSM\\/{dx_1}{dt}\\\\\\/{dx_2}{dt}\\ESM$\n",
        "  $=\\/{‚àÇf}{‚àÇx_1}\\/{dx_1}{dt}+\\/{‚àÇf}{‚àÇx_2}\\/{dx_2}{dt}$\n",
        "\n",
        "  - $f:‚Ñù^2‚Üí‚Ñù,(x_1(s,t),x_2(s,t))‚Ü¶f(\\v{x})$. Then $\\/{df}{d(s,t)}=\\/{‚àÇf}{‚àÇ\\v{x}}\\/{‚àÇ\\v{x}}{‚àÇ(s,t)}=\\BSM\\/{‚àÇf}{‚àÇx_1}&\\/{‚àÇf}{‚àÇx_2}\\ESM\\BSM\\/{dx_1}{ds}&\\/{dx_1}{dt}\\\\\\/{dx_2}{ds}&\\/{dx_2}{dt}\\ESM$\n",
        "\n",
        "- **Vector-vector gradients**: $\\v{f}:‚Ñù^n‚Üí‚Ñù^m,\\v{x}‚Ü¶\\v{f}(\\v{x}),\\v{x}‚àà‚Ñù^n$, then $\\v{f}(\\v{x})=\\BSM f_1(\\v{x})\\\\\\vdots\\\\f_m(\\v{x})\\ESM$, and the **Jacobian** gradient matrix is $\\red{\\v{J}‚â°‚àá_\\v{x}\\v{f}‚â°\\/{d\\v{f}}{d\\v{x}}‚â°\\/{‚àÇ\\v{f}}{‚àÇ\\v{x}}=\\BSM\\/{‚àÇf_1}{‚àÇx_1}&...&\\/{‚àÇf_1}{‚àÇx_n}\\\\\\vdots&\\ddots&\\vdots\\\\\\/{‚àÇf_n}{‚àÇx_1}&...&\\/{‚àÇf_n}{‚àÇx_n}\\ESM}‚àà‚Ñù^{m√ón}$.\n",
        "\n",
        "- **Vector-matrix and matrix-matrix gradients**: Differentiating a vector with respect to a vector $\\v{f}:‚Ñù^n‚Üí‚Ñù^m$, or differentiating a scalar with respect to a matrix $\\v{f}:‚Ñù^{m√ón}‚Üí‚Ñù$ gives a 2-dimensional Jacobian matrix. Differentiating vector with respect to matrix $\\v{f}:‚Ñù^{p√óq}‚Üí‚Ñù^m$ gives 3-dimensional Jacobian tensor. Differentiating matrix with respect to matrix $\\v{f}:‚Ñù^{p√óq}‚Üí‚Ñù^{m√ón}$ gives a 4-dimensional tensor. Bad. Instead we vectorize the matrices (isomorphism) to stay in 2-dimensional.\n",
        "\n",
        "**Hessian**: second derivatives matrix requires one more dimension than Jacobian gradients. For $f:‚Ñù^n‚Üí‚Ñù$, the Hessian matrix is $\\red{\\v{H}‚â°‚àá_\\v{x}^2f‚â°\\/{‚àÇ^2f}{‚àÇ\\v{x}^2}=\\BSM\\/{‚àÇf}{‚àÇx_1^2}&...&\\/{‚àÇf}{‚àÇx_1‚àÇx_n}\\\\\\vdots&\\ddots&\\vdots\\\\\\/{‚àÇf}{‚àÇx_nx_1}&...&\\/{‚àÇf}{‚àÇx_n^2}\\ESM}‚àà‚Ñù^{n√ón}$\n",
        "\n",
        "- 5.6: $f(x,y)=(x+2y^3)^2$. Then $\\/{‚àÇf}{‚àÇx}=2(x+2y^3)$, and $\\/{‚àÇf}{‚àÇy}=2(x+2y^3)6y^2$.\n",
        "\n",
        "- 5.10: $h(t)=(f‚àòg)(t)$ where $f:‚Ñù^2‚Üí‚Ñù,\\v{x}‚Ü¶e^{x_1x_2^2}$ and $g:‚Ñù‚Üí‚Ñù^2,t‚Ü¶\\BM t\\cos t\\\\t\\sin t\\EM$.\n",
        "Then $\\/{dh}{dt}=\\/{‚àÇf}{‚àÇ\\v{x}}\\/{‚àÇ\\v{x}}{‚àÇt}$\n",
        "$=\\BM x_2^2e^{x_1x_2^2}&2x_1x_2e^{x_1x_2^2}\\EM\\BM \\cos t-t\\sin t\\\\\\sin t+t\\cos t\\EM$\n",
        "\n",
        "- 5.11: $\\v{y}=\\v{Œ¶Œ∏}$, where $\\v{Œ∏}‚àà‚Ñù^{D√ó1}$, $\\v{Œ¶}‚àà‚Ñù^{N√óD}$, and $\\v{y}‚àà‚Ñù^{N√ó1}$. Let $\\v{e}=\\v{y}-\\v{Œ¶Œ∏}$. Find $\\arg\\min_\\v{Œ∏}\\vn{e}^2$.\n",
        "$\\/{‚àÇ\\vn{e}^2}{‚àÇ\\v{Œ∏}}|_{1√ó2}=\\/{‚àÇ\\vn{e}^2}{‚àÇ\\v{e}}|_{1√óN}\\/{‚àÇ\\v{e}}{‚àÇ\\v{Œ∏}}|_{N√ó2}$\n",
        "$=-2\\v{e}^‚ä§\\v{Œ¶}=-2(\\v{y}^‚ä§-\\v{Œ∏}^‚ä§\\v{Œ¶}^‚ä§)\\v{Œ¶}=\\v{0}$\n",
        "$‚áí\\v{Œ∏}^‚ä§\\v{Œ¶}^‚ä§\\v{Œ¶}=\\v{y}^‚ä§\\v{Œ¶}$\n",
        "$‚áí\\v{Œ¶}^‚ä§\\v{Œ¶}\\v{Œ∏}=\\v{Œ¶}^‚ä§\\v{y}$\n",
        "$‚áí\\v{Œ∏}=(\\v{Œ¶}^‚ä§\\v{Œ¶})^{-1}\\v{Œ¶}^‚ä§\\v{y}$\n",
        "\n",
        "- 5.12: $\\v{f}=\\v{Ax}$, $\\v{f}‚àà‚Ñù^{M}$, $\\v{A}‚àà‚Ñù^{M√óN}$, $\\v{x}‚àà‚Ñù^{N}$. Then $\\/{d\\v{f}}{d\\v{A}}=\\BM‚àÇ_\\v{A}f_1\\\\\\vdots\\\\‚àÇ_\\v{A}f_n\\EM‚àà‚Ñù^{M√ó(M√óN)}$.\n",
        "Always first write out $f_i=\\sum_{j=1}^NA_{ij}x_j$\n",
        "$‚áí\\/{‚àÇf_i}{‚àÇA_{ij}}=x_j$\n",
        "$‚áí\\/{‚àÇf_i}{‚àÇA_{k:}}=\\BC \\v{x}^‚ä§&k=i\\\\\\v{0}^‚ä§&k\\neq i\\EC$\n",
        "$‚áí\\/{‚àÇf_i}{‚àÇ\\v{A}}=\\BM\\vdots\\\\\\v{0}^‚ä§\\\\\\v{x}^‚ä§\\\\\\v{0}^‚ä§\\\\\\vdots\\EM‚àà‚Ñù^{1√óM√óN}$.\n"
      ],
      "metadata": {
        "id": "fPhQ38f04abG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector/Matrix derivatives**:\n",
        "\n",
        "- $\\BC\n",
        "\\t{identity}&\\/{d}{d\\v{x}}\\v{x}=\\/{d}{d\\v{x}^‚ä§}\\v{x}=\\/{d}{d\\v{x}}\\v{x}^‚ä§=\\v{I}_n \\\\\n",
        "\\t{transpose}& d(\\v{x}^‚ä§)=(d\\v{x})^‚ä§\\\\\n",
        "&\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})^‚ä§=\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})\\right)^‚ä§ \\\\\n",
        "\\t{trace}& \\/{‚àÇ}{‚àÇ\\v{X}}\\tr(\\v{f}(\\v{X}))=\\tr(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X}))\\\\\n",
        "\\t{inner product}& \\/{‚àÇ}{‚àÇ\\v{X}}\\tr(\\v{Y}^‚ä§\\v{X})=\\v{Y}^‚ä§\\\\\n",
        "\\t{determinant}& \\/{‚àÇ}{‚àÇ\\v{X}}\\det(\\v{X})=\\det(\\v{X})\\v{X}^{-1} \\\\\n",
        "\\t{inverse}& \\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})^{-1}=-\\v{f}(\\v{X})^{-1}\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})\\right)\\v{f}(\\v{X})^{-1} \\\\\n",
        "& \\/{‚àÇ}{‚àÇ\\v{X}}(\\v{a}^‚ä§\\v{X}^{-1}\\v{b})=-(\\v{X}^{-1})^‚ä§\\v{ab}^‚ä§(\\v{X}^{-1})^‚ä§ \\\\\n",
        "\\t{dot product}& \\/{‚àÇ}{‚àÇ\\v{x}}(\\v{x}^‚ä§\\v{a})=\\/{‚àÇ}{‚àÇ\\v{x}}(\\v{a}^‚ä§\\v{x})=\\v{a}^‚ä§ \\\\\n",
        "\\t{norm}& \\/{‚àÇ}{‚àÇ\\v{x}}\\vn{x}^2=\\/{‚àÇ}{‚àÇ\\v{x}}(\\v{x}^‚ä§\\v{x})=2\\v{x}^‚ä§ \\\\\n",
        "\\t{inner product}& \\/{‚àÇ}{‚àÇ\\v{X}}(\\v{a}^‚ä§\\v{Xb})=\\v{ab}^‚ä§ \\\\\n",
        "\\t{quadratic form}& \\/{‚àÇ}{‚àÇ\\v{x}}(\\v{x}^‚ä§\\v{Bx})=\\v{x}^‚ä§(\\v{B}+\\v{B}^‚ä§) \\\\\n",
        "\\t{symmetric $\\v{W}$}& \\/{‚àÇ}{‚àÇ\\v{s}}(\\v{x}-\\v{As})^‚ä§\\v{W}(\\v{x}-\\v{As})=-2(\\v{x}-\\v{As})^‚ä§\\v{WA}\n",
        "\\EC$\n",
        "\n",
        "  - Proof (identity): $\\v{f}(\\v{x})=\\BM x_1\\\\\\vdots\\\\x_n\\EM$. Then $\\/{d\\v{f}}{d\\v{x}}=\\BM‚àÇ_{x_1}x_1&...&‚àÇ_{x_n}x_1\\\\\\vdots&\\ddots&\\vdots\\\\‚àÇ_{x_1}x_n&...&‚àÇ_{x_n}x_n\\EM$\n",
        "  $=\\BM1&...&0\\\\\\vdots&\\ddots&\\vdots\\\\0&...&1\\EM=\\v{I}_n$\n",
        "\n",
        "  - Proof (trace): Trace is a linear operator. $\\/{‚àÇ}{‚àÇ\\v{X}}\\tr(\\v{f}(\\v{X}))=\\/{‚àÇ}{‚àÇ\\v{X}}\\sum_{k=1}^n\\v{f}(\\v{X})_{kk}$\n",
        "  $=\\sum_{k=1}^n\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})_{kk}$\n",
        "\n",
        "  - Proof (matrix inner product): $\\/{‚àÇ}{‚àÇX_{ij}}\\tr(\\v{Y}^‚ä§\\v{X})=Y_{ij}$. Numerator layout results in vector transpose for gradients so $\\/{‚àÇ}{‚àÇX}\\tr(\\v{Y}^‚ä§\\v{X})=\\v{Y}^‚ä§$.\n",
        "\n",
        "  - Proof (determinant): Let $\\v{X}^{-1}=\\/{1}{\\det(\\v{X})}\\v{C}^‚ä§$, then $\\v{C}=\\det(\\v{X})(\\v{X}^{-1})^‚ä§$ is the cofactor of the Laplace expansion $\\det(\\v{X})=\\sum_{k=1}^nX_{ik}C_{ik}$ and $\\/{‚àÇ}{‚àÇX_{ij}}\\det(\\v{X})=C_{ij}$.\n",
        "\n",
        "  - Proof (inverse): $\\v{0}=\\/{‚àÇ}{‚àÇ\\v{X}}\\v{I}=\\/{‚àÇ}{‚àÇ\\v{X}}(\\v{f}(\\v{X})\\v{f}(\\v{X})^{-1})=\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})\\right)\\v{f}(\\v{X})^{-1}+\\v{f}(\\v{X})\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})^{-1}\\right)$\n",
        "  $‚áí\\v{f}(\\v{X})\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})^{-1}\\right)=-\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})\\right)\\v{f}(\\v{X})^{-1}$\n",
        "  $‚áí\\green{\\v{f}(\\v{X})^{-1}}\\v{f}(\\v{X})\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})^{-1}\\right)=-\\green{\\v{f}(\\v{X})^{-1}}\\left(\\/{‚àÇ}{‚àÇ\\v{X}}\\v{f}(\\v{X})\\right)\\v{f}(\\v{X})^{-1}$\n",
        "\n",
        "  - Proof (dot product): $\\/{‚àÇ}{‚àÇ\\v{x}}(\\v{x}^‚ä§\\v{a})=\\/{‚àÇ}{‚àÇ\\v{x}}(\\v{a}^‚ä§\\v{x})=\\/{‚àÇ}{‚àÇ\\v{x}}\\sum_ia_ix_i=\\BM a_1&...&a_n\\EM=\\v{a}^‚ä§$\n",
        "  \n",
        "  - Proof (norm): $f(\\v{x})=\\v{x}^‚ä§\\v{x}=\\sum_ix_i^2$. Then $‚àá_\\v{x}f=\\/{df}{d\\v{x}}=\\/{d}{d\\v{x}}\\sum_ix_i^2=\\BM2x_1&...&2x_n\\EM=2\\v{x}^‚ä§$\n",
        "\n",
        "  - Proof (inner product): $\\v{a}^‚ä§\\v{Xb}=\\sum_i\\sum_ja_iX_{ij}b_j‚áí\\/{‚àÇ(\\v{a}^‚ä§\\v{Xb})}{‚àÇX_{ij}}=a_ib_j$\n",
        "  $‚áí\\/{‚àÇ}{‚àÇ\\v{X}}(\\v{a}^‚ä§\\v{Xb})=\\v{ab}^‚ä§$ outer product.\n",
        "\n",
        "  - Proof (quadratic form): $\\v{x}^‚ä§\\v{Bx}=\\sum_i\\sum_jx_iB_{ij}x_j‚áí\\/{‚àÇ(\\v{x}^‚ä§\\v{Bx})}{‚àÇx_k}=\\/{‚àÇ}{‚àÇx_k}(\\sum_jx_kB_{kj}x_j+\\sum_ix_iB_{ik}x_k)$\n",
        "  $=\\sum_jB_{kj}x_j+\\sum_ix_iB_{ik}$\n",
        "  $=\\v{x}^‚ä§(\\v{b}^‚ä§)_{k}+\\v{x}^‚ä§\\v{b}_k$\n",
        "  where $(\\v{b}^‚ä§)_k,\\v{b}_k$ are the $k$-th column of $\\v{B}^‚ä§,\\v{B}$.\n",
        "\n",
        "  - Proof (symmetric $\\v{W}$): $\\/{‚àÇ}{‚àÇ\\v{s}}(\\v{x}-\\v{As})^‚ä§\\v{W}(\\v{x}-\\v{As})=\\/{‚àÇ}{‚àÇ\\v{u}}(\\v{u}^‚ä§\\v{W}\\v{u})\\/{‚àÇ\\v{u}}{‚àÇ\\v{s}}$\n",
        "  $=-\\v{u}^‚ä§(\\v{W}+\\v{W}^‚ä§)\\v{A}$\n"
      ],
      "metadata": {
        "id": "FM0IMQX7APnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taylor polynomial** of $n$-degree $\\red{T_n(x)=\\suml_{k=0}^n\\/{f^{(k)}(x_0)}{k!}(x-x_0)^k}$ approximates $f:‚Ñù‚Üí‚Ñù,f‚ààC^n$ in a neighborhood around $x_0$. If Taylor series $T_‚àû(x)=f(x)$ then $f$ is called analytic.\n",
        "\n",
        "- Consider $f:‚Ñù^D‚Üí‚Ñù,\\v{x}‚Ü¶f(\\v{x}),\\v{x}‚àà‚Ñù^D$ that is smooth at $\\v{x}_0$. Let $\\v{Œ¥}=\\v{x}-\\v{x}_0$, then the $n$-degree Taylor polynomial in the neighborhood of $\\v{x}_0$ is $\\red{T_n(\\v{x})=\\sum_{k=0}^n\\/{‚ü®‚àá_\\v{x}^kf(\\v{x}_0),\\v{Œ¥}^k‚ü©}{k!}}$, where $‚àá_\\v{x}^kf(\\v{x}_0)$ is the $k$-th derivative $k$-dimensional tensor of $f$ with respect to $\\v{x}$ evaluated at $\\v{x}_0$, and $\\v{Œ¥}^k‚àà‚Ñù^{D√ó...√óD}$ is the $k$-dimensional outer product tensor.\n",
        "\n",
        "  - $\\v{Œ¥}^2=\\v{Œ¥}‚äó\\v{Œ¥}‚àà‚Ñù^{D√óD}$ where $(\\v{Œ¥}^2)_{i,j}=Œ¥_iŒ¥_j$\n",
        "\n",
        "  - $\\blue{‚ü®‚àá_\\v{x}^kf(\\v{x}_0),\\v{Œ¥}^k‚ü©}=\\sum_{i_1=1}^D...\\sum_{i_k=1}^D‚àá_\\v{x}^kf(\\v{x}_0)_{i_1,...,i_k}Œ¥_{i_1}...Œ¥_{i_k}‚àà‚Ñù$ the Taylor polynomial ultimately sums up to a single scalar, so these tensor multiplications must be inner products.\n",
        "\n",
        "  - $\\BC\n",
        "  k=0& ‚ü®‚àá_\\v{x}^0f(\\v{x}_0)\\v{Œ¥}^0‚ü©&=f(\\v{x}_0) \\\\\n",
        "  k=1& ‚ü®‚àá_\\v{x}^1f(\\v{x}_0)\\v{Œ¥}^1‚ü©&=\\ub{‚àá_\\v{x}f(\\v{x}_0)}{1√óD}\\ub{(\\v{Œ¥})}{D√ó1}&=\\sum_{i=1}^D(‚àá_\\v{x}f(\\v{x}_0))_iŒ¥_i‚àà‚Ñù\\\\\n",
        "  k=2& ‚ü®‚àá_\\v{x}^2f(\\v{x}_0)\\v{Œ¥}^2‚ü©&=\\tr(\\ub{‚àá_\\v{x}^2f(\\v{x}_0)}{D√óD}\\ub{(\\v{Œ¥}‚äó\\v{Œ¥})}{D√óD})=\\v{Œ¥}^‚ä§‚àá_\\v{x}^2f(\\v{x}_0)\\v{Œ¥}&=\\sum_{i=1}^D\\sum_{j=1}^D(‚àá_\\v{x}^2f(\\v{x}_0))_{ij}Œ¥_iŒ¥_j‚àà‚Ñù\\\\\n",
        "  \\EC$\n",
        "\n",
        "- 5.15: $f(x,y)=x^2+2xy+y^3$ at $\\v{x}_0=(1,2)$. Then $\\v{Œ¥}=\\BM x-1\\\\y-2\\EM$ and $\\BC\n",
        "k=1& ‚àá_\\v{x}f=\\BM 2x+2y&2x+3y^2\\EM=\\BM6&14\\EM\\\\\n",
        "k=2& ‚àá_\\v{x}^2f=\\BM 2&2\\\\2&6y\\EM=\\BM2&2\\\\2&12\\EM\\\\\n",
        "\\EC$"
      ],
      "metadata": {
        "id": "1vRFO-VbeEgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Vector Probability"
      ],
      "metadata": {
        "id": "mHFseZC0wEXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given probability space $(Œ©,\\m{A},P)$ with sample space, event space, and probability measure, random variable is function $X:Œ©‚Üí\\m{T}$ where $\\m{T}$ is the \"target space\" (support) of $X$ whose elements are \"states\".\n",
        "\n",
        "- Function $p:‚Ñù^D‚Üí‚Ñù$ is a multivariate PDF if $p(\\v{x})‚â•0$ for all $\\v{x}‚àà‚Ñù^D$ and $‚à´_{‚Ñù^D}p(\\v{x})\\ d\\v{x}=1$.\n",
        "\n",
        "  - $\\v{x}‚àà‚Ñù^D$ represents one data point, whereas $\\v{x}_1,...,\\v{x}_N$ is a size-$N$ random sample.\n",
        "\n",
        "- Function $F_X(\\v{x})=P(X_1‚â§x_1,...,X_D‚â§x_D)=‚à´_{-‚àû}^{x_1}...‚à´_{-‚àû}^{x_D}p(z_1,...,z_D)\\ dz_1...dz_D$ is CDF for multivariate random variable $X=\\BM X_1\\\\\\vdots\\\\X_D\\EM$ with states $\\v{x}=\\BM x_1\\\\\\vdots\\\\x_D\\EM$\n",
        "\n",
        "- **Sum rule**: marginal distribution $p(\\v{x})=‚à´_Yp(\\v{x},\\v{y})\\ d\\v{y}=‚à´_Yp(\\v{x}|\\v{y})p(\\v{y})\\ d\\v{y}=\\E_Y[p(\\v{x}|\\v{y})]$\n",
        "\n",
        "  - LOTP $P(X)=\\sum_yP(X|Y=y)P(Y=y)=\\E[P(X|Y)]$\n",
        "\n",
        "- **Product rule**: conditional distribution $p(\\v{x},\\v{y})=p(\\v{y}|\\v{x})p(\\v{x})$\n",
        "\n",
        "**Negative log-likelihood**: In supervised learning, we have $(\\v{x}_1,y_1),...,(\\v{x}_N,y_N)$ iid with $\\v{x}_n‚àà‚Ñù^D$ and labels $y_n‚àà‚Ñù$. Given likelihood $p(y_n|\\v{x}_n,\\v{Œ∏})$, negative log-likelihood $l(\\v{Œ∏})=-\\ln p(y_n|\\v{x}_n,\\v{Œ∏})$ is function of $\\v{Œ∏}$ with $\\v{x}_n$ and $y_n$ fixed.\n",
        "\n",
        "- Split the dataset $\\v{y}=[y_1,...,y_N]^‚ä§$ and $\\v{X}=[\\v{x}_1,...,\\v{x}_N]^‚ä§$. The likelihood is $p(\\v{y}|\\v{X},\\v{Œ∏})=\\prodl_{n=1}^Np(y_n|\\v{x}_n,\\v{Œ∏})$ and negative log-likelihood $l(\\v{Œ∏})=-\\suml_{n=1}^N\\ln p(y_n|\\v{x}_n,\\v{Œ∏})$.\n",
        "\n",
        "- **Likelihood function** $p(y_n|\\v{x}_n,\\v{Œ∏})$ is a normalized probability distribution with respect to $y_n$, but it does not integrate to 1 with respect to $\\v{Œ∏}$, and may not even be integrable with respect to $\\v{Œ∏}$.\n",
        "\n",
        "**Bayes theorem**: $\\red{p(\\v{Œ∏}|\\v{x})=\\/{p(\\v{x}|\\v{Œ∏})p(\\v{Œ∏})}{p(\\v{x})}}$ making inference about unobserved latent random variables $\\v{Œ∏}$ based observations of other random variables $\\v{x}$, prior knowledge $p(\\v{Œ∏})$, and relationship between them $p(\\v{x}|\\v{Œ∏})$\n",
        "\n",
        "- Prior $p(\\v{Œ∏})$ encapsulates our subjective prior knowledge of the latent $\\v{Œ∏}$\n",
        "\n",
        "- Likelihood $p(\\v{x}|\\v{Œ∏})$ how likely it is to observe value $\\v{x}$, assuming our latent variable value is $\\v{Œ∏}$. This is analogous as classical likelihood function of $\\v{Œ∏}$.\n",
        "\n",
        "- Posterior $p(\\v{Œ∏}|\\v{x})$ what we believe is the distribution of $\\v{Œ∏}$ after observing value $\\v{x}$\n",
        "\n",
        "- Evidence $p(\\v{x})=‚à´_Œò(\\v{x}|\\v{Œ∏})p(\\v{Œ∏})\\ d\\v{Œ∏}=\\E_\\v{Œ∏}[p(\\v{x}|\\v{Œ∏})]$ is the LOTP probability of $\\v{x}$ weighted by prior $p(\\v{Œ∏})$. It also acts as normalizing constant to ensure posterior sums to 1.\n",
        "\n",
        "- **Maximum A Posteriori** (MAP) $\\v{Œ∏}^*=\\arg\\sup_\\v{Œ∏}p(\\v{Œ∏}|\\v{x})=\\arg\\sup_\\v{Œ∏}\\/{p(\\v{x}|\\v{Œ∏})p(\\v{Œ∏})}{p(\\v{x})}=\\arg\\sup_\\v{Œ∏}p(\\v{x}|\\v{Œ∏})p(\\v{Œ∏})$, which drops out the difficult integral $p(\\v{x})$.\n",
        "\n",
        "  - MLE assumes parameters $\\v{Œ∏}$ are fixed, and it's classically determined from likelihood function $p(\\v{Œ∏}|\\v{x})$. MAP is the Bayesian analog determined from $p(\\v{Œ∏}|\\v{x})‚àùp(\\v{x}|\\v{Œ∏})p(\\v{Œ∏})$.\n",
        "\n",
        "  - From $\\v{Œ∏}^*$, we make predictions $p(\\v{x}|\\v{Œ∏}^*)$. This again turns the problem into a fixed parameter inference.\n",
        "\n",
        "- **Bayesian inference**: Let $\\v{X}=[\\v{x}_1,...,\\v{x}_N]^‚ä§$ be data and $\\v{x}$ be a data point. The goal is to compute posterior $p(\\v{Œ∏}|\\v{X})=\\/{p(\\v{X}|\\v{Œ∏})p(\\v{Œ∏})}{p(\\v{X})}$ and use that to inference $p(\\v{x})=\\E_\\v{Œ∏}[p(\\v{x}|\\v{Œ∏})]$ averages all plausible parameters.\n",
        "\n",
        "  - Computation is difficult. Unless using conjugate priors, we need to use approximations: MCMC, Laplace approximation, variational inference, and expectation propagation.\n",
        "\n",
        "- **Latent variables**: we assume $p(\\v{x}|z,\\v{Œ∏})$ for unobserved $z$, which needs to be marginalized out in likelihood $p(\\v{x}|\\v{Œ∏})=\\E_z[p(\\v{x}|z,\\v{Œ∏})]$. Expectation maximization (EM) is used to handle latent variables."
      ],
      "metadata": {
        "id": "MrUUmbsBbELC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean and covariance**: $\\E_X$ does not mean \"expectation parameterized by value $X$\", as per statistics texts. In Bayesian texts $\\E_X$ means expectation taken with respect to the PDF of $X$.\n",
        "\n",
        "- $\\E_X[g(x)]=‚à´_\\m{X}g(x)p(x)\\ dx$.\n",
        "\n",
        "  - $\\E_X[g(\\v{x})]=\\BSM E_{X_1}[g(x_1)]\\\\\\vdots\\\\E_{X_D}[g(x_D)]\\ESM$. $\\E_X[\\v{x}]=\\BSM \\E_{X_1}[x_1]\\\\\\vdots\\\\\\E_{X_D}[x_D]\\ESM$ where\n",
        "  $\\E_{X_i}[x_i]=‚à´_\\m{X}x_ip(x_i)\\ dx_i$\n",
        "\n",
        "- $\\Cov_{X,Y}[x,y]=\\E_{X,Y}[(x-\\E_X[x])(y-\\E_Y[y])]=\\E_{X,Y}[xy]-\\E_X[x]\\E_Y[y]$\n",
        "\n",
        "  - $\\red{\\v{Œ£}_\\v{xy}:=\\Cov[\\v{x},\\v{y}]=\\E[(\\v{x}-\\E[\\v{x}])(\\v{y}-\\E[\\v{y}])^‚ä§]=\\E[\\v{xy}^‚ä§]-\\E[\\v{x}]\\E[\\v{y}]^‚ä§=\\Cov[\\v{y},\\v{x}]^‚ä§}‚àà‚Ñù^{D√óE}$\n",
        "  where $\\v{x}‚àà‚Ñù^D$ and $\\v{y}‚àà‚Ñù^E$\n",
        "\n",
        "  - Covariance matrix $\\red{\\v{Œ£}_\\v{xx}:=\\V(\\v{x})=\\Cov[\\v{x},\\v{x}]}=\\E[\\v{xx}^‚ä§]-\\E[\\v{x}]\\E[\\v{x}]^‚ä§‚àà‚Ñù^{D√óD}$ is symmetric and positive-semidefinite, but assumed positive-definite (for $N$ data points of $\\v{x}$).\n",
        "\n",
        "  - $\\Corr[x,y]=\\/{\\Cov[x,y]}{\\sqrt{\\V[x]\\V[y]}}$. Correlation matrix is covariance matrix applied to standardized $(x-\\E[x])/œÉ_x$.\n",
        "\n",
        "  - $\\BC\n",
        "  \\blue{\\V[\\v{x}+\\v{y}]=\\V[\\v{x}]+\\V[\\v{y}]+\\Cov[\\v{x},\\v{y}]+\\Cov[\\v{y},\\v{x}]}\\\\\n",
        "  \\V[\\v{x}-\\v{y}]=\\V[\\v{x}]+\\V[\\v{y}]-\\Cov[\\v{x},\\v{y}]-\\Cov[\\v{y},\\v{x}]\\EC$\n",
        "\n",
        "- **Information geometry**: $\\V[x+y]=\\V[x]+\\V[y]+2\\Cov[x,y]$. If $\\Cov[x,y]=0$ then $\\V[x+y]=\\V[x]+\\V[y]$ thereby establishing Pythagorean setup where $\\red{‚ü®X,Y‚ü©:=\\Cov[x,y]}$ (Eaton 2007).\n",
        "\n",
        "  - $\\blue{\\n{X}=\\sqrt{\\V[x]}=œÉ_X}$\n",
        "\n",
        "  - $\\blue{\\cos œâ=\\/{‚ü®X,Y‚ü©}{\\n{X}\\n{Y}}=\\/{\\Cov[x,y]}{\\sqrt{\\V[x]\\V[y]}}=\\Corr[x,y]}$\n",
        "\n",
        "  - $\\blue{X\\perp Y ‚áî ‚ü®X,Y‚ü©=0 ‚áî \\Cov[x,y]=0}$\n",
        "\n",
        "- **Affine transformation**: Consider $X$ with $\\v{Œº}=\\E[\\v{x}]$ and $\\v{Œ£}=\\Cov[\\v{x},\\v{x}]$. Let $\\v{y}=\\v{Ax}+\\v{b}$ where $\\v{A}$ and $\\v{b}$ are known.\n",
        "Then $\\BC\n",
        "\\E_Y[\\v{y}]=\\v{AŒº}+\\v{b}&\\E_Y[\\v{y}]=\\E_X[\\v{Ax}+\\v{b}]=\\v{A}\\E_X[\\v{x}]+\\v{b}=\\v{AŒº}+\\v{b} \\\\\n",
        "\\V_Y[\\v{y}]=\\v{AŒ£A}^‚ä§&\\V_Y[\\v{y}]=\\V_X[\\v{Ax}+\\v{b}]=\\v{A}\\V_X[\\v{x}]\\v{A}^‚ä§=\\v{AŒ£A}^‚ä§ \\\\\n",
        "\\Cov\\small[\\v{x},\\v{y}]=\\v{Œ£A}^‚ä§&\\Cov\\small[\\v{x},\\v{y}]=\\E[\\v{x}(\\v{Ax}+\\v{b})^‚ä§]-\\E[\\v{x}]\\E[\\v{Ax}+\\v{b}]‚ä§\\\\\n",
        "&=\\E[\\v{x}\\v{x}^‚ä§]\\v{A}^‚ä§+\\E[\\v{x}]\\v{b}^‚ä§-\\E[\\v{x}]\\E[\\v{x}]^‚ä§\\v{A}^‚ä§-\\E[\\v{x}]\\v{b}^‚ä§ \\\\\n",
        "&=\\E[\\v{x}\\v{x}^‚ä§]\\v{A}^‚ä§-\\E[\\v{x}]\\E[\\v{x}]^‚ä§\\v{A}^‚ä§ \\\\\n",
        "&=\\v{Œ£A}^‚ä§\n",
        "\\EC$\n",
        "\n",
        "**Empirical data**: Consider random sample $n=1,...,N$ where $\\v{x}_n‚àà‚Ñù^D$ is a data point\n",
        "\n",
        "- $\\bar{\\v{x}}=\\/{1}{N}\\sum_{n=1}^N\\v{x}_n$ and the empirical covariance matrix is $\\red{\\v{Œ£}=\\/{1}{N}\\sum_{n=1}^N(\\v{x}_n-\\bar{\\v{x}})(\\v{x}_n-\\bar{\\v{x}})^‚ä§}‚àà‚Ñù^{D√óD}$\n",
        "\n",
        "- $\\V[x]=\\E[(x-\\E[x])^2]$ requires two passes. $\\V[x]=\\E[x^2]-(\\E[x])^2$ requires one pass, but numerically unstable. Empirically use pairwise difference $\\red{\\/{1}{N^2}\\sum_{i,j=1}^N(x_i-x_j)^2}=2[\\/{\\sum_ix_i^2}{N}-(\\/{\\sum_ix_i}{N})^2]$.\n",
        "\n",
        "  - Proof: $\\/{1}{N^2}\\sum_{i,j=1}^N(x_i-x_j)^2$\n",
        "  $=\\/{1}{N^2}\\sum_{i,j=1}^N(x_i^2+x_j^2-2x_ix_j)$\n",
        "  $=\\/{1}{N^2}\\sum_{i,j=1}^Nx_i^2+\\/{1}{N^2}\\sum_{i,j=1}^Nx_j^2$\n",
        "  $-\\/{2}{N^2}\\sum_{i,j=1}^Nx_ix_j$\n",
        "  $=\\/{2}{N}\\sum_{i=1}^Nx_i^2-\\/{2}{N^2}(\\sum_{i=1}^Nx_i)(\\sum_{j=1}^Nx_j)$\n",
        "  $=2[\\/{\\sum_ix_i^2}{N}-(\\/{\\sum_ix_i}{N})^2]$\n"
      ],
      "metadata": {
        "id": "TiUJCI9ejoEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaussian distribution** $X‚àº\\Normal(\\v{Œº},\\v{Œ£})$ then $\\red{p(\\v{x}|\\v{Œº},\\v{Œ£})=\\/{1}{\\sqrt{(2œÄ)^D\\det|\\v{Œ£}|}}\\e{-\\/{(\\v{x}-\\v{Œº})^‚ä§\\v{Œ£}^{-1}(\\v{x}-\\v{Œº})}{2}}}$\n",
        "\n",
        "- Consider $\\Normal(\\v{x}|\\v{Œº}_\\v{x},\\v{Œ£}_\\v{xx})$ and $\\Normal(\\v{y}|\\v{Œº}_\\v{y},\\v{Œ£}_\\v{yy})$ with dimensions $\\v{x}‚àà‚Ñù^D$ and $\\v{y}‚àà‚Ñù^E$. Then we concatenate them $\\BSM\\v{x}\\\\\\v{y}\\ESM‚àà‚Ñù^{D+E}$ and $p(\\v{x},\\v{y})=\\Normal(\\BSM\\v{Œº}_\\v{x}\\\\\\v{Œº}_\\v{y}\\ESM,\\BSM\\v{Œ£}_\\v{xx}&\\v{Œ£}_\\v{xy}\\\\\\v{Œ£}_\\v{yx}&\\v{Œ£}_\\v{yy}\\ESM)$ where $\\v{Œ£}_\\v{xy}=\\Cov[\\v{x},\\v{y}]‚àà‚Ñù^{D√óE}$.\n",
        "Then ignoring normalizing constants $p(\\v{x},\\v{y})‚àù\\e{-\\/{1}{2}\\BSM\\v{x}-\\v{Œº}_\\v{x}\\\\\\v{y}-\\v{Œº}_\\v{y}\\ESM^‚ä§\\BSM\\v{Œ£}_\\v{xx}&\\v{Œ£}_\\v{xy}\\\\\\v{Œ£}_\\v{yx}&\\v{Œ£}_\\v{yy}\\ESM^{-1}\\BSM\\v{x}-\\v{Œº}_\\v{x}\\\\\\v{y}-\\v{Œº}_\\v{y}\\ESM}$.\n",
        "\n",
        "  - Marginal $p(\\v{x})=‚à´_\\m{Y}p(\\v{x},\\v{y})\\ d\\v{y}=\\Normal(\\v{x}|\\v{Œº}_\\v{x},\\v{Œ£}_\\v{xx})$, where notation $\\Normal(\\v{x}|\\v{Œº},\\v{Œ£}):=p(\\v{x}|\\v{Œº},\\v{Œ£})$\n",
        "\n",
        "- Conditional $p(\\v{x}|\\v{y})=\\Normal(\\v{Œº}_{\\v{x}|\\v{y}},\\v{Œ£}_{\\v{x}|\\v{y}})$ where $\\blue{\\BC\n",
        "\\v{Œº}_{\\v{x}|\\v{y}}=\\v{Œº}_\\v{x}+\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}(\\v{y}-\\v{Œº}_\\v{y}) \\\\\n",
        "\\v{Œ£}_{\\v{x}|\\v{y}}=\\v{Œ£}_\\v{xx}-\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}\\v{Œ£}_\\v{yx}\n",
        "\\EC}$\n",
        "\n",
        "  - Proof: Let $\\BM \\v{d}_\\v{x}\\\\\\v{d}_\\v{y}\\EM=\\BM\\v{x}-\\v{Œº}_\\v{x}\\\\\\v{y}-\\v{Œº}_\\v{y}\\EM$.\n",
        "  Define inverse\n",
        "  $\\BM\\v{Œ£}_\\v{xx}&\\v{Œ£}_\\v{xy}\\\\\\v{Œ£}_\\v{yx}&\\v{Œ£}_\\v{yy}\\EM\\BM\\v{Œõ}_\\v{xx}&\\v{Œõ}_\\v{xy}\\\\\\v{Œõ}_\\v{yx}&\\v{Œõ}_\\v{yy}\\EM=\\BM\\v{I}&\\v{0}\\\\\\v{0}&\\v{I}\\EM$.\n",
        "  Then $p(\\v{x},\\v{y})‚àù\\e{-\\/{1}{2}\\BM \\v{d}_\\v{x}^‚ä§&\\v{d}_\\v{y}^‚ä§\\EM\\BM\\v{Œõ}_\\v{xx}&\\v{Œõ}_\\v{xy}\\\\\\v{Œõ}_\\v{yx}&\\v{Œõ}_\\v{yy}\\EM\\BM \\v{d}_\\v{x}\\\\\\v{d}_\\v{y}\\EM}$\n",
        "  $=\\e{-\\/{1}{2}\\small(\\v{d}_\\v{x}^‚ä§\\v{Œõ}_\\v{xx}\\v{d}_\\v{x}+2\\v{d}_\\v{x}^‚ä§\\v{Œõ}_\\v{xy}\\v{d}_\\v{y}+\\v{d}_\\v{y}^‚ä§\\v{Œõ}_\\v{yy}\\v{d}_\\v{y})}$\n",
        "  $=p(\\v{x}|\\v{y})p(\\v{y})$.\n",
        "  $\\BC\n",
        "  p(\\v{x}|\\v{y})\\t{ quadratic form}&=-\\/{1}{2}(\\v{x}-\\v{Œº}_{\\v{x}|\\v{y}})^‚ä§\\v{Œ£}_{\\v{x}|\\v{y}}^{-1}(\\v{x}-\\v{Œº}_{\\v{x}|\\v{y}})\\\\\n",
        "  &=-\\/{1}{2}(\\green{\\v{d}_\\v{x}-\\v{m}})^‚ä§\\green{\\v{P}}(\\v{d}_\\v{x}-\\v{m})\\\\\n",
        "  &=-\\/{1}{2}(\\v{d}_\\v{x}^‚ä§\\v{P}\\v{d}_\\v{x}-2\\v{d}_\\v{x}^‚ä§\\v{Pm}+\\t{const})\\\\\n",
        "  p(\\v{x},\\v{y})\\t{ quadratic form}&=-\\/{1}{2}(\\v{d}_\\v{x}^‚ä§\\v{Œõ}_\\v{xx}\\v{d}_\\v{x}+2\\v{d}_\\v{x}^‚ä§\\v{Œõ}_\\v{xy}\\v{d}_\\v{y}+\\t{const}) \\\\\n",
        "  \\EC$\n",
        "  $‚áí\\BC \\v{P}=\\v{Œõ}_\\v{xx}\\\\\\v{m}=-\\v{Œõ}_\\v{xx}^{-1}\\v{Œõ}_\\v{xy}\\v{d}_\\v{y}\\EC$.\n",
        "  Therefore $\\v{Œ£}_{\\v{x}|\\v{y}}=\\v{P}^{-1}=\\v{Œ£}_\\v{xx}-\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}\\v{Œ£}_\\v{yx}$ and\n",
        "  $\\v{Œº}_{\\v{x}|\\v{y}}=\\v{Œº}_\\v{x}+\\v{m}=\\v{Œº}_\\v{x}-\\v{Œõ}_\\v{xx}^{-1}\\v{Œõ}_\\v{xy}(\\v{y}-\\v{Œº}_\\v{y})$\n",
        "  $=\\v{Œº}_\\v{x}+\\v{Œ£}_\\v{xy}\\v{Œ£}_\\v{yy}^{-1}(\\v{y}-\\v{Œº}_\\v{y})$.\n",
        "\n",
        "- **Sum of independent Gaussians**: $a\\Normal(\\v{Œº}_\\v{x},\\v{Œ£}_\\v{xx})+b\\Normal(\\v{Œº}_\\v{y},\\v{Œ£}_\\v{yy})‚àº\\Normal(a\\v{Œº}_\\v{x}+b\\v{Œº}_\\v{y},a^2\\v{Œ£}_\\v{xx}+b^2\\v{Œ£}_\\v{yy})$.\n",
        "\n",
        "  - Linear transformation: $\\v{A}\\Normal(\\v{x}|\\v{Œº},\\v{Œ£})‚àº\\Normal(\\v{AŒº},\\v{AŒ£A}^‚ä§)$\n",
        "\n",
        "  - Affine transformation: $\\v{A}\\Normal(\\v{x}|\\v{Œº},\\v{Œ£})+\\v{b}‚àº\\Normal(\\v{AŒº}+\\v{b},\\v{AŒ£A}^‚ä§)$\n",
        "\n",
        "- **Prior/likelihood convolution**: $\\blue{‚à´\\Normal(\\v{x}|\\v{Œº},\\v{A})\\Normal(\\v{y}|\\v{x},\\v{B})\\ d\\v{x}=\\Normal(\\v{y}|\\v{Œº},\\v{A}+\\v{B})}$\n",
        "\n",
        "  - Proof: $p(\\v{y})=‚à´p(\\v{x},\\v{y})\\ d\\v{x}=‚à´p(\\v{y}|\\v{x})p(\\v{x})\\ d\\v{x}=\\Normal(\\v{y}|\\v{x},\\v{B})‚à´\\Normal(\\v{x}|\\v{Œº},\\v{A})\\ d\\v{x}$ The marginal is the convolution of prior and likelihood. This is equivalent to $\\BC\n",
        "  \\v{y}=\\v{x}+\\v{b},&\\v{b}‚àº\\Normal(\\v{0},\\v{B})\\\\\n",
        "  \\v{x}=\\v{Œº}+\\v{a},&\\v{a}‚àº\\Normal(\\v{0},\\v{A})\n",
        "  \\EC‚áí\\v{y}=\\v{Œº}+(\\v{a}+\\v{b})$.\n",
        "  Therefore $p(\\v{y})=\\Normal(\\v{y}|\\v{Œº},\\v{A}+\\v{B})$.\n",
        "\n",
        "  - $\\v{y}=\\v{Ax}+\\v{b}+\\v{w}$ where $\\v{w}‚àº\\Normal(\\v{0},\\v{Q})$ and $\\v{x}‚àº\\Normal(\\v{Œº},\\v{Œ£})$. Then $\\BC\n",
        "  \\t{by integral}&p(\\v{y})=‚à´p(\\v{y}|\\v{x})p(\\v{x})\\ d\\v{x}=‚à´\\Normal(\\v{y}|\\v{Ax+b},\\v{Q})\\Normal(\\v{x}|\\v{Œº},\\v{Œ£})\\ d\\v{x}\\\\\n",
        "  \\t{break up $\\v{x}$}&\\v{x}=\\v{Œº}+\\v{u},\\t{ where }\\v{u}‚àº\\Normal(\\v{0},\\v{Œ£})\\\\\n",
        "  &\\v{y}=\\v{AŒº}+\\v{Au}+\\v{b}+\\v{w}\\\\\n",
        "  \\t{finally}&p(\\v{y})=\\Normal(\\v{y}|\\v{AŒº}+\\v{b},\\v{AŒ£A}^‚ä§+\\v{Q})\n",
        "  \\EC$\n",
        "\n",
        "- **Product of Gaussians**: $\\Normal(\\v{x}|\\v{a},\\v{A})\\Normal(\\v{x}|\\v{b},\\v{B})=c\\Normal(\\v{x}|\\v{c},\\v{C})$ where $\\v{C}=(\\v{A}^{-1}+\\v{B}^{-1})^{-1}$, $\\v{c}=\\v{C}(\\v{A}^{-1}\\v{a}+\\v{B}^{-1}\\v{b})$, and $c=\\Normal(\\v{b}|\\v{a},\\v{A}+\\v{B})=\\/{1}{\\sqrt{(2œÄ)^D\\det|\\v{A}+\\v{B}|}}\\e{-\\/{(\\v{a}-\\v{b})^‚ä§(\\v{A}+\\v{B})^{-1}(\\v{a}-\\v{b})}{2}}$.\n",
        "\n",
        "  - Proof: $\\Normal(\\v{x}|\\v{a},\\v{A})\\Normal(\\v{x}|\\v{b},\\v{B})=c\\Normal(\\v{x}|\\v{c},\\v{C})$. Then $\\BC\n",
        "  \\t{left quadratic}&\n",
        "  =-\\/{(\\v{x}-\\v{a})^‚ä§\\v{A}^{-1}(\\v{x}-\\v{a})+(\\v{x}-\\v{b})^‚ä§\\v{B}^{-1}(\\v{x}-\\v{b})}{2}\n",
        "  =-\\/{\\v{x}^‚ä§(\\v{A}^{-1}+\\v{B}^{-1})\\v{x}-2\\v{x}^‚ä§(\\v{A}^{-1}\\v{a}+\\v{B}^{-1}\\v{b})+\\t{const}}{2}\\\\\n",
        "  \\t{right quadratic}&\n",
        "  =-\\/{(\\v{x}-\\v{c})^‚ä§\\v{C}^{-1}(\\v{x}-\\v{c})}{2}\n",
        "  =-\\/{\\v{x}^‚ä§\\v{C}^{-1}\\v{x}-2\\v{x}^‚ä§\\v{C}^{-1}\\v{c}+\\t{const}}{2}\\\\\n",
        "  \\t{mean and variance}&\\v{C}=\\v{A}^{-1}+\\v{B}^{-1}\\t{, }\\v{c}=\\v{C}(\\v{A}^{-1}\\v{a}+\\v{B}^{-1}\\v{b})\n",
        "  \\EC$\n",
        "  $c=‚à´c\\Normal(\\v{x}|\\v{c},\\v{C})\\ d\\v{x}=‚à´\\Normal(\\v{x}|\\v{a},\\v{A})\\Normal(\\v{x}|\\v{b},\\v{B})\\ d\\v{x}$\n",
        "  $=‚à´\\Normal(\\v{x}|\\v{a},\\v{A})\\Normal(\\v{b}|\\v{x},\\v{B})\\ d\\v{x}$\n",
        "  $=\\Normal(\\v{b}|\\v{a},\\v{A}+\\v{B})$\n",
        "\n",
        "- **Weighted sum of Gaussians**: $p(x)=Œ±\\Normal(x|Œº_1,œÉ_1^2)+(1-Œ±)\\Normal(x|Œº_2,œÉ_2^2)$. Then\n",
        "$\\blue{\\BC \\E[x]=Œ±Œº_1+(1-Œ±)Œº_2\\\\\n",
        "\\V[x]=[Œ±œÉ_1^2+(1-Œ±)œÉ_2^2]+[Œ±Œº_1^2+(1-Œ±)Œº_2^2-(Œ±Œº_1+(1-Œ±)Œº_2)^2]\n",
        "\\EC}$\n",
        "\n",
        "  - Proof: $\\E[x^2]=Œ±\\E_{p_1}[x^2]+(1-Œ±)\\E_{p_2}[x^2]$\n",
        "  $=Œ±(Œº_1^2+œÉ_1^2)+(1-Œ±)(Œº_2^2+œÉ_2^2)$.\n",
        "  $\\V[x]=E[x^2]-E[x]^2=Œ±(Œº_1^2+œÉ_1^2)+(1-Œ±)(Œº_2^2+œÉ_2^2)-(Œ±Œº_1+(1-Œ±)Œº_2)^2$\n",
        "  $=[Œ±œÉ_1^2+(1-Œ±)œÉ_2^2]+[Œ±Œº_1^2+(1-Œ±)Œº_2^2-(Œ±Œº_1+(1-Œ±)Œº_2)^2]$\n",
        "\n",
        "  - Traditional setup: Let $E[I]=Œ±$, $(X|I=1)‚àº\\Normal(Œº_1,œÉ_1^2)$, and $(X|I=0)‚àº\\Normal(Œº_2,œÉ_2^2)$.\n",
        "  By Adam's law,\n",
        "  $\\E[X]=\\E[\\E[X|I]]=Œ±Œº_1+(1-Œ±)Œº_2$.\n",
        "  By Eve's law ,\n",
        "  $\\E[\\Var(X|I)]=Œ±œÉ_1^2+(1-Œ±)œÉ_2^2$, and\n",
        "  $\\Var(E[X|I])=\\E[(E[X|I]-\\E[X])^2]=Œ±(Œº_1-\\E[X])^2+(1-Œ±)(Œº_2-\\E[X])^2$\n",
        "\n",
        "- Reverse transformation: Let $\\v{A}‚àà‚Ñù^{M√óN}$ and $p(\\v{y})=\\Normal(\\v{y}|\\v{Ax},\\v{Œ£})$. Then $\\v{x}=(\\v{A}^‚ä§\\v{A})^{-1}\\v{A}^‚ä§\\v{y}$ and $p(\\v{x})=\\Normal(\\v{x}|(\\v{A}^‚ä§\\v{A})^{-1}\\v{A}^‚ä§\\v{y},(\\v{A}^‚ä§\\v{A})^{-1}\\v{A}^‚ä§\\v{Œ£}\\v{A}(\\v{A}^‚ä§\\v{A})^{-1})$"
      ],
      "metadata": {
        "id": "WdoFidyp_lxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exponential family** $\\red{p(\\v{x}|\\v{Œ∏})=h(\\v{x})\\e{‚ü®\\v{œï}(\\v{x}),\\v{Œ∏}‚ü©-A(\\v{Œ∏})}}$ is the multivariate likelihood function for one data point where $\\v{Œ∏}‚àà‚Ñù^k$ are natural parameters, $\\v{œï}(\\v{x})‚àà‚Ñù^k$ are sufficient statistics, and $A(\\v{Œ∏})$ is the log-partition function.\n",
        "\n",
        "- In Casella, $f(x|\\v{Œ∏})=h(x)c(\\v{Œ∏})\\e{\\sum_{i=1}^kw_i(\\v{Œ∏})t_i(x)}$ or $f(x|\\v{Œ∑})=h(x)c(\\v{Œ∑})\\e{\\sum_{i=1}^kŒ∑_it_i(x)}$ where $\\v{Œ∑}$ are natural parameters that correspond with $\\v{Œ∏}$ in MML.\n",
        "\n",
        "- Log-partition function $\\green{A(\\v{Œ∏})=\\ln ‚à´_\\m{X}h(\\v{x})\\e{\\v{œï}(\\v{x})^‚ä§\\v{Œ∏}}\\ d\\v{x}}$ is the normalizing constant, but with MGF properties. $\\blue{‚àá_\\v{Œ∏}A(\\v{Œ∏})=\\E[\\v{œï}(\\v{x})]}$ and $\\blue{‚àá_\\v{Œ∏}^2A(\\v{Œ∏})=\\V[\\v{œï}(\\v{x})]}$ is the positive-semidefinite Fisher Information matrix.\n",
        "\n",
        "  - Proof: $l(\\v{Œ∏})=\\ln h(\\v{x})+\\v{œï}(\\v{x})^‚ä§\\v{Œ∏}-A(\\v{Œ∏})$.\n",
        "  Score $\\green{‚àá_\\v{Œ∏}l(\\v{Œ∏})=\\v{œï}(\\v{x})^‚ä§-‚àá_\\v{Œ∏}A(\\v{Œ∏})}$. Hession $\\green{‚àá_\\v{Œ∏}^2l(\\v{Œ∏})=-‚àá_\\v{Œ∏}^2A(\\v{Œ∏})}$.\n",
        "  $\\E_X[‚àá_\\v{Œ∏}l(\\v{Œ∏})]=\\v{0}=\\E_X[\\v{œï}(\\v{x})^‚ä§]-\\E_X[‚àá_\\v{Œ∏}A(\\v{Œ∏})]‚áí‚àá_\\v{Œ∏}A(\\v{Œ∏})=\\E_X[\\v{œï}(\\v{x})^‚ä§]$.\n",
        "  Fisher information $‚àá_\\v{Œ∏}^2A(\\v{Œ∏})=-\\E_X[‚àá_\\v{Œ∏}^2l(\\v{Œ∏})]=I_1(\\v{Œ∏})$ is variance of the score:\n",
        "  $\\V_X[‚àá_\\v{Œ∏}l(\\v{Œ∏})]=\\V_X[\\v{œï}(\\v{x})^‚ä§]$.\n",
        "\n",
        "- Every member of the exponential family has a conjugate prior $\\red{p(\\v{Œ∏}|\\v{Œ≥})=h_c(\\v{Œ∏})\\e{‚ü®\\v{Œ≥},\\BSM \\v{Œ∏}\\\\-A(\\v{Œ∏})\\ESM‚ü©-A_c(\\v{Œ≥})}}$.\n",
        "\n",
        "  - $\\v{Œ≥}=\\BSM\\v{Œ≥}_1\\\\Œ≥_2\\ESM$ where $\\v{Œ≥}_1‚àà‚Ñù^k$ and $Œ≥_2‚àà‚Ñù$. The inner product is $\\green{‚ü®\\v{Œ≥},\\BSM \\v{Œ∏}\\\\-A(\\v{Œ∏})\\ESM‚ü©=\\v{Œ≥}_1^‚ä§\\v{Œ∏}-Œ≥_2A(\\v{Œ∏})}$. Together the two elements of $\\v{Œ≥}$ represent a hypothetical dataset observed before real experiment began.\n",
        "\n",
        "  - $\\v{Œ≥}_1$ interacts with $\\v{Œ∏}$ in the prior $p(\\v{Œ∏})$ the same way sufficient statistics $\\v{œï}(\\v{x})$ does in the likelihood $p(\\v{x}|\\v{Œ∏})$. Therefore, $\\v{Œ≥}_1$ represents $\\BSM\\sum_{i=1}^Nœï_1(\\v{x}_i)\\\\\\vdots\\\\\\sum_{i=1}^Nœï_k(\\v{x}_i)\\ESM$ in the prior. $Œ≥_2$ represents $N$ in the prior.\n",
        "\n",
        "  - Bayesian update with one data point of likelihood: $p(\\v{Œ∏}|\\v{x})‚àù\\e{\\v{Œ≥}_1^‚ä§\\v{Œ∏}-Œ≥_2A(\\v{Œ∏})}\\e{\\v{œï}(\\v{x})^‚ä§\\v{Œ∏}-A(\\v{Œ∏})}$\n",
        "  $=\\e{\\green{(\\v{Œ≥}_1+\\v{œï}(\\v{x}))}^‚ä§\\v{Œ∏}-\\green{(Œ≥_2+1)}A(\\v{Œ∏})}$\n",
        "\n",
        "- 6.13: $\\Normal(x|Œº,œÉ^2)=\\/{1}{\\sqrt{2œÄ}}\\e{-\\/{(x-Œº)^2}{2œÉ^2}-\\/{\\ln œÉ^2}{2}}$\n",
        "$‚àù\\e{-\\/{x^2}{2œÉ^2}+\\/{Œºx}{œÉ^2}-\\/{Œº^2}{2œÉ^2}-\\/{\\ln œÉ^2}{2}}$.\n",
        "Therefore $\\v{œï}(x)=\\BM x^2\\\\x\\EM$ and $\\v{Œ∏}=\\BM-1/2œÉ^2\\\\Œº/œÉ^2\\EM$\n",
        "\n",
        "- 6.14/6.15: $p(x|Œº)=Œº^x(1-Œº)^{1-x}=\\e{x\\ln(\\/{Œº}{1-Œº})+\\ln(1-Œº)}$. Then $œï(x)=x$, $Œ∏=\\ln(\\/{Œº}{1-Œº})$, and $Œº=\\/{e^Œ∏}{1+e^Œ∏}=\\/{1}{1+e^{-Œ∏}}$.\n",
        "Conjugate prior $p(Œº|Œ≥_1,Œ≥_2)‚àù\\e{Œ≥_1\\ln(\\/{Œº}{1-Œº})+Œ≥_2\\ln(1-Œº)}$\n",
        "$=(\\/{Œº}{1-Œº})^{Œ≥_1}(1-Œº)^{Œ≥_2}$\n",
        "$=Œº^{Œ≥_1}(1-Œº)^{Œ≥_2-Œ≥_1}$"
      ],
      "metadata": {
        "id": "ZM5Mx0WdwHTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 6.2: $0.4\\Normal(\\BM10\\\\2\\EM,\\v{I})+0.6\\Normal(\\v{0},\\BM8.4&2.0\\\\2.0&1.7\\EM)$\n",
        "\n",
        "  - Compute marginal distributions.\n",
        "  $0.4\\Normal(x_1|10,1)+0.6\\Normal(x_1|0,8.4)$.\n",
        "  $\\E[x_1]=4$.\n",
        "  $\\V[x_1]=(0.4)(1)+(0.6)(8.4)+(0.4)(10-4)^2+(0.6)(0-4)^2=29.44$.\n",
        "  $0.4\\Normal(x_2|2,1)+0.6\\Normal(x_2|0,1.7)$.\n",
        "  $\\E[x_2]=0.8$.\n",
        "  $\\V[x_2]=(0.4)(1)+(0.6)(1.7)+(0.4)(2-0.8)^2+(0.6)(0-0.8)^2=2.38$.\n",
        "\n",
        "  - Compute mean, mode, median for each marginal distribution.\n",
        "  Mean $\\E[x_1]=4$ and $\\E[x_2]=0.8$.\n",
        "  Each distribution is bimodal.\n",
        "  The median and mode calculations must be done numerically.\n",
        "  $x_1$ highest peak is at 10 because much smaller variance $(0.6)(1)< (0.4)(8.4)$.\n",
        "\n",
        "- 6.12: Consider $\\v{x}‚àº\\Normal(\\v{Œº}_x,\\v{Œ£}_x)‚àà‚Ñù^D$ and $\\v{y}=\\v{Ax}+\\v{b}+\\v{w}$ where $\\v{A}‚àà‚Ñù^{E√óD}$, and $\\v{w}‚àº\\Normal(\\v{0},\\v{Q})$ is independent with diagonal $\\v{Q}$.\n",
        "\n",
        "  - Find $p(\\v{y}|\\v{x})$.\n",
        "  Likelihood $p(\\v{y}|\\v{x})=\\Normal(\\v{y}|\\v{Ax}+\\v{b},\\v{Q})$\n",
        "\n",
        "  - Find $p(\\v{y})$.\n",
        "  Marginal $p(\\v{y})=‚à´p(\\v{y}|\\v{x})p(\\v{x})\\ d\\v{x}$\n",
        "  $=‚à´\\Normal(\\v{y}|\\v{Ax}+\\v{b},\\v{Q})\\Normal(\\v{x}|\\v{Œº}_x,\\v{Œ£}_x)\\ d\\v{x}$.\n",
        "  Let $\\v{x}=\\v{Œº}_x+\\v{u}$ where $\\v{u}‚àº\\Normal(\\v{0},\\v{Œ£})$, then $\\v{y}=\\v{AŒº}_x+\\v{b}+\\v{Au}+\\v{w}$ and\n",
        "  $p(\\v{y})=\\Normal(\\v{y}|\\v{AŒº}_x+\\v{b},\\v{AŒ£}_x\\v{A}^‚ä§+\\v{Q})$.\n",
        "\n",
        "  - Let $\\v{z}=\\v{Cy}+\\v{v}$ where $\\v{C}‚àà‚Ñù^{F√óE}$ and $\\v{v}‚àº\\Normal(\\v{v}|\\v{0},\\v{R})$ is independent.\n",
        "  Then $\\v{z}=\\v{CAŒº}_x+\\v{Cb}+\\v{CAu}+\\v{Cw}+\\v{v}$.\n",
        "  $p(\\v{z})=\\Normal(\\v{z}|\\v{CAŒº}_x+\\v{Cb},\\v{CAŒ£A}^‚ä§\\v{C}^‚ä§+\\v{CQC}^‚ä§+\\v{R})$"
      ],
      "metadata": {
        "id": "TsCue6ANSpoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Convex Optimization"
      ],
      "metadata": {
        "id": "w_2sEW1Guy5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent**: for objective function $f:‚Ñù^D‚Üí‚Ñù$, gradient $‚àá_\\v{x}f(\\v{x}_i)^‚ä§$ points towards the direction of steepest ascent at point $(\\v{x}_i,f(\\v{x}_i))$, and is orthogonal to the contour line. Gradient descent finds $\\arg\\min_\\v{x}f(\\v{x})$ iteratively at step size $Œ≥_i$ where each step moves to a new position $\\red{\\v{x}_{i+1}=\\v{x}_i-Œ≥_i‚àá_\\v{x}f(\\v{x}_i)^‚ä§}$.\n",
        "\n",
        "- Gradient descent with adaptive step-size: $\\BC \\t{Undo and $Œ≥_i‚áì$}&\\t{if }f(\\v{x}_{i+1})>f(\\v{x}_i)\\\\Œ≥_{i+1}‚áë&\\t{if }f(\\v{x}_{i+1})< f(\\v{x}_i)\\EC$.\n",
        "\n",
        "- For $\\v{Ax}=\\v{b}$, we can run gradient descent on $\\red{\\BC f(\\v{x})=\\n{\\v{Ax}-\\v{b}}^2=(\\v{Ax}-\\v{b})^‚ä§(\\v{Ax}-\\v{b}) \\\\ ‚àá_\\v{x}=2(\\v{Ax}-\\v{b})^‚ä§\\v{A} \\EC}$ as the objective function. Setting $‚àá_\\v{x}=0$ and solving for $\\v{x}$ gives the normal equation $\\v{x}=(\\v{A}^‚ä§\\v{A})^{-1}\\v{A}^‚ä§\\v{b}$.\n",
        "\n",
        "  - Gradient descent speed depends on **condition number** $Œ∫=\\/{œÉ(\\v{A})_\\max}{œÉ(\\v{A})_\\min}$, which is a ratio of single values of $\\v{A}$ that represent the ratio of the most curved slope vs least curved slope. Because gradient direction is orthogonal to contour line, gradient descent runs zig-zag along thin valleys, which has very high condition number.\n",
        "\n",
        "  - Pre-conditioning uses $\\v{P}$ to reduce condition number down near 1, and solve $\\v{P}^{-1}(\\v{Ax}-\\v{b})=\\v{0}$.\n",
        "\n",
        "- **Gradient descent with momentum**: let $Œ±‚àà[0,1]$, then $\\BC \\v{x}_{i+1}=\\v{x}_i-Œ≥_i‚àá_\\v{x}f(\\v{x}_i)^‚ä§+Œ±Œî\\v{x}_i \\\\ Œî\\v{x}_i=\\v{x}_i-\\v{x}_{i-1}\\EC$\n",
        "\n",
        "- **Stochastic gradient descent**: finding $\\arg\\min_\\v{Œ∏}\\sum_{n=1}^NL_n(\\v{Œ∏})$ requires $\\v{Œ∏}_{i+1}=\\v{Œ∏}_i-Œ≥_i\\sum_{n=1}^N‚àá_\\v{Œ∏}L(\\v{Œ∏}_i)^‚ä§$ by **batch** for large $N$ at each step. **SGD** mini-batch computes a subset $\\red{\\v{Œ∏}_{i+1}=\\v{Œ∏}_i-Œ≥_i\\sum_{n‚àà\\t{subset}}‚àá_\\v{Œ∏}L(\\v{Œ∏}_i)^‚ä§}$ at each step based on unbiased estimator $\\green{\\E[‚àá_\\v{Œ∏}L_n(\\v{Œ∏}_i)]=\\/{1}{N}\\sum_{n=1}^N‚àá_\\v{Œ∏}L_n(\\v{Œ∏}_i)}$ (LLN: $\\bar{X}\\larr{a.s.}Œº=\\E[X_1]$).\n",
        "\n",
        "- 7.1: $f(x_1,x_2)=\\/{1}{2}\\BM x_1\\\\x_2\\EM^‚ä§\\BM2&1\\\\1&20\\EM\\BM x_1\\\\x_2\\EM-\\BM5\\\\3\\EM^‚ä§\\BM x_1\\\\x_2\\EM$.\n",
        "Then $f(\\v{x})=\\/{1}{2}\\v{x}^‚ä§\\v{Ax}-\\v{b}^‚ä§\\v{x}$\n",
        "$‚áí‚àá_\\v{x}f=\\/{1}{2}\\v{x}^‚ä§(\\v{A}+\\v{A}^‚ä§)-\\v{b}^‚ä§$\n",
        "$=\\v{x}^‚ä§\\BM2&1\\\\1&20\\EM-\\BM5&3\\EM$.\n",
        "Starting at $\\v{x}_0=(-3,-1)$ with $Œ≥=0.085$ gives $\\v{x}_1=\\BM-3\\\\-1\\EM-0.085\\BM-12\\\\-26\\EM=\\BM-1.98\\\\1.21\\EM$"
      ],
      "metadata": {
        "id": "-gABm00Iu31Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convex Optimization**: $\\BC\\min_\\v{x}&f(\\v{x})\\\\\\t{subject to}&g_i(\\v{x})‚â§0,\\ i=1,...,m&h_j(\\v{x})=0,\\ j=1,...,n\\EC$ is called convex optimization problem if $f,g_i$ are convex functions, and $h_j(\\v{x})=0$ are convex sets (i.e., affine).\n",
        "\n",
        "- Set $\\m{C}$ is a **convex set** if for any $x,y‚àà\\m{C}$ and scalar $Œ∏‚àà[0,1]$ we have $\\red{Œ∏x+(1-Œ∏)y‚àà\\m{C}}$.\n",
        "\n",
        "  - Convex sets are sets such that a straight line connecting any two elements of the set lie inside the set.\n",
        "\n",
        "  - An affine function is such that for any $\\v{x},\\v{y}‚àà\\m{C}$, a line drawn between them also lies in $\\m{C}$.\n",
        "\n",
        "- Let $f:‚Ñù^D‚Üí‚Ñù$ be a function whose domain $\\t{dom} f$ is a convex set. $f$ is a **convex function** if for all $\\v{x},\\v{y}‚àà\\t{dom} f$ and scalar $Œ∏‚àà[0,1]$ we have $\\red{f(Œ∏\\v{x}+(1-Œ∏)\\v{y})‚â§Œ∏f(\\v{x})+(1-Œ∏)f(\\v{y})}$\n",
        "\n",
        "  - Jensen's inequality $g(\\E[X])‚â§\\E[g(X)]$ if $g$ is convex.\n",
        "\n",
        "  - If $g$ is a concave function then $-g$ is convex.\n",
        "\n",
        "  - $f:‚Ñù^D‚Üí‚Ñù$ is $C^1$, then $f$ is convex iff for any $\\v{x},\\v{y}‚àà\\t{dom}f$ it holds that $\\blue{f(\\v{y})‚â•f(\\v{x})+‚àá_\\v{x}f(\\v{x})(\\v{y}-\\v{x})}$ (second point higher than first order Taylor approximation).\n",
        "\n",
        "  - $f:‚Ñù^D‚Üí‚Ñù$ is $C^2$, then $f$ is convex iff $‚àá_\\v{x}^2f(\\v{x})$ is positive-semidefinite.\n",
        "\n",
        "  - Linear closure: A weighted sum of convex functions is convex.\n",
        "\n",
        "- 7.3: $f(x)=x\\log_2x$. Check convexity using $x=2,4$. Midway method: $f(3)=4.75$ vs $\\/{f(2)+f(4)}{2}=5$.\n",
        "\n"
      ],
      "metadata": {
        "id": "JFFr5r-_gCmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lagrangian duality**: for $f:‚Ñù^D‚Üí‚Ñù$ and $g_i:‚Ñù^D‚Üí‚Ñù$, **primal problem** $\\BC\\min_\\v{x}&f(\\v{x})\\\\\\t{subject to}&g_i(\\v{x})‚â§0,\\ i=1,...,m\\EC$ using Lagrangian $\\red{\\m{L}(\\v{x},\\v{Œª})=f(\\v{x})+\\v{Œª}^‚ä§\\v{g}(\\v{x})}$ becomes **dual problem** $\\BC\\max_\\v{Œª}&\\m{D}(\\v{Œª})=\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})\\\\\\t{subject to}&\\v{Œª}‚â•\\v{0}\\EC$\n",
        "\n",
        "- **Primal problem**: the objective function is a cost to be minimized. The Lagrangian $\\m{L}(\\v{x},\\v{Œª})$ adds cost penalties for constraint violations $\\sum_{i=1}^mŒª_ig_i(\\v{x})$ with dual variables $\\v{Œª}$ (Lagrange multipliers). To set infinite penalty against constraint violation, the objective function becomes $\\green{\\max_{\\v{Œª}‚â•0}\\m{L}(\\v{x},\\v{Œª})=f(\\v{x})+\\sum_{i=1}^m\\max_{Œª_i‚â•0}Œª_ig_i(\\v{x})}$, where the costs are\n",
        "$\\BC\n",
        "g_i(\\v{x})>0&\n",
        "Œª_i=‚àû&\n",
        "\\max_{Œª_i‚â•0}Œª_ig_i(\\v{x})=‚àû&\n",
        "\\max_{\\v{Œª}‚â•0}\\m{L}(\\v{x},\\v{Œª})=‚àû\\t{ if any }g_i(\\v{x})>0\\\\\n",
        "g_i(\\v{x})‚â§0&\n",
        "Œª_i=0&\n",
        "\\max_{Œª_i‚â•0}Œª_ig_i(\\v{x})=0&\n",
        "\\max_{\\v{Œª}‚â•0}\\m{L}(\\v{x},\\v{Œª})=f(\\v{x})\\t{ if all }g_i(\\v{x})‚â§0\\\\\n",
        "\\EC$. The original problem $\\min_\\v{x}f(\\v{x})$ has primal solution $\\red{p^*=\\min_\\v{x}\\max_{\\v{Œª}‚â•\\v{0}}\\m{L}(\\v{x},\\v{Œª})}$.\n",
        "\n",
        "- **Dual problem**: dual function $\\green{\\BC\\m{D}(\\v{Œª})=\\m{L}(\\v{x}_\\min(\\v{Œª}),\\v{Œª})\\\\\\v{x}_\\min(\\v{Œª})=\\arg\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})\\EC}$ maps any $\\v{Œª}$ to the absolute lowest cost (which may violate constraints), and is therefore lower bound to the optimal solution $\\m{D}(\\v{Œª})‚â§p^*$ (which upholds constraints). Dual ascent searches for the tighest lower bound by climbing the $\\m{D}(\\v{Œª})$ hill, whose slope is $\\blue{‚àá_\\v{Œª}\\m{D}(\\v{Œª})=\\v{g}(\\v{x}_\\min)}$ (Danskin's theorem). $\\BC\\t{left of peak}&g_i(\\v{x}_\\min)>0&\\t{raise $Œª_i$ until $g_i(\\v{x}_\\min)=0$}\\\\\\t{right of peak}&g_i(\\v{x}_\\min)< 0&\\t{lower $Œª_i$ toward $g_i(\\v{x}_\\min)=0$}\\EC$. The absolute peak of $\\m{D}(\\v{Œª})$ has flat slope $\\v{g}(\\v{x}_\\min(\\v{Œª}_\\t{peak}))=\\v{0}$, where some $Œª_{\\t{peak},i}< 0$ are negative because $g_i(\\v{x}_\\min(\\v{Œª}))|_{Œª_i=0}< 0$. The dual solution $\\red{d^*=\\max_{\\v{Œª}‚â•\\v{0}}\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})}$ is the tightest lower bound while respecting $\\v{Œª}‚â•\\v{0}$, so it's often placed at the right of the absolute peak where the slope $‚àá_\\v{Œª}\\m{D}(\\v{Œª})\\neq\\v{0}$.\n",
        "\n",
        "  - $\\m{D}(\\v{Œª})$ **is guaranteed to be concave (hill)** regardless of what $f,g_i$ look like. Proof (lower envelope principle): $\\m{L}(\\v{x},Œª)=f(\\v{x})+\\v{Œª}^‚ä§\\v{g}(\\v{x})$ is an affine function ($\\m{L}=mŒª+b$) of $\\v{Œª}$. Every $\\v{x}‚àà‚Ñù^D$ produces a linear function of $\\v{Œª}$: some up-sloping, some down-sloping, and some flat. Because $\\m{D}(\\v{Œª})=\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})$, tracing the bottom outline of all these lines forms a concave \"hill\" shape.\n",
        "\n",
        "- **Strong duality** is achieved when $\\green{\\min_\\v{x}\\max_{\\v{Œª}‚â•\\v{0}}\\m{L}(\\v{x},\\v{Œª})=\\max_{\\v{Œª}‚â•\\v{0}}\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})}$. When duality breaks, dual solution < primal solution (duality gap). The dual solution has constraint violations and the peak of $\\m{D}(\\v{Œª})$ is not smooth (i.e., $‚àá_\\v{Œª}\\m{D}(\\v{Œª})$ doesn't exist at the peak).\n",
        "  \n",
        "  - **Weak duality**: Minimax inequality says $\\red{\\max_y\\min_xœï(x,y)‚â§\\min_x\\max_yœï(x,y)}$. Proof: $\\min_xœï(x,y)‚â§\\max_yœï(x,y)‚áí\\BC \\max_y\\min_xœï(x,y)‚â§\\max_yœï(x,y) \\\\ \\min_xœï(x,y)‚â§\\min_x\\max_yœï(x,y)\\EC$. If LHS set is still less than the RHS set, then maximum of the LHS set is stll less than the minimum of the RHS set.\n",
        "\n",
        "  - **Slater's condition**: strong duality holds for convex $f,g_i$ if there exists $\\v{x}‚àà\\bigcap\\limits_{i=1}^m(g_i(\\v{x})< 0)$.\n",
        "\n",
        "- $\\v{Œª}$ **as shadow price**: If we relax the constraint $\\BC p^*(\\green{0})&\\min_xf(x)\\\\\\t{subject to}&g(x)‚â§\\green{0}\\EC‚áí\\BC p^*(\\green{u})&\\min_xf(x)\\\\\\t{subject to}&g(x)‚â§\\green{u}\\EC$, where the primal optimal value is $p^*(u)=\\min_x\\max_Œª\\m{L}(x,Œª)=f(x)+Œª(g(x)-u)$, then $\\/{‚àÇp^*}{‚àÇu}=-Œª=:\\blue{-Œª^*}$ is the rate of change in the optimal value with respect to a unit relaxation in constraint.\n",
        "\n",
        "  - The shadow price $Œª^*$ is the marginal value to be gained from relaxing the resource constraint by 1 unit.\n",
        "  \n",
        "- **Equality constraint**: $\\BC\n",
        "\\min_\\v{x}&f(\\v{x})\\\\\n",
        "\\t{subject to}&g_i(\\v{x})‚â§0,\\ i=1,...,m\\\\\n",
        "&h_j(\\v{x})=0,\\ j=1,...,n\\EC‚áî\\BC\n",
        "\\min_\\v{x}&f(\\v{x})\\\\\n",
        "\\t{subject to}&g_i(\\v{x})‚â§0,\\ i=1,...,m\\\\\n",
        "&h_j(\\v{x})‚â§0,-h_j(\\v{x})‚â§0,\\ j=1,...,n\\\\\n",
        "\\EC$\n",
        "\n",
        "  - Slater's condition updated: convex $f,g_i$, affine $h_j$, and $\\v{x}‚àà(\\bigcap\\limits_{i=1}^m(g_i(\\v{x})< 0))‚à©(\\bigcap\\limits_{j=1}^n(h_j(\\v{x})=0))$.\n",
        "\n",
        "- **Calculation flow**: $\\BC\n",
        "\\t{Minimize}&\\t{Set }‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})=\\v{0}&\\t{get }\\v{x}_\\min(\\v{Œª})\\\\\n",
        "\\t{Substitute}&\\m{D}(\\v{Œª})=\\m{L}(\\v{x}_\\min(\\v{Œª}),\\v{Œª})\\\\\n",
        "\\t{Dual ascend}&\\max_{\\v{Œª}‚â•\\v{0}}\\m{D}(\\v{Œª})&\\t{get }\\v{Œª}^*\\\\\n",
        "\\t{Recovery}&\\t{Set }‚àá_\\v{x}\\m{L}(\\v{x}^*,\\v{Œª}^*)=\\v{0}&\\t{get $\\v{x}^*$ if strong duality}\n",
        "\\EC$\n",
        "\n",
        "  - **Recovery**: With $\\v{Œª}^*‚àà‚Ñù^m$, we recover $\\v{x}^*‚àà‚Ñù^d$ using a system of $d$ equations from KKT stationarity $\\blue{‚àá_\\v{x}\\m{L}(\\v{x}^*,\\v{Œª}^*)=‚àá_\\v{x}f(\\v{x}^*)+\\v{Œª}^{*‚ä§}‚àá_\\v{x}\\v{g}(\\v{x}^*)=\\v{0}}$.\n",
        "\n",
        "  - Special case: $\\BC\\min_\\v{x}&f(\\v{x})\\\\\\t{subject to}&\\v{h}(\\v{x})=\\v{0}\\EC‚áí\\BC‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})=‚àá_\\v{x}f(\\v{x})+\\v{Œª}^‚ä§‚àá_\\v{x}\\v{h}(\\v{x})=\\v{0}\\\\‚àá_\\v{Œª}\\m{L}(\\v{x},\\v{Œª})=\\v{h}(\\v{x})=\\v{0}\\EC$ is a system of $(m+d)$ equations and $(m+d)$ variables where $\\m{L}(\\v{x},\\v{Œª})=f(\\v{x})+\\v{Œª}^‚ä§\\v{h}(\\v{x})$. Since equality constraints are always active, finding active/inactive elements of $\\v{Œª}$ through dual ascent is no longer necessary.\n",
        "\n",
        "- **KKT Conditions** (Karush-Kuhn-Tucker): for convex problem with strong duality, solution $(\\v{x}^*,\\v{Œª}^*)$ is optimal **IFF**\n",
        "  \n",
        "  1. stationarity: $\\red{‚àá_\\v{x}\\m{L}(\\v{x}^*,\\v{Œª}^*)=‚àá_\\v{x}f(\\v{x}^*)+\\v{Œª}^{*‚ä§}‚àá_\\v{x}\\v{g}(\\v{x}^*)=\\v{0}}$. The optimal solution $(\\v{x}^*,\\v{Œª}^*)$ is the saddle point of the Lagrangian where $‚àá_\\v{x}\\m{L}(\\v{x}^*,\\v{Œª}^*)=\\v{0}$ but often $‚àá_\\v{Œª}\\m{L}(\\v{x}^*,\\v{Œª}^*)\\neq\\v{0}$ (dual ascent).\n",
        "  $\\BC\n",
        "  g(\\v{x}^*)=0&\\t{active constraint}&\\v{Œª}^*>\\v{0}\n",
        "  &\\m{L}(\\v{x}^*,\\v{Œª}^*)=f(\\v{x}^*)\n",
        "  &\\m{L}(\\v{x},\\v{Œª}^*)\\neq f(\\v{x})\n",
        "  &‚àá_\\v{x}\\m{L}=0\\neq‚àá_\\v{x}f\\\\\n",
        "  g(\\v{x}^*)< 0&\\t{inactive constraint}&\\v{Œª}^*=\\v{0}\n",
        "  &\\m{L}(\\v{x}^*,\\v{Œª}^*)=f(\\v{x}^*)\n",
        "  &\\m{L}(\\v{x},\\v{Œª}^*)=f(\\v{x})\n",
        "  &‚àá_\\v{x}\\m{L}=0=‚àá_\\v{x}f\\\\\n",
        "  \\EC$.\n",
        "  $\\blue{‚àá_\\v{x}f(\\v{x}^*)=-\\v{Œª}^{*‚ä§}‚àá_\\v{x}\\v{g}(\\v{x}^*)}$ \"Force balance\": the rate of further improvement in $f$ is equal to the shadow price $-\\v{Œª}^*$. Further improvement is limited by active constraints.\n",
        "\n",
        "  2. primal feasibility: $g_i(\\v{x}^*)‚â§0$ for $i=1,...,m$\n",
        "\n",
        "  3. dual feasibility: $Œª_i^*‚â•0$ for $i=1,...,m$\n",
        "\n",
        "  4. complementary slackness: $\\red{\\diag(\\v{Œª}^*)\\v{g}(\\v{x}^*)=\\v{0}}$ or $\\red{Œª_i^*g_i(\\v{x}^*)=0}$ for $i=1,...,m$.\n",
        "  $\\BC\n",
        "  g_i(\\v{x}^*)< 0&\\t{inactive constraint}&Œª_i^*=0&\\t{no improvement on $p^*$ by relaxing this constraint}\\\\\n",
        "  g_i(\\v{x}^*)=0&\\t{active constraint}&Œª_i^*>0&\\t{relaxing this constraint lowers $p^*$ by $Œª_i^*$}\n",
        "  \\EC$\n",
        "\n",
        "  - Meeting the 4 KKT conditions is sufficient to prove $(\\v{x}^*,\\v{Œª}^*)$ is optimal.\n",
        "\n",
        "- Example: Minimize $f(x)=x^2$ subject to $x‚â•2$. Lagrangian $\\m{L}(x,Œª)=x^2+Œª(2-x)$. Find the dual function $\\/{‚àÇ\\m{L}}{‚àÇx}=2x-Œª=0‚áíx=\\/{Œª}{2}$ and $\\m{D}=(\\/{Œª}{2})^2+Œª(2-\\/{Œª}{2})=2Œª-\\/{Œª^2}{4}$. Maximize $\\/{‚àÇ\\m{D}}{‚àÇŒª}=2-\\/{Œª}{2}=0‚áíŒª=4$. At $\\max\\m{D}$, $Œª=4$ and $x=2$. Duality achieved with active constraint ($x-2=0$).\n",
        "\n",
        "  - $\\m{L}(x,Œª^*)=x^2-4x+8$. $\\/{‚àÇ\\m{L}}{‚àÇx}=2x-4$. $\\/{‚àÇ\\m{L}}{‚àÇx}(2)=0$. Stationarity achieved."
      ],
      "metadata": {
        "id": "hIsXkFstjIaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear programming**: $f,g_i$ are all linear: $\\BC\\min_{\\v{x}‚àà‚Ñù^d}&\\v{c}^‚ä§\\v{x}\\\\\\t{subject to}&\\v{Ax}‚â§\\v{b}\\EC$ where $\\v{A}‚àà‚Ñù^{m√ód}$ is a linear program with $d$ variables and $m$ linear constraints.\n",
        "\n",
        "- Lagrangian $\\m{L}(\\v{x},\\v{Œª})=\\v{c}^‚ä§\\v{x}+\\v{Œª}^‚ä§(\\v{Ax}-\\v{b})$\n",
        "$=(\\green{\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A}})\\v{x}\\red{-\\v{Œª}^‚ä§\\v{b}}$ affine.\n",
        "Dual function is only defined if the slope is 0.\n",
        "$\\m{D}(\\v{Œª})=\\min_\\v{x}\\m{L}(\\v{x},\\v{Œª})=\\BC-\\v{Œª}^‚ä§\\v{b}&\\t{if }‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})=\\v{0}\\\\-‚àû&\\t{if }‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})\\neq\\v{0}\\EC$.\n",
        "\n",
        "  - Dual program\n",
        "  $\\BC\\max_{\\v{Œª}‚àà‚Ñù^m}&-\\v{b}^‚ä§\\v{Œª}\\\\\\t{subject to}&\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A}=\\v{0}\\\\&\\v{Œª}‚â•0\\EC$\n",
        "\n",
        "- Either the primal program ($\\v{x}^*‚àà‚Ñù^d$) or the dual program ($\\v{Œª}^*‚àà‚Ñù^m$) with $\\v{x}^*$ recovery can be solved using linear program polygonal algorithms.\n",
        "\n",
        "- 7.5: $\\min_\\v{x} -\\BM5&3\\EM\\v{x}$ subject to $\\BM2&2\\\\2&-4\\\\-2&1\\\\0&-1\\\\0&1\\EM\\v{x}‚â§\\BM33\\\\8\\\\5\\\\-1\\\\8\\EM$.\n",
        "Stationarity\n",
        "$‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})=\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A}=\\v{0}$\n",
        "$‚áí\\v{A}^‚ä§\\v{Œª}=-\\v{c}$.\n",
        "$\\BM2&2&-2&0&0\\\\2&-4&1&-1&1\\EM\\BMŒª_1\\\\Œª_2\\\\Œª_3\\\\Œª_4\\\\Œª_5\\EM=\\BM5\\\\3\\EM$ subject to $Œª_i‚â•0$.\n",
        "Find a $\\v{Œª}$ that meets dual feasibility.\n",
        "RREF $\\BM1&0&-1/2&-1/6&1/6&13/6\\\\0&1&-1/2&1/6&-1/6&1/3\\EM$\n",
        "$‚áí\\v{Œª}^*=\\BM13/6\\\\1/3\\\\0\\\\0\\\\0\\EM$.\n",
        "Complementary slackness\n",
        "$\\BC\\/{13}{6}(2x_1+2x_2-33)=0\\\\\\/{1}{3}(2x_1-4x_2-8)=0\\EC$.\n",
        "$\\BM13/3&13/3&143/2\\\\2/3&-4/3&8/3\\EM‚Üí\\BM1&0&37/3\\\\0&1&25/6\\EM$\n",
        "$‚áí\\v{x}^*=\\BM37/3\\\\25/6\\EM$. We know $(\\v{x}^*,\\v{Œª}^*)$ is the optimal solution because all 4 KKT conditions are met.\n",
        "\n",
        "  - Finding $\\v{Œª}$ that meets dual feasibility is a combinatorial process. The **Simplex Method** iteratively swaps columns of the basis (before RREF) until a valid active set is found."
      ],
      "metadata": {
        "id": "AfBGCwQO2p6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quadratic programming**: $f$ quadratic, $g_i$ linear: $\\BC\\min_{\\v{x}‚àà‚Ñù^d}&\\/{1}{2}\\v{x}^‚ä§\\v{Qx}+\\v{c}^‚ä§\\v{x}\\\\\\t{subject to}&\\v{Ax}‚â§\\v{b}\\EC$ where $\\v{A}‚àà‚Ñù^{m√ód}$ and $\\v{Q}‚àà‚Ñù^{d√ód}$ is symmetric positive-definite.\n",
        "\n",
        "- Quadratic form $\\v{x}^‚ä§\\v{Qx}$ always has a symmetric matrix $\\v{Q}$.\n",
        "  - Proof: A square matrix $\\v{Q}‚àà‚Ñù^{n√ón}=\\v{P}+\\v{S}=\\/{\\v{Q}+\\v{Q}^‚ä§}{2}+\\/{\\v{Q}-\\v{Q}^‚ä§}{2}$ can be split into symmetric $\\v{P}$ and skew-symmetric $\\v{S}$ who has special property $\\v{S}^‚ä§=-\\v{S}$. Quadratic form $\\v{x}^‚ä§\\v{Qx}=\\v{x}^‚ä§(\\v{P}+\\v{S})\\v{x}=\\v{x}^‚ä§\\v{Px}+\\v{x}^‚ä§\\v{Sx}$. But $(\\v{x}^‚ä§\\v{Sx})^‚ä§=-\\v{x}^‚ä§\\v{Sx}‚àà‚Ñù$. Therefore $\\v{x}^‚ä§\\v{Sx}=0$ and $\\v{x}^‚ä§\\v{Qx}=\\v{x}^‚ä§\\v{Px}$.\n",
        "\n",
        "- Quadratic form $\\v{x}^‚ä§\\v{Qx}$ is convex iff $\\v{Q}$ is positive-semidefinite.\n",
        "\n",
        "  - Proof: $‚àá_\\v{x}(\\v{x}^‚ä§\\v{Qx})=\\v{x}^‚ä§(\\v{Q}+\\v{Q}^‚ä§)=2\\v{x}^‚ä§\\v{Q}‚áí‚àá_\\v{x}^2(\\v{x}^‚ä§\\v{Qx})=2\\v{Q}$. I.e., $2\\v{Q}$ is the Hessian.\n",
        "\n",
        "  - Though the existence of a single optimal $\\v{x}^*$ requires positive-definite $\\v{Q}$.\n",
        "\n",
        "- Lagrangian $\\m{L}(\\v{x},\\v{Œª})=\\/{1}{2}\\v{x}^‚ä§\\v{Qx}+\\v{c}^‚ä§\\v{x}+\\v{Œª}^‚ä§(\\v{Ax}-\\v{b})$\n",
        "$=\\/{1}{2}\\v{x}^‚ä§\\v{Qx}+(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{x}-\\v{Œª}^‚ä§\\v{b}$. Stationarity\n",
        "$‚àá_\\v{x}\\m{L}(\\v{x},\\v{Œª})=\\v{x}^‚ä§\\v{Q}+\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A}=\\v{0}‚áí\\green{\\v{x}^‚ä§=-(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{Q}^{-1}}$ and $\\green{\\v{x}=\\v{Q}^{-1}(\\v{c}+\\v{A}^‚ä§\\v{Œª})}$. Substitute back to get dual function\n",
        "$\\m{D}(\\v{Œª})=\\/{1}{2}(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{Q}^{-1}(\\v{c}+\\v{A}^‚ä§\\v{Œª})-(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{Q}^{-1}(\\v{c}+\\v{A}^‚ä§\\v{Œª})-\\v{Œª}^‚ä§\\v{b}$\n",
        "$=-\\/{1}{2}(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{Q}^{-1}(\\v{c}+\\v{A}^‚ä§\\v{Œª})-\\v{Œª}^‚ä§\\v{b}$.\n",
        "\n",
        "  - Dual program $\\BC\\max_{\\v{Œª}‚àà‚Ñù^m}&-\\/{1}{2}(\\v{c}^‚ä§+\\v{Œª}^‚ä§\\v{A})\\v{Q}^{-1}(\\v{c}+\\v{A}^‚ä§\\v{Œª})-\\v{Œª}^‚ä§\\v{b}\\\\\\t{subject to}&\\v{Œª}‚â•\\v{0}\\EC$\n",
        "\n",
        "- 7.6: $f(\\v{x})=\\/{1}{2}\\v{x}^‚ä§\\BM2&1\\\\1&4\\EM\\v{x}+\\BM5\\\\3\\EM^‚ä§\\v{x}$. Find $\\min_\\v{x}f(\\v{x})$ subject to $\\BM1&0\\\\-1&0\\\\0&1\\\\0&-1\\EM\\v{x}‚â§\\BM1\\\\1\\\\1\\\\1\\EM$. The constraint is $|x_1|‚â§1$ and $|x_2|‚â§1$. We solve unconstrained minimum first then apply constraints retrospectively.\n",
        "$‚àá_\\v{x}f(\\v{x})=\\v{x}^‚ä§\\BM2&1\\\\1&4\\EM+\\BM5\\\\3\\EM^‚ä§=\\v{0}$\n",
        "$‚áí\\v{x}=-\\BM2&1\\\\1&4\\EM^{-1}\\BM5\\\\3\\EM$\n",
        "$=\\BM-4/7&1/7\\\\1/7&-2/7\\EM\\BM5\\\\3\\EM$\n",
        "$=\\BM-17/7\\\\-1/7\\EM$, which fails constraint #2: $\\BM-1&0\\EM\\v{x}=\\/{17}{7}\\not‚â§1$. We apply constraint by setting $x_1=-1$ and resolve for unconstrained minimum. $f(-1,x_2)=\\/{1}{2}[2(-1)^2+2(-1)x_2+4x_2^2]-5+3x_2$\n",
        "$=2x_2^2+2x_2-4$. $\\/{df}{dx_2}=4x_2+2=0‚áíx_2=-\\/{1}{2}$.\n",
        "Therefore $\\v{x}^*=\\BM-1\\\\-1/2\\EM$ primal feasibility. Stationarity $‚àá_\\v{x}f(\\v{x}^*)=-\\v{Œª}^{*‚ä§}‚àá_\\v{x}\\v{g}(\\v{x}^*)$\n",
        "$‚áí\\BM5/2\\\\0\\EM=\\BM-1&1&0&0\\\\0&0&-1&1\\EM\\v{Œª}^{*}$\n",
        "$‚áíŒª_2=\\/{5}{2}$ dual feasibility.\n",
        "KKT conditions met."
      ],
      "metadata": {
        "id": "owBbXv13Cfck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Legendre-Fenchel Transform** or **convex conjugate** of $f:‚Ñù^D‚Üí‚Ñù$ is $\\red{f^*(\\v{s})=\\sup_{\\v{x}‚àà‚Ñù^D}(‚ü®\\v{s},\\v{x}‚ü©-f(\\v{x}))}$.\n",
        "\n",
        "- **Epigraph**: A convex function $f$ is a \"bowl\" that can be filled with water, and the result is the epigraph of $f$. A convex function becomes a convex set through its epigraph. **Supporting hyperplane** of a convex set is a tangential hyperplane. A convex set or convex function epigraph can be described in terms of its supporting hyperplanes.\n",
        "\n",
        "- On the $xy$-plane, $y=‚ü®s,x‚ü©=sx$ is line with slope $s$ passing through origin. $f^*(s)=\\sup_x(‚ü®s,x‚ü©-f(x))$ is the greatest vertical gap between the line $y=sx$ and $y=f(x)$ below it. If $f$ is convex then this is located at $(x_0,f(x_0))$ where $\\/{df}{dx}|_{x=x_0}=s$. Convex conjugate is the magnitude of this gap, which is equal to the negative of the y-intercept of the tangent with slope $s$.\n",
        "\n",
        "  - Proof: Let $g(x)=sx-f(x)$, then $\\/{dg}{dx}=s-\\/{df}{dx}=0‚áís=\\/{df}{dx}$. Second derivative check $\\/{d^2g}{dx^2}=-\\/{d^2f}{dx^2}< 0$. This critical point is supremum because $g$ is concave $‚áî-f$ is concave $‚áîf$ is convex. Therefore $x_0=\\arg\\sup_x(sx-f(x))=x|_{f'(x)=s}$. Consider the tangent line at $(x_0,f(x_0))$, which has slope $s$.\n",
        "  Then $y=mx+c‚áíf(x_0)=sx_0+c$\n",
        "  $‚áíc=-(sx_0-f(x_0))=-f^*(s)$.\n",
        "\n",
        "- $\\red{f^{**}=f}$ for closed convex sets and differentiable functions. Furthermore, $\\red{\\/{df}{dx}|_{x=x_0}=s_0‚áî\\/{df^*}{ds}|_{s=s_0}=x_0}$.\n",
        "\n",
        "  - Proof: $f^*(s)=\\sup_x(sx-f(x))$.\n",
        "  Let $x_0(s)=\\arg\\sup_x(sx-f(x))$ be a function of $s$. Let $\\/{df}{dx}|_{x=x_0}=s_0$, then $f^*(s_0)=s_0x_0-f(x_0)$.\n",
        "  By chain rule,\n",
        "  $\\/{df^*}{ds}|_{s=s_0}=x_0+s_0\\/{dx_0}{ds}-\\/{df}{dx}|_{x=x_0}\\/{dx_0}{ds}$\n",
        "  $=x_0+s_0\\/{dx_0}{ds}-s_0\\/{dx_0}{ds}=x_0$.\n",
        "\n",
        "- $\\BC\\t{Write}&g(x)=sx-f(x)\\\\\n",
        "\\t{Derivative}&g'(x)=s-f'(x)\\\\\n",
        "\\t{Express}&x_\\min(s)\\t{: $x$ as a function of $s$}\\\\\n",
        "\\t{Substitute}&f^*(s)=g(x_\\min(s))\\EC$\n",
        "\n",
        "**Convex conjugate of Lagrangian**: $\\m{L}_\\v{x}^*(\\v{s},\\v{Œª})=\\sup_\\v{x}(‚ü®\\v{s},\\v{x}‚ü©-\\m{L}(\\v{x},\\v{Œª}))$ is the convex conjugate of the Lagrangian with respect to $\\v{x}$. Then\n",
        "$\\m{L}_\\v{x}^*(\\v{0},\\v{Œª})=\\sup_\\v{x}(‚ü®\\v{0},\\v{x}‚ü©-\\m{L}(\\v{x},\\v{Œª}))$\n",
        "$=\\sup_\\v{x}(-\\m{L}(\\v{x},\\v{Œª}))$\n",
        "$=-\\inf_\\v{x}\\m{L}(\\v{x},\\v{Œª})$.\n",
        "That is, the convex conjugate of the Lagrangian at $\\v{s}=\\v{0}$ is the negative of the Lagrangian dual function: $\\red{\\m{L}_\\v{x}^*(\\v{0},\\v{Œª})=-\\m{D}(\\v{Œª})}$.\n",
        "\n",
        "- B&V uses $f^*$ instead of $\\m{L}^*$. For $\\BC\n",
        "\\min_\\v{x}&f(\\v{x})\\\\\n",
        "\\t{subject to}&\\v{Ax}=\\v{b}\\EC$,\n",
        "Lagrangian $\\m{L}(\\v{x},\\v{Œª})=f(\\v{x})+\\v{Œª}^‚ä§(\\v{Ax}-\\v{b})$ has dual function\n",
        "$\\m{D}(\\v{Œª})=\\inf_\\v{x}\\m{L}(\\v{x},\\v{Œª})$\n",
        "$=\\inf_\\v{x}[-(-\\v{Œª}^‚ä§(\\v{Ax}-\\v{b})-f(\\v{x}))]$\n",
        "$=\\inf_\\v{x}[-(‚ü®-\\v{A}^‚ä§\\v{Œª},\\v{x}‚ü©-f(\\v{x}))]-\\v{Œª}^‚ä§\\v{b}$\n",
        "$=-\\sup_\\v{x}(‚ü®-\\v{A}^‚ä§\\v{Œª},\\v{x}‚ü©-f(\\v{x}))-\\v{Œª}^‚ä§\\v{b}$\n",
        "$=-f^*(-\\v{A}^‚ä§\\v{Œª})-\\v{Œª}^‚ä§\\v{b}$.\n",
        "\n",
        "  - $\\blue{\\inf(-f)=-\\sup f}$ and $\\blue{\\max f=-\\min(-f)}$\n",
        "\n",
        "- Example: $f(x)=x^2$. Let $g(x)=sx-x^2$ then $g(x)$ is greatest when $g'(x)=0=s-2x‚áíx=\\/{s}{2}$. $f^*(s)=\\sup_x(sx-f(x))=\\/{s^2}{2}-f(\\/{s}{2})=\\/{s^2}{4}$.\n",
        "\n",
        "  - $g^*(s)=st-\\/{s^2}{4}‚áí\\/{dg^*}{ds}=t-\\/{s}{2}=0‚áís=2t$.\n",
        "  $f^{**}(t)=t^2$.\n",
        "\n",
        "- Example: $f(x)=e^x$. Let $g(x)=sx-e^x$ then $g'(x)=0=s-e^x‚áíx=\\ln s$. Then $f^*(s)=s\\ln s-s$\n",
        "\n",
        "  - $g^*(s)=st-s\\ln s+s‚áí\\/{dg^*}{ds}=t-\\ln s-1+1=0‚áís=e^t$.\n",
        "  $f^{**}(t)=te^t-te^t+e^t=e^t$\n",
        "\n",
        "- 7.7: $f(\\v{y})=\\/{Œª}{2}\\v{y}^‚ä§\\v{K}^{-1}\\v{y}$.\n",
        "Convex conjugate $g(\\v{y})=‚ü®\\v{s},\\v{y}‚ü©-\\/{Œª}{2}\\v{y}^‚ä§\\v{K}^{-1}\\v{y}$\n",
        "$‚áí‚àá_\\v{y}g(\\v{y})=\\v{s}^‚ä§-Œª\\v{y}^‚ä§\\v{K}^{-1}=\\v{0}$\n",
        "$‚áí\\v{y}=\\/{1}{Œª}\\v{Ks}$.\n",
        "Then $f^*(\\v{s})=\\/{1}{Œª}\\v{s}^‚ä§\\v{Ks}-\\/{1}{2Œª}\\v{s}^‚ä§\\v{Ks}=\\/{1}{2Œª}\\v{s}^‚ä§\\v{Ks}$\n",
        "\n",
        "- 7.8: $L(\\v{t})=\\sum_il_i(t_i)$.\n",
        "Then $L^*(\\v{z})=\\sup_\\v{t}‚ü®\\v{z},\\v{t}‚ü©-\\sum_il_i(t_i)$\n",
        "$=\\sup_\\v{t}\\sum_i(z_it_i-l_i(t_i))$\n",
        "$=\\sum_i\\sup_\\v{t}(z_it_i-l_i(t_i))$\n",
        "$=\\sum_il^*(z_i)$\n",
        "\n",
        "- 7.9: Let $f,g$ be convex. Find $\\min_\\v{x}f(\\v{Ax})+g(\\v{x})$. Let $\\v{y}=\\v{Ax}$ then $\\BC\n",
        "\\min_\\v{x}&f(\\v{y})+g(\\v{x})\\\\\n",
        "\\t{subject to}&\\v{Ax}=\\v{y}\n",
        "\\EC$.\n",
        "The Lagrangian is $\\m{L}(\\v{x},\\v{y},\\v{Œª})=f(\\v{y})+g(\\v{x})+\\v{Œª}^‚ä§(\\v{Ax}-\\v{y})$ and\n",
        "$\\min_\\v{x}f(\\v{Ax})+g(\\v{x})=\\min_{\\v{x},\\v{y}}\\max_{\\v{Œª}‚â•\\v{0}}\\m{L}$\n",
        "$=\\max_{\\v{Œª}‚â•\\v{0}}\\min_{\\v{x},\\v{y}}\\m{L}$\n",
        "$=\\max_{\\v{Œª}‚â•\\v{0}}[\\min_\\v{y}f(\\v{y})-\\v{Œª}^‚ä§\\v{y}]+[\\min_\\v{x}g(\\v{x})+\\v{Œª}^‚ä§\\v{Ax}]$\n",
        "$=\\max_{\\v{Œª}‚â•\\v{0}}[-\\max_\\v{y}\\v{Œª}^‚ä§\\v{y}-f(\\v{y})]+[-\\max_\\v{y}(-\\v{A}^‚ä§\\v{Œª})^‚ä§\\v{x}-g(\\v{x})]$\n",
        "$=\\max_{\\v{Œª}‚â•\\v{0}}[-f^*(\\v{Œª})-g^*(-\\v{A}^‚ä§\\v{Œª})]$"
      ],
      "metadata": {
        "id": "WxxrvbtRZpac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 7.5: $\\BC\\max_{\\v{x}‚àà‚Ñù^2,Œæ‚àà‚Ñù}&\\v{p}^‚ä§\\v{x}+Œæ\\\\\n",
        "\\t{subject to}&Œæ‚â•0,x_0‚â§0,x_1‚â§3\\EC$.\n",
        "Let $\\v{z}=\\BM x_0\\\\x_1\\\\Œæ\\EM$ with objective function $\\max\\v{p}^‚ä§\\v{x}+Œæ=\\max\\BM p_0&p_1&1\\EM\\v{z}$\n",
        "$=-\\min\\BM -p_0&-p_1&-1\\EM\\v{z}$.\n",
        "The constraint is $\\BM1&0&0\\\\0&1&0\\\\0&0&-1\\EM\\v{z}‚â§\\BM0\\\\3\\\\0\\EM$.\n",
        "Therefore the linear program standard form is $\\BC-\\min_{\\v{x}‚àà‚Ñù^2,Œæ‚àà‚Ñù}&\\BM -p_0&-p_1&-1\\EM\\v{z}\\\\\n",
        "\\t{subject to}&\\BM1&0&0\\\\0&1&0\\\\0&0&-1\\EM\\v{z}‚â§\\BM0\\\\3\\\\0\\EM\\EC$\n",
        "\n",
        "- 7.8: $\\BC\\min_{\\v{w}‚àà‚Ñù^D}&\\/{1}{2}\\v{w}^‚ä§\\v{w}\\\\\n",
        "\\t{subject to}&\\v{w}^‚ä§\\v{x}‚â•1\\EC$.\n",
        "Lagrangian $\\m{L}(\\v{w},Œª)=\\/{1}{2}\\v{w}^‚ä§\\v{w}-Œª(\\v{w}^‚ä§\\v{x}-1)$.\n",
        "$‚àá_\\v{w}\\m{L}=\\v{w}^‚ä§-Œª\\v{x}^‚ä§=\\v{0}$\n",
        "$‚áí\\v{w}_\\min=Œª\\v{x}$.\n",
        "Dual $\\m{D}(Œª)=\\min_\\v{w}\\m{L}=\\m{L}(\\v{w}_\\min,Œª)$\n",
        "$=\\/{Œª^2}{2}\\v{x}^‚ä§\\v{x}-Œª^2\\v{x}^‚ä§\\v{x}+Œª$\n",
        "$=-\\/{Œª^2}{2}\\v{x}^‚ä§\\v{x}+Œª$.\n",
        "\n",
        "- 7.9: $f(\\v{x})=\\sum_dx_d\\ln x_d=\\sum_df_d(x_d)$.\n",
        "$f^*(\\v{s})=\\sup_\\v{x}\\sum_ds_dx_d-\\sum_dx_d\\ln x_d$\n",
        "$=\\sum_d\\sup_{x_d}‚ü®x_d,s_d‚ü©-x_d\\ln x_d$\n",
        "$=\\sum_df_d^*(s_d)$\n",
        "\n",
        "- 7.10: $f(\\v{x})=\\/{1}{2}\\v{x}^‚ä§\\v{Ax}+\\v{b}^‚ä§\\v{x}+c$.\n",
        "Let gap $g(\\v{x})=\\v{s}^‚ä§\\v{x}-\\/{1}{2}\\v{x}^‚ä§\\v{Ax}-\\v{b}^‚ä§\\v{x}-c$.\n",
        "Then $‚àá_\\v{x}g=\\v{s}^‚ä§-\\v{x}^‚ä§\\v{A}-\\v{b}^‚ä§=\\v{0}$\n",
        "$‚áí\\v{x}=\\v{A}^{-1}(\\v{s}-\\v{b})$.\n",
        "Then $f^*(\\v{s})=\\v{s}^‚ä§\\v{A}^{-1}(\\v{s}-\\v{b})-\\/{1}{2}(\\v{s}-\\v{b})^‚ä§\\v{A}^{-1}(\\v{s}-\\v{b})-\\v{b}^‚ä§\\v{A}^{-1}(\\v{s}-\\v{b})-c$\n",
        "$=\\/{1}{2}(\\v{s}-\\v{b})^‚ä§\\v{A}^{-1}(\\v{s}-\\v{b})-c$."
      ],
      "metadata": {
        "id": "diDjuqA_-xQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function**: given supervised training set $\\{(\\v{x}_1,y_1),...,(\\v{x}_N,y_N)\\}$, we wish to obtain $\\v{Œ∏}^*$ for predictor $f$ such that the output $\\hat{y}=f(\\v{x}_n,\\v{Œ∏}^*)=\\v{Œ∏}^{*‚ä§}\\v{X}‚âày_n$. Let $\\v{X}:=[\\v{x}_1,...,\\v{x}_N]^‚ä§‚àà‚Ñù^{N√óD}$, labels $\\v{y}:=[y_1,...,y_n]^‚ä§‚àà‚Ñù^N$, and loss function $l(y_n,\\hat{y}_n)$, the empirical risk (or average empirical loss) is $\\v{R}_\\t{emp}(f,\\v{X},\\v{y})=\\/{1}{N}\\sum_il(y_n,\\hat{y}_n)$ whereas if infinite data is available then $\\v{R}_\\t{true}(f)=\\E_{\\v{x},\\v{y}}[l(y,f(\\v{x}))]$.\n",
        "\n",
        "- **Least square loss**: $l(y_n,\\hat{y}_n)=(y_n-\\hat{y}_n)^2$. We're solving $\\min_\\v{Œ∏}\\/{1}{N}\\n{\\v{y}-\\v{XŒ∏}}^2$.\n",
        "\n",
        "- **Regularization**: to prevent overfitting, we solve $\\min_\\v{Œ∏}\\/{1}{N}\\n{\\v{y}-\\v{XŒ∏}}^2+Œª\\vn{Œ∏}^2$, which adds a Lagrangian constraint to keep $\\v{Œ∏}$ close to the origin.\n",
        "\n",
        "- **Cross validation**: $K$-fold validation partitions data $\\m{D}$ into $K$ chunks, $K-1$ of which form the training set $\\m{R}$ and last chunk serves as validation set $\\m{V}$ such that $\\m{D}=\\m{R}‚à™\\m{V}$ and $\\m{R}‚à©\\m{V}=‚àÖ$. For each partition $k=1,...,K$, we use $\\m{R}^{(k)}$ to produce $f^{(k)}$ and compute empirical risk $R(f^{(k)},\\m{V}^{(k)})$. Cross validation approximates the expected generalization error $\\E_\\m{V}[R(f,\\m{V})]‚âà\\/{1}{K}\\sum_{k=1}^KR(f^{(k)},\\m{V}^{(k)})$"
      ],
      "metadata": {
        "id": "mdgNgzAMc0Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MML: Linear Regression"
      ],
      "metadata": {
        "id": "SnCzrLICYbWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression**: $p(y|\\v{x})=\\Normal(y|f(\\v{x}),œÉ^2)$ where $\\v{x}‚àà‚Ñù^D$, $y=f(\\v{x})+œµ$, and $œµ‚àº\\Normal(0,œÉ^2)$ iid. The objective is to find a function similar to the unknown $f$ that generated the data. **Linear regression** uses linear parameters $\\v{Œ∏}$ in the model $y=\\v{œï}(\\v{x})^‚ä§\\v{Œ∏}+œµ$ where $œµ‚àº\\Normal(0,œÉ^2)$ iid is the only source of uncertainty. Training set $\\{(\\v{x}_1,y_1),...,(\\v{x}_N,y_N)\\}$ is split into $\\v{y}=[y_1,...,y_N]^‚ä§‚àà‚Ñù^N$ and $\\v{X}=[\\v{x}_1,...,\\v{x}_N]^‚ä§‚àà‚Ñù^{N√óD}$.\n",
        "\n",
        "- $p(\\v{y}|\\v{X},\\v{Œ∏})=\\prodl_{n=1}^N\\Normal(y_n|\\green{\\v{x}_n^‚ä§\\v{Œ∏}},œÉ^2)$ where $\\v{Œ∏}‚àà‚Ñù^D$ has negative log-likelihood $l(\\v{Œ∏})=-\\ln p(\\v{y}|\\v{X},\\v{Œ∏})$. The MLE is $\\blue{\\v{Œ∏}_\\t{MLE}‚àà\\arg\\max_\\v{Œ∏}p(\\v{y}|\\v{X},\\v{Œ∏})=\\arg\\min_\\v{Œ∏}l(\\v{Œ∏})}$.\n",
        "\n",
        "  - $l(\\v{Œ∏})=\\/{1}{2œÉ^2}\\sum_n(y_n-\\v{x}_n^‚ä§\\v{Œ∏})^2$\n",
        "  $=\\/{1}{2œÉ^2}(\\v{y}-\\v{XŒ∏})^‚ä§(\\v{y}-\\v{XŒ∏})$\n",
        "  $=\\/{1}{2œÉ^2}\\green{\\n{\\v{y}-\\v{XŒ∏}}^2}$ thus the MLE is also the least minimum squares error solution.\n",
        "\n",
        "  - $‚àá_\\v{Œ∏}l(\\v{Œ∏})=\\/{1}{2œÉ^2}2(\\v{y}-\\v{XŒ∏})^‚ä§(-\\v{X})$\n",
        "  $=\\/{-1}{œÉ^2}(\\v{y}^‚ä§\\v{X}-\\v{Œ∏}^‚ä§\\v{X}^‚ä§\\v{X})=\\v{0}$\n",
        "  $‚áí(\\v{X}^‚ä§\\v{X})\\v{Œ∏}_\\t{MLE}=\\v{X}^‚ä§\\v{y}$.\n",
        "  Solving this system of equations gives the normal equation\n",
        "  $\\red{\\v{Œ∏}_\\t{MLE}=(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§\\v{y}}$ with requirement $\\blue{\\rk(\\v{X})=D}$.\n",
        "  Then $l(\\v{Œ∏})$ is convex, $‚àá_\\v{Œ∏}^2l(\\v{Œ∏})=\\v{X}^‚ä§\\v{X}‚àà‚Ñù^{D√óD}$ is positive definite and therefore invertible.\n",
        "\n",
        "- With features $p(\\v{y}|\\v{X},\\v{Œ∏})=\\prodl_{n=1}^N\\Normal(y_n|\\green{\\v{œï}(\\v{x}_n)^‚ä§\\v{Œ∏}},œÉ^2)$ where $\\v{Œ∏}‚àà‚Ñù^K$.\n",
        "Non-linear transformations are included in the **feature vector**\n",
        "$\\v{œï}(\\v{x}_n)=\\BMœï_0(\\v{x}_n)\\\\\\vdots\\\\œï_{K-1}(\\v{x}_n)\\EM$\n",
        "and the **feature matrix**\n",
        "$\\v{Œ¶}:=\\BMœï_0(\\v{x}_1)&...&œï_{K-1}(\\v{x}_1)\\\\\\vdots&\\ddots&\\vdots\\\\œï_0(\\v{x}_N)&...&œï_{K-1}(\\v{x}_N)\\EM‚àà‚Ñù^{N√óK}$.\n",
        "\n",
        "  - $l(\\v{Œ∏})=\\/{1}{2œÉ^2}\\sum_n(y_n-\\v{œï}(\\v{x}_n)^‚ä§\\v{Œ∏})^2$\n",
        "  $=\\/{1}{2œÉ^2}\\sum_n(\\v{y}-\\v{Œ¶Œ∏})^‚ä§(\\v{y}-\\v{Œ¶Œ∏})$\n",
        "  $=\\/{1}{2œÉ^2}\\green{\\n{\\v{y}-\\v{Œ¶Œ∏}}^2}$. Similarly to the simple linear regression, $\\red{\\v{Œ∏}_\\t{MLE}=(\\v{Œ¶}^‚ä§\\v{Œ¶})^{-1}\\v{Œ¶}^‚ä§\\v{y}}$\n",
        "  with requirement\n",
        "  $\\blue{\\rk(\\v{Œ¶})=K}$.\n",
        "\n",
        "- $œÉ_\\t{MLE}^2$ uses $l(œÉ^2)=\\/{N}{2}\\ln œÉ^2+\\/{1}{2œÉ^2}\\n{\\v{y}-\\v{Œ¶Œ∏}}^2$.\n",
        "Then $\\/{‚àÇl}{‚àÇœÉ^2}=\\/{N}{2œÉ^2}-\\/{\\n{\\v{y}-\\v{Œ¶Œ∏}}^2}{2œÉ^4}=0$\n",
        "$‚áí\\red{œÉ_\\t{MLE}^2=\\/{1}{N}\\n{\\v{y}-\\v{Œ¶Œ∏}_\\t{MLE}}^2}$\n",
        "is the mean square error (MSE) of $\\v{Œ¶Œ∏}_\\t{MLE}$ against observed $\\v{y}$. As a noise variance estimator it underestimates the true variance because of $\\v{Œ∏}_\\t{MLE}$ over-fitting: $œÉ_\\t{unbiased}^2=\\/{1}{N-K}\\n{\\v{y}-\\v{Œ¶Œ∏}_\\t{MLE}}^2$."
      ],
      "metadata": {
        "id": "OCvp9j2RHSfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting**: $\\t{MSE}(\\v{Œ∏}^*)=\\E[(\\v{Œ∏}^*-\\v{Œ∏}_\\t{true})^2]=\\V(\\v{Œ∏}^*)+\\t{Bias}(\\v{Œ∏}^*)^2$ where $\\t{Bias}(\\v{Œ∏}^*)=\\E[\\v{Œ∏}^*]-\\v{Œ∏}_\\t{true}$.\n",
        "\n",
        "- If $\\v{Œ∏}^*$ degree is too low, the model is biased away from the truth with high systematic error no matter how much data it receives in the training set. If $\\v{Œ∏}^*$ degree is too high, the model memorizes the noise in the training set, and results in high variations when given a new training set.\n",
        "\n",
        "- **Maximum a posteriori**: Parameter magnitude tends to be larger with overfitting. Using prior $p(\\v{Œ∏})$ anchors a parameter. Then posterior $p(\\v{Œ∏}|\\v{X},\\v{y})$ gives the MAP estimator, which is a compromise between prior $p(\\v{Œ∏})$ and MLE from likelihood $p(\\v{y}|\\v{X},\\v{Œ∏})$. The MAP is $\\blue{\\v{Œ∏}_\\t{MAP}‚àà\\arg\\max_\\v{Œ∏}p(\\v{y}|\\v{X},\\v{Œ∏})p(\\v{Œ∏})=\\arg\\min_\\v{Œ∏}[-\\ln p(\\v{y}|\\v{X},\\v{Œ∏})-\\ln p(\\v{Œ∏})]}$\n",
        "\n",
        "  - Let $p(\\v{Œ∏})=\\Normal(\\v{0},b^2\\v{I})$ be the parameters prior. Solve $\\min_\\v{Œ∏}l(\\v{Œ∏})$ where\n",
        "  $\\green{l(\\v{Œ∏})=\\/{1}{2œÉ^2}\\n{\\v{y}-\\v{Œ¶Œ∏}}^2+\\/{1}{2b^2}\\vn{Œ∏}^2}$.\n",
        "  $‚àá_\\v{Œ∏}l(\\v{Œ∏})=\\/{1}{œÉ^2}(\\v{y}-\\v{Œ¶Œ∏})^‚ä§(-\\v{Œ¶})+\\/{1}{b^2}\\v{Œ∏}^‚ä§=\\v{0}$\n",
        "  $‚áí\\/{1}{œÉ^2}\\v{Œ¶}^‚ä§\\v{y}=\\/{1}{b^2}\\v{Œ∏}+\\/{1}{œÉ^2}\\v{Œ¶}^‚ä§\\v{Œ¶Œ∏}$\n",
        "  $‚áí\\v{Œ∏}=\\/{1}{œÉ^2}(\\/{1}{b^2}\\v{I}+\\/{1}{œÉ^2}\\v{Œ¶}^‚ä§\\v{Œ¶})^{-1}\\v{Œ¶}^‚ä§\\v{y}$\n",
        "  $‚áí\\red{\\v{Œ∏}_\\t{MAP}=(\\v{Œ¶}^‚ä§\\v{Œ¶}+\\blue{\\/{œÉ^2}{b^2}\\v{I}})^{-1}\\v{Œ¶}^‚ä§\\v{y}}$.\n",
        "  \n",
        "  - $\\v{Œ¶}^‚ä§\\v{Œ¶}$ is positive-semidefinite. Therefore $\\v{Œ¶}^‚ä§\\v{Œ¶}+\\/{œÉ^2}{b^2}\\v{I}$ is strictly positive-definite and invertible.\n",
        "\n",
        "- **Regularization**: loss function $\\n{\\v{y}-\\v{Œ¶Œ∏}}_2^2+Œª\\vn{Œ∏}_p^p$. The first term is the data-fit (misfit) term. The second term is the regularizer where parameter $Œª$ is the shadow price of the regularization. Smaller values of $p$ lead to sparser $\\v{Œ∏}^*$ with higher frequency of $Œ∏_d=0$.\n",
        "\n",
        "  - **Ridge regression** (Gaussian prior): Using $Œª\\vn{Œ∏}_2^2$ leads to $\\v{Œ∏}^*=(\\v{Œ¶}^‚ä§\\v{Œ¶}+Œª\\v{I})^{-1}\\v{Œ¶}^‚ä§\\v{y}$, which is identical to prior $p(\\v{Œ∏})=\\Normal(\\v{0},\\/{œÉ^2}{Œª}\\v{I})$ where $Œª=\\/{œÉ^2}{b^2}$.\n",
        "\n",
        "  - **Lasso regression** (Laplace prior): Using $Œª\\vn{Œ∏}_1$ corresponds to prior\n",
        "  $p(\\v{Œ∏})=\\prodl_{j=1}^D\\/{1}{2b}\\e{-\\/{|Œ∏_j|}{b}}$ and leads to\n",
        "  $\\green{l(\\v{Œ∏})=\\/{1}{2œÉ^2}\\n{\\v{y}-\\v{Œ¶Œ∏}}_2^2+\\/{1}{b}\\vn{Œ∏}_1}$ where $Œª=\\/{œÉ^2}{b}$."
      ],
      "metadata": {
        "id": "bQ_eQOdYNRHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bayesian linear regression** does not compute $\\v{Œ∏}^*$ and instead relies on full posterior."
      ],
      "metadata": {
        "id": "c92OsYU-Nd7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Convex Sets"
      ],
      "metadata": {
        "id": "3XRLm8J7Zd27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Affine sets**: let $\\green{\\v{1}^‚ä§Œ∏=1}$ and $\\green{Œ∏‚àà‚Ñù^k}$, then $\\blue{Œ∏^‚ä§x}$ is an **affine combination** of the points $x_1,...,x_k$. Set $C$ is called an affine set if $x_1,...,x_k‚ààC$ then all affine combinations of them are in $C$.\n",
        "\n",
        "- Set $C$ is affine if $x_1,x_2‚ààC$ then the straight line through them $Œ∏x_1+(1-Œ∏)x_2‚ààC$ for $Œ∏‚àà‚Ñù$.\n",
        "\n",
        "- If $C$ is an affine set, then for $x_0‚ààC$ the set $V=C-x_0=\\{x-x_0|x‚ààC\\}$ is a subspace: $x_0‚ààC$ gets mapped to origin $0‚ààV$, and the affine set is translated to pass through origin. Thus $C=V+x_0=\\{v+x_0|v‚ààV\\}$.\n",
        "\n",
        "  - Proof: Suppose $v_1,v_2‚ààV$ and $a,b‚àà‚Ñù$. Then $V$ is a subspace $‚áî\\green{av_1+bv_2‚ààV}‚áîav_1+bv_2+x_0‚ààC$.\n",
        "  RHS\n",
        "  $av_1+bv_2+x_0$\n",
        "  $=a(v_1+x_0)+b(v_2+x_0)+(1-a-b)x_0$ is an affine combination of $v_1+x_0‚ààC$, $v_2+x_0‚ààC$, and $x_0‚ààC$ because $a+b+(1-a-b)=1$. Therefore $av_1+bv_2+x_0‚ààC$ and $V$ is a subspace.\n",
        "\n",
        "  - $\\dim(C)=\\dim(V)$ for affine set $C$ and subspace $V=C-x_0$. Dimension of a subspace is the number of spanning basis vectors.\n",
        "\n",
        "- **Affine hull** $\\t{aff}(C):=\\{Œ∏^‚ä§x|x_i‚ààC,\\v{1}^‚ä§Œ∏=1,Œ∏‚àà‚Ñù^k\\}$ is the smallest affine set containing an arbitrary set $C$: if $S$ is any affine set satisfying $C‚äÜS$, then $\\t{aff}(C)‚äÜS$.\n",
        "\n",
        "  - **Affine dimension** of $C$ is the dimension of $\\t{aff}(C)$.\n",
        "\n",
        "  - **Relative interior** $\\t{relint}(C):=\\{x‚ààC|B(x,r)‚à©\\t{aff}(C)‚äÜC\\t{ for some }r>0\\}$. If the affine dimension of $C$ is less than $n$ i.e., flat affine hull, then its volume is 0 and $\\t{int}(C)=‚àÖ$. $B(x,r)‚à©\\t{aff}(C)$ refers to a slice of a ball (with parts sticking out into the empty space shaved off) in the flat affine hull.\n",
        "\n",
        "  - **Relative boundary**: $\\t{cl}(C)\\backslash\\t{relint}(C)$, where the closure $\\t{cl}(C)$ refers to $C$ plus all its limit edge points.\n",
        "\n",
        "- 2.1 (solution set of linear equations): $C=\\{x|Ax=b\\}$ where $A‚àà‚Ñù^{m√ón}$ and $b‚àà‚Ñù^m$ is an affine set. Suppose $x_1,x_2‚ààC$, then $A(Œ∏x_1+(1-Œ∏)x_2)=Œ∏Ax_1+(1-Œ∏)Ax_2=Œ∏b+(1-Œ∏)b=b$\n",
        "$‚áíŒ∏x_1+(1-Œ∏)x_2‚ààC$.\n",
        "\n",
        "  - Every affine set can be expressed as a solution set of a system of linear equations.\n",
        "\n",
        "**Convex sets**: let $\\green{\\v{1}^‚ä§Œ∏=1}$ and $\\green{Œ∏‚âΩ0}$, the sum $\\blue{Œ∏^‚ä§x}$ is a **convex combination** of points $x_1,...,x_k$. Suppose $p:‚Ñù^n‚Üí‚Ñù$ satisfies $p(x)‚â•0$ and $‚à´_Cp(x)\\ dx=1$, then $\\E[x]=‚à´_Cxp(x)\\ dx$ is a convex combination. Set $C$ is called a convex set if $x_1,...,x_k‚ààC$ then all convex combinations of them are in $C$.\n",
        "\n",
        "- **If $x_1,x_2‚ààC$ then a line segment between them $\\red{Œ∏x_1+(1-Œ∏)x_2‚ààC}$ for $Œ∏‚àà[0,1]$**. Where affine set must extend infinitely, a convex set can be bounded. Every pair of two points in the set have a clear line of sight to each other.\n",
        "\n",
        "- **Convex hull** $\\t{conv}(C):=\\{Œ∏^‚ä§x|x_i‚ààC,\\v{1}^‚ä§Œ∏=1,Œ∏‚âΩ0\\}$ is the smallest convex set that contains arbitrary set $C$: if $S$ is any convex set satisfying $C‚äÜS$, then $\\t{conv}(C)‚äÜS$.\n",
        "\n",
        "**Convex cone**: Set $C$ is called a cone if it is nonnegative homogeneous: for every $x‚ààC$ and $Œ∏‚â•0$, we have $Œ∏x‚ààC$. Set $C$ is a convex cone if it is a convex set and a cone.\n",
        "\n",
        "- let $\\green{Œ∏‚âΩ0}$, then $\\blue{Œ∏^‚ä§x}$ is called **conic combination** (nonnegative linear combination) of $x_1,...,x_k$. Set $C$ is a called convex cone if $x_1,...,x_k‚ààC$ then all conic combinations of them are in $C$.\n",
        "\n",
        "- **Conic hull** $\\{Œ∏^‚ä§x|x_i‚ààC,Œ∏‚âΩ0\\}$ is the smallest conic cone that contains an arbitrary set $C$"
      ],
      "metadata": {
        "id": "k6j8We4TZiiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important examples**:\n",
        "\n",
        "- $‚àÖ$, $\\{x_0\\}$, and $‚Ñù^n$ are affine and convex subsets of $‚Ñù^n$.\n",
        "\n",
        "- A line is affine. If it passes through 0 it is a subspace and a convex cone.\n",
        "\n",
        "  - A line segment is convex, but not affine (unless it has 0 length).\n",
        "\n",
        "  - A ray $\\{x_0+Œ∏v|Œ∏‚â•0,v\\neq0\\}$ is convex but not affine. It is a convex cone if $x_0=0$.\n",
        "\n",
        "- A subspace is affine and convex cone.\n",
        "\n",
        "**Hyperplane**: $\\red{\\{x|a^‚ä§x=b\\}=\\{x|a^‚ä§(x-x_0)=0\\}}=x_0+a^\\perp$ where $a^\\perp=\\{v|a^‚ä§v=0\\}$ is orthogonal complement. A hyperplane has dimension $n-1$, is defined by normal $a‚àà‚Ñù^n$ where the inner product is 0, and is offset from the origin determined by $b=a^‚ä§x_0‚àà‚Ñù$. Hyperplane can also be defined as $\\{Fu+g|u‚àà‚Ñù^m\\}$ where $g$ is a particular solution and $Fu$ is the homogenous solution with direction $F$. Hyperplane is affine and convex.\n",
        "\n",
        "- **Halfspace**: $\\red{\\{x|a^‚ä§x‚â§b\\}}$ extends in the direction $-a$, whereas $\\{x|a^‚ä§x‚â•b\\}$ extends in the direction of $a$. Halfspace is convex, but not affine.\n",
        "\n",
        "**Polyhedron**: $\\red{\\{x|Ax‚âºb,Cx=d\\}}=\\{x|a_i^‚ä§x‚â§b_i,i=1,...,m,c_j^‚ä§x=d_j,j=1,...,p\\}$ is the intersection of a finite number of halfspaces and hyperplanes.\n",
        "\n",
        "- **Simplex**: Let $v_0,...,v_k‚àà‚Ñù^n$ be affinely independent so that $v_1-v_0,...,v_k-v_0$ are linearly independent. Then the $k$-dimensional simplex is $\\t{conv}\\{v_0,...,v_k\\}=\\{Œ∏^‚ä§v|\\v{1}^‚ä§Œ∏=1,Œ∏‚âΩ0\\}$, and is equivalent to a polyhedron.\n",
        "\n",
        "  - Simplex is generalized triangle (k=3) and tetrahedron (k=4).\n",
        "\n",
        "  - Simplex‚ÜíPolyhedron: Define vector $y=(Œ∏_1,...,Œ∏_k)$ and matrix $B=[v_1-v_0,...,v_k-v_0]‚àà‚Ñù^{n√ók}$. Then\n",
        "  $\\BC\n",
        "  By=Œ∏^‚ä§v-Œ∏_0v_0-v_0\\v{1}^‚ä§y=Œ∏^‚ä§v-Œ∏_0v_0-v_0(1-Œ∏_0)‚áíŒ∏^‚ä§v=v_0+By \\\\\n",
        "  \\t{conv}\\{v_0,...,v_k\\}=\\{x|x=v_0+By,y‚âΩ0,\\v{1}^‚ä§y+Œ∏_0=1\\}\\EC$. Because $\\rk(B)=k$, there exists $A=\\BM A_1\\\\A_2\\EM‚àà‚Ñù^{n√ón}$ such that $AB=\\BM I_k\\\\0_{n-k}\\EM‚àà‚Ñù^{n√ók}$\n",
        "  and\n",
        "  $\\BC\n",
        "  A_1x=A_1v_0+A_1By=A_1v_0+y&‚áíy=A_1x-A_1v_0\\\\\n",
        "  A_2x=A_2v_0+A_2By=A_2v&‚áíA_2x=A_2v_0\n",
        "  \\EC$.\n",
        "  The simplex can be written in polyhedron form $\\{x|A_2x=A_2v_0,A_1x‚â•A_1v_0,\\v{1}^‚ä§A_1x‚â§1+\\v{1}^‚ä§A_1v_0\\}$\n",
        "\n",
        "  - **Probability simplex** $\\blue{\\{x|x‚âΩ0,\\v{1}^‚ä§x=1\\}}$ is the $n-1$-dimensional simplex constructed from unit vectors $\\t{conv}\\{e_1,...,e_n|e_i‚ààI_n\\}$. Each vector $x=(x_1,...,x_n)=x_1e_1+...+x_ne_n$ in the probability simplex is a discrete probability distribution.\n",
        "\n",
        "- The convex hull of set $\\{v_1,...,v_k\\}$ is $\\t{conv}(v_1,...,v_k)=\\{Œ∏^‚ä§v|\\v{1}^‚ä§Œ∏=1,Œ∏‚âΩ0\\}$ written in its generator form. However it is difficult to express it in linear constraints form unless it is simplex or a unit-ball.\n",
        "\n",
        "  - $‚Ñì_‚àû$-norm $\\n{x}_‚àû:=\\max_{x_i}\\{|x_i||x_i‚ààx\\}$\n",
        "\n",
        "- **Every linear inequality defines a halfspace ($a^‚ä§x‚â§b$) or a polyhedron ($Ax‚âºb$).**\n",
        "\n",
        "**Euclidean/norm ball**: $\\red{B(x_c,r)=\\{x|\\n{x-x_c}_2‚â§r\\}}=\\{x|(x-x_c)^‚ä§(x-x_c)‚â§r^2\\}=\\{x_c+u|\\ \\n{u}_2‚â§r\\}$.\n",
        "\n",
        "- $B(x_c,r)$ is a convex set.\n",
        "\n",
        "  - Proof: $\\n{x_1-x_c}‚â§r$ and $\\n{x_2-x_c}‚â§r$ then by triangle inequality\n",
        "  $\\n{Œ∏x_1+(1-Œ∏)x_2-x_c}$\n",
        "  $=\\n{Œ∏(x_1-x_c)+(1-Œ∏)(x_2-x_c)}$\n",
        "  $‚â§Œ∏\\n{(x_1-x_c)}+(1-Œ∏)\\n{(x_2-x_c)}$\n",
        "  $‚â§r$\n",
        "\n",
        "- **Ellipsoid**: $\\red{\\{x|(x-x_c)^‚ä§P(x-x_c)‚â§1\\}}$ where $P‚àà‚Ñù^{n√ón}_{++}$ (symmetric positive-definite), where the axes of the ellipsoid are the eigenvectors of $P$ and stretched by magnitude equal to the sqrt eigenvalues of $P$.\n",
        "\n",
        "- **Second-order norm cone**: $\\{(x,t)|\\n{x}‚â§t\\}‚äÜ‚Ñù^{n+1}$ is a convex cone. This is the definition of $\\epi(\\n{x})$. Therefore $f(x)=Œ±\\n{x}$ describes the \"skin\" of a cone whose height is scaled by $Œ±$.\n",
        "\n",
        "  - The **second-order cone** is the $‚Ñì_2$-norm cone $\\{(x,t)|\\n{x}_2‚â§t\\}=\\{\\BM x\\\\t\\EM|\\BM x\\\\t\\EM^‚ä§\\BM I&0\\\\0&-1\\EM\\BM x\\\\t\\EM‚â§0, t‚â•0\\}$. It is the epigraph of the Euclidean norm.\n",
        "\n",
        "**Proper cones**: Convex cone $K‚äÜ‚Ñù^n$ is proper if\n",
        "\n",
        "  - $\\BC\n",
        "  1.&K\\t{ is closed}\\\\\n",
        "  2.&K\\t{ is solid}\\\\\n",
        "  3.&K\\t{ is pointed (contains no line)}\\EC$\n",
        "\n",
        "- Nonnegative orthant $‚Ñù_+^n=\\{x‚àà‚Ñù^n|X_i‚â•0,i=1,...,n\\}$\n",
        "\n",
        "- Positive semidefinite cones: $\\v{S}^n=\\{X‚àà‚Ñù^{n√ón}|X=X^‚ä§\\}$ are symmetric matrices, $\\v{S}_+^n=\\{X‚àà\\v{S}^n|X‚âΩ0\\}$ are positive-semidefinite (nonnegative eigenvalues), and $\\v{S}_{++}^n=\\{X‚àà\\v{S}^n|X‚âª0\\}$ are positive-definite.\n",
        "\n",
        "- **Generalized inequalities**: $\\red{x‚âº_Ky ‚áî y-x‚ààK}$ and $\\red{x‚â∫_Ky‚áîy-x‚àà\\t{int}(K)}$, where $K$ is proper cone. B&V uses notation $\\preceq$ and $\\succeq$\n",
        "\n",
        "  - $\\BC\n",
        "  K=‚Ñù_+&‚âº_{‚Ñù_+}\\t{ is identical to }‚â§\\t{ for }‚Ñù\\\\\n",
        "  K=‚Ñù_+^n&‚âº_{‚Ñù_+^n}\\t{ is vector element-wise inequality}\\\\\n",
        "  K=\\v{S}_+^n&‚âº_{\\v{S}_+^n}\\t{ is matrix inequality}\n",
        "  \\EC$\n",
        "\n",
        "  - $x‚ààS$ is the **minimum** element of $S$ with respect to $‚âº_K$ if for every $y‚ààS$ we have $x‚âº_Ky$. Everything else in the set is comparable and bigger. Written as $S‚äÜx+K$ to mean all points comparable and $‚â•$.\n",
        "\n",
        "  - $x‚ààS$ is the **minimal** element of $S$ with respect to $‚âº_K$ if $y‚ààS$ and $y‚âº_Kx$ $‚áíy=x$. No other point is strictly smaller. Other points $y‚ààS$ may be different and incomparable, but not necessarily $y‚âº_Kx$. Written as $(x-K)‚à©S=\\{x\\}$ to mean the only thing comparable and $‚â§$ is $x$ it self.\n",
        "\n",
        "  - A cone defines what positive means. For point $x$ in a set, $y‚â§_Kx$ means $x-y=k$ for some vector $k‚ààK$. Or $y=x-k$ for that some vector $k‚ààK$. Therefore the set of points $y‚â§_Kx$ must exist in the cone pointing in opposite direction radiating from $x$."
      ],
      "metadata": {
        "id": "i2rnUOIehAnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preservation of convexity**:\n",
        "\n",
        "- If $S_1$ and $S_2$ are convex, then $S_1‚à©S_2$ is convex.\n",
        "\n",
        "- Suppose $S‚äÜ‚Ñù^n$ is convex and $f:‚Ñù^n‚Üí‚Ñù^m$ is affine function $\\{f(x)=Ax+b|A‚àà‚Ñù^{m√ón},b‚àà‚Ñù^m\\}$, then image $f(S)=\\{f(x)|x‚ààS\\}$ and inverse image $f^{-1}(S)=\\{x|f(x)‚ààS\\}$ are convex.\n",
        "\n",
        "- Affine combinations of convex sets are convex.\n",
        "\n",
        "- Projection of a convex set onto a lower dimension uses affine function\n",
        "$f(\\BM x_1\\\\x_2\\EM)=\\BM 1&0\\EM\\BM x_1\\\\x_2\\EM=x_1$.\n",
        "If $S‚äÜ‚Ñù^m√ó‚Ñù^n$ is convex, then\n",
        "$T=\\{x_1‚àà‚Ñù^m|(x_1,x_2)‚ààS\\t{ for some }x_2‚àà‚Ñù^n\\}$ is convex.\n",
        "\n",
        "- **Cartesian product** of lower dimension convex sets onto a higher dimension is convex using inverse image of previous affine function. if $S_1$ and $S_2$ are convex, then $\\blue{S_1√óS_2:=\\{(x_1,x_2)|x_1‚ààS_1,x_2‚ààS_2\\}}$ is convex. Here the notation $(x_1,x_2)=\\BM x_1\\\\x_2\\EM$ is a longitudinal stacking of two vectors.\n",
        "\n",
        "- **Partial sums** of convex $S_1,S_2$ as $S=\\{(x,y_1+y_2)|(x,y_1)‚ààS_1,(x,y_2)‚ààS_2,x‚àà‚Ñù^m,y_1,y_2‚àà‚Ñù^n\\}$ are convex.\n",
        "\n",
        "- **Linear matrix inequality** (LMI) is an equality $\\blue{F(x)=F_0+x_1F_1+...+x_nF_n‚âΩ0}$, which is an affine combination of symmetric matrices. $\\blue{\\{x|F(x)‚âΩ0\\}}$ (or $F(x)‚âº0$) is always a convex set because it is the inverse image of $F(x)‚àà\\v{S}_{+}^n$.\n",
        "\n",
        "- **Perspective function** $P:‚Ñù^{n+1}‚Üí‚Ñù^n$ with $\\dom(P)=‚Ñù^n√ó‚Ñù_{++}$ is defined as $\\blue{P(z,t)=\\/{z}{t}}$, which scales the vector so its last component is one, then drops it from the vector. If $C$ is convex, then its image $P(C)=\\{P(x)|x‚ààC\\}$ and inverse image $P^{-1}(C)=\\{x|P(x)‚ààC\\}$ are convex.\n",
        "\n",
        "  - Proof: Suppose\n",
        "  $x=(\\tilde{x},x_{n+1})$ and\n",
        "  $y=(\\tilde{y},y_{n+1})$. Then\n",
        "  $P(Œ∏x+(1-Œ∏)y)=\\/{Œ∏\\tilde{x}+(1-Œ∏)\\tilde{y}}{Œ∏x_{n+1}+(1-Œ∏)y_{n+1}}$\n",
        "  $=ŒºP(x)+(1-Œº)P(y)$ where\n",
        "  $Œº=\\/{Œ∏x_{n+1}}{Œ∏x_{n+1}+(1-Œ∏)y_{n+1}}‚àà[0,1]$.\n",
        "\n",
        "- **Linear fractional function**: Suppose $g:‚Ñù^n‚Üí‚Ñù^{m+1}$ is affine such that $g(x)=\\BM A\\\\c^‚ä§\\EM x+\\BM b\\\\d\\EM$ where $A‚àà‚Ñù^{m√ón}$, $c‚àà‚Ñù^n$, $b‚àà‚Ñù^m$, $d‚àà‚Ñù$. Then the non-linear $f:‚Ñù^n‚Üí‚Ñù^m$ where $\\blue{f(x)=P‚àòg(x)=\\/{Ax+b}{c^‚ä§x+d}}$ with $\\dom(f)=\\{x|c^‚ä§x+d>0\\}$ is called a linear-fractional (or projective) function.\n",
        "\n",
        "  - $f(x)=P‚àòg(x)=\\/{Ax+b}{c^‚ä§x+d}$ says $f(x)$ is equivalent to perspective $P(z,t)$ where $z=Ax+b$ and $t=c^‚ä§x+d$. In general, linear-over-linear functions are convex. Morever, image and inverse image are convex.\n",
        "\n",
        "  - Projective geometry: We convert this non-linear problem in $‚Ñù^n‚Üí‚Ñù^m$ into a linear one in $‚Ñù^{n+1}‚Üí‚Ñù^{m+1}$. Create block matrix $Q=\\BM A&b\\\\c^‚ä§&d\\EM‚àà‚Ñù^{(m+1)√ó(n+1)}$ and augment $x‚Ü¶\\BM x\\\\1\\EM$. The augmented point is a ray in $‚Ñù^{n+1}$ to which we apply linear transformation and normalize\n",
        "  $Q\\BM x\\\\1\\EM=\\BM Ax+b\\\\c^‚ä§x+d\\EM‚àà‚Ñù^{m+1}$\n",
        "  $‚Ü¶\\BM (Ax+b)/(c^‚ä§x+d)\\\\1\\EM=\\BM f(x)\\\\1\\EM$.\n",
        "\n",
        "  - If $C$ is convex, then image $f(C)$ and inverse image $f^{-1}(C)$ are also convex.\n",
        "\n",
        "- 2.7: Show $\\v{S}_+^n$ is a convex set. $\\v{S}_+^n=\\bigcap_{z\\neq0}\\{X‚àà\\v{S}^n|z^‚ä§Xz‚â•0\\}$ is an intersection of the test $z^‚ä§Xz‚â•0$ for every possible vector $z\\neq0$. However $z^‚ä§Xz‚â•0$ is a linear inequality of $X$ therefore represents a halfspace. Intersection of halfspaces is a polyhedron and convex.\n",
        "\n",
        "- 2.8: Let $p(t)=\\sum_{k=1}^mx_k\\cos(kt)$. Show the set $\\{x‚àà‚Ñù^m||p(t)|‚â§1, |t|‚â§\\/{œÄ}{3}\\}$ is convex. The set is equivalent to $\\{x‚àà‚Ñù^m||-1‚â§[\\cos(t),...,\\cos(mt)]^‚ä§x‚â§1,|t|‚â§\\/{œÄ}{3}\\}$, where the test is a intersection of two linear inequalities (two halfspaces in opposite directions). Therefore it is convex.\n",
        "\n",
        "- 2.9 (polyhedron): $\\{x|Ax‚âºb,Cx=d\\}$ is inverse image of affine function $f(x)=(b-Ax,d-Cx)=\\BM b-Ax\\\\d-Cx\\EM‚àà\\BM ‚Ñù_+^m\\\\\\{0\\}\\EM$, which is a Cartesian product. Therefore $\\{x|Ax‚âºb,Cx=d\\}=\\{x|f(x)‚àà‚Ñù_+^m√ó\\{0\\}\\}$ is convex.\n",
        "\n",
        "- 2.10 (LMI): $A(x)=\\sum_{i=1}^nx_iA_i‚âºB$ where $B,A_i‚àà\\v{S}^m$ is called a linear matrix inequality in $x$. Its solution set $\\{x|A(x)‚âºB\\}$ is inverse image of affine function $f(x)=B-A(x)‚àà\\v{S}_+^m$ and is therefore convex.\n",
        "\n",
        "  - Ordinary linear inequality $a^‚ä§x=\\sum_{i=1}^na_ix_i‚â§b$ where $a_i,b‚àà‚Ñù$ is just a special case.\n",
        "\n",
        "- 2.13 (conditional probabilities): Let $U=\\{1,...,n\\}$ and $V=\\{1,...,m\\}$ be two discrete random variables.  Then $P(U=i|V=j)=\\/{P(U=i,V=j)}{\\sum_{k=1}^nP(U=k,V=j)}$ is a linear fractional function."
      ],
      "metadata": {
        "id": "SZfeSVUE03Bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Separating hyperplane theorem**: Suppose $C$ and $D$ are nonempty disjoint convex sets $C‚à©D=‚àÖ$. Then there exists $a\\neq0$ and $b‚àà‚Ñù$ such that $\\BC a^‚ä§x‚â§b\\ ‚àÄx‚ààC\\\\a^‚ä§x‚â•b\\ ‚àÄx‚ààD\\EC$. The hyperplane $\\{x|a^‚ä§x=b\\}$ is a **separating hyperplane** for $C,D$.\n",
        "\n",
        "- $\\t{dist}(C,D)=\\inf\\{\\n{u-v}_2|u‚ààC,v‚ààD\\}$. There exists $c‚ààC$ and $d‚ààD$ such that $\\n{c-d}_2=\\t{dist}(C,D)$. Then the separating plane $f(x)=a^‚ä§(x-x_0)$ passes through midpoint $x_0=\\/{c+d}{2}$ and has normal $a=d-c$ (pointing toward $D$). I.e., $\\green{f(x)=(d-c)^‚ä§(x-\\/{c+d}{2})}$.\n",
        "\n",
        "- Existence of a separating hyperplane does not mean $C,D$ are disjoint as defined by $‚â§$ and $‚â•$, as $C=\\{0\\}$ and $D=\\{0\\}$ is a counterexample.\n",
        "\n",
        "  - Any two convex sets $C,D$ at least one of which is open, is disjoint iff there exists a separating hyperplane.\n",
        "\n",
        "**Supporting hyperplanes**: Suppose $C‚äÜ‚Ñù^n$ and $x_0$ is a point in its boundary $x_0‚àà\\t{boundary}(C)=\\t{closure}(C)\\backslash\\t{int}(C)$. If $a\\neq0$ satisfies $a^‚ä§(x-x_0)‚â§0$ for all $x‚ààC$, then $\\red{\\{x|a^‚ä§(x-x_0)=0\\}}$ is the supporting hyperplane to $C$ at $x_0$.\n",
        "\n",
        "- Tangent. But unlike tangent, supporting hyperplanes also exist for $x_0$ at a sharp corner."
      ],
      "metadata": {
        "id": "6YMPCYJY-TGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dual cones**: Let $K$ be a cone. Then $K^*=\\{y|x^‚ä§y‚â•0\\ ‚àÄx‚ààK\\}$ is the dual cone of $K$. Every $(x‚ààK,y‚ààK^*)$ pair shares an angle no more than 90¬∞. **Every vector $y‚ààK^*$ is the normal of a supporting hyperplane of $K$ at the origin.**\n",
        "\n",
        "- $\\BC\n",
        "K^*\\t{ is closed and convex}\\\\\n",
        "K_1‚äÜK_2‚áíK_2^*‚äÜK_1^*\\\\\n",
        "\\t{int}(K)\\neq‚àÖ‚áíK^*\\t{ is pointed}\\\\\n",
        "\\t{cl}(K)\\t{ is pointed}‚áí\\t{int}(K^*)\\neq‚àÖ\\\\\n",
        "K^{**}=\\t{cl}(\\t{conv}(K)) \\\\\n",
        "\\red{K\\t{ is proper}‚áîK^*\\t{ is proper}‚áîK^{**}=K}\n",
        "\\EC$\n",
        "\n",
        "  - Geometrically a symmetric cone and its dual share the same central axis because of the angular requirement.\n",
        "\n",
        "- Proper $K$ induces primal inequality $‚âº_K$, then $‚âº_{K^*}$ is its dual inequality.\n",
        "\n",
        "  - Primal inequality from $y-x‚ààK$ has a dual inequality $Œª^‚ä§x‚â§Œª^‚ä§y\\ ‚àÄŒª‚âΩ_{K^*}0$ $\\red{\\BC\n",
        "  x‚âº_Ky ‚áî Œª^‚ä§x‚â§Œª^‚ä§y\\ ‚àÄŒª‚âΩ_{K^*}0\\\\\n",
        "  x‚â∫_Ky ‚áî Œª^‚ä§x< Œª^‚ä§y\\ ‚àÄŒª‚âΩ_{K^*}0, Œª\\neq0\n",
        "  \\EC}$\n",
        "\n",
        "  - Proof ($‚âº$): Suppose $x‚âº_Ky$. Then $y-x‚ààK$ and $Œª^‚ä§(y-x)‚â•0$ for all $Œª‚ààK^*$ by definition of dual cone.\n",
        "\n",
        "  - Proof ($‚â∫$): Suppose $x‚â∫_Ky$. Then $y-x‚àà\\t{int}(K)$. The vectors in $K^*$ that hold 90¬∞ angle with border of $K$ necessarily hold less than 90¬∞ with interior of $K$. Therefore $Œª^‚ä§(y-z)>0$ for all $Œª‚ààK^*$, which also implies $Œª\\neq0$.\n",
        "\n",
        "- Example: Why $x‚â∫_Ky ‚áî Œª^‚ä§x< Œª^‚ä§y\\ ‚àÄ\\green{Œª‚âΩ_{K^*}0, Œª\\neq0}$ and not $\\green{Œª‚âª_{K^*}0}$? Suppose $K=‚Ñù_+^2=K^*$, then $x=(0,5)‚ààK$ does not satisfy $x‚âª_K0$.\n",
        "\n",
        "  - $Œª‚âª_{K^*}0$: Let $Œª=(1,1)$, then $Œª^‚ä§x=5$. Therefore $x‚âª_K0$. Because $Œª‚àà\\t{int}(K)$, we have $Œª^‚ä§x>0\\ ‚àÄx‚ààK$. They cannot correctly distinguish $x‚ààK$ located on cone boundary and reject $x‚âª_K0$.\n",
        "\n",
        "  - $Œª‚âΩ_{K^*}0, Œª\\neq0$: Let $Œª=(1,0)$, then $Œª^‚ä§x=0$. Therefore $x\\not‚âª_K0$. We can check that $x‚ààK$ is located on boundary, and correctly reject $x‚âª_K0$.\n",
        "\n",
        "- $x‚ààS$ is the **minimum** element with respect to $‚âº_K$ iff $x=\\arg\\min_zŒª^‚ä§z$ for all $Œª‚ààK^*$ and that hyperplane $\\{z|Œª^‚ä§(z-x)=0\\}$ supports $S$ at $x$.\n",
        "\n",
        "- If $x=\\arg\\min_zŒª^‚ä§z$ for all $Œª‚ààK^*$ then $x‚ààS$ is the **minimal** element with respect to $‚âº_K$. However $x$ can be minimal, but not a minimizer of $Œª^‚ä§$ unless $S$ is convex.\n",
        "\n",
        "**Dual norm** of a vector $u$ with respect to $\\n{‚ãÖ}_p$ is $\\red{\\n{u}_q:=\\sup\\{u^‚ä§x|\\ \\n{x}_p‚â§1\\}}$.\n",
        "\n",
        "- For any $p‚â•1$, the dual of $‚Ñì_p$ norm is $‚Ñì_q$ norm defined by H√∂lder conjugate: $\\blue{\\/{1}{p}+\\/{1}{q}=1}$. The dual norm max dot product is defined by the H√∂lder inequality: $\\blue{u^‚ä§x‚â§\\n{x}_p\\n{u}_q}‚áí\\sup u^‚ä§x=(1)\\n{u}_q$.\n",
        "\n",
        "- Dual norm of $u$ is the max dot product between $u$ and any collinear unit vector $x$ that satisfies $\\n{x}_p=1$ (because $\\sup u^‚ä§x=\\n{u}_2\\sup(\\n{x}_2\\cos œâ)$). The condition is written as $\\{x|\\ \\n{x}‚â§ 1\\}$ instead of $\\{x|\\ \\n{x}=1\\}$ because a filled ball is convex and allows convex analysis tools like support functions, while a hollow shell is non-convex.\n",
        "\n",
        "- **Alignment trick**: $\\blue{\\sup\\{u^‚ä§v|\\ \\n{u}_2‚â§1\\}=\\sup\\{\\n{u}_2\\n{v}_2\\cos œâ|\\ \\n{u}_2‚â§1\\}=\\n{v}_2}$.\n",
        "\n",
        "- 2.22: Dual cone of subspace $V‚äÜ‚Ñù^n$ is $V^\\perp=\\{y|v^‚ä§y=0\\ ‚àÄv‚ààV\\}$. If $x‚ààV$ then $-x‚ààV$.\n",
        "\n",
        "- 2.23: Dual cone of nonnegative orthant $‚Ñù_+^n$ is $‚Ñù_+^n$ itself (self-dual).\n",
        "\n",
        "- 2.24: Positive semidefinite cone $\\v{S}_+^n$ is self-dual. Proof:\n",
        "$(\\v{S}_+^n)^*=\\{Y|‚ü®X,Y‚ü©‚â•0\\ ‚àÄX‚ààK\\}$\n",
        "$=\\{Y|\\tr(X^‚ä§Y)‚â•0\\ ‚àÄX‚ààK\\}$\n",
        "$=\\{Y|\\sum_iŒª_i\\tr(u_iu_i^‚ä§Y)‚â•0\\ ‚àÄUDU^{-1}‚ààK\\}$\n",
        "$=\\{Y|\\sum_iŒª_i\\tr(u_i^‚ä§Yu_i)‚â•0\\ ‚àÄUDU^{-1}‚ààK\\}$\n",
        "$=\\{Y|Y‚àà\\v{S}_+^n\\}$.\n",
        "\n",
        "- 2.25: Dual cone of norm cone $\\{(x,t)‚àà‚Ñù^{n+1}|\\ \\n{x}‚â§t\\}$ is $\\{(u,v)‚àà‚Ñù^{n+1}|\\ \\n{u}_*‚â§v\\}$ where $\\n{u}_*$ is dual norm\n",
        "\n",
        "  - $\\{(x,t)‚àà‚Ñù^{n+1}|\\ \\n{x}_2‚â§t\\}$ is self-dual, as Euclidean norm is dual norm of itself."
      ],
      "metadata": {
        "id": "Q5mKhF-sEblu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2.5: What is the distance between two parallel hyperplanes $\\{x‚àà‚Ñù^n|a^‚ä§x=b_1\\}$ and $\\{x‚àà‚Ñù^n|a^‚ä§x=b_2\\}$?\n",
        "$b_1=a^‚ä§x_1$ and $b_2=a^‚ä§x_2$ where $x_1$ and $x_2$ are offsets from origin for the two planes. Project them onto $a$ and the displacement between them is $\\/{a^‚ä§x_1}{\\n{a}_2^2}a-\\/{a^‚ä§x_2}{\\n{a}_2^2}a=\\/{(b_1-b_2)a}{\\n{a}_2^2}$. Therefore the distance is $\\/{|b_1-b_2|}{\\n{a}_2}$.\n",
        "\n",
        "- 2.6: Two halfspaces $H_1=\\{x|a_\\v{1}^‚ä§x‚â§b_1\\}$ and $H_2=\\{x|a_2^‚ä§x‚â§b_2\\}$. Give conditions when $H_1‚äÜH_2$ and when $H_1=H_2$. Let $Œª>0$. Then $H_1=\\{x|Œªa_\\v{1}^‚ä§x‚â§Œªb_1\\}$. $H_1‚äÜH_2$ if $a_2=Œªa_1$ (collinear) and $b_2‚â•Œªb_1$. $H_1=H_2$ if $a_2=Œªa_1$ and $b_2=Œªb_1$.\n",
        "\n",
        "- 2.8: Polyhedron?\n",
        "\n",
        "  - $S=\\{y_1a_1+y_2a_2|y_1‚àà[-1,1], y_2‚àà[-1,1]\\}$ where $a_1,a_2‚àà‚Ñù^n$ is a parallelogram therefore polyhedron.\n",
        "\n",
        "  - $S=\\{x‚àà‚Ñù^n|x‚âΩ0,\\v{1}^‚ä§x=1,a^‚ä§x=b_1,a^‚ä§xa=b_2\\}$ is the set of all discrete PMFs with $P(X=a_i)=x_i$. All constraints are linear therefore polyhedron\n",
        "\n",
        "  - $S=\\{x‚àà‚Ñù^n|x‚âΩ0,x^‚ä§y‚â§1\\ ‚àÄy:\\n{y}_2=1\\}=\\{x‚àà‚Ñù^n|x‚âΩ0,\\n{x}_2‚â§1\\}$ is the intersection of the nonnegative orthant and a unit ball, which cannot be described by finite number of linear inequalities.\n",
        "\n",
        "  - $S=\\{x‚àà‚Ñù^n|x‚âΩ0,x^‚ä§y‚â§1\\ ‚àÄy:\\n{y}_1=1\\}=\\{x‚àà‚Ñù^n|x‚âΩ0,\\n{x}_‚àû‚â§1\\}$ is the intersection of the nonnegative orthant and a unit square and therefore polyhedron.\n",
        "\n",
        "- 2.10: Show $C=\\{x‚àà‚Ñù^n|x^‚ä§Ax+b^‚ä§x+c‚â§0\\}$ is convex if $A‚âΩ0$. $f(x)=x^‚ä§Ax+b^‚ä§x+c$ has Hessian $2A$. Therefore if $A‚âΩ0$ then $f(x)‚â§0$ is the **sublevel set of a convex function**, which is convex.\n",
        "\n",
        "- 2.12: Convex?\n",
        "\n",
        "  - Slab $\\{x‚àà‚Ñù^n|Œ±‚â§a^‚ä§x‚â§Œ≤\\}$ is the intersection of two halfplanes. Therefore convex.\n",
        "\n",
        "  - Rectangle $\\{x‚àà‚Ñù^n|a‚âºx‚âºb\\}$ is the intersection of $n$ slabs. Therefore convex.\n",
        "\n",
        "  - Wedge $\\{x‚àà‚Ñù^n|a_\\v{1}^‚ä§x‚â§b_1,a_2^‚ä§x‚â§b_2\\}$ is the intersection of two halfplanes. Therefore convex.\n",
        "\n",
        "  - $\\{x|\\ \\n{x-x_0}_2‚â§\\n{x-y}_2\\ ‚àÄy‚ààS\\}$.\n",
        "  Here $\\n{x-x_0}_2‚â§\\n{x-y}_2$\n",
        "  $‚áí\\n{x-x_0}^2‚â§\\n{x-y}^2$\n",
        "  $‚áí\\n{x}^2-2x_0^‚ä§x+\\n{x_0}^2‚â§\\n{x}^2-2x^‚ä§y+\\n{y}^2$\n",
        "  $‚áí2x^‚ä§(y-x_0)‚â§\\n{y}^2-\\n{x_0}^2$ is a linear inequality on $x$. Halfplane therefore convex.\n",
        "\n",
        "  - $\\{x|\\t{dist}(x,S)‚â§\\t{dist}(x,T)\\}$. Counter-example $S=\\{-1,1\\}$ and $T=\\{0\\}$. Not convex.\n",
        "\n",
        "  - $\\{x|x+S_2‚äÜS_1\\}$ where $S_1,S_2‚äÜ‚Ñù^n$ and $S_1$ is convex.\n",
        "  $x+S_2‚äÜS_1$ means $x+y‚ààS_1\\ ‚àÄy‚ààS_2$\n",
        "  $‚áíx‚ààS_1-y\\ ‚àÄy‚ààS_2$\n",
        "  $‚áíx‚àà\\bigcap_{y‚ààS_2}S_1-\\{y\\}$, the intersection of translations of $S_1$.\n",
        "  Therefore convex.\n",
        "\n",
        "- 2.15: Let $x$ be random variable with $P(x=a_1)=p_i$ where $a_1< ...< a_n$ and $p‚àà‚Ñù^n$ lies in the probability simplex $P=\\{p|\\v{1}^‚ä§p=1,p‚âΩ0\\}$. Which of these conditions on a set of $p‚ààP$ is convex?\n",
        "\n",
        "  - $Œ±‚â§\\E[f(x)]‚â§Œ≤$. Here $\\E[f(x)]=f(x)^‚ä§p$ is a linear inequality of $p$. The set $\\{p|Œ±‚â§f(x)^‚ä§p‚â§Œ≤\\}$ is a slab therefore convex.\n",
        "\n",
        "  - $P(x>Œ±)‚â§Œ≤$. Equivalent to $\\sum_{i:a_i>Œ±}p_i‚â§Œ≤$ is a linear inequality of $p$. It is a halfplane therefore convex.\n",
        "\n",
        "  - $\\E|x^3|‚â§Œ±\\E|x|$. Equivalent to $\\sum_i(|a_i^3|-Œ±|a_i|)p_i‚â§0$. Halfplane therefore convex.\n",
        "\n",
        "  - $\\Var(x)‚â§Œ±$. Equivalent to $a^‚ä§pa-(a^‚ä§p)^2‚â§Œ±$\n",
        "  $‚áí(a^‚ä§p)^2-a^‚ä§pa+Œ±‚â•0$. The **superlevel set of a convex function is NOT convex** (disjoint epigraph).\n",
        "\n",
        "  - $\\Var(x)‚â•Œ±$. Equivalent to $(a^‚ä§p)^2-a^‚ä§pa+Œ±‚â§0$. The sublevel set of a convex function is convex.\n",
        "\n",
        "- 2.16: Prove partial sums $S=\\{(x,y_1+y_2)|x‚àà‚Ñù^m,y_1,y_2‚àà‚Ñù^n,(x,y_1)‚ààS_1,(x,y_2)‚ààS_2\\}$ is convex if $S_1,S_2$ are convex sets. Let $(a,b_1)‚ààS_1$, $(a,b_2)‚ààS_2$ then $(a,b_1+b_2)‚ààS$. Let $(c,d_1)‚ààS_1$, $(c,d_2)‚ààS_2$ then $(c,d_1+d_2)‚ààS$. Fix $Œ∏‚àà[0,1]$. Then $(Œ∏a+(1-Œ∏)c,Œ∏b_1+(1-Œ∏)d_1)‚ààS_1$, $(Œ∏a+(1-Œ∏)c,Œ∏b_2+(1-Œ∏)d_2)‚ààS_2$, and $(Œ∏a+(1-Œ∏)c,Œ∏(b_1+b_2)+(1-Œ∏)(d_1+d_2))‚ààS$.\n",
        "\n",
        "- 2.17: Describe $P(C)=\\{\\/{v}{t}|(v,t)‚ààC,t>0\\}$.\n",
        "\n",
        "  - $C=\\t{conv}\\{(v_1,t_1),...,(v_K,t_K)\\}$. Then $P(C)=\\t{conv}\\{\\/{v_1}{t_1},...,\\/{v_K}{t_K}\\}$.\n",
        "\n",
        "  - $C=\\{(v,t)|f^‚ä§v+gt=h\\}$. Then $P(C)=\\{x|f^‚ä§x+g=\\/{h}{t}\\}$ is a hyperplane."
      ],
      "metadata": {
        "id": "5OX-xGpk_37i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Convex Functions\n",
        "\n",
        "- Denominator layout $‚àáf(x)^‚ä§(y-x)$"
      ],
      "metadata": {
        "id": "lCXdVkbcqRO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convex function**: $f:‚Ñù^n‚Üí‚Ñù$ is convex if $\\dom(f)$ is a convex set and $‚àÄx,y‚àà\\dom(f)$ and $Œ∏‚àà[0,1]$ we have f(mixture)‚â§mixture(f) or $\\red{f(Œ∏x_1+(1-Œ∏)x_2)‚â§Œ∏f(x_1)+(1-Œ∏)f(x_2)}$.\n",
        "\n",
        "- **Extended-value extension**: $\\tilde{f}(x)=\\BC f(x)&x‚àà\\dom(f)\\\\‚àû&x‚àâ\\dom(f)\\EC$ is assumed for all functions in the book.\n",
        "\n",
        "- First order condition: $\\red{f(x)‚â•f(x_0)+‚àáf(x_0)^‚ä§(x-x_0)}$ holds for all $x,x_0‚àà\\dom(f)$. Denominator layout.\n",
        "\n",
        "- Second order condition: $\\red{‚àá^2f(x)‚âΩ0}$ (positive-semidefinite).\n",
        "\n",
        "- Examples:\n",
        "\n",
        "  - **Affine**: $f(x)=a^‚ä§x+b$ or $f(X)=\\tr(A^‚ä§X)+b$\n",
        "\n",
        "  - **Norm**: $\\n{‚ãÖ}:‚Ñù^n‚Üí‚Ñù$ is convex, including matrix spectral norm (max singular value). Proof: $\\n{Œ∏x+(1-Œ∏)y}‚â§Œ∏\\n{x}+(1-Œ∏)\\n{y}$ by triangle inequality.\n",
        "\n",
        "  - **Max**: $\\max(Œ∏x+(1-Œ∏)y)‚â§Œ∏\\max(x)+(1-Œ∏)\\max(y)$ is convex. Triangle inequality applies as well.\n",
        "\n",
        "  - **Quadratic-over-linear**: $f(x,y)=\\/{x^2}{y}$ is convex on $y>0$. Hessian $‚àá^2f=\\/{2}{y^3}\\BM y^2&-xy\\\\-xy&x^2\\EM$. $\\det(‚àá^2f)=0$. All principal minors are $‚â•0‚áí‚àá^2f‚âΩ0$.\n",
        "\n",
        "  - **Log-sum-exp**: $f(x)=\\ln(\\sum_ie^{x_i})$ is convex.\n",
        "  Let $z=(e^{x_1},...,e^{x_n})$,\n",
        "  $S=\\v{1}^‚ä§z$, and\n",
        "  $f(x)=\\ln(S)$. Then\n",
        "  $‚àÇ_{x_i}f=\\/{z_i}{S}$.\n",
        "  Off-diagonal\n",
        "  $‚àÇ_{x_i,x_j}^2f=-\\/{z_iz_j}{S^2}$.\n",
        "  On-diagonal\n",
        "  $‚àÇ_{x_i,x_i}^2f=\\/{z_i}{S}-\\/{z_iz_i}{S^2}$.\n",
        "  Hessian $‚àá^2f(x)=\\/{\\diag(z_i)}{S}-\\/{zz^‚ä§}{S^2}$\n",
        "  $=\\/{1}{(\\v{1}^‚ä§z)^2}((\\v{1}^‚ä§z)\\diag(z_i)-zz^‚ä§)$.\n",
        "  Check positive-semidefinite\n",
        "  $v^‚ä§‚àá^2fv=\\/{1}{(\\v{1}^‚ä§z)^2}((\\v{1}^‚ä§z)v^‚ä§\\diag(z_i)v-v^‚ä§zz^‚ä§v)$\n",
        "  $=\\/{1}{(\\v{1}^‚ä§z)^2}((\\sum_iz_i)(\\sum_iz_iv_i^2)-(\\sum_iv_iz_i)^2)‚â•0$.\n",
        "  $(\\sum_i(\\sqrt{z_i})^2)(\\sum_i(\\sqrt{z_i}v_i)^2)‚â•(\\sum_i(\\sqrt{z_i})(\\sqrt{z_i}v_i))^2$ by\n",
        "  Cauchy-Schwarz, therefore Hessian positive-semidefinite and convex.\n",
        "\n",
        "  - **Geometric mean**: $f(x)=[\\prod_{i=1}^nx_i]^{1/n}$ is concave on $‚Ñù_{++}^n$.\n",
        "  Let $S=\\prod_ix_i$,\n",
        "  $z=(\\/{1}{x_1},...,\\/{1}{x_n})$. Then\n",
        "  $‚àÇ_{x_i}f=\\/{1}{n}(\\/{S}{x_i})^{1/n}x_i^{1/n-1}$\n",
        "  $=\\/{S^{1/n}}{n}z_i$.\n",
        "  Off-diagonal\n",
        "  $‚àÇ_{x_ix_j}^2f=\\/{S^{1/n}}{n^2}z_iz_j$.\n",
        "  On-diagonal\n",
        "  $‚àÇ_{x_ix_i}^2f=\\/{S^{1/n}}{n^2}z_i^2-\\/{S^{1/n}}{n}z_i^2$.\n",
        "  Hessian $‚àá^2f(x)=-\\/{S^{1/n}}{n^2}(n\\diag(z_i^2)-zz^‚ä§)$.\n",
        "  Check negative-semidefinite\n",
        "  $v‚ä§‚àá^2fv=-\\/{S^{1/n}}{n^2}(nv^‚ä§\\diag(z_i^2)v-v^‚ä§zz^‚ä§v)$\n",
        "  $=-\\/{S^{1/n}}{n^2}((\\sum_i1)(\\sum_iz_i^2v_i^2)-(\\sum_iz_iv_i)^2)‚â§0$ by Cauchy-Schwarz, therefore Hessian negative-semidefinite and concave.\n",
        "\n",
        "- **Restriction of convex function to a line** (lecture): $f:‚Ñù^n‚Üí‚Ñù$ is convex iff $g:‚Ñù‚Üí‚Ñù,g(t)=f(x+tv)$ with $\\dom(g)=\\{t|x+tv‚àà\\dom(f)\\}$ is convex in $t$ for any $x‚àà\\dom(f),v‚àà‚Ñù^n$. This is used to prove convexity of $f:\\v{S}_{++}^n‚Üí‚Ñù$ where $f(X)=f(Z+tV)$.\n",
        "\n",
        "  - **Log-determinant**: Let $f(x)=\\ln(\\det(X))$ where $\\dom(f)=\\v{S}_{++}^n$, then\n",
        "  $g(t)=f(X+tV)=\\ln\\det(X+tV)$\n",
        "  $=\\ln\\det(X^{1/2}(I+tX^{-1/2}VX^{-1/2})X^{1/2})$\n",
        "  $=\\ln\\det(X)+\\ln \\prod_i(1+tŒª_i)$\n",
        "  $=\\ln\\det(X)+\\sum_i\\ln(1+tŒª_i)$\n",
        "  where $Œª_i$ are eigenvalues of $X^{-1}V$. Concave.\n",
        "\n",
        "- **Sublevel sets**: The $Œ±$-sublevel set of $f:‚Ñù^n‚Üí‚Ñù$ is $C_Œ±=\\{x‚àà\\dom(f)|f(x)‚â§Œ±\\}$. Convex function $‚áí$ convex sublevel sets. Converse not true.\n",
        "\n",
        "  - Convex ‚áí quasiconvex, concave ‚áí quasiconcave, converse not true.\n",
        "\n",
        "- **Epigraph**: Graph of $f:‚Ñù^n‚Üí‚Ñù$ is $\\{(x,f(x))‚àà‚Ñù^{n+1}|x‚àà\\dom(f)\\}$. Epigraph is the filled area above the graph $\\epi(f)=\\{(x,y)‚àà‚Ñù^{n+1}|x‚àà\\dom(f),f(x)‚â§y\\}$. **Function is convex ‚áî epigraph is convex set**.\n",
        "\n",
        "  - $(x,y)‚àà\\epi(f)$\n",
        "  $‚áíy‚â•f(x)‚â•f(x_0)+‚àáf(x_0)^‚ä§(x-x_0)$\n",
        "  $‚áíy-f(x_0)‚â•‚àáf(x_0)^‚ä§(x-x_0)$\n",
        "  at the point $(x_0,f(x_0))$.\n",
        "  Rewrite in matrix form\n",
        "  $\\BM ‚àáf(x_0)\\\\-1\\EM^‚ä§\\BM x-x_0\\\\y-f(x_0)\\EM‚â§0$\n",
        "  $‚áí\\red{\\BM ‚àáf(x_0)\\\\-1\\EM^‚ä§(\\BM x\\\\y\\EM-\\BM x_0\\\\f(x_0)\\EM)‚â§0}$ is a supporting hyperplane whose normal is $\\BM ‚àáf(x_0)\\\\-1\\EM$.\n",
        "  \n",
        "  - **Supporting hyperplane**: Consider $g(x,y)=f(x)-y$ then $‚àág(x,y)=(‚àáf(x),-1)$ points in the direction of steepest ascent and is perpendicular to the level set $g(x,y)=0$.\n",
        "  This trick allows us to find the normal vector at $(x_0,f(x_0))$, which is $‚àág(x,y)=\\blue{\\BM ‚àáf(x_0)\\\\-1\\EM}$. Example: $f(x)=x^2$, $‚àáf(x)=2x$. At $x=2$, $‚àáf(2)=2(2)=4$. The normal vector is $(4,-1)$.\n",
        "\n",
        "- **Jensen's inequality**: $\\BC\n",
        "f(Œ∏x_1+(1-Œ∏)x_2)‚â§Œ∏f(x_1)+(1-Œ∏)f(x_2) \\\\\n",
        "f(\\sum_iŒ∏_ix_i)‚â§\\sum_iŒ∏_if(x_i) \\\\\n",
        "f(‚à´_pp(x)x\\ dx)‚â§‚à´_pf(x)p(x)\\ dx \\\\\n",
        "f(\\E[x])‚â§\\E[f(x)]\n",
        "\\EC$\n",
        "\n",
        "- 3.4 (**matrix fractional function**): $f:‚Ñù^n√ó\\v{S}^n‚Üí‚Ñù,f(x,P)=x^‚ä§P^{-1}x$ is convex on $\\dom(f)=‚Ñù^n√ó\\v{S}_{++}^n$.\n",
        "\n",
        "  - Proof: $\\epi(f)=\\{(x,P,y)|P‚âª0,x^‚ä§P^{-1}x‚â§y\\}$.\n",
        "  By Schur's complement $y-x^‚ä§P^{-1}x‚â•0$\n",
        "  $‚áî\\BM P&x\\\\x^‚ä§&y\\EM‚âΩ0$, therefore\n",
        "  $\\epi(f)=\\{(x,P,y)|P‚âª0,\\BM P&x\\\\x^‚ä§&y\\EM‚âΩ0\\}$,\n",
        "  which is a linear matrix inequality on $(x,P,y)$:\n",
        "  $\\BM P&x\\\\x^‚ä§&y\\EM=P\\BM I&0\\\\0&0\\EM+x\\BM0&I\\\\0&0\\EM+x^‚ä§\\BM0&0\\\\I&0\\EM+y\\BM0&0\\\\0&1\\EM$ therefore $\\epi(f)$ convex and $f$ convex.\n"
      ],
      "metadata": {
        "id": "wogUsgC_qWwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preservation of convexity**:\n",
        "\n",
        "- **Nonnegative weighted combination**: If $f_i$ is convex then $f=\\sum_iw_if_i$ is convex where $w_i>0$.\n",
        "\n",
        "  - Proof: $\\epi(w_if_i)=\\BM I&0\\\\0&w_i\\EM\\epi(f_i)$\n",
        "  $=\\{\\BM I&0\\\\0&w_i\\EM\\BM x\\\\y\\EM|(x,y)‚àà\\epi(f_i)\\}$. Therefore $\\epi(f)$ is the linear image of convex sets $\\epi(f_i)$. Negative weight $w_i< 0$ converts convex $f_i$ into concave.\n",
        "\n",
        "  - $wf_i$ can be negative. The rule says summation preserves convexity if summants are convex.\n",
        "\n",
        "- **Integration**: If $f(x,y)$ is convex in $x\\ ‚àÄy‚àà\\m{A}$, and $w(y)‚â•0\\ ‚àÄy‚àà\\m{A}$, then $g(x)=‚à´_\\m{A}w(y)f(x,y)\\ dy$ is convex.\n",
        "\n",
        "  - $w(y)f(x,y)$ can be negative if convex. The rule says integration preserves convexity if integrant is convex.\n",
        "\n",
        "- **Affine pre-composition**: If $f$ is convex then $f(Ax+b)$ is convex. If $f$ is concave then $f(Ax+b)$ is concave.\n",
        "\n",
        "- **Pointwise maximum**: If $f_1,...,f_m$ are convex, then $f(x)=\\max\\{f_1(x),...,f_m(x)\\}$ is convex.\n",
        "\n",
        "  - Proof: $\\epi(f)=\\bigcap_{i=1}^m\\epi(f_i)$\n",
        "\n",
        "- **Pointwise supremum**: $f(x)=\\sup_{y‚àà\\m{A}}g(x,y)$ is convex if $g(x,y)$ is convex in $x$ for all $y‚àà\\m{A}$.\n",
        "\n",
        "  - Proof: $\\epi(f)=\\bigcap_{y‚àà\\m{A}}\\epi(g(‚ãÖ,y))$\n",
        "\n",
        "- **Partial infimum**: $f(x)=\\inf_{y‚ààC}g(x,y)$ is convex if $g(x,y)$ is jointly convex in both $x,y$ and set $C$ is convex.\n",
        "\n",
        "  - Although the definition is pointwise, $g$ is jointly convex in $(x,y)$ so that it is shaped like a \"bowl\" and $\\epi(f)$ is the projection (shadow) of $\\epi(g)‚àà(x,y,t)$ onto $(x,t)$, which is still convex (like the keel of a ship's hull).\n",
        "\n",
        "- **Upper envelope principle**: $\\red{f(x)=\\sup\\{g(x)|g\\t{ affine},g(z)‚â§f(z)\\ ‚àÄz‚àà‚Ñù^n\\}}$. $f$ is convex iff the upper envelope of all lower affine functions $g$ perfectly recreates the function $f$. Almost every convex function can be expressed as the pointwise supremum of a famly of affine functions.\n",
        "\n",
        "  - Proof: Because $\\epi(f)$ is convex, each $x_0‚àà‚Ñù^n$ has a supporting hyperplane\n",
        "  $\\BM‚àáf(x_0)\\\\-1\\EM^‚ä§\\BM z-x_0\\\\t-f(x_0)\\EM‚â§0$\n",
        "  for all $(z,t)‚àà\\epi(f)$.\n",
        "  I.e., $‚àáf(x_0)(z-x_0)-(t-f(x_0))‚â§0$\n",
        "  $‚áít‚â•f(x_0)+‚àáf(x_0)^‚ä§(z-x_0)$.\n",
        "  For $t=f(z)$ on the graph,\n",
        "  $f(z)‚â•f(x_0)+‚àáf(x_0)^‚ä§(z-x_0)=g(z)$.\n",
        "\n",
        "  - This principle requires that $f$ is closed ($\\epi(f)$ is a closed set) to prevent vertical supporting hyperplane.\n",
        "\n",
        "- **Composition**: $h:‚Ñù^k‚Üí‚Ñù$ and $g:‚Ñù^n‚Üí‚Ñù^k$ and $f=h‚àòg=h(g(x))$\n",
        "\n",
        "  - **Scalar** ($k=1$): Let $h,g:‚Ñù‚Üí‚Ñù‚ààC^2$. Then\n",
        "  $f''(x)=h''(g(x))g'(x)^2+h'(g(x))g''(x)$.\n",
        "  In general for $h:‚Ñù‚Üí‚Ñù$, $g:‚Ñù^n‚Üí‚Ñù$, even if $g,h$ are not twice differentiable: $f$ is $\\BC\n",
        "  \\t{convex if }h\\t{ convex}&\\t{&}&g\\t{ affine or} \\BC\n",
        "  h'(x)‚â•0\\t{ ‚áë nondecreasing}&\\t{&}&g\\t{ convex}\\\\\n",
        "  h'(x)‚â§0\\t{ ‚áì nonincreasing}&\\t{&}&g\\t{ concave}\n",
        "  \\EC \\\\\n",
        "  \\t{concave if }h\\t{ concave}&\\t{&}&g\\t{ affine or} \\BC\n",
        "  h'(x)‚â•0\\t{ ‚áë nondecreasing}&\\t{&}&g\\t{ concave}\\\\\n",
        "  h'(x)‚â§0\\t{ ‚áì nonincreasing}&\\t{&}&g\\t{ convex}\\\\\n",
        "  \\EC\n",
        "  \\EC$\n",
        "\n",
        "  - **Vector** ($k>1$): $‚àá^2f(x)=‚àág(x)^‚ä§‚àá^2h(g(x))‚àág(x)+‚àáh(g(x))^‚ä§‚àá^2g(x)$. Then $f$ is $\\red{\\BC\n",
        "  \\t{convex if }h\\t{ convex}&\\t{&}&\\t{each }g_i\\t{ affine or} \\BC\n",
        "  ‚àÇ_{g_i}h‚â•0\\t{ ‚áë nondecreasing}&\\t{&}&g_i\\t{ convex}\\\\\n",
        "  ‚àÇ_{g_i}h‚â§0\\t{ ‚áì nonincreasing}&\\t{&}&g_i\\t{ concave}\\\\\n",
        "  \\EC\\\\\n",
        "  \\t{concave if }h\\t{ concave}&\\t{&}&\\t{each }g_i\\t{ affine or} \\BC\n",
        "  ‚àÇ_{g_i}h‚â•0\\t{ ‚áë nondecreasing}&\\t{&}&g_i\\t{ concave}\\\\\n",
        "  ‚àÇ_{g_i}h‚â§0\\t{ ‚áì nonincreasing}&\\t{&}&g_i\\t{ convex}\\\\\n",
        "  \\EC\n",
        "  \\EC}$\n",
        "  in mix-and-match for each $g_i$. This generalizes all other rules (including pointwise max, nonnegative weighted, etc).\n",
        "\n",
        "  - Softwares (CVX, CVXPY) use disciplined convex programming as one engine of convex analysis. DCP maintains a library of known convex/concave \"atoms\", and recursively checks against the composition rules.\n",
        "\n",
        "- **Perspective function**: if $f:‚Ñù^n‚Üí‚Ñù$, then $g:‚Ñù^{n+1}‚Üí‚Ñù$ where $g(x,t)=tf(x/t)$ is the perspective of $f$ where $\\dom(g)=\\{(x,t)|x/t‚àà\\dom(f),t>0\\}$. If $f$ is convex then so is $g$. If $f$ is concave then so is $g$.\n",
        "\n",
        "- 3.7 (**support function of a set**): the support function associated with a set $C‚äÜ‚Ñù^n$ is $\\red{S_C(x)=\\sup\\{x^‚ä§y|y‚ààC\\}}$ is the distance from origin out to the furthest (most positive) frontier of the set along positive $x$. It is convex because it is pointwise supremum of linear functions of $x$ (even if $C$ is not convex).\n",
        "\n",
        "  - Support function of set $C$ for a given $x$ returns the supporting hyperplane for $C$ in the direction of $x$. The supporting halfplane is $\\{y|x^‚ä§y‚â§S_C(x)\\}$.\n",
        "\n",
        "- 3.8 (**distance to farthest point of a set**): $f(x)=\\sup_{y‚ààC}\\n{x-y}=\\sup_{y‚ààC}\\{(x-y)^‚ä§(x-y)\\}$\n",
        "$=\\n{x}^2+\\sup_{y‚ààC}\\{-2x^‚ä§y+\\n{y}^2\\}$ is pointwise supremum of family of convex functions.\n",
        "\n",
        "- 3.9 (**weighted least squares**): Let\n",
        "$g(w)=\\inf_x\\sum_{i=1}^nw_i(a_i^‚ä§x-b_i)^2=\\inf_x(Ax-b)^‚ä§W(Ax-b)$ where\n",
        "$W=\\diag(w)$. Then\n",
        "$g(W)=b^‚ä§Wb-b^‚ä§WA(A^‚ä§WA)^{-1}A^‚ä§Wb=D-E^‚ä§P^{-1}E$\n",
        "where $D$ is linear and $E^‚ä§P^{-1}E$ is a matrix fractional function and therefore convex only when $P‚âª0$. Therefore $g(W)$ is concave.\n",
        "\n",
        "- 3.10 (**max eigenvalue of symmetric matrix**): $f(X)=Œª_\\max(X)=\\sup\\{y^‚ä§Xy|\\n{y}_2=1\\}$ is pointwise supremum of linear functions of $X$ therefore $f$ is convex.\n",
        "$Xy=Œªy‚áíŒª(y^‚ä§y)=y^‚ä§Xy$.\n",
        "\n",
        "  - Proof: Let $X=PDP^‚ä§$ where $D$ is sorted $Œª_1‚â•...‚â•Œª_n$. Then $y^‚ä§Xy=y^‚ä§PDP^‚ä§y=(P^‚ä§y)^‚ä§D(P^‚ä§y)$.\n",
        "  Let $z=(P^‚ä§y)$, then $\\sup_{\\n{y}_2=1}\\{y^‚ä§Xy\\}=\\sup_{\\n{z}_2=1}\\{z^‚ä§Dz\\}=Œª_1$ where $z=(1,0,...,0)$.\n",
        "\n",
        "- 3.11 (**spectral norm of matrix**): $f(X)=\\n{X}_2=\\sup\\{u^‚ä§Xv|\\n{u}_2=1,\\n{v}_2=1\\}$ where $\\dom(f)=‚Ñù^{p√óq}$ is pointwise supremum of linear functions of $X$ therefore $f$ is convex.\n",
        "\n",
        "  - Proof: $u^‚ä§Xv=u^‚ä§UŒ£V^‚ä§v=\\tilde{u}^‚ä§Œ£\\tilde{v}$.\n",
        "  For rank-$k$ matrix,\n",
        "  $\\sup_{\\n{u}_2=\\n{v}_2=1}\\{u^‚ä§Xv\\}$\n",
        "  $=\\sup_{\\n{\\tilde{u}}_2=\\n{\\tilde{v}}_2=1}\\{\\tilde{u}^‚ä§Œ£\\tilde{v}\\}$\n",
        "  $=\\sup_{\\n{\\tilde{u}}_2=\\n{\\tilde{v}}_2=1}\\sum_{i=1}^kœÉ_i\\tilde{u}_i\\tilde{v}_i$\n",
        "  $=œÉ_1$ where $\\tilde{u}=(1,0,...,0)$ and $\\tilde{v}=(1,0,...,0)$.\n",
        "\n",
        "- 3.15 (**Schur complement**): Let $f(u,v)=\\BM u\\\\v\\EM^‚ä§\\BM A&B\\\\B^‚ä§&C\\EM\\BM u\\\\v\\EM$\n",
        "$=u^‚ä§Au+2u^‚ä§Bv+v^‚ä§Cv$, then $f$ is convex iff $‚àá_{u,v}^2f‚âΩ0$. However $‚àá_{u,v}^2f=2\\BM A&B\\\\B^‚ä§&C\\EM$ therefore by definition of positive-semidefinite $f(u,v)‚â•0\\ ‚àÄ(u,v)$. Let $f_\\min(u)=\\inf_vf(u,v)$. Then $‚àá_vf=2u^‚ä§B+2v^‚ä§C=0‚áív^*=-C^{-1}B^‚ä§u$.\n",
        "Then $f_\\min(u)=f(u,v^*)=u^‚ä§Au-2u^‚ä§BC^{-1}B^‚ä§u+u^‚ä§BC^{-1}CC^{-1}B^‚ä§u$\n",
        "$=u^‚ä§Au-u^‚ä§BC^{-1}B^‚ä§u$\n",
        "$=u^‚ä§(A-BC^{-1}B^‚ä§)u‚â•0$.\n",
        "Therefore $f_\\min(u)$ exists iff $C‚âª0$ strictly, and $A-BC^{-1}B^‚ä§‚âΩ0$.\n",
        "\n",
        "  - Schur complement lemma says if $C‚âª0$ then $\\BM A&B\\\\B^‚ä§&C\\EM‚âΩ0‚áîA-BC^{-1}B^‚ä§‚âΩ0$.\n",
        "\n",
        "- 3.16 (**Distance to a set**): $\\t{dist}(x,S)=\\inf_{y‚ààS}\\n{x-y}$. Here $\\n{x-y}$ is convex in $(x,y)$. Then $\\t{dist}(x,S)$ is convex if $S$ is convex.\n",
        "\n",
        "- 3.17 (**relative entropy**): Let $f(x)=-\\ln x$ on $‚Ñù_{++}$. Then $g(x,t)=-t\\ln\\/{x}{t}=t\\ln t-t\\ln x$ is the relative entropy of $t$ and $x$. It is the perspective of convex $f$ and is convex on $‚Ñù_{++}^2$.\n",
        "\n",
        "  - Let $u,v‚àà‚Ñù_{++}^n$, then sum of relative entropy $\\sum_iu_i\\ln(\\/{u_i}{v_i})=\\sum_ig(v_i,u_i)$ is convex in $(u,v)$.\n",
        "\n",
        "  - **Kullback-Leibler divergence** between $u,v‚àà‚Ñù_{++}^n$ is $\\blue{D_\\t{kl}(u,v)=\\sum_i(u_i\\ln(\\/{u_i}{v_i})-u_i+v_i)}$ is convex in $(u,v)$. It is a measure of two positive vectors. $D_\\t{kl}(u,v)‚â•0$ and $D_\\t{kl}(u,v)=0‚áîu=v$. When $\\v{1}^‚ä§u=\\v{1}^‚ä§v=1$ are probability vectors then $D_\\t{kl}(u,v)=\\sum_iu_i\\ln(\\/{u_i}{v_i})$ is equal to the relative entropy.\n",
        "\n",
        "  - If $\\v{1}^‚ä§u\\neq1$, **normalized entropy** is $\\blue{\\sum_iu_i\\ln(\\/{\\v{1}^‚ä§u}{u_i})=(\\v{1}^‚ä§u)\\sum_i\\/{u_i}{\\v{1}^‚ä§u}\\ln(\\/{\\v{1}^‚ä§u}{u_i})}$, which is a sum of concave perspective functions $(\\v{1}^‚ä§u)\\sum_iz_i\\ln(\\/{1}{z_i})$ therefore concave.\n",
        "\n",
        "- Lecture example: $f(x)=\\/{(\\v{1}^‚ä§x)^2}{\\min(2,\\sqrt{x_3})}$.\n",
        "The outer function $h(u(x),v(x))=\\/{u(x)^2}{v(x)}$ quadratic-over-linear is convex.\n",
        "$‚àÇ_uh>0$ and $u(x)$ is convex.\n",
        "$‚àÇ_vh< 0$ and $v(x)$ is concave.\n",
        "Therefore $f$ is convex.\n"
      ],
      "metadata": {
        "id": "tpqHEMZsWeHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Legendre-Fenchel Conjugate**: Let $f:‚Ñù^n‚Üí‚Ñù$. Then $f^*:‚Ñù^n‚Üí‚Ñù,\\red{f^*(y)=\\sup_{x‚àà\\dom(f)}(y^‚ä§x-f(x))}$ is the conjugate of $f$ where $\\red{\\dom(f^*)=\\{y‚àà‚Ñù^n|y^‚ä§x-f(x)< ‚àû\\ ‚àÄx‚àà\\dom(f)\\}}$.\n",
        "The conjugate function $f^*(y)$ is only defined for $y$ where $y^‚ä§x-f(x)$ is bounded above $\\dom(f)$.\n",
        "It is a pointwise supremum of affine functions of $y$. Conjugate function $f^*(y)$ is always convex, even if $f(x)$ is concave.\n",
        "\n",
        "- **Fenchel's inequality**: $\\blue{f(x)+f^*(y)‚â•x^‚ä§y}$ for all $x,y$.\n",
        "\n",
        "- **Conjugate of conjugate**: If $f$ is convex and $\\epi(f)$ is closed, then $\\blue{f^{**}=f}$.\n",
        "\n",
        "  - Proof: $f^{**}(x)=\\sup_y(x^‚ä§y-f^*(y))$, which is always convex (thereby requiring $f$ to be convex).\n",
        "  Fenchel's inequality says $f(x)‚â•x^‚ä§y-f^*(y)=h(x,y)$.\n",
        "  I.e., $h(x,y)$ is an affine underestimator of $f(x)$,\n",
        "  and $f^{**}(x)=\\sup_yh(x,y)$ is a pointwise supremum of all affine underestimators. Since $\\epi(f)$ is closed, $f$ is equal to the supremum of its affine underestimators $f=f^{**}$.\n",
        "\n",
        "- **Legendre transform**: If $f$ is differentiable then the conjugate is called Legendre transform (as opposed to Fenchel conjugate for non-differentiable). Let $h(x)=y^‚ä§x-f(x)‚áí‚àá_xh(x)=y-‚àá_xf(x)=0$. Then $\\blue{y=‚àá_xf(x^*)}$ and $\\BC\n",
        "y=‚àá_xf(x^*)&\\t{is the slope at }(x^*,f(x^*)) \\\\\n",
        "f^*(y)=‚àá_xf(x^*)^‚ä§x^*-f(x^*)&\\t{is the gap between a straight line from origin and }f(x^*)\n",
        "\\EC$\n",
        "\n",
        "- **Affine**: Let $g(x)=af(x)+b$ where $a>0,b‚àà‚Ñù$, then $g^*(y)=af^*(y/a)-b$.\n",
        "\n",
        "  - Let $g(x)=f(Ax+b)$ then $g^*(y)=f^*(A^{-‚ä§}y)-b^‚ä§A^{-‚ä§}y$.\n",
        "\n",
        "- **Support function vs Indicator**: If $f(x)$ is the support function $\\red{f(x)=\\sup_{z‚ààS}z^‚ä§x}$ of a closed convex set $S‚äÜ‚Ñù^n$, then $f^*(y)$ is the indicator function of set $S$.\n",
        "\n",
        "  - 3.24 (**indicator**): Indicator of $S‚äÜ‚Ñù^n$ is defined to be $\\red{I_S(x)=\\BC\n",
        "  0&x‚ààS\\\\\n",
        "  ‚àû&\\t{else}\n",
        "  \\EC}$ on $\\dom(I_S)=S$.\n",
        "  Its conjugate is $I_S^*(y)=\\sup_{x‚ààS}y^‚ä§x$, which is the support function of $S$.\n",
        "\n",
        "  - $f(x)$ is a support function of set $\\red{S=\\{y|y^‚ä§x‚â§f(x)\\ ‚àÄx‚àà\\dom(f)\\}}$ iff it satisfies 3 conditions: $\\BC\n",
        "  1. &\\t{positively homogeneous}&f(tx)=tf(x)\\t{ for }t>0\\\\\n",
        "  2. &\\t{convex}&f(Œ∏x_1+(1-Œ∏)x_2)‚â§Œ∏f(x_1)+(1-Œ∏)f(x_2)\\\\\n",
        "  3. &\\t{closed}&f\\t{ is lower semicontinuous}\n",
        "  \\EC$\n",
        "\n",
        "- **Two variable conjugate**: Let $f(x,z)$. Then $\\red{f^*(y,u)=\\sup_{x,z‚àà\\dom(f)}(y^‚ä§x+u^‚ä§z-f(x,z))}$. (exercise 3.39)\n",
        "\n",
        "- 3.21: Examples\n",
        "\n",
        "  - **Affine**: $f(x)=ax+b$. Then $‚àÇ_x(xy-ax-b)=y-a=0‚áíy=a$ with singular domain\n",
        "  $\\dom(f^*)=\\{y|yx-ax-b< ‚àû\\ ‚àÄx\\}=\\{a\\}$\n",
        "  and $f^*(a)=-b$.\n",
        "\n",
        "  - **Negative log**: $f(x)=-\\ln x$ on $\\dom(f)=‚Ñù_{++}$. Then $‚àÇ_x(yx+\\ln(x))=y+\\/{1}{x}=0‚áíx^*=-\\/{1}{y}$.\n",
        "  Therefore $yx+\\ln(x)$ is bounded only if $y< 0$.\n",
        "  Then $f^*(y)=-\\ln(-y)-1$ on $\\dom(f^*)=‚Ñù_{--}$.\n",
        "\n",
        "  - **Exponential**: $f(x)=e^x$. Then $‚àÇ_x(xy-e^x)=y-e^x=0‚áíx^*=\\ln(y)$.\n",
        "  Therefore $f^*(y)=y(\\ln y-1)$ on $\\dom(f^*)=‚Ñù_+$.\n",
        "\n",
        "  - **Negative entropy**: $f(x)=x\\ln x$. Then $‚àÇ_x(xy-x\\ln x)=y-\\ln x-1=0‚áíx^*=e^{y-1}$.\n",
        "  Therefore $f^*(y)=ye^{y-1}-(y-1)e^{y-1}=e^{y-1}$ on $\\dom(f^*)=‚Ñù$.\n",
        "\n",
        "  - **Inverse**: $f(x)=1/x$. Then $‚àÇ_x(xy-1/x)=y+1/x^2=0‚áíx^*=\\/{1}{\\sqrt{-y}}$.\n",
        "  Therefore $f^*(y)=\\/{y}{\\sqrt{-y}}-\\sqrt{-y}=-2\\sqrt{-y}$ on $\\dom(f^*)=‚Ñù_-$.\n",
        "\n",
        "- 3.22 (**strictly convex quadratic function**): $f(x)=\\/{1}{2}x^‚ä§Qx$ where $Q‚àà\\v{S}_{++}^n$.\n",
        "Then\n",
        "$‚àá_x(x^‚ä§y-\\/{1}{2}x^‚ä§Qx)$\n",
        "$=y^‚ä§-x^‚ä§Q=0$\n",
        "$‚áíx^*=Q^{-1}y$.\n",
        "Therefore $f^*(y)=y^‚ä§Q^{-1}y-\\/{1}{2}y^‚ä§Q^{-1}QQ^{-1}y$\n",
        "$=\\/{1}{2}y^‚ä§Q^{-1}y$ on $\\dom(f^*)=‚Ñù^n$.\n",
        "\n",
        "- 3.23 (**log-determinant**): $f(X)=\\ln\\det(X^{-1})=-\\ln\\det(X)$ on $\\dom(f)=\\v{S}_{++}^n$. Then\n",
        "$f^*(Y)=\\sup_{X‚àà\\dom(f)}(\\tr(XY)+\\ln\\det(X))$ where $Y‚àà\\v{S}^n$. Let $h(X)=\\tr(XY)+\\ln\\det(X)$.\n",
        "\n",
        "  - If $Y‚âª0$ has a positive eigenvalue then $h(X)$ is unbounded.\n",
        "  Proof: Let $Œª>0$ be an eigenvalue of $Y$ with eigenvector $v$ where $\\n{v}_2=1$.\n",
        "  Let $X=I+tvv^‚ä§$.\n",
        "  Then\n",
        "  $\\tr(XY)=\\tr(Y+tvv^‚ä§Y)$\n",
        "  $=\\tr(Y)+t\\tr(vv^‚ä§Y)$\n",
        "  $=\\tr(Y)+t\\tr(Yvv^‚ä§)$\n",
        "  $=\\tr(Y)+tŒª\\tr(vv^‚ä§)$\n",
        "  $=\\tr(Y)+tŒª\\n{v}_2^2$.\n",
        "  By matrix determinant lemma\n",
        "  $\\det(X)$\n",
        "  $=\\det(I+tvv^‚ä§)$\n",
        "  $=1+t\\n{v}_2^2$.\n",
        "  Therefore $h(X)=\\tr(Y)+tŒª+\\ln(1+t)$\n",
        "  $‚Üí‚àû$ with $t‚Üí‚àû$.\n",
        "\n",
        "  - Let $Y‚â∫0$ with negative eigenvalues. Then\n",
        "  $‚àá_Xh(X)=Y^‚ä§+\\/{1}{\\det(X)}\\det(X)X^{-1}$\n",
        "  $=Y^‚ä§+X^{-1}=0$\n",
        "  $‚áíX^*=-Y^{-1}$.\n",
        "  Then\n",
        "  $f^*(Y)=h(X^*)=-\\tr(YY^{-1})+\\ln\\det(-Y^{-1})$\n",
        "  $=-n+\\ln\\det(-Y^{-1})$ on $\\dom(f^*)=\\v{S}_{--}^n$.\n",
        "\n",
        "- 3.25 (**log-sum-exp**): $f(x)=\\ln(\\sum_ie^{x_i})$.\n",
        "Let $h(x)=y^‚ä§x-\\ln(\\sum_ie^{x_i})$.\n",
        "Then\n",
        "$‚àá_xh(x)=y^‚ä§-\\/{(e^x)^‚ä§}{\\sum_ie^{x_i}}=0$\n",
        "$‚áíy_j=\\/{e^{x_j}}{\\sum_ie^{x_i}}$.\n",
        "Therefore $\\dom(f^*)=\\{y|y‚âΩ0,\\v{1}^‚ä§y=1\\}$ is probability simplex.\n",
        "$x_j^*=\\ln(\\sum_ie^{x_i}y_j)$\n",
        "$=\\ln(\\sum_ie^{x_i})+\\ln(y_j)$.\n",
        "And\n",
        "$f^*(y)=\\sum_jy_i(\\ln(\\sum_ie^{x_i})+\\ln(y_j))-\\ln(\\sum_ie^{x_i})$\n",
        "$=(\\sum_jy_i)\\ln(\\sum_ie^{x_i})+\\sum_jy_i\\ln(y_j)-\\ln(\\sum_ie^{x_i})$\n",
        "$‚áíf^*(y)=\\sum_jy_i\\ln(y_j)$ is sum of negative entropy on $\\dom(f^*)=\\{y|y‚âΩ0,\\v{1}^‚ä§y=1\\}$.\n",
        "\n",
        "- 3.26 (**norm**): Let $f(x)=\\n{x}$ with dual norm $\\n{y}_*=\\sup\\{y^‚ä§x|\\n{x}‚â§1\\}$. Let $h(x)=y^‚ä§x-\\n{x}$.\n",
        "\n",
        "  - If $\\n{y}_*>1$ then $h(x)$ is unbounded. Proof: there is some $\\n{z}‚â§1$ such that $y^‚ä§z>1$. Let $x=tz$ then $h(x)=y^‚ä§(tz)-\\n{tz}$\n",
        "  $=t(y^‚ä§z-\\n{z})$\n",
        "  $‚Üí‚àû$ with $t‚Üí‚àû$.\n",
        "\n",
        "  - From Holder inequality $\\green{y^‚ä§x‚â§\\n{x}\\n{y}_*}$\n",
        "  $‚áíy^‚ä§x-\\n{x}‚â§\\n{x}(\\n{y}_*-1)‚â§0$.\n",
        "  Therefore $f^*(y)=I_{\\n{y}_*‚â§1}$.\n",
        "\n",
        "- 3.27 (**norm-squared**): Let $f(x)=\\/{1}{2}\\n{x}^2$.\n",
        "Then $y^‚ä§x-\\/{1}{2}\\n{x}^2‚â§\\n{x}\\n{y}_*-\\/{1}{2}\\n{x}^2=h(\\n{x})$.\n",
        "Then $‚àÇ_th(t)=\\n{y}_*-t=0$\n",
        "$‚áít^*=\\n{y}_*$.\n",
        "Therefore\n",
        "$f^*(y)=\\/{1}{2}\\n{y}_*^2$.\n",
        "\n",
        "- Lecture example: Let $x‚àà‚Ñù_{+}^n$ be the quantities of $n$ widgets produced over a time period. Let $f(x)$ be the cost of production. Let $y‚àà‚Ñù_{+}^n$ be the market prices of the widgets produced. Then $f^*(y)=\\sup_xy^‚ä§x-f(x)$ is the maximum profit made by adjusting the quantity of production as a function of market prices."
      ],
      "metadata": {
        "id": "nrpFUY-0OD6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quasiconvex**: $f:‚Ñù^n‚Üí‚Ñù$ is quasiconvex if its domain and all its sublevel sets $S_Œ±=\\{x‚àà\\dom(f)|f(x)‚â§Œ±\\}$ are convex sets. $f$ is quasiconcave if $-f$ is quasiconvex, and is quasilinear if both quasiconcave and quasiconvex. Many convex function properties have analogs for quasiconvex. Quasiconvex functions are **unimodal**: single valley but may have flat spots.\n",
        "\n",
        "- **Jensen's inequality**: function $f$ is consiconvex iff $\\dom(f)$ is convex and for any $x,y‚àà\\dom(f)$ and $Œ∏‚àà[0,1]$ we have f(mixture)‚â§max f(endpoints) $\\blue{f(Œ∏x+(1-Œ∏)y)‚â§\\max\\{f(x),f(y)\\}}$.\n",
        "\n",
        "  - Proof: let $x,y$ be two points in $S_Œ±$, then $\\max\\{f(x),f(y)\\}‚â§Œ±$. Then any $z=Œ∏x+(1-Œ∏)y$ will satisfy $f(z)‚â§Œ±$. Therefore $S_Œ±$ convex. Similar logic for converse.\n",
        "\n",
        "- **Monotonicity**: Continuous $f$ is quasiconvex on $‚Ñù$ iff one of the conditions hold: $\\BC\n",
        "1.&f\\t{ ‚áë nondecreasing}\\\\\n",
        "2.&f\\t{ ‚áì nonincreasing}\\\\\n",
        "3.&‚àÉc‚àà\\dom\\small(f):f\\t{ ‚áì nonincreasing }‚àÄt‚â§c\\t{ and }f\\t{ ‚áë nondecreasing }‚àÄt‚â•c\n",
        "\\EC$\n",
        "\n",
        "- **First order condition**: If $f$ is differentiable, then $f$ is quasiconvex iff $\\dom(f)$ is convex and for all $x,y‚àà\\dom(f)$ we have $\\blue{f(y)‚â§f(x)‚áí‚àáf(x)^‚ä§(y-x)‚â§0}$.\n",
        "\n",
        "  - If you're standing at $f(x)$ and $f(y)$ is downhills from you, then the path leading toward $y$ must initially go down.\n",
        "\n",
        "  - Proof: Let $Œ±=f(x)$ and $f(y)‚â§f(x)$ then for small $Œ∏‚àà(0,1]$ we have\n",
        "  $f(x+Œ∏(y-x))‚â§f(x)$\n",
        "  $‚áíf(x+Œ∏(y-x))-f(x)‚â§0$\n",
        "  $‚áí\\/{f(x+Œ∏(y-x))-f(x)}{Œ∏}‚â§0$\n",
        "  $‚áí‚àáf(x)^‚ä§(y-x)‚â§0$.\n",
        "  Similar logic for converse.\n",
        "\n",
        "  - $‚àáf(x)$ is the supporting hyperplane normal for the convex sublevel set $\\{z|f(z)‚â§f(x)\\}$.\n",
        "\n",
        "- **Second order condition**: If $f$ is quasiconvex, then for all $x‚àà\\dom(f)$ and $y‚àà‚Ñù^n$ we have $\\blue{y^‚ä§‚àáf(x)=0‚áíy^‚ä§‚àá^2f(x)y‚â•0}$.\n",
        "\n",
        "  - Second order Taylor's\n",
        "  $f(x+ty)=f(x)+t‚àáf(x)^‚ä§y+\\/{t^2}{2}y^‚ä§‚àá^2f(x)y$\n",
        "  $=\\green{f(x)+\\/{t^2}{2}y^‚ä§‚àá^2f(x)y‚â•f(x)}$. Starting at location $x$, moving straight along tangent $y$ off the contour by $t$ amount results in going uphills.\n",
        "\n",
        "- **Preservation of quasiconvexity**:\n",
        "\n",
        "  - Nonnegative weighted maximum: $f=\\max\\{w_1f_1,...,w_mf_m\\}$\n",
        "\n",
        "  - If $g:‚Ñù^n‚Üí‚Ñù$ is quasiconvex and $h:‚Ñù‚Üí‚Ñù$ is nondecreasing $‚áë$, then $f=h‚àòg$ is quasiconvex.\n",
        "\n",
        "  - If $f:‚Ñù^n‚Üí‚Ñù$ is quasiconvex then $g(x)=f(Ax+b)$ is quasiconvex.\n",
        "\n",
        "  - If $f(x,y)$ is quasiconvex jointly in $(x,y)$ and $C$ is convex, then $g(x)=\\inf_{y‚ààC}f(x,y)$ is quasiconvex.\n",
        "\n",
        "- **Convex representation**: to check if $x$ is inside sublevel set $f(x)‚â§t‚áîœï_t(x)‚â§0$ where proxy function $œï_t(x)$ is strictly convex and monotonous: $s‚â•t‚áíœï_s(x)‚â§œï_t(x)$.\n",
        "\n",
        "  - Indicator $œï_t(x)=\\BC 0&f(x)‚â§t\\\\‚àû&\\t{otherwise}\\EC$ is not differentiable. So we use $œï_t(x)=\\t{dist}(x,\\{z|f(z)‚â§t\\})$.\n",
        "\n",
        "- 3.29: $f(x)=\\ln(x)$ is quasiconvex and quasiconcave. $f(x)=\\t{ceil}(x)=\\inf\\{z‚àà‚Ñ§|z‚â•x\\}$ is quasiconvex and quasiconcave.\n",
        "\n",
        "- 3.30: length of a vector $f(x)=\\max\\{i|x_i\\neq0\\}$ has $\\dom(f)=‚Ñù^n$ being a subspace, and all its sublevels are subspaces. It is therefore quasiconvex.\n",
        "\n",
        "- 3.32: **linear-fractional** $f(x)=\\/{a^‚ä§x+b}{c^‚ä§x+d}$ with $\\dom(f)=\\{x|c^‚ä§x+d>0\\}$ and sublevels $S_Œ±=\\{x|c^‚ä§x+d>0,a^‚ä§x+b‚â§Œ±c^‚ä§x+Œ±d\\}$ are convex sets therefore quasiconvex."
      ],
      "metadata": {
        "id": "DoTPqWdMJBLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log-convexity and log-concavity**: $f:‚Ñù^n‚Üí‚Ñù$ is log-concave if $f(x)>0$ for all $x‚àà\\dom(f)$ and $\\ln f$ is concave. $f$ is log-convex if $\\ln f$ is convex. $f$ is log-convex if $1/f$ is log-concave.\n",
        "\n",
        "- $f:‚Ñù^n‚Üí‚Ñù$  with convex domain and $f(x)>0$ for all $x‚àà\\dom(f)$ is log-concave iff for all $x,y‚àà\\dom(f)$ and $Œ∏‚àà[0,1]$ we have $\\red{f(Œ∏x+(1-Œ∏)y)‚â•f(x)^Œ∏f(y)^{1-Œ∏}}$. And $f$ is log-convex iff $\\red{f(Œ∏x+(1-Œ∏)y)‚â§f(x)^Œ∏f(y)^{1-Œ∏}}$.\n",
        "\n",
        "- Let $f‚ààC^2$ with $\\dom(f)$ convex. Then $‚àá^2\\ln f(x)=\\/{1}{f(x)}‚àá^2f(x)-\\/{1}{f(x)^2}‚àáf(x)‚àáf(x)^‚ä§$ by denom-layout. Then $f$ is $\\blue{\\BC\n",
        "\\t{log-convex }‚áî&f(x)‚àá^2f(x)‚âΩ‚àáf(x)‚àáf(x)^‚ä§\\\\\n",
        "\\t{log-concave }‚áî&f(x)‚àá^2f(x)‚âº‚àáf(x)‚àáf(x)^‚ä§\\\\\n",
        "\\EC}$\n",
        "\n",
        "- Log-convexity is closed under pointwise multiplication and positive-scaling. If $f,g$ are log-concave, then so is pointwise product $h(x)=f(x)g(x)$.\n",
        "\n",
        "- Sums of log-concave functions is NOT in general log-concave. Sums of log-convex functions is log-convex. If $f(x,y)$ is log-convex in $x$ for each $y‚ààC$ then $g(x)=‚à´_Cf(x,y)\\ dy$ is log-convex.\n",
        "\n",
        "  - **Log-sum-exp**: $f(x)=\\ln(\\sum_ie^{x_i})$.\n",
        "  Inner $e^{x_i}$ is log-convex, which is closed under summation and $\\sum_ie^{x_i}$ is log-convex. Therefore $\\ln(\\sum_ie^{x_i})$ is convex.\n",
        "\n",
        "- **Prekopa's theorem**: If $f:‚Ñù^n√ó‚Ñù^m‚Üí‚Ñù$ is log-concave, then $g(x)=‚à´f(x,y)\\ dy$ is log-concave.\n",
        "\n",
        "  - Marginal PDFs of log-concave joint PDFs are log-concave.\n",
        "\n",
        "  - Log-concavity is closed under convolution: If $f,g$ are log-concave, then $(f‚äïg)(x)=‚à´f(x-y)g(y)\\ dy$ is log-concave.\n",
        "\n",
        "  - CDFs of log-concave PDFs are log-concave.\n",
        "\n",
        "- 3.39: Examples\n",
        "\n",
        "  - **Affine functions**: $f(x)=a^‚ä§x+b$ is log-concave on $\\{x|a^‚ä§x+b>0\\}$.\n",
        "\n",
        "  - **Powers**: $f(x)=x^a$ is log-convex for $a‚â§0$ and log-concave for $a‚â•0$.\n",
        "\n",
        "  - **Exponential**: $f(x)=e^{ax}$ is log-convex and log-concave.\n",
        "\n",
        "  - **Normal CDF**: $Œ¶(x)=\\/{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^xe^{-u^2/2}\\ du$ is log-concave (integration preservation)\n",
        "\n",
        "  - **Gamma function**: $Œì(x)=‚à´_0^‚àûu^{x-1}e^{-u}\\ du$ is log-convex for $x‚â•1$ (integration preservation)\n",
        "\n",
        "  - **Determinant**: $f(X)=\\det(X)$ is log-concave on $\\v{S}_{++}^n$\n",
        "\n",
        "- 3.40: **PDFs**: Many common PDFs are log-concave."
      ],
      "metadata": {
        "id": "tThPy89H3ybe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Monotonocity on general inequalities**: With respect to proper cone $K‚äÜ‚Ñù^n$, function $f:‚Ñù^n‚Üí‚Ñù$ is K-nondecreasing if $x‚âº_Ky‚áíf(x)‚â§f(y)$ and K-increasing if $x‚âº_Ky‚áíf(x)< f(y)$\n",
        "\n",
        "- $f$ with convex domain is **K-nondecreasing** iff $‚àáf(x)‚âΩ_{K^*}0$ (i.e., $‚àáf(x)^‚ä§(y-x)‚â•0$).\n",
        "\n",
        "  - Proof: Assuming $‚àáf(x)^‚ä§(y-x)‚â•0$ show $y-x‚ààK‚áíf(y)-f(x)‚â•0$.\n",
        "  Let $x,y‚ààD$ convex, then $z(t)=x+t(y-x)‚ààD$ for $t‚àà[0,1]$.\n",
        "  By fundamental theorem of calculus,\n",
        "  $f(y)-f(x)=‚à´_y^x‚àáf(z)\\ dz$\n",
        "  $=‚à´_0^1‚àáf(z(t))^‚ä§(y-x)\\ dt$\n",
        "  $‚â•0$.\n",
        "\n",
        "  - Proof: Assuming $y-x‚ààK‚áíf(y)-f(x)‚â•0$ show $‚àáf(y)^‚ä§v‚â•0$ for all $v‚ààK$. Let $x‚ààD,v‚ààK$, then $x+tv‚ààD,tv‚ààK‚áíf(x+tv)-f(x)‚â•0$\n",
        "  $‚áí‚àáf(x)^‚ä§v=\\lim\\limits_{t‚Üí0}\\/{f(x+tv)-f(x)}{t}‚â•0$\n",
        "\n",
        "- If $‚àáf(x)‚âª_{K^*}0$ then $f$ is **K-increasing**.\n",
        "\n",
        "**Convexity on generalized inequalities**: $f$ is K-convex if $f(Œ∏x+(1-Œ∏)y)‚âº_KŒ∏f(x)+(1-Œ∏)f(y)$.\n",
        "\n",
        "- $f$ is strictly K-convex if $f(Œ∏x+(1-Œ∏)y)‚â∫_KŒ∏f(x)+(1-Œ∏)f(y)$.\n",
        "\n",
        "- Suppose $f:‚Ñù^p‚Üí‚Ñù^q$ then $f(x)$ produces a $q$-dimensional vector with sclar components $F_1,...,F_q$. Then $f$ is K-convex if $F_1(x),...,F_q(x)$ are all convex.\n",
        "\n",
        "- Composition rules on convexity apply to K-convexity as well.\n",
        "\n",
        "- **Dual characterization of K-convexity**: $f$ is K-convex iff $‚àÄŒª‚âΩ_{K^*}0$, we have $Œª^‚ä§f$ is convex.\n",
        "\n",
        "  - Proof: from definition of dual inequality: $x‚âº_Ky$ iff $Œªx‚â§Œªy$ for all $Œª‚ààK^*$.\n",
        "\n",
        "- Example (**Matrix convexity**): function $g(F)‚àà\\v{S}_+^n$ is convex iff $v^‚ä§g(F)v$ is convex for every possible $v‚àà‚Ñù^n$\n",
        "\n",
        "  - Proof: Cone $\\v{S}_+^n$ is self dual. Then $g$ is $\\v{S}_+^n$-convex iff $\\tr(W^‚ä§g(F))$ is convex for every $W‚àà\\v{S}_+^n$.\n",
        "  Decompose $W=PDP^‚ä§=\\sum_iŒª_ip_ip_i^‚ä§=\\sum_iv_iv_i^‚ä§$.\n",
        "  Therefore $g$ is $\\v{S}_+^n$-convex iff $v^‚ä§g(F)v$ is convex for all $v$.\n",
        "\n",
        "- 3.45 (monotone vector function): $f:‚Ñù^n‚Üí‚Ñù$ is nondecreasing with respect to $‚Ñù_+^n$ iff $x‚âº_{‚Ñù_+^n}y‚áíf(x)‚â§f(y)$.\n",
        "\n",
        "- 3.46 (monotone matrix functions): $f:\\v{S}^n‚Üí‚Ñù$ can be monotone with respect to $\\v{S}_{+}^n$.\n",
        "\n",
        "  - $\\tr(X^{-1})$ is matrix decreasing on $\\v{S}_{++}^n$.\n",
        "  \n",
        "  - $\\det(X)$ is matrix increasing on $\\v{S}_{++}^n$ and matrix nondecreasing on $\\v{S}_+^n$.\n",
        "\n",
        "- 3.47 (componentwise convexity): $f:‚Ñù^n‚Üí‚Ñù^m$ is convex with respect to $‚Ñù_+^m$ iff each component $f_i(z)$ is convex $f(Œ∏x+(1-Œ∏)y)‚âºŒ∏f(x)+(1-Œ∏)f(y)$"
      ],
      "metadata": {
        "id": "xUrae60JDV48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3.16: Hessian and convexity\n",
        "\n",
        "  - $f(x)=e^x-1$ on $‚Ñù$ convex, quasiconvex, and quasiconcave.\n",
        "\n",
        "  - $f(x_1,x_2)=x_1x_2$ on $‚Ñù_{++}^2$. $‚àáf=\\BM x_2&x_1\\EM$. $‚àá^2f=\\BM 0&1\\\\1&0\\EM$. It looks like a saddle. Not convex/concave. The level set $\\{x_1,x_2|x_1x_2=1\\}$ looks like a hyperbola, which is quasiconcave, and not quasiconvex.\n",
        "\n",
        "  - $f(x_1,x_2)=1/(x_1x_2)$ on $‚Ñù_{++}^2$. $‚àáf=\\BM -\\/{1}{x_1^2x_2}&-\\/{1}{x_1x_2^2}\\EM$. $‚àá^2f=\\BM\\/{2}{x_1^3x_2}&\\/{1}{x_1^2x_2^2}\\\\\\/{1}{x_1^2x_2^2}&\\/{2}{x_1x_2^3}\\EM$. $\\det(‚àá^2f)=\\/{3}{x_1^4x_2^4}$ convex. The level set $\\{x_1,x_2|1/(x_1x_2)=1\\}$ looks like a hyperbola. It is quasiconvex, and not quasiconcave.\n",
        "\n",
        "  - $f(x_1,x_2)=\\blue{x_1/x_2}$ on $‚Ñù_{++}^2$. Linear-fractional $‚àáf=\\BM1/x_2&-x_1/x_2^2\\EM$. $‚àá^2f=\\BM0&\\/{-1}{x_2^2}\\\\\\/{-1}{x_2^2}&\\/{2x_1}{x_2^3}\\EM$. Not convex/concave. The level set $\\{x_1,x_2|x_1/x_2=1\\}$ is a line. Quasilinear.\n",
        "\n",
        "  - $f(x_1,x_2)=\\blue{x_1^2/x_2}$ on $‚Ñù√ó‚Ñù_{++}$. Quadratic-over-linear $‚àáf=\\BM2x_1/x_2&-x_1^2/x_2^2\\EM$. $‚àá^2f=\\BM 2/x_2&-2x_1/x_2^2\\\\-2x_1/x_2^2&2x_1^2/x_2^3\\EM$.\n",
        "  $\\det(‚àá^2f)=4x_1^2/x_2^4-4x_1^2/x_2^4=0$ positive-semidefinite. Convex and quasiconvex.\n",
        "\n",
        "  - $f(x_1,x_2)=\\blue{x_1^Œ±x_2^{1-Œ±}}$ on $‚Ñù_{++}^2$ and $Œ±‚àà[0,1]$. $‚àáf=\\BM Œ±x_1^{Œ±-1}x_2^{1-Œ±}&(1-Œ±)x_1^Œ±x_2^{-Œ±}\\EM$.\n",
        "  $‚àá^2f=\\BM Œ±(Œ±-1)x_1^{Œ±-2}x_2^{1-Œ±}&Œ±(1-Œ±)x_1^{Œ±-1}x_2^{-Œ±}\\\\\n",
        "  Œ±(1-Œ±)x_1^{Œ±-1}x_2^{-Œ±}&-Œ±(1-Œ±)x_1^Œ±x_2^{-Œ±-1}\\EM$.\n",
        "  $\\det(‚àá^2f)=0$. Negative-semidefinite. Concave and quasiconcave.\n",
        "\n",
        "- 3.17: Show $f(x)=\\blue{\\n{x}_p=(\\sum_ix_i^p)^{1/p}}$ where $\\blue{p< 1,p\\neq0}$ is **concave** on $‚Ñù_{++}^n$.\n",
        "Let $S=\\sum_ix_i^p$ then\n",
        "$‚àÇ_{x_j}f=S^{\\/{1}{p}-1}x_j^{p-1}$.\n",
        "Off-diagonal $‚àÇ_{x_i,x_j}^2=(1-p)S^{\\/{1}{p}-2}x_i^{p-1}x_j^{p-1}$.\n",
        "On-diagonal $‚àÇ_{x_i,x_i}^2=(1-p)S^{\\/{1}{p}-2}x_i^{2p-2}+(p-1)S^{\\/{1}{p}-1}x_i^{p-2}$.\n",
        "Let $z=(x_1^{p-1},...,x_n^{p-1})$\n",
        "Hessian $‚àá^2f=(p-1)S^{\\/{1}{p}-2}(S\\diag(x_i^{p-2})-zz^‚ä§)$.\n",
        "Check negative-semidefinite\n",
        "$v^‚ä§‚àá^2fv=(p-1)S^{\\/{1}{p}-2}((\\sum_ix_i^p)(\\sum_iv_i^2x_i^{p-2})-(\\sum_iv_ix_i^{p-1})^2)$\n",
        "$=(p-1)S^{\\/{1}{p}-2}(\\sum_i(x_i^{p/2})^2\\sum_i(v_ix_i^{p/2-1})^2-(\\sum_iv_ix_i^{p-1})^2)‚â§0$ by Cauchy-Schwarz, therefore concave.\n",
        "\n",
        "- 3.18: matrix functions convexity\n",
        "\n",
        "  - $f(X)=\\blue{\\tr(X^{-1})}$ on $\\v{S}_{++}^n$.\n",
        "  Let $g(t)=\\tr((Z+tV)^{-1})$ where $Z,Z+tV‚àà\\v{S}_{++}^n$.\n",
        "  We only need $g''(0)$ to prove convexity of $f$. Let $Y(t)=(Z+tV)^{-1}$.\n",
        "  Then $Y'(t)=-YVY$ and\n",
        "  $g(t)=\\tr(Y)$\n",
        "  $‚áíg'(t)=\\tr(-YVY)=-\\tr(YVY)$.\n",
        "  $g''(t)=-\\tr(‚àáYVY+YV‚àáY)$\n",
        "  $=2\\tr(YVYVY)$.\n",
        "  Then\n",
        "  $g''(0)=2\\tr(Z^{-1}VZ^{-1}VZ^{-1})$\n",
        "  $=2\\tr((Z^{-1}VZ^{-1/2})(Z^{-1/2}VZ^{-1}))$\n",
        "  $=2\\tr((Z^{-1/2}VZ^{-1})^‚ä§(Z^{-1/2}VZ^{-1}))$\n",
        "  $=2\\n{(Z^{-1/2}VZ^{-1})}_F^2‚â•0$\n",
        "  therefore convex.\n",
        "\n",
        "  - $f(X)=\\blue{(\\det(X))^{1/n}}$ on $\\v{S}_{++}^n$.\n",
        "  Let $g(t)=(\\det(Z+tV))^{1/n}$ where $Z,Z+tV‚àà\\v{S}_{++}^n$.\n",
        "  $\\det(Z+tV)=\\det(Z)\\det(I+tZ^{-1/2}VZ^{-1/2})$\n",
        "  $=\\det(Z)\\prod_i(1+tŒª_i)$ where $Œª_i‚àà‚Ñù$ are eigenvalues of $Z^{-1/2}VZ^{-1/2}$.\n",
        "  Therefore\n",
        "  $g(t)=\\det(Z)^{1/n}(\\prod_{i=1}^n(1+tŒª_i))^{1/n}$,\n",
        "  which is a geometric mean, concave.\n",
        "\n",
        "- 3.19: nonnegative weighted combinations\n",
        "\n",
        "  - $f(x)=\\sum_{i=1}^rŒ±_ix_{[i]}$ given $Œ±_1‚â•...‚â•Œ±_r‚â•0$ and $x_{[1]}‚â•...‚â•x_{[r]}$.\n",
        "  $f(x)$ is the pointwise max of $\\binom{n}{r}$ possible sums of $r$ of $n$ elements. Therefore it is convex.\n",
        "\n",
        "  - Let $T(x,œâ)=x_1+x_2\\cos œâ+...+x_n\\cos (n-1)œâ$. Show $f(x)=-‚à´_{0}^{2œÄ}\\ln T(x,œâ)\\ dœâ$ is convex on $\\{x|T(x,œâ)>0,0‚â§œâ‚â§2œÄ\\}$.\n",
        "  Each $T(x,œâ)$ is an affine combination of $x$ and is convex.\n",
        "  Therefore $-\\ln T(x,œâ)$ is convex.\n",
        "  $f(x)$ is an integration of convex functions of $x$ and is convex.\n",
        "\n",
        "- 3.20: composition with affine\n",
        "\n",
        "  - $f(x)=\\n{Ax-b}$. $h(y)=\\n{y}$ convex. Therefore $f$ is convex.\n",
        "\n",
        "  - $f(x)=-\\det(A_0+x_1A_1+...+x_mA_m)^{1/m}$ where  linear matrix combination is positive-definite. From 3.18 $\\det(X)^{1/n}$ is concave, therefore $f(x)$ is convex.\n",
        "\n",
        "  - $f(x)=\\tr((A_0+x_1A_1+...+x_nA_n)^{-1})$ where linear matrix combination is positive-definite. From 3.18 $\\tr(X^{-1})$ is convex, therefore $f(x)$ is convex.\n",
        "\n",
        "- 3.21: pointwise max/supremum\n",
        "\n",
        "  - $f(x)=\\max_{i=1,...,k}\\n{A^{(i)}x-b^{(i)}}$ where $A^{(i)}‚àà‚Ñù^{m√ón}$, $b‚àà‚Ñù^m$.\n",
        "  $f(x)$ is pointwise maximum of convex functions. Therefore convex.\n",
        "\n",
        "  - $f(x)=\\sum_{i=1}^r|x|_{[i]}$ on $‚Ñù^n$ where $|x|_{[i]}$ is the $i$th largest absolute value of vector $x$.\n",
        "  $f(x)$ is the pointwise max of $\\binom{n}{r}$ possible sums and is therefore convex.\n",
        "\n",
        "- 3.22: composition rules:\n",
        "\n",
        "  - $f(x)=-\\ln(-\\ln(\\sum_ie^{a_i^‚ä§x+b_i}))$ on $\\{x|S< 1\\}$ where $S=\\sum_ie^{a_i^‚ä§x+b_i}$.\n",
        "  Here $S(x)$ is convex.\n",
        "  $-\\ln(S)>0$ is concave ($h$ convex and nonincreasing, $g$ convex).\n",
        "  Therefore $f(x)$ is convex.\n",
        "\n",
        "  - $f(x,u,v)=-\\sqrt{uv-x^‚ä§x}$ on $\\{x,u,v|uv>x^‚ä§x,u,v>0\\}$.\n",
        "  $f(x,u,v)=-\\sqrt{u(v-\\/{x^‚ä§x}{u})}$.\n",
        "  Here\n",
        "  $\\/{x^‚ä§x}{u}$ is quadratic-over-linear convex,\n",
        "  $v-\\/{x^‚ä§x}{u}$ is concave.\n",
        "  From 3.16\n",
        "  $\\sqrt{x_1x_2}$ is concave,\n",
        "  $-\\sqrt{x_1x_2}$ is convex and decreasing.\n",
        "  Therefore $f(x)$ is convex.\n",
        "\n",
        "  - $f(x,u,v)=-\\ln(uv-x^‚ä§x)$ on $\\{x,u,v|uv>x^‚ä§x,u,v>0\\}$.\n",
        "  $f(x,u,v)=-\\ln(u)-\\ln(v-\\/{x^‚ä§x}{u})$.\n",
        "  Here $-\\ln(u)$ is convex.\n",
        "  $v-\\/{x^‚ä§x}{u}$ is concave,\n",
        "  and $-\\ln(‚ãÖ)$ is convex and decreasing,\n",
        "  then $-\\ln(v-\\/{x^‚ä§x}{u})$ is convex.\n",
        "  Therefore $f(x,u,v)$ is convex.\n",
        "\n",
        "  - $f(x,t)=-(t^p-\\n{x}_p^p)^{1/p}$ where $p>1$ on $\\{x,t|t‚â•\\n{x}_p\\}$.\n",
        "  $f(x,t)=-t^{1-\\/{1}{p}}(t-\\/{\\n{x}_p^p}{t^{p-1}})^{\\/{1}{p}}$.\n",
        "  Here $\\/{\\n{x}_p^p}{t^{p-1}}$ is convex,\n",
        "  $t-\\/{\\n{x}_p^p}{t^{p-1}}$ is concave.\n",
        "  From 3.16\n",
        "  $x_1^{1-\\/{1}{p}}x_2^{\\/{1}{p}}$ is concave,\n",
        "  $-x_1^{1-\\/{1}{p}}x_2^{\\/{1}{p}}$ is convex and decreasing,\n",
        "  therefore $f(x,t)$ is convex.\n",
        "\n",
        "  - $f(x,t)=-\\ln(t^p-\\n{x}_p^p)$ where $p>1$ on $\\{x,t|t>\\n{x}_p\\}$.\n",
        "  $f(x,t)=-\\ln(t^{1-\\/{1}{p}})-\\ln(t-\\/{\\n{x}_p^p}{t^{p-1}})$.\n",
        "  Here $t-\\/{\\n{x}_p^p}{t^{p-1}}$ is concave,\n",
        "  $-\\ln(t-\\/{\\n{x}_p^p}{t^{p-1}})$ is convex.\n",
        "  $t^{1-\\/{1}{p}}$ is concave, $-\\ln(t^{1-\\/{1}{p}})$ is convex.\n",
        "  Therefore $f(x)$ is convex.\n",
        "\n",
        "- 3.23: Perspective: if $g(x)$ is convex then so is $tg(x/t)$.\n",
        "\n",
        "  - Show $p>1$ then $f(x,t)=\\blue{\\/{\\n{x}_p^p}{t^{p-1}}}$ is convex on $\\{x,t|t>0\\}$.\n",
        "  Base function $g(x)=\\n{x}_p^p=\\sum_i|x_i|^p$ is convex.\n",
        "  Then $tg(\\/{x}{t})=t\\sum_i|\\/{x_i}{t}|^p$\n",
        "  $=\\/{1}{t^{p-1}}\\sum_i|x_i|^p$\n",
        "  $=\\/{\\n{x}_p^p}{t^{p-1}}$ is convex.\n",
        "\n",
        "  - Show $f(x)=\\/{\\n{Ax+b}_2^2}{c^‚ä§x+d}$ is convex on $\\{x|c^‚ä§x+d>0\\}$.\n",
        "  By affine composition $g(x)=\\n{Ax+b}_2^2$ is convex.\n",
        "  Path #1: $(c^‚ä§x+d)g(\\/{x}{c^‚ä§x+d})$\n",
        "  $=(c^‚ä§x+d)\\sum_i|\\/{Ax_i+b}{c^‚ä§x+d}|^2$\n",
        "  $=\\/{1}{c^‚ä§x+d}\\sum_i|Ax_i+b|^2$\n",
        "  $=\\/{\\n{Ax+b}_2^2}{c^‚ä§x+d}$\n",
        "  is convex.\n",
        "  Path #2: $f(x)=\\/{(Ax+b)^‚ä§(Ax+b)}{c^‚ä§x+d}$ is quadratic-over-linear, which is convex.\n",
        "\n",
        "- 3.24: Probability simplex: Let $x$ be random variable with support $\\{a_1< ...< a_n\\}$ and probability $P(x=a_i)=p_i$. Determine convexity and quasiconvexity of functions on simplex $\\{p‚àà‚Ñù_+^n|\\v{1}^‚ä§p=1\\}$.\n",
        "\n",
        "  - $f(p)=\\E[x]=\\sum_ia_ip_i$ is a linear function of $p$. Linear and therefore quasilinear.\n",
        "\n",
        "  - $f(p)=P(x‚â•Œ±)=\\sum_{i‚â•k}p_i$ is a linear function of $p$. Linear and quasilinear.\n",
        "\n",
        "  - $f(p)=P(Œ±‚â§x‚â§Œ≤)=\\sum_{k‚â§i‚â§l}p_i$ is a linear function of $p$. Linear and quasilinear.\n",
        "\n",
        "  - **Negative entropy** $f(p)=\\blue{\\sum_ip_i\\ln(p_i)}=p^‚ä§\\ln(p)$.\n",
        "  $‚àáf=\\ln(p)+n$.\n",
        "  $‚àá^2f=\\/{1}{p}‚âª0$\n",
        "  therefore convex and quasiconvex.\n",
        "\n",
        "  - $f(p)=\\Var(x)=\\E[x^2]-\\E[x]^2$.\n",
        "  $\\E[x^2]=\\sum_ia_i^2p_i$ is a linear function of $p$.\n",
        "  $-\\E[x]^2$ is a concave function of $p$.\n",
        "  Together $f(p)$ is negative quadratic function of $p$ and is concave and quasiconcave.\n",
        "\n",
        "- 3.26: **Eigenvalues**\n",
        "\n",
        "  - Largest eigenvalue $Œª_1(X)=\\sup\\{y^‚ä§Xy|\\n{y}_2=1\\}$ is convex.\n",
        "\n",
        "  - Smallest eigenvalue $Œª_n(X)=\\inf\\{y^‚ä§Xy|\\n{y}_2=1\\}$ is concave.\n",
        "\n",
        "  - Sum of eigenvalues $\\tr(X)$ is linear.\n",
        "\n",
        "  - Sum of inverse of eigenvalues $\\tr(X^{-1})$ is convex (3.18)\n",
        "\n",
        "  - Geometric mean of eigenvalues $\\det(X)^{1/n}$ is concave (3.18)\n",
        "\n",
        "  - Sum of $k$ largest eigenvalues $\\sum_{i=1}^kŒª_i(X)=\\sup\\{\\tr(V^‚ä§XV)|V‚àà‚Ñù^{n√ók},V^‚ä§V=I\\}$.\n",
        "  $\\tr(V^‚ä§XV)=\\tr(VV^‚ä§X)=‚ü®VV^‚ä§,X‚ü©$ is linear function of $X$.\n",
        "  Pointwise maximum of convex functions is convex.\n",
        "  \n",
        "- 3.36: Conjugate functions\n",
        "\n",
        "  - **Max**: $\\blue{f(x)=\\max_{i=1,...,n}x_i}$ on $‚Ñù^n$.\n",
        "  Variational characterization\n",
        "  $f(x)=\\sup\\{z^‚ä§x|0‚âºz‚âº\\v{1},\\v{1}^‚ä§z=1\\}$, which forces $z=(1,0,...,0)$ for $x_1=\\max_ix_i$.\n",
        "  This means $f$ is the support function of $S=\\{0‚âºz‚âº\\v{1},\\v{1}^‚ä§z=1\\}$.\n",
        "  Therefore $f^*(y)=\\BC\n",
        "  0&\\v{1}^‚ä§y=1,y‚âΩ0 \\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$ is indicator function for probability simplex.\n",
        "\n",
        "  - **Sum of largest**: $\\blue{f(x)=\\sum_{i=1}^rx_{[i]}}$ on $‚Ñù^n$.\n",
        "  Variational characterization\n",
        "  $f(x)=\\sup\\{z^‚ä§x|0‚âºz‚âº\\v{1},\\v{1}^‚ä§z=r\\}$, which forces $z_i$ to be either 0 or 1.\n",
        "  This means $f$ is the support function $f(x)=\\sup_{z‚ààS}z^‚ä§x$ where $S=\\{z|0‚âºz‚âº\\v{1},\\v{1}^‚ä§z=r\\}$.\n",
        "  Therefore $f^*(y)=\\BC\n",
        "  0&0‚âºy‚âº\\v{1},\\v{1}^‚ä§y=r\\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$.\n",
        "\n",
        "  - **Piecewise-linear**: $\\blue{f(x)=\\max_i(a_ix+b_i)}$ on $‚Ñù$. No piece is redundant: $f(x)=f_i(x)=a_ix+b_i$ for some $x$. The slopes are ordered $a_1‚â§...‚â§a_m$.\n",
        "  $\\BC\n",
        "  \\dom(f^*)=[a_1,a_n] &\\t{are slopes of }f_i(x)\\\\\n",
        "  f^*(a_k)=-b_k &\\t{are $f$-intersept of }f_i(x)\n",
        "  \\EC$.\n",
        "  $f^*(y)$ is also piecewise-linear, with lines joining at vertices $(a_k,-b_k)$.\n",
        "  For $y‚àà(a_k,a_{k+1})$, we have $f_k$ and $f_{k+1}$ joining at $x_k$ and satisfy\n",
        "  $f_k(x_k)=f_{k+1}(x_k)$\n",
        "  $‚áía_kx_k+b_k=a_{k+1}x_k+b_{k+1}$\n",
        "  $‚áíx_k=\\/{b_k-b_{k+1}}{a_{k+1}-a_k}$ is the slope of the conjugate line.\n",
        "  The sharp corner at $x_k$ between $f_k$ and $f_{k+1}$ has spawned the entire set of slopes $y‚àà[a_k,a_{k+1}]$ in the conjugate domain forming a line segment whose slope is $x_k$.\n",
        "  We have\n",
        "  $f^*(y)=yx_k-f(x_k)$\n",
        "  $=x_k(y-a_k)-b_k$\n",
        "  $=\\/{b_k-b_{k+1}}{a_{k+1}-a_k}(y-a_k)-b_k$.\n",
        "  Duality\n",
        "  $\\blue{\\BC\n",
        "  f(x)&\\t{line segment slope }a_k&\\t{sharp corner }x=x_k\\\\\n",
        "  f^*(y)&\\t{sharp corner at }y=a_k&\\t{line segment slope }x_k\n",
        "  \\EC}$\n",
        "\n",
        "  - **Power**: $\\blue{f(x)=x^p}$ on $‚Ñù_{++}$.\n",
        "  $h(x)=y^‚ä§x-x^p$.\n",
        "  $h'(x)=y-px^{p-1}=0$\n",
        "  $‚áíx^*=(\\/{y}{p})^{\\/{1}{p-1}}$.\n",
        "  $\\dom(f)=‚Ñù_{++}‚áí\\BC\n",
        "  \\dom(f^*)=‚Ñù_{++} &\\t{if }p>1\\\\\n",
        "  \\dom(f^*)=‚Ñù_{--} &\\t{if }p< 0\n",
        "  \\EC$.\n",
        "  $f^*(y)=h(x^*)=y(\\/{y}{p})^{\\/{1}{p-1}}-(\\/{y}{p})^{\\/{p}{p-1}}$\n",
        "  $=(p-1)(\\/{y}{p})^{\\/{p}{p-1}}$\n",
        "\n",
        "  - **Negative geometric mean**: $\\blue{f(x)=-(\\prod_ix_i)^{1/n}}$ on $‚Ñù_{++}^n$.\n",
        "  $f$ is convex and satisfies $f(tx)=tf(x)$, therefore is support function of $S=\\{y|y^‚ä§x‚â§f(x)\\ ‚àÄx‚àà\\dom(f)\\}$\n",
        "  $=\\{y|y^‚ä§x+(\\prod_ix_i)^{1/n}‚â§0\\ ‚àÄx‚àà‚Ñù_{++}^n\\}$.\n",
        "  $\\BC\n",
        "  1. &\\t{immediately clear requirement}&\\green{y_i‚â§0}\\\\\n",
        "  2. &\\t{Arithmetic-geometric inequality}&\\/{1}{n}\\sum_iz_i‚â•(\\prod_iz_i)^{1/n}\\\\\n",
        "  &\\t{Let }z_i=-y_ix_i&\\sum_i-y_ix_i‚â•(\\prod_ix_i)^{1/n}\\\\\n",
        "  &&\\sum_iz_i‚â•(\\prod_i\\/{z_i}{-y_i})^{1/n}\\\\\n",
        "  &&(\\prod_i-y_i)^{1/n}\\sum_iz_i‚â•(\\prod_iz_i)^{1/n}\\\\\n",
        "  &\\t{AG inequality is tightest}&(\\prod_i-y_i)^{1/n}\\sum_iz_i‚â•\\/{1}{n}\\sum_iz_i‚â•(\\prod_iz_i)^{1/n}\\\\\n",
        "  &&\\green{(\\prod_i-y_i)^{1/n}‚â•\\/{1}{n}}\n",
        "  \\EC$.\n",
        "  $S=\\{y|y‚âº0,(\\prod_i-y_i)^{1/n}‚â•\\/{1}{n}\\}$.\n",
        "  $f^*(y)=\\BC\n",
        "  0&y‚âº0,(\\prod_i-y_i)^{1/n}‚â•\\/{1}{n}\\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$.\n",
        "\n",
        "- 3.37: $f(X)=\\tr(X^{-1})$ on $\\v{S}_{++}^n$.\n",
        "$h(X)=\\tr(XY)-\\tr(X^{-1})$.\n",
        "$‚àáh(X)=Y+X^{-2}=0$\n",
        "$‚áíX^*=(-Y)^{-1/2}$.\n",
        "$Y‚àà‚Ñù_{-}^n$.\n",
        "$f^*(Y)=\\tr((-Y)^{-1/2}Y)-\\tr((-Y)^{1/2})$\n",
        "$=-\\tr((-Y)^{-1/2}(-Y))-\\tr((-Y)^{1/2})$\n",
        "$=-\\tr((-Y)^{1/2})-\\tr((-Y)^{1/2})$\n",
        "$=-2\\tr((-Y)^{1/2})$\n",
        "on $‚Ñù_{-}^n$.\n",
        "\n",
        "- 3.39 Properties of conjugate:\n",
        "\n",
        "  - **Convex + affine**: $\\blue{g(x)=f(x)+c^‚ä§x+d}$ where $f$ convex.\n",
        "  $g^*(y)=\\sup_xy^‚ä§x-f(x)-c^‚ä§x-d$\n",
        "  $=\\sup_x(y-c)^‚ä§x-f(x)-d$.\n",
        "  $\\blue{g^*(y)=f^*(y-c)-d}$ on\n",
        "  $\\dom(g^*)=\\{y|y-c‚àà\\dom(f^*)\\}$\n",
        "\n",
        "  - **Perspective**: $\\blue{g(x,t)=tf(\\/{x}{t})}$ is perspective function of $f(x)$ on $\\{x,t|t>0,x/t‚àà\\dom(f)\\}$.\n",
        "  Let $z=x/t$ then\n",
        "  $g^*(y,s)=\\sup_{x,t}y^‚ä§x+st-tf(x/t)$\n",
        "  $=\\sup_{z,t}y^‚ä§(tz)+st-tf(z)$\n",
        "  $=\\sup_tt\\sup_z(y^‚ä§z+s-f(z))$\n",
        "  $=\\sup_tt(f^*(y)+s)$\n",
        "  $‚áíg^*(y,s)=\\BC\n",
        "  0&f^*(y)+s‚â§0\\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$\n",
        "\n",
        "  - **Partial infimum**: $\\blue{g(x)=\\inf_zf(x,z)}$ where $f(x,z)$ is convex in $(x,z)$.\n",
        "  $g^*(y)=\\sup_x(y^‚ä§x-\\inf_zf(x,z))$\n",
        "  $=\\sup_x(y^‚ä§x+\\sup_z(-f(x,z)))$\n",
        "  $=\\sup_{x,z}y^‚ä§x-f(x,z)$\n",
        "  $=\\sup_{x,z}y^‚ä§x+0z-f(x,z)$\n",
        "  $‚áí\\blue{g^*(y)=f^*(y,0)}$\n",
        "\n",
        "  - **Partial infimum**: $\\blue{g(x)=\\inf_z\\{f(z)|Az+b=x\\}}$ where $f(z)$ is convex.\n",
        "  Let $F(x,z)=\\BC\n",
        "  f(z)&Az+b=x\\\\\n",
        "  ‚àû&\\t{otherwise}\n",
        "  \\EC$, then\n",
        "  $g(x)=\\inf_zF(x,z)$.\n",
        "  $F^*(y,v)=\\sup_{x,z}y^‚ä§x+v^‚ä§z-F(x,z)$\n",
        "  $=\\sup_zy^‚ä§(Az+b)+v^‚ä§z-f(z)$\n",
        "  $=y^‚ä§b+\\sup_z(A^‚ä§y+v)^‚ä§z-f(z)$\n",
        "  $‚áí\\green{F^*(y,v)=y^‚ä§b+f^*(A^‚ä§y+v)}$.\n",
        "  $g^*(y)=F^*(y,0)=y^‚ä§b+f^*(A^‚ä§y)$.\n",
        "\n",
        "- 3.40 **Gradient and Hessian of conjugate**: $f:‚Ñù^n‚Üí‚Ñù$ is convex and $C^2$. Suppose $\\bar{y}=‚àáf(\\bar{x})$ and $‚àá^2f(\\bar{x})‚âª0$.\n",
        "\n",
        "  - Show $\\blue{‚àáf^*(\\bar{y})=\\bar{x}}$.\n",
        "  For $f^*(y)=\\sup_xy^‚ä§x-f(x)$, at supremum $x^*=\\arg\\sup_xy^‚ä§x-f(x)$ must satisfy\n",
        "  $\\green{y=‚àáf(x^*)}$.\n",
        "  Let $x^*(y)$ be the supremum $x^*$ for given $y$ satisfying $y=‚àáf(x^*(y))$, then\n",
        "  $f^*(y)=y^‚ä§x^*(y)-f(x^*(y))$\n",
        "  $‚áí‚àáf^*(y)=x^*(y)+y^‚ä§‚àáx^*(y)-‚àáf(x^*(y))^‚ä§‚àáx^*(y)$\n",
        "  $=x^*(y)+(y-‚àáf(x^*(y)))^‚ä§‚àáx^*(y)$\n",
        "  $‚áí\\green{‚àáf^*(y)=x^*(y)}$.\n",
        "\n",
        "  - Show $\\blue{‚àá^2f^*(\\bar{y})=‚àá^2f(\\bar{x})^{-1}}$.\n",
        "  $\\BC\n",
        "  y=‚àáf(x^*(y))&‚áí\n",
        "  I=‚àá^2f(x^*(y))‚àáx^*(y)\\\\\n",
        "  ‚àáf^*(y)=x^*(y)&‚áí\n",
        "  ‚àá^2f^*(y)=‚àáx^*(y)\n",
        "  \\EC$\n",
        "  $‚áí\\green{I=‚àá^2f(x^*(y))‚àá^2f^*(y)}$\n",
        "\n",
        "- 3.41 **Negative normalized entropy**: $\\blue{f(x)=\\sum_{i=1}^nx_i\\ln(\\/{x_i}{\\v{1}^‚ä§x})}$ on $‚Ñù_{++}^n$.\n",
        "Normalized entropy $\\sum_ix_i\\ln(\\/{\\v{1}^‚ä§x}{x_i})$ is concave therefore $f(x)$ is convex. Furthermore $f(x)$ is positively homogeneous $f(tx)=tf(x)$ therefore $f$ is a support function for set $S=\\{y|y^‚ä§x‚â§f(x)\\}$.\n",
        "Let $p_i=\\/{x_i}{\\v{1}^‚ä§x}$ be probability simplex, then\n",
        "$y^‚ä§x‚â§\\sum_ix_i\\ln(\\/{x_i}{\\v{1}^‚ä§x})$\n",
        "$‚áí\\sum_iy_ix_i‚â§\\sum_ix_i\\ln(\\/{x_i}{S})$\n",
        "$‚áí\\sum_ix_i(y_i-\\ln(\\/{x_i}{S}))‚â§0$\n",
        "$‚áíS\\sum_ip_i(y_i-\\ln(p_i))‚â§0$\n",
        "$‚áí\\sum_ip_iy_i-\\sum_ip_i\\ln(p_i)‚â§0$\n",
        "$‚áí\\sup_pp^‚ä§y-p\\ln(p)‚â§0$\n",
        "$‚áíg^*(y)‚â§0$ where convex negative entropy $\\green{g(p)=p\\ln(p)}$ is conjugate of convex log-sum-exp $\\green{g^*(y)=\\ln(\\sum_ie^{y_i})}$ (example 3.25).\n",
        "Therefore $f$ is support function for\n",
        "$S=\\{y|\\ln(\\sum_ie^{y_i})‚â§0\\}$.\n",
        "$f^*(y)=\\BC\n",
        "0&\\sum_ie^{y_i}‚â§1\\\\\n",
        "‚àû&\\t{otherwise}\n",
        "\\EC$.\n",
        "\n",
        "- 3.52 **moment functions**: $f$ is nonnegative on $‚Ñù_+$. Show $\\blue{œï(x)=‚à´_0^‚àûu^xf(u)\\ du}$ is log-convex.\n",
        "By Holder inequality $\\/{1}{p}=Œ∏$ and $\\/{1}{q}=1-Œ∏$ we have\n",
        "$œï(Œ∏x+(1-Œ∏)y)=‚à´_0^‚àûu^{Œ∏x+(1-Œ∏)y}f(u)\\ du$\n",
        "$=‚à´_0^‚àû(u^xf(u))^Œ∏(u^yf(u))^{1-Œ∏}\\ du$\n",
        "$‚â§(‚à´_0^‚àû(u^xf(u))^{Œ∏\\/{1}{Œ∏}}\\ du)^Œ∏(‚à´_0^‚àû(u^xf(u))^{(1-Œ∏)\\/{1}{1-Œ∏}}\\ du)^{1-Œ∏}$\n",
        "$‚áíœï(Œ∏x+(1-Œ∏)y)‚â§œï(x)^Œ∏œï(x)^{1-Œ∏}$.\n",
        "\n",
        "- 3.53: Suppose $x,y$ are independent random variables with log-concave PDF $f,g$. Show $z=x+y$ is log-concave.\n",
        "Let $h(z)$ be the PDF of $z$, then\n",
        "$h(z)=(f‚äïg)(z)=‚à´_tf(z-t)g(t)\\ dt$.\n",
        "If $f,g$ are log-concave, then log-concavity is closed under pointwise product $f(z-t)g(t)$ and integral $‚à´_tf(z-t)g(t)\\ dt$. Therefore $h(z)$ is log-concave."
      ],
      "metadata": {
        "id": "wtAlAljSXaJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&V: Convex Optimization Problems"
      ],
      "metadata": {
        "id": "V4sbAQwnPNNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimization problem** standard form $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0,&i=1,...,m\\\\\n",
        "&h_j(x)=0,&j=1,...,p\n",
        "\\EC$\n",
        "\n",
        "- $f_0:‚Ñù^n‚Üí‚Ñù$ is the objective function, $x‚àà‚Ñù^n$ is the optimization variable. $f_i(x)‚â§0,h_j(x)=0$ are inequality and equality functions and constraints.\n",
        "$\\m{D}=\\bigcap_i\\dom(f_i)‚à©\\bigcap_j\\dom(h_i)$ is the domain of the problem.\n",
        "Point $x$ is **feasible** if it satisfies all constraints. Feasible set $\\m{F}=\\{x‚àà\\m{D}|f_i(x)‚â§0\\ ‚àÄi,h_j(x)=0\\ ‚àÄj\\}$.\n",
        "\n",
        "- If $\\m{F}\\neq‚àÖ$ then the problem is feasible and **optimal value** $p*=\\inf_x\\{f_0(x)|x‚àà\\m{F}\\}$.\n",
        "Point $x^*‚àà\\m{F}$ is an **optimal point** if $f_0(x^*)=p^*$.\n",
        "Optimal set\n",
        "$X_\\t{opt}=\\{x‚àà\\m{F}|,f_0(x)=p^*\\}$.\n",
        "If $\\m{F}=‚àÖ$ then $p^*=‚àû$.\n",
        "\n",
        "  - Problem is unbounded below if there are $x_k‚àà\\m{F}$ such that $f_0(x_k)‚Üí-‚àû$ as $k‚Üí‚àû$, then $p^*=-‚àû$.\n",
        "  Problem is solvable if the optimal set is nonempty, optimal value is attained, and not unbounded below.\n",
        "\n",
        "  - $x‚àà\\m{F}$ satisfying $f_0(x)‚â§p^*+œµ$ is $œµ$-suboptimal.\n",
        "\n",
        "  - Point $z‚àà\\m{F}$ is **locally optimal** if $f_0(z)=\\inf\\{f_0(x)|f_i(x)‚â§0\\ ‚àÄi,h_j(x)=0\\ ‚àÄj,\\n{x-z}_2‚â§R\\}$ for $R>0$.\n",
        "  Optimization problem with an added constraint $\\n{x-z}_2‚â§R$ for a smaller $\\m{F}$.\n",
        "\n",
        "  - **Feasibility problem**: $\\BC\n",
        "  \\t{find}&x\\\\\n",
        "  \\t{subject to}&x‚àà\\m{F}\\EC$ is an algorithmic function that returns an instance of $x‚àà\\m{F}$ (problem is feasible) or it returns $‚àÖ$. The objective function \"$0$\" has optimal value $p^*=\\BC\n",
        "  0&\\m{F}\\neq‚àÖ\\\\\n",
        "  ‚àû&\\m{F}=‚àÖ\n",
        "  \\EC$.\n",
        "\n",
        "- If $x‚àà\\m{F}$ and $f_i(x)=0$, then the $i$-th inequality constraint is **active** at $x$.\n",
        "If $f_i(x)< 0$ then constraint $f_i(x)‚â§0$ is inactive. Constraint is redundant if deleting it does not change $\\m{F}$.\n",
        "\n",
        "**Equivalent problems**: solution of one problem is readily obtained from the solution of another.\n",
        "\n",
        "- **Maximizing** $f_0(x)$ is equivalent to minimizing $-f_0(x)$.\n",
        "\n",
        "- **Change of variable**: Let $œï:‚Ñù^n‚Üí‚Ñù^n$, $\\m{D}‚äÜœï(\\dom(œï))$ such that $\\tilde{f}_i(x)=f_i(œï(x))$ and $\\tilde{h}_i(x)=h_i(œï(x))$.\n",
        "\n",
        "  - $\\BC \\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0\\\\\n",
        "  &h_j(x)=0\\EC‚áí\\BC \\t{minimize}&\\tilde{f}_0(x)\\\\\n",
        "  \\t{subject to}&\\tilde{f}_i(x)‚â§0\\\\\n",
        "  &\\tilde{h}_j(x)=0 \\EC$\n",
        "\n",
        "- **Transformation of functions**: $œà_0$ is monotone increasing, $œà_i(u)‚â§0‚áîu‚â§0$, and $œà_{m+j}(u)=0‚áîu=0$, such that $\\tilde{f}_i(x)=œà_i(f_i(x))$ and $\\tilde{h}_j(x)=œà_{m+j}(h_j(x))$.\n",
        "\n",
        "  - $\\BC \\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0\\\\\n",
        "  &h_j(x)=0 \\EC‚áí\\BC \\t{minimize}&\\tilde{f}_0(x)\\\\\n",
        "  \\t{subject to}&\\tilde{f}_i(x)‚â§0\\\\\n",
        "  &\\tilde{h}_j(x)=0 \\EC$\n",
        "\n",
        "- **Slack variables**: $f_i(x)‚â§0$ becomes $f_i(x)+s_i=0$ and $s_i‚â•0$.\n",
        "\n",
        "  - $\\BC \\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0\\\\\n",
        "  &h_j(x)=0 \\EC‚áí\\BC \\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&s_i‚â•0\\\\\n",
        "  &f_i(x)+s_i=0\\\\\n",
        "  &h_j(x)=0 \\EC$\n",
        "\n",
        "- **Eliminating equality constraints**: Let $z‚àà‚Ñù^k$ and $œï:‚Ñù^k‚Üí‚Ñù^n$ such that $h_j(œï(z))=0$ is always true.\n",
        "\n",
        "  - $\\BC \\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0\\\\\n",
        "  &h_j(x)=0 \\EC‚áí\\BC \\t{minimize}&f_0(œï(z))\\\\\n",
        "  \\t{subject to}&f_i(œï(z))‚â§0 \\EC$\n",
        "\n",
        "  - Linear example:\n",
        "  $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "  \\t{subject to}&f_i(x)‚â§0 \\\\\n",
        "  &Ax-b=0\\EC‚áí\\BC \\t{minimize}&f_0(z_0+Fz) \\\\\n",
        "  \\t{subject to}&f_i(z_0+Fz)‚â§0\\EC$.\n",
        "  Let $A‚àà‚Ñù^{m√ón}$ and $F‚àà‚Ñù^{n√ók}$ such that $b‚àà\\span(A)$ and $\\span(F)=\\null(A)$.\n",
        "  Solutions to $Ax=b$ are $x‚àà\\{x_0+\\null(A)\\}$ where $x_0$ is one solution.\n",
        "  Now the dimensionality of the solution has been reduced by $\\rk(A)$ from $n$ variables to $k$ variables.\n",
        "\n",
        "- **Introducing equality constraints**: Create a slack variable $y_i$ for each affine pre-composition $A_ix+b_i$. The objective and constraints become independent with different optimization variables. This is for parallel computing.\n",
        "\n",
        "  - $\\BC\\t{minimize}&f_0(A_0x+b_0)\\\\\n",
        "  \\t{subject to}&f_i(A_ix+b_i)‚â§0\\\\\n",
        "  &h_j(x)=0\\EC ‚áí \\BC\\t{minimize}&f_0(y_0)\\\\\n",
        "  \\t{subject to}&f_i(y_i)‚â§0\\\\\n",
        "  &y_i=A_ix+b\\\\\n",
        "  &h_j(x)=0\\EC$\n",
        "\n",
        "- **Partial optimization**: $\\inf_{x,y}f(x,y)=\\inf_x(\\inf_yf(x,y))$. Useful when $\\inf_yf(x,y)$ has closed form or easy to solve.\n",
        "\n",
        "  - Let $x‚àà‚Ñù^{n_1+n_2},x_1‚àà‚Ñù^{n_1},x_2‚àà‚Ñù^{n_2}$, and $\\tilde{f}_0(x_1)=\\inf_z\\{f_0(x_1,z)|g_j(z)‚â§0\\}$. Then\n",
        "  $\\BC\\t{minimize}&f_0(x_1,x_2)\\\\\n",
        "  \\t{subject to}&f_i(x_1)‚â§0\\\\\n",
        "  &g_j(x_2)‚â§0\\EC ‚áí \\BC\\t{minimize}&\\tilde{f}_0(x_1)\\\\\n",
        "  \\t{subject to}&f_i(x_1)‚â§0\\EC$\n",
        "\n",
        "- **Epigraph form**: $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&h_j(x)=0\n",
        "\\EC ‚áí\\BC\\t{minimize}&t\\\\\n",
        "\\t{subject to}&f_0(x)-t‚â§0\\\\\n",
        "&f_i(x)‚â§0\\\\\n",
        "&h_j(x)=0\\EC$\n",
        "\n",
        "- **Implicit constraints**: $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&h_j(x)=0\n",
        "\\EC ‚áí \\BC\\t{minimize}&F(x)\\EC$ where\n",
        "$F(x)=\\BC f_0(x)&f_i(x)‚â§0,h_j(x)=0\\\\\n",
        "‚àû&\\t{otherwise}\n",
        "\\EC$\n",
        "\n",
        "- 4.4 (Partial optimization): $\\BC\\t{minimize}&\\BM x_1\\\\x_2\\EM^‚ä§\\BM P_{11}&P_{12}\\\\P_{12}^‚ä§&P_{22}\\EM\\BM x_1\\\\x_2\\EM\\\\\n",
        "\\t{subject to}&f_i(x_1)‚â§0\\EC ‚áí \\BC\n",
        "\\t{minimize}&x_1^‚ä§(P_{11}-P_{12}P_{22}^{-1}P_{12}^‚ä§)x_1\\\\\n",
        "\\t{subject to}&f_i(x_1)‚â§0\\EC$.\n",
        "By 3.15 Schur Complement, $\\inf_{x_2}x_1^‚ä§P_{11}x_1+2x_1^‚ä§P_{12}x_2+x_2^‚ä§P_{22}x_2=x_1^‚ä§(P_{11}-P_{12}P_{22}^{-1}P_{12}^‚ä§)x_1$"
      ],
      "metadata": {
        "id": "yxjQWlkaPULn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convex optimization problem**\n",
        "$\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0,&i=1,...,m&f_i\\t{ convex}\\\\\n",
        "&a_j^‚ä§x=b_j,&j=1,...,p&h_j\\t{ affine}\\EC$\n",
        "\n",
        "- $\\m{D}=\\bigcap_i\\dom(f_i)$ is the intersection of $m$ sublevel sets and $p$ hyperplanes. $a_i\\neq0$ otherwise the problem is infeasible. Therefore convex optimization is minimizing convex objective function over a convex set.\n",
        "\n",
        "  - If $f_0$ is quasiconvex, the problem is quasiconvex optimization problem where its sublevel $œµ$-suboptimal set $f_0(x)‚â§p^*+œµ$ and optimal set are still convex.\n",
        "\n",
        "  - Strictly convex objective $f(Œ∏x+(1-Œ∏)y)< Œ∏f(x)+(1-Œ∏)f(y)$ optimal set contains at most 1 point.\n",
        "\n",
        "  - Any locally optimal point is also globally optimal.\n",
        "\n",
        "**Convex optimality criterion**: $f_0$ is differentiable and convex, then $\\red{f_0(y)‚â•f_0(x^*)‚áî‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0}$ at the optimal point $x^*$ for all other $y‚àà\\m{F}$.\n",
        "\n",
        "- This is used by solvers to decide whether convergence has been reached for differentiable objective. Proof:\n",
        "\n",
        "  - Show $‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0‚áíf_0(y)‚â•f_0(x^*)$. This is evident from the first-order convexity condition\n",
        "  $f_0(y)‚â•f_0(x^*)+‚àáf_0(x^*)^‚ä§(y-x^*)$.\n",
        "\n",
        "  - Show $f_0(y)‚â•f_0(x^*)‚áí‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0$. This relies on convexity of the feasible set. Let $z(t)=x^*+t(y-x^*)$ be a line segment in the feasible set between the optimal $x^*$ and any $y‚àà\\m{F}$, then\n",
        "  $f_0(x^*+t(y-x^*))‚â•f_0(x^*)\\ ‚àÄt‚àà[0,1]$\n",
        "  $‚áí\\lim\\limits_{t‚Üí0^+}\\/{f_0(x^*+t(y-x^*))-f_0(x^*)}{t}‚â•0$\n",
        "  $‚áí‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0$.\n",
        "\n",
        "- Intuition: $f_0(x)$ is a surface floating in the air. The feasible set and $‚àáf_0(x)$ are both parallel to the ground. $‚àáf_0$ points toward a direction that results in the steepest ascent in the floating surface.\n",
        "$‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0$ says moving away from $x^*$ toward any $y‚àà\\m{F}$ results in $f_0(y)‚â•f_0(x^*)$.\n",
        "It also says geometrically $‚àáf_0(x^*)$ and $y-x^*$ form an angle no more than 90¬∞.\n",
        "\n",
        "  - If $x^*$ is unconstrained, it sits at the bottom of the bowl where $‚àáf_0(x^*)=0$ and $‚àáf_0(x^*)^‚ä§(y-x)=0$. Suppose $x^*$ is on boundary,\n",
        "  then $‚àáf_0(x^*)$ points toward steepest ascent and into the feasible set thus $f_0(y)‚â•f_0(x^*)$ for all $y‚àà\\m{F}$.\n",
        "  The feasible set is convex thus $‚àáf_0(x^*)(y-x^*)>0$ with all $y‚àà\\m{F}$.\n",
        "  Finally $-‚àáf_0(x^*)\\neq0$ points away from feasible set and therefore is normal to the supporting hyperplane of the feasible set at $x^*$.\n",
        "\n",
        "- **Unconstrained** (sanity check): Let $y=x^*-t‚àáf_0(x^*),\\ t>0$ in a small step toward $-‚àáf_0(x^*)$.\n",
        "Then optimality $‚àáf_0(x^*)^‚ä§(y-x^*)‚â•0$\n",
        "$‚áí‚àáf_0(x^*)^‚ä§(-t‚àáf_0(x^*))$\n",
        "$=-t\\n{‚àáf_0(x^*)}_2^2‚â•0$\n",
        "$‚áí‚àáf_0(x^*)=0$, which agrees with unconstrained optimization.\n",
        "\n",
        "- **Equality only constraint**: $\\BC\n",
        "\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&\\green{Ax=b}\n",
        "\\EC$.\n",
        "Feasible set is affine $\\{x_0+\\null(A)\\}$ assuming $b‚àà\\span(A)$.\n",
        "Every $y‚àà\\m{F}$ satisfies $y=x^*+v$ for some $v‚àà\\null(A)$.\n",
        "Optimal point $x^*$ satisfies $\\green{‚àáf_0(x^*)^‚ä§v‚â•0\\ ‚àÄv‚àà\\null(A)}$.\n",
        "\n",
        "  - If linear function is nonnegative on a subspace then it must be zero on the subspace.\n",
        "  If $g(v)=c^‚ä§v‚â•0\\ ‚àÄv‚ààS$ then $g(v)=0\\ ‚àÄv‚ààS$, because if $v‚ààS$ then $-v‚ààS$.\n",
        "  \n",
        "  - Stationarity: $‚àáf_0(x^*)^‚ä§v‚â•0\\ ‚àÄv‚àà\\null(A)$\n",
        "  $‚áí‚àáf_0(x^*)^‚ä§v=0\\ ‚àÄv‚àà\\null(A)$\n",
        "  $‚áí‚àáf_0(x^*)\\perp\\null(A)$\n",
        "  $‚áí\\blue{‚àáf_0(x^*)‚àà\\null(A)^\\perp=\\span(A^‚ä§)}$\n",
        "  $‚áí‚àáf_0(x^*)=A^‚ä§Œª$ for some $Œª‚àà‚Ñù^p$.\n",
        "  Therefore\n",
        "  $‚àáf_0(x)+A^‚ä§Œª=0$.\n",
        "  \n",
        "- **Nonnegative orthant**: $\\BC\n",
        "\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&\\green{x‚âΩ0}\n",
        "\\EC$.\n",
        "Optimality $‚àáf_0(x^*)^‚ä§y-‚àáf_0(x^*)^‚ä§x^*‚â•0\\ ‚àÄy‚âΩ0$ has 2 requirements:\n",
        "\n",
        "  - Dual feasibility: $\\green{‚àáf_0(x^*)‚âΩ0}$ otherwise $‚àáf_0(x^*)^‚ä§y$ is unbounded below when $y‚âΩ0$ is arbitrarily large.\n",
        "  From stationarity\n",
        "  $‚àá\\m{L}=‚àáf_0(x^*)-‚àá(Œªx^*)=0$\n",
        "  $‚áíŒª=‚àáf_0(x^*)‚âΩ0$.\n",
        "\n",
        "  - Complementarity: $-‚àáf_0(x^*)^‚ä§x^*‚â•0$\n",
        "  $‚áí‚àáf_0(x^*)=0$ because $x^*‚âΩ0$.\n",
        "  Therefore $‚àáf_0(x^*)^‚ä§x^*=0$\n",
        "  $‚áí\\sum_i‚àáf_0(x^*)_ix^*_i=0$\n",
        "  $‚áí\\green{‚àáf_0(x^*)_ix^*_i=0\\ ‚àÄi}$.\n",
        "  The indices corresponding to nonzero components of $x^*$ and $‚àáf_0(x^*)$ are complements and have zero intersection: it's zero in either one or the other.\n",
        "\n",
        "**Convex equivalent problems**\n",
        "\n",
        "- Maximizing concave objective function with convex constraints is equivalent problem.\n",
        "\n",
        "- Some non-convex optimization problems are convex in disguise:\n",
        "$\\BC\\t{minimize}&f_0(x)=x_1^2+x_2^2\\\\\n",
        "\\t{subject to}&f_1(x)=x_1/(1+x_2^2)‚â§0\\\\\n",
        "&h_1(x)=(x_1+x_2)^2=0\\EC ‚áí\n",
        "\\BC\\t{minimize}&f_0(x)=x_1^2+x_2^2\\\\\n",
        "\\t{subject to}&\\tilde{f}_1(x)=x_1‚â§0&\\t{- denom is a positive scalar}\\\\\n",
        "&\\tilde{h}_1(x)=x_1+x_2=0&\\t{- square-root both sides}\n",
        "\\EC$\n",
        "\n",
        "- Eliminating equality:\n",
        "$\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&Ax=b \\EC ‚áí \\BC\\t{minimize}&f_0(Fz+x_0)\\\\\n",
        "\\t{subject to}&f_i(Fz+x_0)\n",
        "\\EC$\n",
        "\n",
        "- 4.5 (unconstrained): $f_0(x)=\\/{1}{2}x^‚ä§Px+q^‚ä§x+r$. $‚àáf_0(x)=Px+q=0$.\n",
        "\n",
        "  - If $q‚àâ\\span(P)$ then there is no solution and $f_0$ is unbounded below. This happens when $P$ is singular (has a 0 eigenvalue), and $f_0$ is linear along that eigenvector E.g., $f_0(x_1,x_2)=\\/{1}{2}x_1^2+x_2$ where $P=\\BM1&0\\\\0&0\\EM$ and $q=\\BM0\\\\1\\EM$.\n",
        "\n",
        "  - If $P‚âª0$ then $f_0$ is strictly convex, $P$ is invertible, and $x^*=-P^{-1}q$\n",
        "\n",
        "  - If $P$ is singular (non-invertible) but $q‚àà\\span(P)$, then $X_\\t{opt}=\\{-P^‚Ä†q+\\null(P)\\}$.\n",
        "\n",
        "- 4.6 **analytic centering**: $f_0(x)=-\\sum_i\\ln(b_i-a_i^‚ä§x)$ on $\\{x|Ax‚â∫b\\}$. Here $‚àáf_0=\\sum_i\\/{a_i}{b_i-a_i^‚ä§x}=0$.\n",
        "\n",
        "  - $b-Ax$ forms $m$ walls. $\\ln(b_i-a_i^‚ä§x)$ finds an analytic center point that simultaneously maximizes distance to all walls. $‚àáf_0‚àù\\/{1}{b_i-a_i^‚ä§x}$ represents the force of repulsion from each wall. At optimality, the pushing forces cancel out each other in equilibrium.\n",
        "\n",
        "  - If the space between the walls has an opening like letter 'C', then there's no solution to $‚àáf_0=0$ and $f_0$ is unbounded below. You can move infinitely through the opening to reduce $f_0$ against that one far wall.\n",
        "\n",
        "  - If the space is fully enclosed in a polygon (soccer ball), then there is exactly 1 analytic center as solution.\n",
        "\n",
        "  - If the walls form a tube/tunnel axis with 2 openings, then that has infinite solutions."
      ],
      "metadata": {
        "id": "8jaa28W7nnDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quasiconvex optimization**: $\\BC\\t{minimize}&f_0(x)&f_0\\t{ quasiconvex}\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0,&f_i\\t{ convex, }i=1,...,m\\\\\n",
        "&Ax=b&\\t{affine}\\EC$\n",
        "\n",
        "- Quasiconvex means unimodal but possibly with flat spots half-way down the valley. Locally optimal solutions are not guaranteed to be globally optimal.\n",
        "\n",
        "- **Quasiconvex optimality criterion**: $x^*‚àà\\m{F}$ is globally optimal if $\\red{‚àáf_0(x)^‚ä§(y-x^*)>0\\ ‚àÄy‚àà\\m{F}\\backslash\\{x\\}}$ as sufficient but not necessary for $x^*$ to be optimal.\n",
        "\n",
        "  - If you move to any other $y‚àà\\m{F}$ you immediately go strictly uphill, then that's sufficient to be global optimal. However there could be multiple points in $X_\\t{opt}$, therefore criterion not necessary for $x^*$ to be global optimal.\n",
        "\n",
        "  - Criterion requires $\\blue{‚àáf_0(x^*)\\neq0}$, which means this criterion is **designed for constrained boundaries**, and does not work for unconstrained minima where the bottom is flat.\n",
        "\n",
        "- **Equivalent convex constraint**: If $f_i(x)$ constraint is quasiconvex, use equivalent 0-sublevel convex constraint $œï_0(x)=\\t{dist}(x,\\{z|f_i(z)‚â§0\\})‚â§0$ is an all-purpose example, but often something simpler is used.\n",
        "\n",
        "- **Convex feasibility algorithm**: $x_{\\m{F}_t}=\\BC\\t{find}&x\\\\\n",
        "\\t{subject to}&œï_t(x)‚â§0\\\\\n",
        "&f_i(x)‚â§0\\\\\n",
        "&Ax=b\\EC$ where $\\BC\n",
        "œï_t(x)\\t{ is convex}\\\\\n",
        "œï_t(x)‚â§0‚áîf_0(x)‚â§t\\\\\n",
        "s‚â•t‚áîœï_s(x)‚â§œï_t(x)\\EC$ returns whether $\\m{F}_t$ is empty for a given $t$.\n",
        "Then $\\BC\n",
        "p^*‚â§t&\\t{if }\\m{F}_t\\neq‚àÖ\\\\\n",
        "p^*>t&\\t{if }\\m{F}_t=‚àÖ\n",
        "\\EC$. Binary search on $t$ to find the $œµ$-suboptimal value.\n"
      ],
      "metadata": {
        "id": "pw5hQ-0CpKc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Programming (LP)** $\\BC\\t{minimize}&c^‚ä§x+d&\\t{affine }f_0(x)\\\\\n",
        "\\t{subject to}&Gx‚âºh&\\t{affine }f_i(x)\\\\\n",
        "&Ax=b\\EC$\n",
        "where $d$ is redundant and often dropped.\n",
        "$\\m{F}$ is a polyhedron.\n",
        "LP is finding the lowest point on a tilted hyperplane in a polyhedron. The optimal point is on a boundary.\n",
        "\n",
        "- Two common forms: **standard form LP** $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax=b\\\\\n",
        "&x‚âΩ0\\EC$ and\n",
        "**inequality form LP** $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax‚âºb\n",
        "\\EC$\n",
        "\n",
        "  - Converting to standard form: use slack variables $s$ and use the **nonnegative variables trick** by splitting $x=x^+-x^-$ such that $x^+‚âΩ0$ and $x^-‚âΩ0$.\n",
        "  $\\BC\\t{minimize}&c^‚ä§x+d\\\\\n",
        "  \\t{subject to}&Gx‚âºh\\\\\n",
        "  &Ax=b\\EC ‚áí \\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&Gx+s=h\\\\\n",
        "  &Ax=b\\\\\n",
        "  &s‚âΩ0\\EC ‚áí \\BC\\t{minimize}&c^‚ä§x^+-c^‚ä§x^-\\\\\n",
        "  \\t{subject to}&Gx^+-Gx^-+s=h\\\\\n",
        "  &Ax^+-Ax^-=b\\\\\n",
        "  &x^+‚âΩ0,\\ x^-‚âΩ0,\\ s‚âΩ0\\EC$\n",
        "\n",
        "- **Diet problem**: A healthy diet contains at least $b_1,...,b_m$ amounts of $m$ nutrient types. One consumes $x_1,...,x_n$ units of $n$ food types. Each unit of food $j$ contains $a_{ij}$ amount of type $i$ nutrient, and costs $c_j$ amount.\n",
        "$\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Ax‚âΩb\\\\\n",
        "&x‚âΩ0\\EC$\n",
        "\n",
        "- **Chebyshev center of polyhedron**: largest Euclidean ball $\\{x_c+u|\\ \\n{u}_2‚â§r\\}$ to fit in polyhedron $\\{x‚àà‚Ñù^n|a_i^‚ä§x‚â§b_i,\\ i=1,...,m\\}$ has closest point to each wall given by\n",
        "$\\blue{x_c+r\\/{a_i}{\\n{a_i}_2}}$ satisfying\n",
        "$a_i^‚ä§(x_c+r\\/{a_i}{\\n{a_i}_2})‚â§b_i$\n",
        "$‚áía_i^‚ä§x_c+r\\n{a_i}_2‚â§b_i$.\n",
        "$\\BC\\t{maximize}&r\\\\\n",
        "\\t{subject to}&a_i^‚ä§x_c+r\\n{a_i}_2‚â§b_i,&i=1,...,m\n",
        "\\EC$.\n",
        "\n",
        "- **Activity planning**: There are $n$ activities over $N$ time periods.\n",
        "Activities consume and produce products in proportion to activity levels.\n",
        "Activity $j$ produces $a_{ij}$ and consumes $b_{ij}$ amounts of product $i$ per unit of activity level.\n",
        "$x_j(t)‚â•0,\\ t=1,...,N$ denotes the activity level of activity $j$ in period $t$.\n",
        "Initial products are provided in $g_0$ so that $\\BC\n",
        "Bx(1)‚âºg_0\\\\\n",
        "Bx(t+1)‚âºAx(t)\n",
        "\\EC$.\n",
        "Excess products not consumed are $\\BC\n",
        "s(0)=g_0-Bx(1)\\\\\n",
        "s(t)=Ax(t)-Bx(t+1)\\\\\n",
        "s(N)=Ax(N)\n",
        "\\EC$.\n",
        "Let $c$ be the values of products and $Œ≥$ be discount rate, then $\\BC\\t{maximize}&c^‚ä§s(0)+Œ≥c^‚ä§s(1)+...+Œ≥^NC^‚ä§s(N)\\\\\n",
        "\\t{subject to}&x(t)‚âΩ0,\\ s(t)‚âΩ0\\\\\n",
        "&s(0)=g_0-Bx(1),\\ s(t)=Ax(t)-Bx(t+1),\\ s(N)=Ax(N)\\EC$\n",
        "\n",
        "- **Chebyschev inequality**: $P(f(x)‚â•r)‚â§\\/{\\E[f(x)]}{r}$. Let discrete random variable $x$ have support $\\{u_1,...,u_n\\}‚äÜ‚Ñù$ with probability simplex $p‚àà‚Ñù^n$. Let $\\E[f_i(x)]=[f_i(x_j),\\ j=1,...,n]^‚ä§p$ be a function of prior $p$. We wish to find lowerbound on $\\E[f_0(x)]$ with optimization variable $p$.\n",
        "$\\BC\\t{minimize}&\\E[f_0(x)]\\\\\n",
        "\\t{subject to}&p‚âΩ0,\\ \\v{1}^‚ä§p=1\\\\\n",
        "&Œ±_i‚â§\\E[f_i(x)]‚â§Œ≤_i\n",
        "\\EC$\n",
        "\n",
        "- **Piecewise-minimization**: Consider piecewise linear convex function $f(x)=\\max_i(a_i^‚ä§x+b_i)$. Then the probem is in epigraph form\n",
        "$\\BC\\t{minimize}&t\\\\\n",
        "  \\t{subject to}&\\max_i(a_i^‚ä§x+b_i)‚â§t\n",
        "  \\EC ‚áí \\BC\\t{minimize}&t\\\\\n",
        "  \\t{subject to}&a_i^‚ä§x+b_i‚â§t\\EC$.\n",
        "\n",
        "**Linear fractional programming**: $f_0(x)=\\/{c^‚ä§x+d}{e^‚ä§x+f}$ on $\\dom(f_0)=\\{x|e^‚ä§x+f>0\\}$ is quasilinear.\n",
        "\n",
        "- Let $y=\\/{x}{e^‚ä§x=f}$ and $z=\\/{1}{e^‚ä§x=f}$ then $\\BC\\t{minimize}&\\/{c^‚ä§x+d}{e^‚ä§x+f}\\\\\n",
        "\\t{subject to}&Gx‚âºh\\\\\n",
        "&Ax=b\\\\\n",
        "&e^‚ä§x+f>0\n",
        "\\EC ‚áí \\BC\\t{minimize}&c^‚ä§y+dz\\\\\n",
        "\\t{subject to}&Gy-hz‚âº0\\\\\n",
        "&Ay-bz=0\\\\\n",
        "&e^‚ä§y+fz=1\\\\\n",
        "&z‚â•0\\EC$"
      ],
      "metadata": {
        "id": "fzFmAC1vpvTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quadratic programming (QP)** $\\BC\n",
        "\\t{minimize}&\\/{1}{2}x^‚ä§Px+q^‚ä§x+r,&\\t{convex quadratic }f_0(x)\\\\\n",
        "\\t{subject to}&Gx‚âºh,&\\t{affine }f_i(x)\\\\\n",
        "&Ax=b\\EC$\n",
        "where $\\m{F}$ is polyhedron like LP.\n",
        "If $f_i(x)$ are quadratic then it is quadratically constrained quadratic program (QCQP) where $\\m{F}$ is intersection of ellipsoids.\n",
        "\n",
        "- **Least squares**: $\\n{Ax-b}_2^2=x^‚ä§A^‚ä§Ax-2b^‚ä§Ax+b^‚ä§b$. Unconstrained least squares has analytical solution $x=A^‚Ä†b$ where $A^‚Ä†$ is pseudoinverse of $A$.\n",
        "\n",
        "- **Distance between polyhedra**: $\\m{P}_1=\\{x|A_1x‚âºb_1\\}$ and $\\m{P}_2=\\{x|A_2x‚âºb_2\\}$.\n",
        "Then $\\t{dist}(\\m{P}_1,\\m{P}_2)=\\inf\\{\\n{x_1-x_2}_2|x_1‚àà\\m{P}_1,x_2‚àà\\m{P}_2\\}$. Then\n",
        "$\\BC\n",
        "\\t{minimize}&\\n{x_1-x_2}_2^2\\\\\n",
        "\\t{subject to}&A_1x_1‚âºb_1,\\ A_2x_2‚âºb_2\n",
        "\\EC$\n",
        "\n",
        "- **Chebyschev inequality**: $P(|D|‚â•t)‚â§\\/{\\Var(D)}{t^2}$. Then $\\BC\\t{maximize}&[f_0(x_j)^2,\\ j=1,...,n]^‚ä§p-(\\E[f_0(x)])^2\\\\\n",
        "\\t{subject to}&p‚âΩ0,\\ \\v{1}^‚ä§p=1\\\\\n",
        "&Œ±_i‚â§\\E[f_i(x)]‚â§Œ≤_i\n",
        "\\EC$\n",
        "\n",
        "- **LP with stochastic objective**: Suppose LP with $f_0(x)=c^‚ä§x$ where $c‚àº(\\bar{c},Œ£)$ where $Œ£=\\E(c-\\bar{c})(c-\\bar{c})^‚ä§$. In trade off between minimizing expected cost vs cost variance, we minimize $\\E[c^‚ä§x]+Œ≥\\Var(c^‚ä§x)$ using $Œ≥$ risk aversion parameter\n",
        "$\\BC\\t{minimize}&\\bar{c}^‚ä§x+Œ≥x^‚ä§Œ£x\\\\\n",
        "\\t{subject to}&Gx‚âºh\\\\\n",
        "&Ax=b\\EC$\n",
        "\n",
        "- **Markowitz portfolio optimization**: Let $x‚àà‚Ñù^n$ be portfolio in dollars, and $p‚àº(\\bar{p},Œ£)$ be relative price change over a period. Then dollar return is $r=p^‚ä§x‚àº(\\bar{p}^‚ä§x,x^‚ä§Œ£x)$. Classical portfolio optimization\n",
        "$\\BC\\t{minimize}&x^‚ä§Œ£x\\\\\n",
        "\\t{subject to}&\\bar{p}^‚ä§x‚â•r_\\min\\\\\n",
        "&\\v{1}^‚ä§x=1,\\ x‚âΩ0\n",
        "\\EC$\n",
        "\n",
        "  - To allow shorting, use $x_\\t{long}‚âΩ0$, $x_\\t{short}‚âΩ0$, $x=x_\\t{long}-x_\\t{short}$, and $\\v{1}^‚ä§x_\\t{short}‚â§Œ∑\\v{1}^‚ä§x_\\t{long}$.\n",
        "\n",
        "  - To have transaction costs, let $u_\\t{buy}‚âΩ0$ and $u_\\t{sell}‚âΩ0$ be transactions in current period so that $x=x_0+u_\\t{buy}-u_\\t{sell}$. Let $f_\\t{buy}‚â•0$ and $f_\\t{sell}‚â•0$ be transaction fee rates, then total transation fees are $f_\\t{buy}u_\\t{buy}+f_\\t{sell}u_\\t{sell}$. If applying self-financing, then $(1-f_\\t{sell})\\v{1}^‚ä§u_\\t{sell}=(1+f_\\t{buy})\\v{1}^‚ä§u_\\t{buy}$.\n",
        "\n",
        "**Second-order cone program (SOCP)**: $\\BC\n",
        "\\t{minimize}&f^‚ä§x,&\\t{affine }f_0(x)\\\\\n",
        "\\t{subject to}&\\n{A_ix+b_i}_2‚â§c_i^‚ä§x+d_i,&i=1,...,m\\\\\n",
        "&Fx=g\n",
        "\\EC$\n",
        "where the second-order cone constraint $\\n{A_ix+b_i}_2‚â§c_i^‚ä§x+d_i$ fits $(Ax+b,c^‚ä§x+d)$ into a second-order cone $\\{(x,t)|\\n{x}_2‚â§t\\}$. If cone width $A=0$ then SOCP becomes LP. If cone height $c=0$ then SOCP becomes QCQP.\n",
        "\n",
        "- **Robust LP**: suppose $\\BC\n",
        "\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&a_i^‚ä§x‚â§b_i\n",
        "\\EC$ where $c$ and $b_i$ are fixed, but $a_i‚àà\\{\\bar{a}_i+P_iu|\\n{u}_2‚â§1\\}$ lies in an ellipsoid.\n",
        "Then $\\sup\\{a_i^‚ä§x|a_i‚àà\\{\\bar{a}_i+P_iu|\\n{u}_2‚â§1\\}\\}$\n",
        "$=\\bar{a}_i^‚ä§x+\\sup\\{(P_iu)^‚ä§x|\\n{u}_2‚â§1\\}$\n",
        "$=\\bar{a}_i^‚ä§x+\\sup\\{u^‚ä§P_i^‚ä§x|\\n{u}_2‚â§1\\}$\n",
        "$=\\bar{a}_i^‚ä§x+\\sup\\{\\n{u}_2\\n{P_i^‚ä§x}_2\\cos œâ|\\n{u}_2‚â§1\\}$\n",
        "$=\\bar{a}_i^‚ä§x+\\n{P_i^‚ä§x}_2$.\n",
        "Then express the problem as SOCP\n",
        "$\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&\\bar{a}_i^‚ä§x+\\n{P_i^‚ä§x}_2‚â§b_i\n",
        "\\EC$\n",
        "where $\\n{P_i^‚ä§x}_2$ is regularization that provides additional safety margin away from boundary.\n",
        "\n",
        "- **LP with stochastic constraints**: In robust LP, suppose $a_i‚àº\\Normal(\\bar{a}_i,Œ£_i)$ and $a_i^‚ä§x‚àº\\Normal(\\bar{a}_i^‚ä§x,x^‚ä§Œ£_ix)$ where\n",
        "$\\blue{x^‚ä§Œ£_ix=x^‚ä§Œ£_i^{1/2}Œ£_i^{1/2}x=\\n{Œ£_i^{1/2}x}_2^2}$ for Cholesky decomposition $Œ£_i^{1/2}$, then the $m$ constraints become\n",
        "$P(a_i^‚ä§x‚â§b_i)‚â•Œ∑$\n",
        "$‚áíŒ¶(\\/{b_i-\\bar{a}_i^‚ä§x}{\\n{Œ£_i^{1/2}x}_2})‚â•Œ∑$\n",
        "$‚áí\\bar{a}_i^‚ä§x+Œ¶^{-1}(Œ∑)\\n{Œ£_i^{1/2}x}_2‚â§b_i$.\n",
        "$\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&P(a_i^‚ä§x‚â§b_i)‚â•Œ∑\n",
        "\\EC ‚áí \\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&\\bar{a}_i^‚ä§x+Œ¶^{-1}(Œ∑)\\n{Œ£_i^{1/2}x}_2‚â§b_i\n",
        "\\EC$\n",
        "\n",
        "- 4.8: **Markowitz with loss risk**: $p‚àº\\Normal(\\bar{p},Œ£)$ and $r‚àº\\Normal(\\bar{p}^‚ä§x,x^‚ä§Œ£x)$. Then loss risk constraint $P(r‚â§Œ±)‚â§Œ≤‚áí\\bar{p}^‚ä§x+Œ¶^{-1}(Œ≤)\\n{Œ£^{1/2}x}_2‚â•Œ±$. $\\BC\\t{minimize}&\\bar{p}^‚ä§x\\\\\n",
        "\\t{subject to}&\\bar{p}^‚ä§x+Œ¶^{-1}(Œ≤)\\n{Œ£^{1/2}x}_2‚â•Œ±\\\\\n",
        "&x‚âΩ0,\\ \\v{1}^‚ä§x=1\n",
        "\\EC$"
      ],
      "metadata": {
        "id": "SdCfpBpe_sLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geometric programming (GP)**: $\\BC\\t{minimize}&f_0(x)&\\t{posynomial }f_0\\\\\n",
        "\\t{subject to}&f_i(x)‚â§1&\\t{posynomial }f_i\\\\\n",
        "&h_i(x)=1&\\t{monomial }h_i\n",
        "\\EC$ on domain $\\m{D}=‚Ñù_{++}^n$.\n",
        "\n",
        "- Monomial $f(x)=cx_1^{a_1}...x_n^{a_n},\\ c>0,\\ a_i‚àà‚Ñù$ is closed under multiplication and addition, and posynomial $f(x)=\\sum_kc_kx_1^{a_{1k}}...x_n^{a_{nk}}$ is closed under addition, multiplication, and nonnegative scaling.\n",
        "\n",
        "  - $f(x)=cx_1^{a_1}...c_n^{a_n}$\n",
        "  $=f(e^y)=c({e^{y_1}})^{a_1}...({e^{y_n}})^{a_n}$\n",
        "  $=e^{a^‚ä§y+\\ln(c)}$ where $y_i=\\ln(x_i)$\n",
        "\n",
        "  - $f(x)=\\sum_kc_kx_1^{a_{1k}}...x_n^{a_{nk}}$\n",
        "  $=\\sum_kc_k({e^{y_1}})^{a_{1k}}...({e^{y_n}})^{a_{nk}}$\n",
        "  $=\\sum_ke^{a_k^‚ä§y+\\ln(c_k)}$ where $a_k=(a_{1k},...,a_{nk})$\n",
        "\n",
        "- Let $\\tilde{f}_i(y)=\\ln(f_i(x))$ be convex log-sum-exp and\n",
        "$\\tilde{h}_j(y)=\\ln(h_j(x))$ be affine then GP can be re-expressed in convex form\n",
        "$\\BC\\t{minimize}&\\tilde{f}_0(y)\\\\\n",
        "\\t{subject to}&\\tilde{f}_i(y)‚â§0\\\\\n",
        "&\\tilde{h}_j(y)=0\\EC$.\n",
        "\n",
        "- **Frobenius norm scaling**: Consider $y=Mu$, but we scale the coordinates by diagonal matrix $D$ so that $\\tilde{u}=Du$ and $\\tilde{y}=Dy$. Then $\\tilde{y}=DMD^{-1}\\tilde{u}$. We want the scaling done so $DMD^{-1}$ is small, measured with Frobenius norm. Then\n",
        "$\\n{DMD^{-1}}_F^2=\\sum_{ij}(DMD^{-1})_{ij}^2$\n",
        "$=\\sum_{ij}M_{ij}^2d_i^2/d_j^2$, which is a GP problem."
      ],
      "metadata": {
        "id": "WVKpVUc_5wfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generalized inequality constraints**: $\\BC\\t{minimize}&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚âº_{K_i}0&f_i\\t{ is }K_i\\t{-convex}\\\\\n",
        "&Ax=b\\EC$ - vector-valued constraint functions\n",
        "\n",
        "- **Conic programming**: $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&Fx+g‚âº_K0\\\\\n",
        "&Ax=b\\EC$.\n",
        "\n",
        "  - Standard form $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&x‚âΩ_K0\\\\\n",
        "  &Ax=b\\EC$.\n",
        "  Inequality form $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&Fx+g‚âº_K0\\EC$.\n",
        "\n",
        "- **Semidefinite programming (SDP)**: $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "\\t{subject to}&x_1F_1+...+x_nF_n+G‚âº0,&\\t{LMI; }G,F_i‚àà\\v{S}^k\\\\\n",
        "&Ax=b,&A‚àà‚Ñù^{p√ón}\\EC$.\n",
        "\n",
        "  - If $G,F_i$ are diagonal then SDP reduces to LP.\n",
        "\n",
        "  - Standard form $\\BC\\t{minimize}&\\tr(CX)\\\\\n",
        "  \\t{subject to}&\\tr(A_iX)=b_i\\\\\n",
        "  &X‚âΩ0\n",
        "  \\EC$.\n",
        "  Inequality form $\\BC\\t{minimize}&c^‚ä§x\\\\\n",
        "  \\t{subject to}&x_1A_1+...+x_nA_n‚âºB\n",
        "  \\EC$.\n"
      ],
      "metadata": {
        "id": "N8ldZVf2LA1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector optimization**: $\\BC\\t{minimize wrt }K&f_0(x),&f_0:‚Ñù^n‚Üí‚Ñù^q\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&h_i(x)=0\n",
        "\\EC$ - vector-valued objective function\n",
        "\n",
        "- Minimize wrt proper cone $K$: if $K=‚Ñù_+^2$ then $f_0(x)$ is a vector. If $K=\\v{S}_+^n$ then $f_0(x)$ is a matrix.\n",
        "\n",
        "- **Achievable objective values** $\\m{O}=\\{f_0(x)|‚àÉx‚àà\\m{F}\\}‚äÜ‚Ñù^q$ are objective values of feasible points. There is $x^*‚àà\\m{F}$ such that\n",
        "$f_0(x^*)‚âº_Kf_0(y)\\ ‚àÄy‚àà\\m{F}$\n",
        "$‚áíf_0(y)-f_0(x^*)‚ààK$\n",
        "$‚áíf_0(y)‚ààf_0(x^*)+K\\ ‚àÄf_0(y)‚àà\\m{O}$\n",
        "$‚áí\\m{O}‚äÜf_0(x^*)+K$\n",
        "\n",
        "- **Pareto Optimality**: $\\m{O}$ does not have a minimum, but does have minimal elements.\n",
        "  \n",
        "  - Point $x‚àà\\m{F}$ is Pareto optimal if $f_0(x)$ is a minimal element of $\\m{O}$.\n",
        "  \n",
        "  - Point $x‚àà\\m{F}$ is Pareto optimal if $‚àÄy‚àà\\m{F}$ satisfying $f_0(y)‚âº_Kf_0(x)$ implies $f_0(y)=f_0(x)$. Any $y$ better than or equal to $x$ has the same objective value as $x$.\n",
        "\n",
        "  - Point $x‚àà\\m{F}$ is Pareto optimal iff $(f_0(x)-K)‚à©\\m{O}=\\{f_0(x)\\}$.\n",
        "\n",
        "  - Pareto optimal values set $\\m{P}$ satisfies: $\\red{\\m{P}‚äÜ\\m{O}‚à©\\t{boundary}(\\m{O})}$\n",
        "\n",
        "- **Scalarization**:\n",
        "$\\BC\\t{minimize wrt }K&f_0(x)\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&h_i(x)=0\n",
        "\\EC ‚áí \\BC\\t{minimize}&\\red{Œª^‚ä§f_0(x)},&\\red{Œª‚âª_{K^*}0}\\\\\n",
        "\\t{subject to}&f_i(x)‚â§0\\\\\n",
        "&h_i(x)=0\\EC$.\n",
        "Then optimal point $x^*$ in the ordinary scalar problem is Pareto optimal in the vector problem. Here $Œª$ are called **weights**.\n",
        "\n",
        "  1. **Pick a $Œª‚àà\\t{int}(K^*)$ and minimize $Œª^‚ä§f_0(x)$.** Minimizing $Œª^‚ä§f_0(x)$ is finding the supporting hyperplane of $\\m{O}$ with normal $Œª$.\n",
        "  Point $x$ is optimal iff $Œª^‚ä§f_0(y)‚â•Œª^‚ä§f_0(x)‚áíŒª^‚ä§(f_0(y)-f_0(x))‚â•0\\ ‚àÄy‚àà\\m{F}$\n",
        "  $‚áí\\blue{Œª^‚ä§(u-f(x))‚â•0\\ ‚àÄu‚àà\\m{O}}$, a hyperplane.\n",
        "\n",
        "  - If $Œª‚àà\\t{boundary}(K^*)$ is used instead of $Œª‚àà\\t{int}(K^*)$, then $Œª^‚ä§v=0$ for some $v‚àà\\t{boundary}(K)$.\n",
        "  Suppose $f_0(x)‚àà\\m{O}$ and $f_0(y)=f_0(x)-v‚àà\\m{O}$ such that $f_0(y)< f_0(x)$.\n",
        "  Then\n",
        "  $Œª^‚ä§f_0(y)=Œª^‚ä§(f_0(x)-v)$\n",
        "  $=Œª^‚ä§f_0(x)$ and the solver stops at\n",
        "  $f_0(x)$, which is not a true Pareto optimal point but is at a flat horizontal or vertical edge of $\\m{O}$.\n",
        "  Using $Œª‚àà\\t{int}(K^*)$ ensures $v‚àà\\t{boundary}(K)$ does not stop the solver.\n",
        "\n",
        "  2. **Iterate $Œª$ and collect minimal $f_0(x)$ to build the Pareto frontier $\\m{P}$**. All elements in $\\t{boundary}(\\m{O})‚à©\\t{conv}(\\m{O})$ along the lower left side is minimal $f_0(x)\\ ‚àÄx‚àà\\m{F}$. This boundary is the scalarizable part of Pareto frontier $\\m{P}$. Pareto optimal solutions in concave dips are not reachable by scalarization.\n",
        "\n",
        "- **Scalarization of convex problems**: Let $f_0(x)$ be convex, then $Œª^‚ä§x$ is an affine transformation and is convex. Furthermore $\\m{D}$ and $\\m{A}=\\m{O}+‚Ñù_+^q$ ($\\m{O}$ plus everything worse) is a convex set. This guarantees no concave dips on the lower left boundary and scalarization finds the entire Pareto frontier.\n",
        "\n",
        "  - Proof: $\\epi(f_0)=\\{(x,t)‚àà‚Ñù^n√ó‚Ñù^q|x‚àà\\m{D},f_0(x)‚âºt\\}$.\n",
        "  Then\n",
        "  $\\m{A}=\\{t‚àà‚Ñù^q|‚àÉx‚àà\\m{D}\\t{ such that }f_0(x)‚âºt\\}$\n",
        "  $=\\{t|(x,t)‚àà\\epi(f_0)\\}$\n",
        "  is the projection of the epigraph onto the objective space of $f_0(x)$, which is an affine mapping whose image preserves convexity.\n",
        "  I.e., $\\m{A}$ is the projection (shadow) of $\\epi(f_0)$ onto a lower dimension and therefore preserves convexity of $f_0$.\n",
        "\n",
        "  - After scalarizing with $Œª‚âª_{K^*}0$ to find Pareto frontier, we then scalarize $Œª‚âΩ_{K^*}0$ to find end-points. These results we have to check they are minimal.\n",
        "\n",
        "- **Multicriterion optimization**: When $K=‚Ñù_+^q$ it is a multicriterion problem. Let $F_1,...,F_q$ be scalar components of $f_0$, where each scalar component is a different objective. Problem is convex if $F_1,...,F_q$ are convex, $f_i$ are convex, and $h_i$ are affine. $x‚àà\\m{F}$ dominates $y‚àà\\m{F}$ if $F_i(x)‚â§F_i(y)\\ ‚àÄi$ and $‚àÉj:F_j(x)< F_j(y)$. Point $x$ is Pareto optimal if no feasible point dominates it.\n",
        "\n",
        "  - When an optimum point $x^*$ exists then it minimizes every objective simultaneously $F_i(x^*)‚â§F_i(y)\\ ‚àÄy,i$. When $x^*$ exists then the objectives are noncompeting. Otherwise trade-offs have to be made with weights.\n",
        "\n",
        "  - $Œª^‚ä§f_0(x)=\\sum_{i=1}^qŒª_iF_i(x)$ where $Œª‚âª0$ are the weights attached to the objectives. $Œª_i/Œª_j$ should be large if we care more about wanting $F_i$ to be small than $F_j$.\n",
        "\n",
        "- **Regularized least-squares**: We optimize for two objectives $\\BC\n",
        "F_1(x)=\\n{Ax-b}_2^2&\\t{goodness of fit} \\\\\n",
        "F_2(x)=\\n{x}_2^2&\\t{minimal size}\\EC$\n",
        "using constrained\n",
        "$\\BC\n",
        "\\t{minimize wrt }‚Ñù_+^2&f_0(x)=(F_1(x),F_2(x))\n",
        "\\EC$\n",
        "$‚áí\\BC\n",
        "\\t{minimize}&Œª^‚ä§f_0(x)\n",
        "\\EC$.\n",
        "Then\n",
        "$Œª^‚ä§f_0(x)$\n",
        "$=Œª_1F_1(x)+Œª_2F_2(x)$\n",
        "$=Œª_1\\n{Ax-b}_2^2+Œª_2\\n{x}_2^2$.\n",
        "$‚àá_x=2Œª_1(Ax-b)^‚ä§A+2Œª_2x^‚ä§$\n",
        "$=2Œª_1x^‚ä§A^‚ä§A-2Œª_1b^‚ä§A+2Œª_2x^‚ä§$\n",
        "$=2x^‚ä§(Œª_1A^‚ä§A+Œª_2I)-2Œª_1b^‚ä§A=0$\n",
        "$‚áí\\blue{x=(A^‚ä§A+\\/{Œª_2}{Œª_1}I)^{-1}A^‚ä§b}$,\n",
        "which produces all Pareto optimal points except two associated with $\\/{Œª_2}{Œª_1}‚Üí‚àû$ and $\\/{Œª_2}{Œª_1}‚Üí0$, which are produced by $Œª‚âΩ_{‚Ñù_+^2}0$, namely $Œª=(0,1)$ and $Œª=(1,0)$.\n",
        "\n",
        "- **Markowitz portfolio risk-return trade-off**: $p‚àº(\\bar{p},Œ£)$, dollar $r=p^‚ä§x$. We want to maximize returns and minimize variance\n",
        "$\\BC\n",
        "\\t{minimize wrt }‚Ñù_+^2&(F_1(x),F_2(x))=(-\\bar{p}^‚ä§x,x^‚ä§Œ£x)\\\\\n",
        "\\t{subject to}&\\v{1}^‚ä§x=1,\\ x‚âΩ0\n",
        "\\EC ‚áí \\BC\n",
        "\\t{minimize}&-\\bar{p}^‚ä§x+Œºx^‚ä§Œ£x&Œª_1=1,\\ Œª_2=Œº\\\\\n",
        "\\t{subject to}&\\v{1}^‚ä§x=1,\\ x‚âΩ0\n",
        "\\EC$\n",
        "\n",
        "- 4.9 **BLUE**: $y=Ax+v$ where $A‚àà‚Ñù^{m√ón}$ has rank $n$. Assume $v‚àº(0,I)$.\n",
        "We want to estimate $x$ given $y$, then linear estimator of $x$ has form $\\hat{x}=Fy$ where $FA=I$ and $\\E[\\hat{x}]=x$.\n",
        "Then $x=F(y-v)$, $\\hat{x}-x=Fv$, and\n",
        "$\\E[(\\hat{x}-x)(\\hat{x}-x)^‚ä§]$\n",
        "$=\\E[Fvv^‚ä§F^‚ä§]$\n",
        "$=FF^‚ä§$.\n",
        "Then the BLUE estimator is $\\BC\\t{minimize wrt }\\v{S}_+^n&FF^‚ä§\\\\\n",
        "\\t{subject to}&FA=I\\EC$.\n",
        "Here $FF^‚ä§$ is convex because $v^‚ä§FF^‚ä§v=\\n{F^‚ä§v}_2^2$ is convex for all $v$.\n",
        "\n",
        "  - Solution is the least-squares estimator $F^*=A^‚Ä†=(A^‚ä§A)^{-1}A^‚ä§$ and $F^*F^{*‚ä§}=(A^‚ä§A)^{-1}$\n",
        "\n",
        "- 4.10 (minimal upperbound): $\\BC\\t{minimize wrt }\\v{S}_+^n&X\\\\\n",
        "\\t{subject to}&X‚âΩA_i\\EC$. Choose $W‚àà\\v{S}_{++}^n$ and run $\\BC\\t{minimize}&\\tr(WX)\\\\\n",
        "\\t{subject to}&X‚âΩA_i\\EC$,\n",
        "which is standard form SDP. Geometrically each $A_i$ is an ellipsoid, and the Pareto frontier is the set of envelope ellipsoids."
      ],
      "metadata": {
        "id": "9Nik1ilE3_gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 4.1: $\\BC\\t{minimize}&f_0(x_1,x_2)\\\\\n",
        "\\t{subject to}&2x_1+x_2‚â•1\\\\\n",
        "&x_1+3x_2‚â•1\\\\\n",
        "&x_1‚â•0,\\ x_2‚â•0\\EC$.\n",
        "$\\m{F}$ is above the line $2x_1+x_2=1$ through $(1/2,0)$ and $(0,1)$ and above the line $x_1+3x_2=1$ through $(1,0)$ and $(0,1/3)$ in $‚Ñù_+^2$.\n",
        "\n",
        "  - $f_0(x_1,x_2)=x_1+x_2$. LP, the optimal point is at the intersection of the two lines, which offers lowest point along the diagonal.\n",
        "  $\\/{1}{2}-\\/{x_2}{2}=1-3x_2$\n",
        "  $‚áíx_2=\\/{1}{5}$ and\n",
        "  $x_1=\\/{2}{5}$.\n",
        "\n",
        "  - $f_0(x_1,x_2)=-x_1-x_2$. LP, the problem is unbounded below.\n",
        "\n",
        "  - $f_0(x_1,x_2)=x_1$. LP, the optimal point is the ray $(0,1)+s(0,1)\\ ‚àÄs‚àà‚Ñù_+$\n",
        "\n",
        "  - $f_0(x_1,x_2)=\\max\\{x_1,x_2\\}$.\n",
        "  Then $\\min\\max\\{x_1,x_2\\}$ occurs at $x_1=x_2$.\n",
        "\n",
        "  - $f_0(x_1,x_2)=x_1^2+9x_2^2$. QP."
      ],
      "metadata": {
        "id": "2sKdlxC2QkEs"
      }
    }
  ]
}