{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xpN7g4G_GqOC",
        "BNbBm-jVTDZT",
        "RwPkwYSspGCx",
        "wOFd_NGgMHGb",
        "09BvKQcdHIkJ",
        "KTXGM8LqQ5rJ",
        "oLo9cgDDWnqt",
        "RHhB4IDEOtOh",
        "pO_So0ejnd3T",
        "uVbeIY9o3e-s",
        "oz31vvbrttOC",
        "hQzNpIpI5qOR",
        "Qeo4CDIGAeja",
        "3RL9B5Gq9mQQ",
        "jbbaQ6_8c8WA",
        "-z6CpbDWMWwS",
        "HBJcCN56Rak2"
      ],
      "authorship_tag": "ABX9TyNWgntE/uCMHrOaXm7vifat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/realdope/math/blob/main/stats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\require{color}\n",
        "\\newcommand{\\arr}[1]{\\overset{\\small #1}{‚Üí}}\n",
        "\\newcommand{\\larr}[1]{\\overset{\\small #1}{‚ü∂}}\n",
        "\\newcommand{\\imply}[1]{\\overset{\\small #1}{‚áí}}\n",
        "\\newcommand{\\red}[1]{\\color{OrangeRed}{#1}}\n",
        "\\newcommand{\\blue}[1]{\\color{RoyalBlue}{#1}}\n",
        "\\newcommand{\\green}[1]{\\color{ForestGreen}{#1}}\n",
        "\\newcommand{\\purple}[1]{\\color{DarkMagenta}{#1}}\n",
        "\\newcommand{\\lgray}[1]{\\color{lightgray}{#1}}\n",
        "\\newcommand{\\gray}[1]{\\color{gray}{#1}}\n",
        "\\newcommand{\\dgray}[1]{\\color{darkgray}{#1}}\n",
        "\\newcommand{\\E}{\\text{ùîº}}\n",
        "\\newcommand{\\Var}{\\small\\text{Var}\\normalsize}\n",
        "\\newcommand{\\Cov}{\\small\\text{Cov}\\normalsize}\n",
        "\\newcommand{\\Corr}{\\small\\text{Corr}\\normalsize}\n",
        "\\newcommand{\\Bias}{\\small\\text{Bias}\\normalsize}\n",
        "\\newcommand{\\ob}[2]{\\overbrace{#1}^{\\small#2}}\n",
        "\\newcommand{\\ub}[2]{\\underbrace{#1}_{\\small#2}}\n",
        "\\newcommand{\\m}[1]{\\mathcal{#1}}\n",
        "\\newcommand{\\e}[1]{\\small{\\exp}\\normalsize\\left\\{#1\\right\\}}\n",
        "\\newcommand{\\t}[1]{\\text{#1}}\n",
        "\\newcommand{\\/}[2]{\\frac{#1}{#2}}\n",
        "\\newcommand{\\v}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\BPM}{\\small\\begin{pmatrix}}\n",
        "\\newcommand{\\EPM}{\\end{pmatrix}\\normalsize}\n",
        "\\newcommand{\\BVM}{\\begin{vmatrix}}\n",
        "\\newcommand{\\EVM}{\\end{vmatrix}\\normalsize}\n",
        "\\newcommand{\\BM}{\\small\\left[\\begin{smallmatrix}}\n",
        "\\newcommand{\\EM}{\\end{smallmatrix}\\right]\\normalsize}\n",
        "\\newcommand{\\BC}{\\small\\begin{cases}}\n",
        "\\newcommand{\\EC}{\\end{cases}\\normalsize}\n",
        "\\newcommand{\\norm}[1]{\\lVert#1\\rVert}\n",
        "\\newcommand{\\Normal}{\\mathcal{N}}\n",
        "\\newcommand{\\DUnif}{\\text{DUnif}}\n",
        "\\newcommand{\\Expo}{\\text{Expo}}\n",
        "\\newcommand{\\Unif}{\\text{Unif}}\n",
        "\\newcommand{\\Binom}{\\text{Binom}}\n",
        "\\newcommand{\\Geom}{\\text{Geom}}\n",
        "\\newcommand{\\HGeom}{\\text{HGeom}}\n",
        "\\newcommand{\\QGeom}{\\text{QGeom}}\n",
        "\\newcommand{\\Pois}{\\text{Pois}}\n",
        "\\newcommand{\\Gamma}{\\text{Gamma}}\n",
        "\\newcommand{\\Beta}{\\text{Beta}}\n",
        "\\newcommand{\\Mult}{\\text{Mult}}\n",
        "\\newcommand{\\liml}{\\lim\\limits}\n",
        "\\newcommand{\\suml}{\\sum\\limits}\n",
        "\\newcommand{\\prodl}{\\prod\\limits}\n",
        "$$\n",
        "# Probability and counting\n",
        "- Taylor series:\n",
        "  - $e^x=1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+...+\\frac{x^n}{n!}+...$\n",
        "  - $e^{-x}=1-x+\\frac{x^2}{2!}-\\frac{x^3}{3!}+...+\\frac{(-x)^n}{n!}+...$\n",
        "  - $e^{-1}=1-1+\\frac{1}{2!}-\\frac{1}{3!}+...+\\frac{(-1)^n}{n!}+...$"
      ],
      "metadata": {
        "id": "xpN7g4G_GqOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiplication rule**: an experiment consisting of sub-experiments A with $a$ possible outcomes and B with $b$ possible outcomes have $ab$ total possible outcomes.\n",
        "- Naive probability is about counting equally likely paths in an outcome tree. Some outcomes are compounded and are not equally likely: e.g., throwing 2 dice resulting in 3 vs 6 are not equally likely.\n",
        "\n",
        "**Definition (permutation n!)** counts the number of *ordered* arrangements of $n$ distinct elements without replacement. *Ordered* means two arrangements whose only difference is reordering of elements are considered different events.\n",
        "- $\\frac{n!}{(n-k)!}$ counts the number of size-$k$ ordered arrangements of $n$ distinct elements without replacement.\n",
        "- $n^k$ counts the number of size-$k$ ordered arrangements of $n$ distinct elements with replacement. With replacement means any element of $n$ can be repeatedly included in the $k$-selection.\n",
        "- Stirling approximation: $n!‚âà\\sqrt{2œÄn}\\left(\\frac{n}{e}\\right)^n$\n",
        "- ``` math.factorial(n) ```\n",
        "\n",
        "**Definition (binomial coefficient $n\\choose{k}$)** counts the number of size-$k$ subsets of $n$ distinct elements without replacement. Subsets are by definition *unordered*. $\\binom{n}{k}=\\frac{n!}{(n-k)!k!}$ arrangements of $k$ elements chosen from $n$ adjusted for overcounting from ordering.\n",
        "- $2^n$ counts the number of subsets of $n$ distinct elements without replacement.\n",
        "- $k^n$ counts the number of ways to fully distribute $n$ distinct elements into $k$ distinct variable size subsets.\n",
        "- **Multinomial coefficients** $\\binom{n}{n_1,n_2,...,n_r}=\\binom{n}{n_1}\\binom{n-n_1}{n_2}...\\binom{n-n_1-...-n_{r-1}}{n_r}=\\frac{n!}{n_1!n_2!...n_r!}$ counts the number of ways to fully distribute $n$ distinct elements into $r$ labeled/distinct subsets of fixed nonnegative sizes $n_1+n_2+...+n_r=n$. For indistinguishable subsets, $\\frac{n!}{n_1!n_2!...n_r!r!}$ to remove subset ordering in the outcome tree.\n",
        "  - (Ross Ch1 e5d) Grouping 2n people into n unordered pairs. During the first round of a knockout tournament for 2n players there are $\\frac{(2n)!}{n!2^n}$ possible arrangements for the first round, and $\\frac{(2n)!2^n}{n!2^n}=\\frac{(2n)!}{n!}$ possible win/loss outcomes. $\\frac{1}{n!}\\binom{2n}{2}\\binom{2n-2}{2}...\\binom{2}{2}=\\frac{(2n)!}{n!2^n}=(2n-1)(2n-3)...(3)(1)$\n",
        "  - E.g., Round robin players A, B, C, D would play $\\binom{4}{2}=6$ total games: (A,B), (A,C), (A,D), (B,C), (B,D), (C,D). During the first round, there are $\\frac{4!}{2!2^2}=3$ ways to match up: {(A,B),(C,D)}, {(B,C),(A,D)}, {(A,C)(B,D)}.\n",
        "- **Bose-Einstein** ${k+n-1}\\choose{k}$ counts the number of size-$k$ indistinct subsets of $n$ distinct elements with replacement. Distribute $k$ indistinct selections into $n$ distinct bins. The approach is to use the \"stars and bars setup\" where there are $n-1$ bars separating $n$ labeled bins where $k$ stars are placed, and we're allocating $n+k-1$ slots with stars and bars.\n",
        "  - how many ways are there to choose $k$ times from a set of $n$ objects with replacement, if order doesns't matter? We only care about how many times each object was chosen, not the order in which they were chosen?\n",
        "  - Bose-Einstein mostly should not be used in probability counting. A survey of sample size $k$ collected from a population $n$ has $n^k$ equally likely ordered samples, but $\\binom{n+k-1}{k}$ unordered samples are *not equally likely* and therefore cannot be used in probability.\n",
        "- **Integer solutions** $\\binom{n-1}{r-1}$ counts the number of ways to fully distribute $n$ indistinct elements into $r$ distinct nonzero variable size subsets; while $\\binom{n+r-1}{r-1}$ counts nonnegative subsets that can be 0. To count $(x_1 x_2 ... x_r)$ such that $x_1+x_2+..+x_r=n$ and $x_i>0$, use the stars and bars setup with $n-1$ possible positions between $n$ stars to insert $r-1$ bars. To count $x_i‚â•0$, use the stars and bars setup with $n+r-1$ possible slots to place $r-1$ bars and $n$ stars.\n",
        "- ``` math.comb(n, r) ```\n"
      ],
      "metadata": {
        "id": "sMg456xe6zHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition** A probability space consists of a sample space S and a probability function P which takes an event A ‚äÜ S as input and returns P(A) as a real number between 0 and 1 as output. Function P satisfies requirements:\n",
        "1. $P(‚àÖ)=0, P(S)=1$\n",
        "2. $0‚â§P(E)‚â§1$\n",
        "3. If $A_1, A_2, ...$ are disjoint / mutually exclusive ($A_i‚à©A_j=‚àÖ$) events, then\n",
        "$$\\small\n",
        "P\\left(\\bigcup_{j=1}^{‚àû}A_j\\right)=\\sum_{j=1}^{‚àû}P(A_j)$$\n",
        "- The sample space is the set of all possible outcomes. An event is a subset of possible outcomes in the sample space. The intersection $A‚à©B$ refers to outcomes shared in both events $A$ and $B$.\n",
        "- $P(A^C)=1-P(A)$.\n",
        "- Distributive: $(E‚à™F)‚à©G=(E‚à©G)‚à™(F‚à©G)$ and $(E‚à©F)‚à™G=(E‚à™G)‚à©(F‚à™G)$\n",
        "- De Morgan's: $(E‚à©F‚à©G)^C=E^C‚à™F^C‚à™G^C$ and $(E‚à™F‚à™G)^C=E^C‚à©F^C‚à©G^C$"
      ],
      "metadata": {
        "id": "wvDK7WRTf3S6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theorem (inclusion-exclusion)**: for any events $A_1,...,A_n$,\n",
        "$$\\small\n",
        "P\\left(\\bigcup_{j=1}^{‚àû}A_j\\right)=\\sum_{i}P(A_i)-\\sum_{i<j}P(A_i‚à©A_j)+\\sum_{i<j<k}P(A_i‚à©A_j‚à©A_k)-...+(-1)^{n+1}P(A_1‚à©...‚à©A_n)$$\n",
        "- $P(A‚à™B)=P(A)+P(B)-P(A‚à©B)$\n",
        "- **Boole/Bonferroni's inequality**: $P(A_1‚à™...‚à™A_n)‚â§P(A_1)+...+P(A_n)$\n",
        "- **Derangement**: $!n=n!\\suml_{k=0}^{n}\\frac{(-1)^k}{k!}$ counts the permutations of $n$ elements where no element appears in their original position. Given a permutation of $n$ elements, probability that all elements are out of position is given by\n",
        "  $$\\small\\frac{!n}{n!}=1-1+\\frac{1}{2!}-\\frac{1}{3!}+...+\\frac{(-1)^n}{n!}=\\suml_{k=0}^{n}\\frac{(-1)^k}{k!}‚Üíe^{-1}$$\n",
        "  - Let $A$ be the event that at least 1 element is in its ordered place, $A_i$ be element $i$ is in place, then $P(A_i)=\\frac{(n-1)!}{n!}=\\frac{1}{n}$ as all other elements permute freely. Similarly $P(A_i,A_j)=\\frac{(n-2)!}{n!}=\\frac{1}{n(n-1)}$ and $P(A_i,A_j,A_k)=\\frac{1}{n(n-1)(n-2)}$\n",
        "  - $P(A)=\\suml_{i=1}^nP(A_i)-\\suml_{i<j}P(A_i,A_j)+\\suml_{i<j<k}P(A_i,A_j,A_k)+...+(-1)^{n+1}P(A_1,...,A_n)$$=\\frac{n}{n}-\\frac{\\binom{n}{2}}{n(n-1)}+\\frac{\\binom{n}{3}}{n(n-1)(n-2)}+...+\\frac{(-1)^{n+1}}{n!}$$=1-\\frac{1}{2!}+\\frac{1}{3!}+...+\\frac{(-1)^{n+1}}{n!}=1-\\suml_{i=0}^n\\frac{(-1)^i}{i!}$"
      ],
      "metadata": {
        "id": "k_Ca4v8sjnS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Increasing sequence** is a sequence of events such that $E_1‚äÇ...‚äÇE_n$. We define a new event $\\liml_{n‚Üí‚àû}E_n=\\bigcup\\limits_{i=1}^{‚àû}E_i$. A decreasing sequence is $E_1‚äÉ...‚äÉE_n$, for which we define new event $\\liml_{n‚Üí‚àû}E_n=\\bigcap\\limits_{i=1}^{‚àû}E_i$. For these two newly defined events, $\\liml_{n\\to‚àû}P(E_n)=P(\\liml_{n\\to‚àû}E_n)$"
      ],
      "metadata": {
        "id": "QEBck9Jxb2r7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blitzstein Chapter 1 (Probability and counting):\n",
        "\n",
        "- e1.4.10 (birthday problem): k people with independent birthdays equally likely to be any of 365 days. The denominator is $365^{k}$. The k selections are distinguishable. The numerator of k people with unique birthdays is (365)(365-1)...(365-k+1).\n",
        "\n",
        "- e1.5.2 (team captain) forming a team of k from n players where k‚â§n and then picking a captain, $n\\binom{n-1}{k-1}=k\\binom{n}{k}$\n",
        "\n",
        "- e1.5.3 (Vandermonde's identity): forming a team of k by choosing from m girls and n boys: $\\binom{m+n}{k}=\\suml_{j=0}^{k}\\binom{m}{j}\\binom{n}{k-j}$\n",
        "\n",
        "- e1.6.4 (De Montmort): You have a deck of well shuffled n cards each uniquely labeled 1 to n. You flip over each card while counting out loud 1 through n. If the number you shout out is equal to the number written on the card, you win. What is the chance of winning on a deck? Winning is having at least 1 card is in ordered place. $1-\\frac{!n}{n!}‚âà\\frac{1}{e}$.\n",
        "\n",
        "- 1.1: Ways to permute MISSISSIPPI: $\\frac{11!}{4!4!2!}$, which is equivalent to $\\binom{11}{1}\\binom{10}{4}\\binom{6}{4}$.\n",
        "\n",
        "- 1.3: Fred is planning to go out on 5 evenings of the week at 10 restaurants. A weekly schedule respects order, so $\\binom{10}{5}$ is incorrect, and 10\\*9\\*8\\*7\\*6 is correct.\n",
        "\n",
        "- 1.4: Round-robin where n players will play against each other exactly once. There are $\\binom{n}{2}$ total matches played, and $2^{\\binom{n}{2}}$ possible outcomes.\n",
        "\n",
        "- 1.6: 20 people at a chess club split into pairs. The number of possible pairs is $\\binom{20}{2}$. If the seats matter (distinct pairs), the number of ways they're matched up is $\\frac{20!}{(2!)^{10}}$. If the pairs are indistinct, then $\\frac{20!}{(2!)^{10}10!}$. The number of ways they're matched up respecting white vs black is $\\frac{20!2^{10}}{(2!)^{10}10!}=\\frac{20!}{10!}$.\n",
        "\n",
        "- 1.8: Splitting 12 people into 3 teams where one team has 2 people and other two teams each has 5 people is $\\binom{12}{2}\\binom{10}{5}\\frac{1}{2}$. The adjustment factor is $\\frac{1}{2}$ and not $\\frac{1}{3!}$ because the 2-people team cannot appear in later levels of the outcome tree. Splitting 12 people into 3 teams of 4 will need adjustment factor of $\\frac{1}{3!}$ so it's $\\binom{12}{4}\\binom{8}{4}\\frac{1}{3!}$\n",
        "\n",
        "- 1.10: A student needs to pick 7 out of a list of 20 courses. One of them has to be a stats course. 5 of the 20 courses are stats courses. The number of choices is $\\binom{20}{7}-\\binom{15}{7}$. The naive solution $\\binom{5}{1}\\binom{19}{6}$ overcounts reorderings where first-picked stat course in one path of the tree appears in later levels of another path.\n",
        "\n",
        "- 1.11: Let function f(A) -> B, where |A|=n and |B|=m. The outcome tree is setup such that each level maps an element of A to all possible elements of B. Each level has m branches, and there are n levels, so m^n. If the function is one-to-one, then each level has 1 fewer branch, so m(m-1)...(m-n+1)\n",
        "\n",
        "- 1.13: A casino uses 10 decks of 52 cards each and 520 cards in total. How many 10-card draws can there be? This setup can be interpreted as Bose-Einstein with k=10 indistinguishable unordered selections distributed onto n=52 distinguishable cards.\n",
        "\n",
        "- 1.14: A pizza can be ordered with 4 sizes and 8 possible toppings. The number of 1-pizza orders is $4*2^8=2^{10}$. The number of 2-pizzas orders is $\\binom{2^{10}}{2}+2^{10}$ (2 different pizzas + 2 same pizzas) and not simply $2^{20}$ because there are overcounting from reordering of pizza1 and pizza2. Using Bose-Einstein k=2 indistinguishable unordered selections distributed onto n=2^10 distinguishable configurations we have $\\binom{2^{10}+1}{2}$. Both equal 524800.\n",
        "\n",
        "- 1.16: $\\binom{n}{k}+\\binom{n}{k-1}=\\binom{n+1}{k}$\n",
        "\n",
        "- 1.17: $\\binom{n}{k}^2$=$\\binom{n}{k}\\binom{n}{n-k}$\n",
        "\n",
        "- 1.25: A particular week had 6 robberies randomly located in 6 districts.\n",
        "What is the probability that some district had more than 1 robbery? If the robberies are distinguishable, then $1-\\frac{6!}{6^6}$. If indistinguishable, then Bose-Einstein $1-\\frac{1}{\\binom{6+6-1}{6}}$.\n",
        "\n",
        "- 1.26: A survey is being conducted in a city with n=1 million residents on a sample of k=1000 people one at a time, with replacement and with equal probabilities. Let $A$ be at least 1 person getting selected more than once. What is $P(A)$?\n",
        "  - Taylor approximation: $1-\\frac{j}{n}‚âàe^{-j/n}$.\n",
        "  - $P(A^C)=\\frac{n(n-1)(n-2)...(n-k+1)}{n^k}=\\frac{n}{n}\\frac{n-1}{n}\\frac{n-2}{n}...\\frac{n-k+1}{n}‚âà\\prodl_{i=1}^{k-1}e^{-\\frac{i}{n}}=e^{-\\suml_{i=1}^{k-1}\\frac{i}{n}}=e^{-\\frac{k(k-1)}{2n}}$.\n",
        "\n",
        "- 1.27: Hash table with random h(x) -> [1,n] storing k entries. The denominator is $n^k$. The k entries are distinguishable and there is no overcounting from reordering in levels of the tree. The numerator for unique k entries is n(n-1)(n-2)...(n-k+1).\n",
        "\n",
        "- 1.31: N elks in a forest, n of which were tagged. Take another sample of m elks. The denominator is $\\binom{N}{m}$. The event that exactly k of them were tagged has numerator $\\binom{n}{k}\\binom{N-n}{m-k}$\n",
        "\n",
        "- 1.33: A ball is drawn from a jar containing $r$ red balls and $g$ green balls randomly, and then another ball is drawn randomly. The probability that the two balls have the same color is $\\frac{r(r-1)+g(g-1)}{(r+g)(r+g-1)}$, different colors is $\\frac{rg}{(r+g)(r+g-1)}$.\n",
        "\n",
        "- 1.35: A random 13-hand is dealt from a 52-deck. The probability of having at least 3 cards of each suit is $\\frac{4\\binom{13}{4}\\binom{13}{3}^3}{\\binom{52}{13}}=\\frac{10\\binom{13}{3}^4}{\\binom{52}{13}}$.\n",
        "\n",
        "- 1.36: 30 dice are thrown. The event that 5 of each 1, 2, 3, 4, 5, 6 appears has probability $\\frac{\\binom{30}{5}\\binom{25}{5}\\binom{20}{5}\\binom{15}{5}\\binom{10}{5}}{6^{30}}$.\n",
        "\n",
        "- 1.37: Cards are dealt one by one from a shuffled 52-deck until A appears. The event that no J, Q, or K appears before the first A only needs to consider J Q K and A, and ignore all other cards.\n",
        "\n",
        "- 1.38: T, C, and 10 other people sit at a round table. The event that T and C sit together is $\\frac{12‚ãÖ2‚ãÖ10!}{12!}$, where 12 indicates starting position, 2 indicates T-C or C-T. Alternatively it is $\\frac{12}{\\binom{12}{2}}$.\n",
        "\n",
        "- 1.40: $k$ balls are drawn one by one with replacement from a jar with $n$ balls labeled $1..n$ to obtain a sequence of numbers. The number of increasing sequences is equal to the number of no-replacement $k$-subsets $\\binom{n}{k}$. The number of non-decreasing sequences is equal to the number of with-replacement $k$-subsets $\\binom{n+k-1}{k}$."
      ],
      "metadata": {
        "id": "qiUjK5CJgCPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ross Chapter 1 (Combinatorial analysis):\n",
        "\n",
        "- e1.4: n indistinguishable antennas of which m are defective and n-m are functional. To count the linear orderings in which no two defectives are consecutive, use n-m functional antennas as bars, the number of ways to place m balls into n-m+1 slots is $\\binom{n-m+1}{m}$.\n",
        "\n",
        "- 1.28: 8 new teachers are to be divided among 4 schools. The number of divisions is $\\binom{8+4-1}{4-1}$. If each school must receive 2 teachers, then it is $\\binom{8}{2}\\binom{6}{2}\\binom{4}{2}$.\n",
        "\n",
        "- t1.5: the number of vectors $(x_1, x_2, ..., x_n)$ where $x_i=\\{0,1\\}$ such that $\\sum_{i=1}^{n}x_i‚â•k$ is $\\sum_{j=k}^{n}\\binom{n}{j}$.\n",
        "\n",
        "- t1.6: the number of vectors $(x_1, x_2, ..., x_k)$ where $1‚â§x_i‚â§n$ and $x_1\\lt x_2\\lt ...\\lt x_k$ is $\\binom{n}{k}$.\n",
        "\n",
        "Ross Chapter 2 (Axioms of probability):\n",
        "\n",
        "- e2.5m: N men throw their hat in a ring and each then randomly picks up a hat. Setup: we line up the hats, and then place the N men onto the hats. The total number of ways this could happen is $N!$. The event that a specific group of men $i_1,i_2,...,i_k$ pick up their hats permutes the remaining $(N-k)!$ men and the probability is $\\frac{(N-k)!}{N!}$. This event includes outcomes where those remaining men (more than k men in total) could have picked up their hats as well.\n",
        "\n",
        "- e2.5n: 10 married couples are randomly seated at a round table. There are $19!$ ways to arrange 20 people in a circle. Let $E_i$ be the event that couple $i$ is sitting next to each other. $P(E_i)=\\frac{2*18!}{19!}$, and $P(E_{i1}‚à©...‚à©E_{in})=\\frac{2^n(19-n)!}{19!}$. The chance of at least 1 couple sitting together is $\\frac{\\binom{20}{1}(2)18!}{19!}-\\frac{\\binom{20}{2}2^217!}{19!}+...$.\n",
        "\n",
        "- 2.2: A die is rolled continually until a 6 appears. The sample space is the set of all sequences that end in 6.\n",
        "\n",
        "- 2.23: A pair of fair dice is rolled. The probability that the second die lands higher than the first is $\\frac{\\binom{6}{2}}{6^2}$.\n",
        "\n",
        "- 2.25: A pair of fair dice is rolled until 5 or 7 comes up. Let $E_n$ be the event that 5 occurs on $n$th roll and no 5/7 occurs in the first $n-1$ rolls, then the probability that 5 occurs first is $P(E_1)+P(E_2)+...+P(E_‚àû)=\\sum_{i=1}^{‚àû}P(E_n)$. $P(5)=\\frac{4}{6^2}, P(7)=\\frac{6}{6^2}, P(E_n)=(1-P(5)-P(7))^{n-1}P(5)$ because mutual exclusive.\n",
        "\n",
        "- 2.43: N people are randomly arranged in a line, probability that A and B are next to each other is $\\frac{2(N-1)(N-2)!}{N!}=\\frac{2}{N}$: treat AB as a block and permute (N-1)!, or find a position for AB (N-1) and permute everyone else (N-2)!. If N people are arranged in a circle, then $\\frac{2(N-2)!}{(N-1)!}=\\frac{2}{N-1}$: fix 1 person, permute everyone else (N-1)!, and treat AB as a block.\n",
        "\n",
        "- 7.23: urn 1 contains 5 white and 6 black balls, while urn 2 contains 8 white and 10 black balls. Two balls are randomly selected from urn 1 and placed in urn 2. Three balls are then randomly selected from urn 2. What is the chance that a labeled white ball from urn 1 is ultimately selected? The probability is $\\frac{2}{11}\\frac{3}{20}$. It is equivalent to $(1-\\frac{10}{11}\\frac{9}{10})(1-\\frac{19}{20}\\frac{18}{19}\\frac{17}{18})$."
      ],
      "metadata": {
        "id": "5zM9E6oxYvPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional probability\n",
        "\n",
        "- Geometric series:\n",
        "  - $a\\suml_{i=0}^{n-1}r^i=\\BC a\\frac{1-r^n}{1-r} &r\\neq1 \\\\ ai &r=1\\EC$\n",
        "  - $a\\suml_{i=0}^‚àûr^i=\\frac{a}{1-r}$ if $r\\lt 1$"
      ],
      "metadata": {
        "id": "BNbBm-jVTDZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition (conditional)** If an outcome in event B occurred, then in order for event A to have also occurred the outcome needs to be a point both in A and B. Because B occurred, uncertainty is reduced and B becomes the new sample space. If A and B are events with P(B)>0, then the conditional probability of A given B is defined as\n",
        "$$\\small\n",
        "P(A|B)=\\frac{P(A,B)}{P(B)}\\t{ or }P(A,B)=P(A|B)P(B)$$\n",
        "- $P(A|B,C)=\\frac{P(A,B|C)}{P(B|C)}$ or $P(A,B|C)=P(A|B,C)P(B|C)$\n",
        "- **Multiplication rule**: $P(E_1...E_n)=P(E_1)P(E_2|E_1)P(E_3|E_1,E_2)...P(E_n|E_1...E_{n-1})$.\n",
        "  - $P(E_1...E_n|F)=P(E_1|F)P(E_2|E_1,F)...P(E_n|E_1...E_{n-1},F)$.\n",
        "- **Baye's rule**: $P(E|F)P(F)=P(F|E)P(E)$.\n",
        "- $P(‚ãÖ|F)$ is a probability measure: $E|F$ is not an event; $E$ is the event. Even $P(‚ãÖ)$ likely assumes certain conditions $P(‚ãÖ|S)$. All probabilities are conditional.\n",
        "  - $P(E|F)=1-P(E^C|F)$.\n",
        "  - $P(E|F,G)=\\frac{P(F|E,G)P(E|G)}{P(F|G)}$.\n",
        "- **Baye's rule (Odds)**: $\\frac{P(A|B)}{P(A^C|B)}=\\frac{P(B|A)P(A)}{P(B|A^C)P(A^C)}$.\n",
        "\n",
        "**Definition (mutually exclusive)** events that share no outcome in common are mutually exclusive: the events cannot happen simultaneously. Events $A$ and $B$ are mutually exclusive if and only if $AB=‚àÖ$ and $P(AB)=0$.\n",
        "- **Law of total probability** If $F_1,...,F_n$ are mutually exclusive and $\\bigcup\\limits_{i=1}^{n}F_i=S$ i.e., $\\suml_{i=1}^{n}P(F_i)=1$, then $P(E)=\\suml_{i=1}^{n}P(E|F_i)P(F_i)$ and $P(F_j|E)=\\frac{P(E|F_j)P(F_j)}{\\suml_{i=1}^{n}P(E|F_i)P(F_i)}$\n",
        "- $P(E)=P(E,F)+P(E,F^C)=P(E|F)P(F)+P(E|F^C)P(F^C)$.\n",
        "- $P(E|F)=P(E|F,G)P(G|F)+P(E|F,G^C)P(G^C|F)$\n",
        "\n",
        "**Definition (independent)**: Two events $A$ and $B$ are independent iff $P(A|B)=P(A)$ and vice versa, and $P(A,B)=P(A)P(B)$.\n",
        "- Two events are independent if learning one had occurred gives no information on the probability of the other occurring.\n",
        "- If two events are mutually exclusive, knowing one had occurred assigns the probability of the other occurring to be 0. If $A$ and $B$ are independent, so are $A$ and $B^C$.\n",
        "- Pairwise independence of $AB$, $BC$, and $AC$ does not imply $ABC$ are independent.\n",
        "  - E.g., Two tosses of a fair coin. A: first toss ‚Üí Heads, B: second toss ‚Üí Heads, C: both tosses land on the same side.\n",
        "  - However, pairwise independence of $AB$, $BC$, and $AC$ is a necessary condition for independence of $ABC$.\n",
        "- If $A$ and $B$ are independent, so are all subsets of $A$ and subsets of $B$.\n",
        "- If $E_1,...,E_n$ are independent, then $P(E_1‚à™...‚à™E_n)=1-\\prodl_{i=1}^n(1-P(E_i))$\n",
        "- $A$ and $B$ are conditionally independent given $E$ if $P(A,B|E)=P(A|E)P(B|E)$. Independence and conditional independence do not cross imply one another. Conditional indepence given $E$ does not necessarily hold for $E^C$."
      ],
      "metadata": {
        "id": "W0lxNkG2O7NQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coherency** (Blitzstein e2.6.1) Baye's rule can be updated by linking new information. Conditionitis has 1% prevalence in the population $P(D)=.01$. Diagnostic test has sensitivity $P(T|D)=.95$ and specificity $P(T^C|D^C)=.95$. Fred tested positive, and then independently gets tested positive a second time.\n",
        "  - Using regular Baye's, $P(D|T_1)=\\frac{(0.95)(0.01)}{(0.95)(0.01)+(0.05)(0.99)}=0.16$.\n",
        "  - Using Baye's Odds form, $\\frac{P(D|T_1)}{P(D^C|T_1)}=\\frac{P(T_1|D)P(D)}{P(T_1|D^C)P(D^C)}$ or $\\frac{P(D|T_1)}{1-P(D|T_1)}=\\frac{(0.95)(0.01)}{(0.05)(0.99)}=0.19$ and $P(D|T_1)=\\frac{0.19}{1+0.19}=0.16$.\n",
        "  - To find $P(D|T_1‚à©T_2)$ in 1 step: $\\frac{P(D|T_1‚à©T_2)}{P(D^C|T_1‚à©T_2)}=\\frac{P(T_1‚à©T_2|D)P(D)}{P(T_1‚à©T_2|D^C)P(D^C)}=\\frac{(0.95^2)(0.01)}{(0.05^2)(0.99)}=3.65$, and $P(D|T_1‚à©T_2)=\\frac{3.65}{1+3.65}=0.78$.\n",
        "  - To find $P(D|T_1‚à©T_2)$ in 2 steps: $\\frac{P(D|T_1‚à©T_2)}{P(D^C|T_1‚à©T_2)}=\\frac{P(D|T_1)}{P(D^C|T_1)}\\frac{P(T_2|D‚à©T_1)}{P(T_2|D^C‚à©T_1)}=(0.19)\\frac{0.95}{0.05}=3.65$ This updates the probability of disease with new information.\n",
        "  - For a rare disease, specificity $P(T^C|D^C)$ matters much more than sensitivity in determining positive predictive value $P(D|T)$.\n",
        "\n",
        "**Monty Hall** (Blitzstein e2.7.1): on a game show a contestant chooses one of three closed doors, two of which have a goat behind them and one of which has a car. Monty then opens one of the remaining doors revealing a goat and offers the contestant the option to switch to the other unopened door. Let $A$ be winning the car, $C_i$ be the car hiding behind door $i$.\n",
        "- At time 0, contestant faces 3 doors and initially chooses door 1, **prior probability** $P(C_1)=1/3$. By committing to switch away from door 1, the contestant has prior probability of winning $P(C_2)+P(C_3)=2/3$.\n",
        "- At time 1, Monty knows which door has the car, and will open door 2 or 3 with a goat behind it. Let $M_2$ be Monty revealing door 2. **Posterior probability** $P(C_1|M_2)=\\frac{P(M_2|C_1)P(C_1)}{P(M_2)}=\\frac{(1/2)(1/3)}{(1/2)}=\\frac{1}{3}$ has not changed to 1/2. Switching choice to door 3 raises the posterior probability of winning $P(C_3|M_2)=1-P(C_1|M_2)=\\frac{2}{3}$.\n",
        "- Intuition: if there were 100 doors and Monty opens 98 of them, sticking to the initial door will not improve chance of success from 1% to 50%. Switching gives 99% chance of success.\n",
        "\n",
        "**Branching** (Blitzstein e2.7.2): A single amoeba lives in a pond. Every minute, it will either die, stay the same, or split into two with equal 1/3 probabilities. What is the probability that the population will die out? Let $B_i$ be the number of amoeba after 1 minute.\n",
        "$$\\small\\begin{aligned}\n",
        "P(D)&=P(D|B_0)P(B_0)+P(D|B_1)P(B_1)+P(D|B_2)P(B_2) \\\\\n",
        "&=(1)(1/3)+P(D)(1/3)+P(D)^2(1/3) \\\\\n",
        "&=1\n",
        "\\end{aligned}$$"
      ],
      "metadata": {
        "id": "9NUHOOo27KOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E before F** (Ross 3.76) $E$ and $F$ are mutually exclusive events representing possible outcomes of independent trials of an experiment. What is the probability that $E$ occurs before $F$? Let $A$ be the event that $E$ occurs before $F$, then\n",
        "$$\\small\\begin{aligned}\n",
        "P(A)&=P(A|E)P(E)+P(A|F)P(F)+P(A|(E‚à™F)^C)P((E‚à™F)^C) \\\\\n",
        "&=P(E)+P(A)(1-P(E‚à™F)) \\\\\n",
        "&=P(E)+P(A)(1-P(E)-P(F)) \\\\\n",
        "&=\\frac{P(E)}{P(E)+P(F)}\n",
        "\\end{aligned}$$\n",
        "\n",
        "**Gambler's Ruin** (Ross e3.4m) A and B bet on the outcomes of coin flips. On Head (probability $p$) A collects 1 unit from B, otherwise A gives 1 unit to B. A starts with $i$ units and B starts with $N-i$ units and they continue until one of them runs out of money. What is the probability $P_i=P(E)$ that A wins all the money while starting with $i$ units?\n",
        "$$\\small\\begin{aligned}\n",
        "P(E)&=P(E|H)P(H)+P(E|H^C)P(H^C) \\\\\n",
        "P_i&=pP_{i+1}+qP_{i-1} ‚Üí pP_i+qP_i=pP_{i+1}+qP_{i-1} \\\\\n",
        "p(P_{i+1}-P_i)&=q(P_i-P_{i-1}) ‚Üí P_{i+1}-P_i=\\frac{q}{p}(P_i-P_{i-1}) \\\\\n",
        "P_2-P_1&=\\frac{q}{p}P_1 ‚Üí P_i-P_{i-1}=\\left(\\frac{q}{p}\\right)^{i-1}P_1 \\\\\n",
        "P_i-P_1&=(P_i-P_{i-1})+(P_{i-1}-P_{i-2})+...+(P_2-P_1) \\\\\n",
        "P_i&=\\left(\\frac{q}{p}\\right)^{i-1}P_1+\\left(\\frac{q}{p}\\right)^{i-2}P_1+...+P_1\n",
        "\\end{aligned}$$\n",
        "Using geometric series $a+ar+ar^2+...ar^{i-1}=a\\frac{1-r^i}{1-r}$ if $r\\neq1$ and $ai$ if $r=1$:\n",
        "$$\\small\n",
        "P_i=\\BC\n",
        "\\frac{1-(q/p)^i}{1-(q/p)}P_1 &\\t{if $\\frac{q}{p}\\neq1$} \\\\\n",
        "iP_1 &\\t{if $\\frac{q}{p}=1$}\n",
        "\\EC\n",
        "$$\n",
        "Because $P_N=1$ and solving backwards,\n",
        "$$\\small\n",
        "P_1=\\BC\n",
        "\\frac{1-(q/p)^i}{1-(q/p)^N} &\\t{if $\\frac{q}{p}\\neq1$} \\\\\n",
        "\\frac{1}{N} &\\t{if $\\frac{q}{p}=1$}\n",
        "\\EC\n",
        "$$\n",
        "Therefore the probability that someone wins all $N$ units while starting with $i$ units is\n",
        "$$\\small\n",
        "P_{i,N}=\\BC\n",
        "\\frac{1-(q/p)^i}{1-(q/p)^N} &\\t{if $\\frac{q}{p}\\neq1$}\\\\\n",
        "\\frac{i}{N} &\\t{if $\\frac{q}{p}=1$}\n",
        "\\EC$$"
      ],
      "metadata": {
        "id": "YSFC0iuJsJLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ross Chapter 3 (Conditional probability and independence):\n",
        "\n",
        "- e3.2e. 2 balls are drawn from an urn containing 8 red balls and 4 white balls. Each red ball has weight r and white ball w. The probability that a given ball is next selected is the ratio of its weight over all balls currently in the urn. what is the probability that both balls are red? $P(A_1)=\\frac{8r}{8r+4w}, P(A_2|P_1)=\\frac{7r}{7r+4w}, P(A_1‚à©A_2)=\\frac{8r}{8r+4w}\\frac{7r}{7r+4w}$.\n",
        "\n",
        "- e3.2f. $N$ men throw their hats in a ring and each then randomly picks up a hat. What is the probability that exactly $k$ men pick up their hat? Let $F$ be $k$ men picking up their own hats, and $G$ be none of the other $N-k$ men do. The probability that a specific group of $k$ men pick up their hat is $P(E_1,...,E_k)=\\frac{(N-k)!}{N!}$, and there are $\\binom{N}{k}$ such groups. $P(F)=\\binom{N}{k}\\frac{(N-k)!}{N!}$. The remaining $N-k$ men have derangement $P(G|F)=!(N-k)=\\suml_{i=1}^{N-k}\\frac{(-1)^i}{i!}$. The probability that exactly $k$ men pick up the right hat is $P(G,F)=P(G|F)P(F)=\\binom{N}{k}\\frac{(N-k)!}{N!}\\frac{1}{e}=\\frac{1}{ek!}$.\n",
        "\n",
        "- e3.2g. A 52-deck is divided into 4 piles of 13 cards each. Compute the probability that each ple only has 1 Ace. Setup: Let E1 be As in a pile, E2 be As and Ah in different piles, E3 be As, Ah, and Ac in different piles, and E4 be all four Aces in different piles.\n",
        "\n",
        "- e3.3l. A card is drawn from 3 cards, the first RR is red on both sides, second BB is black on both sides, third RB is red on one side and black on the other. If the drawn card is red, what is the probability that the other side is black? Intuitively the probability is not 1/2 because the RR card has 2 R faces and RB card only has 1 R face. $P(RB|R)=\\frac{P(R|RB)P(RB)}{P(R|RR)P(RR)+P(R|RB)P(RB)}=\\frac{\\frac{1}{2}\\frac{1}{3}}{\\frac{1}{3}+\\frac{1}{2}\\frac{1}{3}}=\\frac{1}{3}$. Similarly, $P(RR|R)=\\frac{2}{3}$.\n",
        "\n",
        "- e3.4h. Independent trials of rolling 2 dice are performed. What is the probability that an outcome of 5 appears before 7?\n",
        "  - Let $P(E_n)=\\left(1-\\frac{4+6}{36}\\right)^{n-1}\\frac{4}{36}$ be the probability that no 5/7 appears before trial $n$, and 5 appears on trial $n$. The solution to the problem is $P\\left(\\bigcup\\limits_{n=1}^{‚àû}E_n\\right)=\\frac{1}{9}\\suml_{n=1}^{‚àû}\\left(\\frac{13}{18}\\right)^{n-1}=\\frac{2}{5}$.\n",
        "  - Let $E$ be the event that 5 appears before 7, $F$ be the first trial resulting in 5, $H$ be the first trial resulting in neither 5/7. $P(E)=P(E|F)P(F)+P(E|H)P(H)=P(F)+P(E)P(H)$. Solving $P(E)=\\frac{1}{9}+P(E)\\frac{13}{18}$ gives $P(E)=\\frac{2}{5}$.\n",
        "  - $P(5)=\\frac{4}{36}, P(7)=\\frac{6}{36}$. The odds that 5 appears before 7 should be 4 to 6 or $\\frac{2}{5}$.\n",
        "\n",
        "- 3.22: Red (R), blue (B), and yellow (Y) die are rolled. What is the probability that $R>Y>B$? Let $X$ be no two dice land on the same number, then $P(X)=\\frac{\\binom{6}{3}3!}{6^3}=\\frac{5}{9}$, $P(R>Y>B|X)=\\frac{1}{3!}$, and $P(R>Y>B)=P(R>Y>B|X)P(X)=\\frac{5}{54}$ (Symmetry of iid, Blitzstein 5.7)\n",
        "\n",
        "- 3.37: A gambler has a fair coin and a two-headed coin in his pocket. He randomly selects a coin and tosses it twice, both times showing head. The probability of fair coin is $P(F|HH)=\\frac{P(HH|F)P(F)}{P(HH|F)P(F)+P(HH|F^C)P(F^C)}=\\frac{1}{5}$. Or after the first toss resulting in head $P(F|H_1)=\\frac{1}{3}$, and second head $P(F|H_1H_2)=\\frac{P(FH_1H_2)}{P(H_1H_2)}=\\frac{P(H_2|FH_1)P(H_1|F)P(F)}{P(H_1H_2|F)P(F)+P(H_1H_2|F^C)P(F^C)}=\\frac{1}{5}$\n",
        "\n",
        "- 3.50: An insurance company classifies people into good risk ($G$, 20%, risk of accident 0.05), average risk ($A$, 50%, risk of accident 0.15) and bad risk ($B$, 30%, risk of accident 0.3). A person having had no accident ($E^C$) being good risk ($G$) has probability $\\frac{P(E^C|G)P(G)}{1-P(E|G)P(G)-P(E|A)P(A)-P(E|B)P(B)}=0.23$, and $P(E^C|G)=1-P(E|G)$.\n",
        "\n",
        "- 3.58: A biased coin lands on head with probability $p$. If two successive flips result in different outcomes (HT or TH) then use the second outcome; otherwise (HH or TT) try again. $P(HT)=P(TH)=p(1-p)$.\n",
        "\n",
        "- 3.64: A husband-wife team on a quiz show each independently give correct answer with probability $p$. If one of them is chosen to answer a question the probability of getting it right is $p$. If they both consider it and then give a common answer if they agree and flip a coin if they disagree, the probability is $p^2+\\frac{1}{2}p(1-p)+\\frac{1}{2}p(1-p)=p$.\n",
        "\n",
        "- 3.74: A and B alternate rolling a pair of dice. If A rolls first, what is the probability that A rolls a 9 before B rolls a 6? $p_A=\\frac{4}{36}, p_B=\\frac{5}{36}, P(A)=p_A\\left(1+(1-p_A)(1-p_B)+(1-p_A)^2(1-p_B)^2+...\\right)=\\frac{p_A}{1-(1-p_A)(1-p_B)}=\\frac{9}{19}$. A can win on turn 1, 3, 5,..., which is an infinite sum. Because of the turn-structure of the experiment and A starts first, A's chance of winning is slightly higher than $\\frac{p_A}{p_A+p_B}=\\frac{4}{9}$, which describes another experiment where at each dice throw A wins on 9 and B wins on 6.\n",
        "\n",
        "- 3.79: In successive rolls of 2 dice, what is the probability of getting 2 sevens $P(S)=s=\\frac{6}{36}$ before 6 even $P(E)=e=\\frac{18}{36}$ numbers? Getting neither is $P(X)=x=\\frac{12}{36}$. Let $R_{i,j}$ be the event of rolling $i$ sevens before $j$ evens, then recursion\n",
        "$$\\small\\begin{aligned}\n",
        "P(R_{i,j})&=P(R_{i,j}|S)P(S)+P(R_{i,j}|E)P(E)+P(R_{i,j}|X)P(X) \\\\\n",
        "P(R_{i,j})&=P(R_{i-1,j})s+P(R_{i,j-1})e+P(R_{i,j})x\n",
        "\\end{aligned}$$\n",
        "\n",
        "- 3.81: An investor owns shares in a stock, and will sell if it goes up 15 points or down 15 points. The chance of going up 1 point is $p=0.55$. The probability of profit follows Gambler's Ruin and is $P_{15}=\\frac{1-(q/p)^{15}}{1-(q/p)^{30}}$\n",
        "\n",
        "- 3.82a: A starts to flip a coin until Tail occurs, at which point B starts to to flip until Tail occurs. $P(T|A)=p_1$, $P(T|B)=p_2$. If the winner is the first to get 2 heads in a row, what is the probability that A wins? Let $A$ be A going first and gets 2 heads in a row and $B$ be B going first and A gets 2 heads in a row, then\n",
        "$$\\small\\begin{aligned}\n",
        "P(A)&=P(A|H_1H_2)P(H_1H_2)+P(A|H_1T_2)P(H_1T_2)+P(A|T_1)P(T_1) \\\\\n",
        "&=P(H_1H_2)+P(B)P(H_1T_2)+P(B)P(T_1) \\\\\n",
        "&=P_1^2+(1+P_1)(1-P_1)P(B) \\\\\n",
        "P(B)&=P(B|H_1H_2)P(H_1H_2)+P(B|H_1T_2)P(H_1T_2)+P(B|T_1)P(T_1) \\\\\n",
        "&=(1+P_2)(1-P_2)P(A) \\\\\n",
        "P(A)&=P_1^2+(1-P_1^2)(1-P_2^2)P(A) \\\\\n",
        "&=\\frac{P_1^2}{1-(1-P_1^2)(1-P_2^2)}\n",
        "\\end{aligned}$$\n",
        "\n",
        "- 3.82b: If the winner is the first to get 2 total heads, what is the probability that A wins? Let $A(i,j)$ be A going first and A wins when it has won i heads lost j heads and $B(i,j)$ be B going first but A wins when A has won i heads and lost j heads, then recursion\n",
        "$$\\small\\begin{aligned}\n",
        "P(A(i,j))&=P(A(i+1,j))P_1+P(B(i,j))(1-P_1) \\\\\n",
        "P(B(i,j))&=P(B(i,j+1))P_2+P(A(i,j))(1-P_2)\n",
        "\\end{aligned}$$\n",
        "Solve for $P(A(0,0))$ with boundary conditions $P(A(2,j))=1$, $P(B(2,j))=1$, $P(A(i,2))=0$, $P(B(i,2))=0$.\n",
        "\n",
        "- 3.84: An urn with 12 balls including 4 white balls. Players A B and C successively draw a ball from the urn. The first one to withdraw a white ball wins. If with replacement,\n",
        "$$\\small\\begin{aligned}\n",
        "P(A)&=P(A|W_A)P(W_A)+P(A|W_A^CW_B^CW_C^C)P(W_A^CW_B^CW_C^C) \\\\\n",
        "&=P(W_A)+P(A)P(W_A^C)P(W_B^C)P(W_C^C) \\\\\n",
        "&=\\frac{P(W_A)}{1-P(W_A^C)P(W_B^C)P(W_C^C)}\n",
        "\\end{aligned}$$\n",
        "If without replacement, $P(A)=P(W_1)+P(W_1^CW_2^CW_3^CW_4)+P(W_1^CW_2^CW_3^CW_4^CW_5^CW_6^CW_7)$.\n",
        "\n",
        "- t3.7a: An urn containing $n$ white and $m$ black balls is drawn from until only those of the same color are left. Let $A(i,j)$ be the event that only white balls remain in the end if there are $i$ white and $j$ black balls. Boundary conditions $P(A(i,0))=1$ and $P(A(0,j))=0$, then recurse\n",
        "$$\\small\\begin{aligned}\n",
        "P(A(i,j))&=P(A(i,j-1))P(B)+P(A(i-1,j))P(W) \\\\\n",
        "&=\\frac{j}{i+j}P(A(i,j-1))+\\frac{i}{i+j}P(A(i-1,j)) \\\\\n",
        "P(A(1,1))&=\\frac{1}{2}, P(A(2,1))=\\frac{1}{3}+\\frac{2}{3}\\frac{1}{2}=\\frac{2}{3}, P(A(3,1))=\\frac{1}{4}+\\frac{3}{4}\\frac{2}{3}=\\frac{3}{4} \\\\\n",
        "P(A(1,2))&=\\frac{2}{3}\\frac{1}{2}=\\frac{1}{3}, P(A(2,2))=\\frac{2}{4}\\frac{2}{3}+\\frac{2}{4}\\frac{1}{3}=\\frac{1}{2}, P(A(2,3))=\\frac{3}{5}\\frac{1}{2}+\\frac{2}{5}\\frac{1}{4}=\\frac{2}{5}\n",
        "\\end{aligned}$$\n",
        "The pattern that emerges is $P(A(i,j))=\\frac{i}{i+j}$. Then $P(A(n,m))=\\frac{n}{n+m}$. Intuitively, the question can be rephrased to be if $n$ white and $m$ black balls are randomly arranged in a line, what is the probability that the last ball is white?\n",
        "\n",
        "- t3.7b: (**Race to extinction problem**) A pond containing $r$ Red, $b$ Blue, and $g$ Green fish is drawn from. What is the probability that Red are the first to become extinct? If G is the last to go extinct then the problem becomes linear ordering of R and B, while the remaining (G-1) sprinkled in do not affect the result. Let RGB and RBG represent orders of extinction, then\n",
        "$$\\small\\begin{aligned}\n",
        "P(R)&=P(RBG)+P(RGB) \\\\\n",
        "&=P(\\t{R first|G last})P(\\t{G last})+P(\\t{R first|B last})P(\\t{B last}) \\\\\n",
        "&=\\frac{b}{r+b}\\frac{g}{r+b+g}+\\frac{g}{r+g}\\frac{b}{r+b+g}\n",
        "\\end{aligned}$$\n",
        "With a lot of manipuations it simplifies to $\\frac{1/r}{1/r+1/b+1/g}$. Solution manual incorrect.\n",
        "\n",
        "- t3.13: Probility of Head is $p$. A starts to flip a coin until Tail shows up, then B starts flipping until Tail shows up, then A. Let $P_{n,m}$ be probability that A gets $n$ Heads before B gets $m$. Observe that $P_{m,n}$ likewise describes B getting $m$ Heads before A gets $n$. If A succeeds at current trial then A subsequently has $n-1$ Heads to win. Assume A is flipping then $P_{n,m}=pP_{n-1,m}+(1-p)(1-P_{m,n})$.\n",
        "\n",
        "- t3.14: Start with $i$ units against an opponent with $‚àû$ units, at each trial you either win or lose 1 unit with probability $p$ and $q$. What's the probability of going broke? Let $P_i$ be the probability of winning while having $i$ units. If current trial succeeds then subsequently we have $i+1$ units to go, otherwise we have $i-1$ units to go.\n",
        "$$\\small\\begin{aligned}\n",
        "P_i&=pP_{i+1}+qP_{i-1} ‚Üí pP_i+qP_i=pP_{i+1}+qP_{i-1} \\\\\n",
        "&=\\left(\\frac{q}{p}\\right)^{i-1}P_1+\\left(\\frac{q}{p}\\right)^{i-2}P_1+...+P_1\n",
        "\\end{aligned}$$\n",
        "If $q=p$, then $P_i=iP_1$ otherwise $P_i=P_1\\frac{1-(q/p)^i}{1-(q/p)}$. Given boundary condition $P_‚àû=1$ and solving backwards, $P_1=0$ if $p=q$ otherwise $P_1=\\frac{1-(q/p)}{1-(q/p)^‚àû}=0$ if $p<1/2$. If $p>1/2$ then $P_1=1-(q/p)$ and $P_i=1-(q/p)^i$.\n",
        "\n",
        "- t3.16: Independent trials with probability of success $p$. Let $P_n$ be the probability that $n$ trials result in even number of successes. If current trial succeeds or fails then subsequently we have $n-1$ trials to go. $P_0=1, P_1=1-p$\n",
        "$$\\small\\begin{aligned}\n",
        "P(E)&=P(E|W)P(W)+P(E|W^C)P(W^C) \\\\\n",
        "P_n&=p(1-P_{n-1})+(1-p)P_{n-1}\n",
        "\\end{aligned}$$\n",
        "\n",
        "- t3.17: $n$ independent trials with trial $i$ having probability of success $1/(2i+1)$. Let $P_n$ be probability that total number of successes is odd. $P_0=0, P_1=1/3$. Recurse\n",
        "$$\\small\\begin{aligned}\n",
        "P_n&=\\frac{1}{2n+1}(1-P_{n-1})+\\frac{2n}{2n+1}P_{n-1}=\\frac{1}{2n+1}+\\frac{2n-1}{2n+1}P_{n-1} \\\\\n",
        "P_1&=\\frac{1}{3}, P_2=\\frac{2}{5}, P_3=\\frac{3}{7}. P_n=\\frac{n}{2n+1}.\n",
        "\\end{aligned}$$"
      ],
      "metadata": {
        "id": "V5wNJEegttcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blitzstein Chapter 2 (Conditional probability):\n",
        "\n",
        "- 2.25: A crime is committed either by suspect A or B with equal evidence against both. Further investigation revealed the guilty party had blood type found in 10% of the population. A has matching blood type; B has unknown blood type. Let $G_A$ be A guilty, $G_B$ be B guilty, $T_A$ be A with blood type, $T_B$ be B with blood type. Prior probabilities prior to blood type information are $P(G_A)=0.5, P(G_B)=0.5, P(T_A)=1, P(T_B)=0.1$. Let $M$ be guilty party has matching blood type. Posterior probabilities conditional on blood type information are $P(G_A|MT_A)=\\frac{P(M|G_AT_A)P(G_A)}{P(M|G_A)P(G_A)+P(M|G_B)P(G_B)}=\\frac{(1)(0.5)}{(1)(0.5)+(0.1)(0.5)}=\\frac{10}{11}$. $P(T_B|MT_A)=P(T_B|MT_AG_A)P(G_A|MT_A)+(T_B|MT_AG_B)P(G_B|MT_A)=(0.1)\\frac{10}{11}+(1)\\left(1-\\frac{10}{11}\\right)=\\frac{2}{11}$.\n",
        "\n",
        "- 2.27: There are 5 blood types in population with prevalence $p_1, ..., p_5$. A crime was committed by two individuals, and evidence shows they have blood types 1 and 2. A suspect with blood type 1 has prior probability $p$ of being guilty. Let $A$ be suspect guilty, $E$ be evidence that criminals have types 1 and 2. $P(A|E)=\\frac{P(E|A)P(A)}{P(E|A)P(A)+P(E|A^C)P(A^C)}=\\frac{(1)p_2p}{(1)p_2p+2p_1p_2(1-p)}=\\frac{p}{p+2p_1(1-p)}$\n",
        "  - If suspect is guilty, then he has type 1 and $P(E|A)=p_2$ is the probability that the other criminal has type 2. If suspect is not guilty, then $P(E|A^C)=p_1p_2+p_2p_1$ is the probability that two random individuals have blood types 1 and 2. The suspect‚Äôs blood type is provided separately as a characteristic of the suspect, not as part of the crime scene evidence. The suspecting having blood type 1, like height or shoe size, is a known attribute of the suspect and is a part of prior probability. If suspect's blood type is a part of evidence then $P(E|A^C)=p_12p_1p_2$ still needs to consider suspect having blood type 1 even though that is no longer related to case. $P(A|E)=\\frac{p}{p+2p_1^2(1-p)}$.\n",
        "\n",
        "- 2.33: $C$ is a set of 100 people, $A$ is subset who are friends with Alice, $B$ is subset who are friends with Bob. Each person in $C$ is friends with Alice with probability 0.5, and likewise for Bob independently.\n",
        "  - If $D‚äÜC$, then $P(A=D)$ is equivalent to 100 people throwing a coin twice and getting the same result. $P(A=D)=p^k(1-p)^{100-k}=0.5^{100}$\n",
        "  - $P(A‚äÜB)$. Each person has $0.5^2$ chance to be in $B-A$. $P(A‚äÜB)=0.75^{100}$\n",
        "  - $P(A‚à™B=C)$. Each person has $0.5^2$ chance to be neither in A nor in B. $P(A‚à™B=C)=0.75^{100}$\n",
        "\n",
        "- 2.40: Monty Hall, except $P(M_2|C_1)=p‚â•0.5$. The posterior probability that the strategy of always switching succeeds is $P(C_3|M_2)=\\frac{P(M_2|C_3)P(C_3)}{P(M_2)}=\\frac{(1)(1/3)}{(1)(1/3)+p(1/3)}=\\frac{1}{1+p}$\n",
        "\n",
        "- 2.48: A fair die is repeatedly rolled. Let $p_n$ be the probability that the running total is exactly $n$. Then recursive $p_n=\\frac{1}{6}(p_{n-1}+p_{n-2}+p_{n-3}+p_{n-4}+p_{n-5}+p_{n-6})$ with boundary conditions $p_{\\lt0}=0, p_0=1$.\n",
        "\n",
        "- 2.49: A sequence of $n‚â•1$ independent trials is performed where each trial ends in success with probability $p_i$. Let $A_n$ be even number of successful trials in $n$ trials.\n",
        "$P(A_n)=p_n(1-P(A_{n-1}))+(1-p_n)P(A_{n-1})=p_n+(1-2p_n)P(A_{n-1})$ with boundary condition $P(A_0)=1$. Then $P(A_1)=1-p_1$, and $P(A_2)=p_2+(1-2p_2)(1-p_1)=1-p_1-p_2+2p_1p_2=(1-p_1)(1-p_2)+p_1p_2$. Substitute in $b_i=1/2-p_i$, $P(A_2)=(1/2-b_1)(1/2-b_2)+(1/2+b_1)(1/2+b_2)=1/2+2b_1b_2$\n",
        "\n",
        "- 2.50: C and H play a match where C has probability of winning each trial $p$. The first player to win two games more than the opponent wins. Let $X$ be number of the first two games won by C.\n",
        "$$\\small\\begin{aligned}\n",
        "P(C)&=P(C|X=0)P(X=0)+P(C|X=1)P(X=1)+P(C|X=2)P(X=2) \\\\\n",
        "&=P(C)(pq+qp)+p^2 \\\\\n",
        "&=\\frac{p^2}{1-2pq}=\\frac{p^2}{(p+q)^2-2pq}=\\frac{p^2}{p^2+q^2} \\\\\n",
        "\\end{aligned}$$\n",
        "Gambler's Ruin with i=2, N=4: $P_{i,N}=\\frac{1-(q/p)^i}{1-(q/p)^N}$. $P_{2,4}=\\frac{(p^2-q^2)/p^2}{(p^2-q^2)(p^2+q^2)/p^4}=\\frac{p^2}{p^2+q^2}$.\n",
        "\n",
        "- 2.53: There are 100 points equally placed around a circle where 99 of them has a sheep and 1 of them has a wolf positioned. At each time step the wolf randomly moves CW or CCW by 1 point, eating a sheep if one is there. What is the probability that the sheep initially opposite the wolf is the last one standing? With 100% probability the wolf will eventually eat one of the final sheep's neighbors. It is now Gambler's Ruin {i=1, N=99} to hit the other neighbor. When $q/p=1$, $P_{i,N}=i/N$. The probability is $\\frac{1}{99}$."
      ],
      "metadata": {
        "id": "H4cF3gzm3Gpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discrete random variables"
      ],
      "metadata": {
        "id": "RwPkwYSspGCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discrete random variable**: for an experiment $S$, random variable $X:S‚Üí‚Ñù$ assumes a value for every outcome of the experiment. The range of values that $X$ can assume is called its support. $X$ is a discrete random variable if it has finite or countably infinite possible values: often its support is a set of integers.\n",
        "- for discrete $X$ with support $x_1,...,x_‚àû$ the **probability mass function** PMF is $P(X=x_i)‚â•0$ such that $\\suml_{i=1}^{‚àû}P(X=x_i)=1$.\n",
        "  - let $X$ be a random variable and $g:‚Ñù‚Üí‚Ñù$. Then the support of $g(X)$ is the set of all $y=g(x)$ for at least one $x$ in the support of $X$, and its PMF is $P(g(X)=y)=\\suml_{x:g(x)=y}P(X=x)$. When $x=g^{-1}(y)$ have multiple values, add their probabilities.\n",
        "- the **cumulative distribution function** CDF $F(x)=P(X‚â§x)$.\n",
        "  - non-decreasing: if $a\\lt b$ then $F(a)‚â§F(b)$.\n",
        "  - $\\liml_{a‚Üí‚àû}F(a)=1$ and $\\liml_{a‚Üí-‚àû}F(a)=0$\n",
        "  - right-continuous: $F(a)=\\liml_{x‚Üía^+}F(x)$. If there's a jump at $a$, $F(a)$ takes value from the right side of the jump.\n",
        "\n",
        "**Expected value** is the average of all possible values of $X$ weighted against their probabilities is\n",
        "$$\\small\n",
        "\\E[X]=\\sum_{x:P(X=x)>0}xP(X=x) \\quad\\t{or}\\quad \\E[X]=\\sum_{s‚ààS}X(s)P(\\{s\\})\n",
        "$$\n",
        "- **Linearity**: For any random variables $X,Y$ regardless if they are independent or dependent, and constant $c$,\n",
        "$$\\small\\begin{aligned}\\E[X+Y]&=\\E[X]+\\E[Y]\\\\ \\E[aX+b]&=a\\E[X]+b\\end{aligned}$$\n",
        "- **LOTUS**: Law of unconscious statistician. Given discrete random variable $X$ and function $g:‚Ñù‚Üí‚Ñù$,\n",
        "$$\\small\n",
        "\\E[g(X)]=\\sum_{x}g(x)P(X=x)\n",
        "$$\n",
        "- For non-negative $X$, then $\\E[X]=\\suml_{n=0}^{‚àû}\\left(1-F_X(n)\\right)$. Sum of survival function (Blitzstein 4.4.8)\n",
        "- $\\E[XY]=\\E[X]\\E[Y]$ iff $X$ and $Y$ are independent\n",
        "\n",
        "**Variance** $\\Var(X)=\\E(X-Œº)^2=\\E[X^2]-Œº^2$\n",
        "- $\\Var(X)‚â•0$\n",
        "- $\\Var(X+c)=\\Var(X)$\n",
        "- $\\Var(cX)=c^2\\Var(X)$\n",
        "- $\\Var(X+Y)=\\Var(X)+\\Var(Y)$ iff $X$ and $Y$ are independent\n",
        "- $\\Var\\left(\\suml_{i=1}^nX_i\\right)=\\suml_{i=1}^n\\Var(X_i)=n\\Var(X_1)$ for iid $X_1,...,X_n$\n",
        "\n",
        "**Independence**: $X$ and $Y$ are independent if knowing the value of $X$ gives no clue to the value of $Y$ and vice versa. $X$ and $Y$ are independent if for all $x,y‚àà‚Ñù$,\n",
        "$$\\small\\begin{aligned}P(X‚â§x,Y‚â§y)&=P(X‚â§x)P(Y‚â§y)\\\\P(X=x,Y=y)&=P(X=x)P(Y=y)\\end{aligned}$$\n",
        "- If there is a single $\\{x,y\\}$ that breaks the equality, then $X$ and $Y$ are not independent.\n",
        "- Like with events, for $X_1...X_n$, pairwise independence does not imply independence in general\n",
        "- If $X$ and $Y$ are independent, then so are $g(X)$ and $h(Y)$ (Blitzstein 3.8.5, proof in measure theory)\n",
        "- **iid**: idependent and identically distributed\n",
        "- $X$ and $Y$ are conditionally independent given $Z$ if for all $x,y‚àà‚Ñù,z$ in support of $Z$,\n",
        "$$\\small\\begin{aligned}P(X‚â§x,Y‚â§y|Z=z)&=P(X‚â§x|Z=z)P(Y‚â§y|Z=z)\\\\P(X=x,Y=y|Z=z)&=P(X=x|Z=z)P(Y=y|Z=z)\\end{aligned}$$\n"
      ],
      "metadata": {
        "id": "Q1TudRDDh785"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bernoulli/indicator:** A trial where the outcome is either success or failure is called a Bernoulli trial. $I_A‚àà\\{0,1\\}$ is an indicator random variable on event $A$ with PMF $\\BC P(I_A=1)=p_A \\\\ P(I_A=0)=1-p_A \\EC$\n",
        "- $\\E[I_A]=p_A$ - fundamental bridge between expectation and probability for all other discrete random variables.\n",
        "- $\\Var(I_A)=p_A(1-p_A)$\n",
        "- $(I_A)^k=I_A$ for any positive integer $k$\n",
        "- $I_A^C=1-I_A$\n",
        "- Let $A,B$ be events, then $I_{A‚à©B}=I_AI_B$ and $I_{A‚à™B}=I_A+I_B-I_AI_B$\n",
        "- Bonferroni's inequality: let $A_1...A_n$ be events, then $I_{A_1‚à™...‚à™A_n}‚â§I_{A_1}+...+I_{A_n}$.\n",
        "- Linearity: For $n$ events, independent or dependent, each occurring with probabilities $p_1...p_n$ represented by indicators $I_1...I_n$. Let $N$ be the number of these events occurring, then $N=I_1+...+I_n$ and $\\E[N]=\\suml_{i=1}^n\\E[I_i]=\\suml_{i=1}^np_i$.\n",
        "  - $p_1...p_n$ are prior probabilities unaffected by dependent \"without replacement\" conditions\n",
        "  - e4.2.2: let $X‚àº\\Binom(n,p)$, then $X=I_1+...+I_n$ and $\\E[X]=np$.\n",
        "  - e4.2.3: let $X‚àº\\HGeom(N,m,n)$ be the number of white balls in an $n$-sample drawn from $N$ balls, among which $m$ are white. $X=I_1+...+I_n$ where $I_i$ is indicator for ball $i$ being white. Unconditionally the $i$th ball being white has probability $\\frac{m}{N}$, and $\\E[X]=\\frac{nm}{N}$\n"
      ],
      "metadata": {
        "id": "UihseoTNpHVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binomial:** $n$ independent trials each with success probability $p$, then $X‚àº\\Binom(n,p)$ represents the number of successes with PMF\n",
        "$$\\small P(X=i)=p(i)=\\binom{n}{i}p^i(1-p)^{n-i}$$\n",
        "- ``` scipy.stats.binom.pmf(i, n, p) ```\n",
        "- $\\E[X]=np$ and $\\Var(X)=np(1-p)$\n",
        "$$\\small\\begin{aligned}\n",
        "\\E[X^k]&=\\sum_{i=0}^{n}i^k\\binom{n}{i}p^i(1-p)^{n-i}, \\quad i\\binom{n}{i}=n\\binom{n-1}{i-1} \\\\\n",
        "&=np\\sum_{j=0}^{n-1}(j+1)^{k-1}\\binom{n-1}{j}p^j(1-p)^{n-1-j}, \\quad j=i-1 \\\\\n",
        "&=np\\E[(Y(n-1,p)+1)^{k-1}] \\quad‚áí \\E[X^2]=np[(n-1)p+1]\n",
        "\\end{aligned}$$\n",
        "- The PMF $p(k)$ peaks at $k_{max}=(n+1)p$, as proven by solving inequality $\\frac{p(k)}{p(k-1)}=\\frac{n-k+1}{k}\\frac{p}{1-p}‚â•1$\n",
        "- if $X‚àº\\Binom(n,p)$ and $Y‚àº\\Binom(m,p)$, then $(X+Y)‚àº\\Binom(m+n,p)$\n",
        "  - $P(X+Y=k)=\\suml_{j=0}^{k}P(X+Y=k|X=j)P(X=j)$\n",
        "  $=\\suml_{j=0}^{k}P(Y=k-j)P(X=j)$\n",
        "- Binomial-hypergeometric (Blitzstein T3.9.2) if $X‚àº\\Binom(n,p)$ and $Y‚àº\\Binom(m,p)$ independent, then $(X|X+Y=k)‚àº\\HGeom(m+n,n,k)$\n",
        "  - $P(X=x|X+Y=k)=\\frac{P(X=x,Y=k-x)}{P(X+Y=k)}$\n",
        "  $=\\frac{P(X=x)P(Y=k-x)}{P(X+Y=k)}$\n",
        "  $=\\frac{\\binom{n}{x}p^xq^{n-x}\\binom{m}{k-x}p^{k-x}q^{m-k+x}}{\\binom{m+n}{k}p^kq^{m+n-k}}$\n",
        "  $=\\frac{\\binom{n}{x}\\binom{m}{k-x}}{\\binom{m+n}{k}}$\n",
        "- Binomial describes independent trials where $p$ remains constant. It is selecting a $k$ subset with replacement. It is juxtaposed against hypergeometric where selection is without replacement, the trials are dependent, and probability of success changes.\n",
        "\n",
        "**Hypergeometric:** A $n$ sample is randomly chosen from $N$ balls, of which $m$ are white. $X‚àº\\HGeom(N,m,n)$ is the number of white balls selected. The PMF is\n",
        "$$\\small P(X=i)=\\frac{\\binom{m}{i}\\binom{N-m}{n-i}}{\\binom{N}{n}}$$\n",
        "- ``` scipy.stats.hypergeom.pmf(i, N, m, n) ```\n",
        "- Selecting $n$ from $N$ each with **hypergeometric probability** $p=\\frac{m}{N}$ without replacement (dependent trials), whereas binomial is making selections with replacement (independent trials). When $m$ and $N$ are large relative to $n$, with or without replacement no longer matters to the problem.\n",
        "  - What is the probability that a randomly selected $A,B$ pair from $N$ are both white? $P(A)P(B|A)=\\frac{m}{N}\\frac{m-1}{N-1}$.\n",
        "\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "P(X=i)=&\\frac{\\binom{m}{i}\\binom{N-m}{n-i}}{\\binom{N}{n}}\\\\\n",
        "=&\\binom{n}{i}\\frac{m}{N}...\\frac{m-i+1}{N-i+1}\\frac{N-m}{N-i}\\frac{N-m-1}{N-i-1} \\\\\n",
        "&...\\frac{N-m-(n-i-1)}{N-i-(n-i-1)} \\\\\n",
        "‚Üí&\\binom{n}{i}p^i(1-p)^{n-i}\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "\\E[X^k]&=\\sum_{i=0}^ni^k\\frac{\\binom{m}{i}\\binom{N-m}{n-i}}{\\binom{N}{n}}\\\\\n",
        "&=\\sum_{i=0}^ni^k\\frac{\\frac{m}{i}\\binom{m-1}{i-1}\\binom{N-m}{n-i}}{\\frac{N}{n}\\binom{N-1}{n-1}}\\\\\n",
        "&=\\frac{nm}{N}\\sum_{j=0}^{n-1}(j+1)^{k-1}\\frac{\\binom{m-1}{j}\\binom{N-m}{n-j-1}}{\\binom{N-1}{n-1}}\\\\\n",
        "&=\\frac{nm}{N}\\E[(Y+1)^{k-1}]\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- Hypergeometric-binomial (Blitzstein T3.9.3): If $X‚àº\\HGeom(N,m,n)$, then $\\liml_{N‚Üí‚àû}X‚àº\\Binom(n,\\frac{m}{N})$.\n",
        "- $\\E[X]=\\frac{nm}{N}=np$, and $\\Var(X)=\\frac{nm}{N}\\left[\\frac{(n-1)(m-1)}{N-1}+1-\\frac{nm}{N}\\right]$ $=np(1-p)\\left(1-\\frac{n-1}{N-1}\\right)‚Üínp(1-p)$\n",
        "- Hypergeometric(N,m,n) ‚áî Hypergeometric(N,n,m)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SCUYJE-AYpch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geometric:** independent trials are performed each with success probability $p$, then $X‚àº\\Geom(p)$ is the number of trials required until 1 success occurs. It has PMF\n",
        "$$\\small P(X=n)=(1-p)^{n-1}p$$\n",
        "- ``` scipy.stats.geom.pmf(n, p) ```\n",
        "- $\\E[X]=\\frac{1}{p}$ and $\\Var(X)=\\frac{1-p}{p^2}$.\n",
        "  - If $Y‚àº\\QGeom(p)$ where $P(Y=n)=(1-p)^np$ measures the number of fails before the first success, then $\\E[Y]=\\frac{1-p}{p}$ and $\\Var(Y)=\\frac{1-p}{p^2}$.\n",
        "\n",
        "$$\\small\\begin{equation}\\begin{aligned}\n",
        "\\E[X]&=\\sum_{i=1}^{‚àû}iq^{i-1}p=\\sum_{i=1}^{‚àû}(i-1+1)q^{i-1}p\\\\\n",
        "&=\\sum_{j=0}^{‚àû}jq^jp+1=q\\E[X]+1\n",
        "\\end{aligned}\\quad\\begin{aligned}\n",
        "\\E[X^2]&=\\sum_{i=1}^{‚àû}i^2q^{i-1}p=\\sum_{i=1}^{‚àû}(i-1+1)^2q^{i-1}p\\\\\n",
        "&=q\\E[X^2]+2q\\E[X]+1\n",
        "\\end{aligned}\\end{equation}$$\n",
        "- If $Y=X-1$ where $P(Y=k)=(1-p)^kp$ measures the number of failures before the first success, then $\\E[Y]=\\E[X]-1=\\frac{1}{p}-1=\\frac{1-p}{p}$.\n",
        "- Geometric distribution is named after its survival function: $P(X‚â•n)=\\suml_{i=n}^‚àû(1-p)^ip$ $=(1-p)^np\\suml_{i=0}^‚àû(1-p)^i=(1-p)^n$.\n",
        "  - Using LOTUS with geometric PMF needs geometric series $a\\suml_{i=0}^‚àûr^i=\\frac{a}{1-r}$ if $r\\lt 1$ (e.g., Blitzstein 9.31)\n",
        "- **Memoryless**: $P(X‚â•n+m|X‚â•m)=P(X‚â•n)$ and $\\E[X|X‚â•m]=m+\\E[X]$.\n",
        "\n",
        "**Negative binomial:** independent trials are performed each with success probability $p$, then $X‚àº\\t{nbinom}(r,p)$ is the number of trials required until $r$ successes occur.  It has PMF\n",
        "$$\\small P(X=n)=\\binom{n-1}{r-1}p^r(1-p)^{n-r}$$\n",
        "- ``` scipy.stats.nbinom.pmf(n-r, r, p) ```\n",
        "- the final trial is successful $p$, and there are $r-1$ successes in the first $n-1$ trials $\\binom{n-1}{r-1}p^{r-1}(1-p)^{n-r}$.\n",
        "- $X(r,p)=Y_1(p)+...+Y_r(p)$ where $Y_i‚àº\\Geom(p)$.\n",
        "- $\\E[X]=\\frac{r}{p}$, and $\\Var(X)=\\frac{r(1-p)}{p^2}$\n",
        "- If $Y=X-r$ where $P(Y=k)=(1-p)^kp^r$ measures the number of failures before $r$ successes, then $\\E[Y]=\\E[X]-r=\\frac{r}{p}-r=\\frac{r(1-p)}{p}$.\n",
        "- $n\\binom{n-1}{r-1}=r\\binom{n}{r}$\n",
        "\n",
        "**Negative hypergeometric:** $N$ balls, of which $m$ are white and others black, are drawn from until $r$ white balls are obtained. $X‚àº\\t{nhypergeom}(N,m,r)$ represents the number of black balls drawn before the final white ball is drawn. It has PMF\n",
        "$$\\small\\begin{equation}\n",
        "P(X=k)=\\frac{\\binom{m}{r-1}\\binom{N-m}{k}}{\\binom{N}{r+k-1}}\\frac{m-(r-1)}{N-(r+k-1)}\n",
        "\\quad\\t{or}\\quad\n",
        "P(X=k)=\\frac{\\binom{r+k-1}{r-1}\\binom{N-(r+k)}{m-r}}{\\binom{N}{m}}\n",
        "\\end{equation}$$\n",
        "- Blitzstein e4.4.7: The first PMF selects the first $r-1$ white balls and $k$ black balls before selecting the final white ball. The second imagines all $N$ balls are drawn and enumerates possible placements for all $m$ white balls for permutations where the $r$th white ball is positioned at $r+k$.\n",
        "- What's $\\E[X]$? Let $X_i$ be the number of black balls in the gap between white ball $i-1$ and $i$. There are $m+1$ such gaps where a black ball can appear in all permutations of $N$ balls. Base case $X_1$: Let $I_j$ be indicator that black ball $j$ appears in the gap before the first white ball, then $\\E[I_j]=\\frac{1}{m+1}$ either by (1) the $m+1$ gaps reasoning, or (2) \"race\" reasoning: permutations of having this black ball vs $m$ white balls to appear first. Therefore $\\E[X_1]=\\frac{N-m}{m+1}$. By symmetry, $\\E[X_1]=...=\\E[X_{m+1}]$. Therefore $\\E[X]=r\\frac{N-m}{m+1}$.\n",
        "- ``` scipy.stats.nhypergeom.pmf(k, N, m, r) ```"
      ],
      "metadata": {
        "id": "f88EuxWCpvkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Poisson:** for $Œª>0$, $X‚àº\\Pois(Œª)$ is Poisson random variable with PMF $$P(X=k)=e^{-Œª}\\frac{Œª^k}{k!}$$\n",
        "- ``` scipy.stats.poisson.pmf(k, Œª) ```\n",
        "- **Poisson paradigm**: for $n$ events each occurring with probability $p_1...p_n$, let $I_i$ be indicators and let $N=I_1+...+I_n$ be the number of these events occurring: $\\E[N]=\\suml_{i=1}^n\\E[I_i]=\\suml_{i=1}^np_i$. If $p_i$ small, $n$ large, and dependence weak or independent, then we approximate $N‚àº\\Pois(Œª=\\E[N])$.\n",
        "  - Poisson $(Œª)$ is a good approximation for binomial $(n,p)$ for large $n$ and small $p$ such that $Œª=np$ is well defined.\n",
        "\n",
        "$$\\small\\begin{aligned}\n",
        "P\\{X=i\\}&=\\binom{n}{i}p^i(1-p)^{n-i}=\\frac{n!}{i!(n-i)!}\\left(\\frac{Œª}{n}\\right)^i\\left(1-\\frac{Œª}{n}\\right)^{n-i} \\\\\n",
        "&=\\frac{n!}{n^i(n-i)!}\\frac{Œª^i}{i!}\\frac{(1-Œª/n)^n}{(1-Œª/n)^i}, \\quad\\t{ where }\\frac{n!}{n^i(n-i)!}‚Üí1, \\frac{(1-Œª/n)^n}{(1-Œª/n)^i}‚Üí\\frac{e^{-Œª}}{1}\\\\\n",
        "&‚âàe^{-Œª}\\frac{Œª^i}{i!}\n",
        "\\end{aligned}$$\n",
        "  - For n‚âà10 p‚âà0.1, accuracy is within ‚â•2 digits when np\\>1, or 1 digit when np=1. For n‚âà1000 p‚âà0.001, accuracy is ‚â• 3 digits.\n",
        "- $\\E[X]=Œª$ and $\\Var(X)=Œª$\n",
        "\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\E[X]&=\\sum_{i=0}^{‚àû}ie^{-Œª}\\frac{Œª^i}{i!}=\\sum_{i=1}^{‚àû}ie^{-Œª}\\frac{Œª^i}{i!} \\\\\n",
        "&=Œª\\sum_{i=1}^{‚àû}e^{-Œª}\\frac{Œª^{i-1}}{(i-1)!} \\\\\n",
        "&=Œªe^{-Œª}\\sum_{j=0}^{‚àû}\\frac{Œª^j}{j!} \\quad j=i-1\\\\\n",
        "&=Œª \\\\\n",
        "\\sum_{i=0}^{‚àû}\\frac{Œª^i}{i!}&=e^Œª\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "\\E[X^2]&=\\sum_{i=0}^{‚àû}i^2e^{-Œª}\\frac{Œª^i}{i!} \\\\\n",
        "&=Œªe^{-Œª}\\sum_{i=1}^{‚àû}i\\frac{Œª^{i-1}}{(i-1)!} \\\\\n",
        "&=Œªe^{-Œª}\\sum_{j=0}^{‚àû}(j+1)\\frac{Œª^j}{j!} \\\\\n",
        "&=Œª\\left[\\sum_{j=0}^{‚àû}je^{-Œª}\\frac{Œª^j}{j!}+e^{-Œª}\\sum_{j=0}^{‚àû}\\frac{Œª^j}{j!}\\right] \\\\\n",
        "&=Œª(\\E[X]+1)\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- If $X‚àº\\Pois(Œª_1)$ and $Y‚àº\\Pois(Œª_2)$ are independent, then $(X+Y)‚àº\\Pois(Œª_1+Œª_2)$. (Blitzstein e6.5.1)\n",
        "- Poisson-binomial (Blitzstein 4.8.2): if $X‚àº\\Pois(Œª_1)$ and $Y‚àº\\Pois(Œª_2)$ are independent, then $(X|X+Y=n)‚àº\\Binom\\left(n,\\frac{Œª_1}{Œª_1+Œª_2}\\right)$\n",
        "  - $P(X=x|X+Y=n)=\\frac{P(X=x,Y=n-x)}{P(X+Y=n)}$\n",
        "  $=\\frac{P(X=x)P(Y=n-x)}{P(X+Y=n)}$\n",
        "  $=\\frac{e^{-Œª_1}Œª_1^x}{x!}\\frac{e^{-Œª_2}Œª_2^{n-x}}{(n-x)!}\\frac{n!}{e^{-Œª_1+Œª_2}(Œª_1+Œª_2)^n}$\n",
        "  $=\\binom{n}{x}\\frac{Œª_1^xŒª_2^{n-x}}{(Œª_1+Œª_2)^n}$\n",
        "  $=\\binom{n}{x}(\\frac{Œª_1}{Œª_1+Œª_2})^x(\\frac{Œª_2}{Œª_1+Œª_2})^{n-x}$\n",
        "- Poisson-binomial (Blitzstein 7.1.9 Chicken-Egg): $N‚àº\\Pois(Œª)$, $X+Y=N$, $(X|N=n)‚àº\\Binom(n,p)$, and $(Y|N=n)‚àº\\Binom(n,q)$.\n",
        "$P(X=i,Y=j)=P(X=i|N=n)P(N=n)$\n",
        "$=\\binom{n}{i}p^iq^j\\frac{e^{-Œª}Œª^{i+j}}{(i+j)!}$\n",
        "$=\\frac{e^{-Œªp}(Œªp)^i}{i!}\\frac{e^{-Œªq}(Œªq)^j}{j!}$.\n",
        "  - $X‚àº\\Pois(Œªp)$, $Y‚àº\\Pois(Œªq)$, and $X$ and $Y$ are independent iff $N=X+Y‚àº\\Pois(Œª)$ and $(X|N=n)‚àº\\Binom(n,p)$."
      ],
      "metadata": {
        "id": "sy7b03fKugFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ross Chapter 4 (Random variables):\n",
        "- e4.1e: $k$ types of coupons independently and equally likely to be collected. Let $C_k$ be the number of coupons collected while achieving $k$ types, and $A_j$ be having no type $j$ among the first $n$ collected coupons.\n",
        "$$\\small\\begin{aligned}\n",
        "P(C_k>n)&=\\left(\\bigcup_{i=1}^{k}A_i\\right) \\\\\n",
        "&=\\sum_{i=1}^{k}P(A_i)-\\sum_{i<j}P(A_iA_j)+...+(-1)^{k+1}P(A_1...A_k) \\\\\n",
        "P(A_i)&=\\left(\\frac{k-1}{k}\\right)^n,\\quad P(A_iA_j)=\\left(\\frac{k-2}{k}\\right)^n \\\\\n",
        "P(C_k>n)&=k\\left(\\frac{k-1}{k}\\right)^n-\\binom{k}{2}\\left(\\frac{k-2}{k}\\right)^n+...+(-1)^kk\\left(\\frac{1}{k}\\right)^n \\\\\n",
        "&=\\sum_{i=1}^{k-1}(-1)^{i+1}\\binom{k}{i}\\left(\\frac{k-i}{k}\\right)^n\n",
        "\\end{aligned}$$\n",
        "What is the probability that $t$ types are achieved after $n$ coupons are collected? Let $T_n$ be the number of types achieved after collecting $n$ coupons.\n",
        "$$\\small\\begin{aligned}\n",
        "P(T_n=t)&=P(T_n=t|C_t‚â§n)P(C_t‚â§n) \\\\\n",
        "&=\\binom{k}{t}\\left(\\frac{t}{k}\\right)^n\\left[1-\\sum_{i=1}^{t-1}(-1)^{i+1}\\binom{t}{i}\\left(\\frac{t-i}{t}\\right)^n\\right]\n",
        "\\end{aligned}$$\n",
        "\n",
        "- e2.5i: $n$ people equally likely to have any of the 365 days as birthday. What is the smallest $n$ such that the probability of shared birthday is less than 1/2? $\\frac{365!}{(365-n)!365^n}$ There are $\\binom{n}{2}$ pairs, each with chance of having the same birthday with probability $1/365$. Let $X(Œª=\\frac{n(n-1)}{2*365})$ be number of pairs sharing birthdays, then $P(X=0)=\\t{exp}\\left(-\\frac{n(n-1)}{2*365}\\right)‚â§\\frac{1}{2}$ calculates to be $n‚â•23$.\n",
        "\n",
        "- e3.2f: N men throw their hats in a ring and each then randomly picks up a hat. What is the probability that exactly k men pick up their hat? Let $E_i$ be $i$th man picking up his own hat then $P(E_i)=1/n$ and $P(E_i|E_j)=1/(n-1)$. When $n$ is large, the dependency is weak. Let $X(Œª=np=1)$ be the number of men picking up their own hat, then $P(X=k)=e^{-Œª}\\frac{Œª^k}{k!}=\\frac{1}{ek!}$.\n",
        "\n",
        "- e4.7d: Independent trials of a coin flipped repeatedly has probability $p$ of Heads. What is the probability of $k$ consecutive heads?\n",
        "  - A naive try let $E_i$ be $k$ heads starting from $i$th flip with probability $p^k$. However, the dependencies are not weak: $P(E_2|E_1)=p>P(E_2)$.\n",
        "  - Instead let $E_i$ be $k$ heads followed by 1 tails starting from $i$th flip for $i‚â§n-k$, and let $E_{n-k+1}$ be all heads. Let $X$ be the number of $E_i$ occurring. $\\E[X]=\\suml_{i=1}^{n-k+1}P(E_i)=(n-k)p^k(1-p)+p^k$. Then using Poisson paradigm $P(X=0)‚âà\\t{exp}\\left(-(n-k)p^k(1-p)-p^k\\right)$.\n",
        "  - Let $L_n$ be the longest string of consecutive heads in $n$ flips. The exact expression is found via inclusion-exclusion, by counting $\\{E_{i_1}...E_{i..r}\\}$ combinations without sequential overlaps.\n",
        "  \n",
        "$$\\small\\begin{aligned}\n",
        "  P(L_n‚â•k)&=P\\left(\\bigcup_{i=1}^{n-k+1}E_i\\right)=\\sum_{r=1}^{n-k+1}(-1)^{r+1}\\sum_{i_1\\lt...\\lt i_r}P(E_{i_1}...E_{i_r}) \\\\\n",
        "  \\sum_{i_1\\lt...\\lt i_r\\lt n-k+1}P(E_{i_1}...E_{i_r})&=\\binom{n-rk}{r}p^{kr}(1-p)^r,\\quad\\t{middle sequences} \\\\\n",
        "  \\sum_{i_1\\lt...\\lt i_{r-1}\\lt n-k+1}P(E_{i_1}...E_{i_{r-1}}E_{n-k+1})&=\\binom{n-rk}{r-1}p^{kr}(1-p)^{r-1},\\quad\\t{ending sequence} \\\\\n",
        "  P(L_n‚â•k)&=\\sum_{r=1}^{n-k+1}(-1)^{r+1}\\left[\\binom{n-rk}{r}+\\frac{1}{p}\\binom{n-rk}{r-1}\\right]p^{kr}(1-p)^r\n",
        "  \\end{aligned}$$\n",
        "  - Recursively, let $A_n$ be there exists $k$ consecutive heads in $n$ flips, $F_j$ be the first tail appearing on $j$th flip for $j=1...k$ and $H$ be all first $k$ flips are heads where $F_1,...,F_k,H$ are mutually exclusive.\n",
        "  $$\\small\\begin{aligned}\n",
        "  P(A_n)&=\\sum_{j=1}^{k}P(A_n|F_j)P(F_j)+P(A_n|H)P(H) \\\\\n",
        "  &=\\sum_{j=1}^{k}P(A_{n-j})p^{j-1}(1-p)+p^k \\\\\n",
        "  \\end{aligned}$$\n",
        "\n",
        "- e4.8d (Ross e3.4j): independent trials each with success probability $p$ are performed. What is the probability of $r$ successes before $m$ failures? Using negative binomial, $r$ successes occur before $m$ failures iff the $r$th success occurs by the $(r+m-1)$th trial.\n",
        "$$\\small P(X‚â§r+m-1)=\\sum_{n=r}^{r+m-1}\\binom{n-1}{r-1}p^r(1-p)^{n-r}$$\n",
        "Using binomial, $r$ successes occur before $m$ failures iff at least $r$ successes occur in the first $(r+m-1)$ trials.\n",
        "$$\\small P(X‚â•r)=\\sum_{k=r}^{r+m-1}\\binom{r+m-1}{k}p^k(1-p)^{r+m-1-k}$$\n",
        "\n",
        "- e4.8e (Banach match problem): A man has 2 matchboxes each initially containing $N$ matches. When he takes a match it is equally likely from either matchbox. When he notices one of the matchboxes is empty, what is the probability that there are exactly $k$ matches in the other box? Box1 becomes empty after $N$ accesses, but the problem assumes he doesn't know the box is empty yet (p4.77) until the next access. At the $(N+1)$th access on box1 is when it no longer has a match to give. Total accesses on both boxes is $(N+1)+(N-k)=2N-k+1$. Therefore $P(X‚àº\\t{nbinom}(N+1,\\frac{1}{2})=2N-k+1)=2\\binom{2N-k}{N}(\\frac{1}{2})^{N+1}(\\frac{1}{2})^{N-k}=\\binom{2N-k}{N}\\frac{1}{2^{2N-k}}$\n",
        "\n",
        "- e4.8h: an unknown number $N$ of animals inhabit in a region. $m$ of them were caught, marked, and released. A new catch $n$ is made and the number of marked animals is observed to be $i$. A reasonable estimate of $N$ would be the value of $N$ that maximizes $P_i(N)=P(X=i)$. The maximum likelihood estimate $N_{max}$ is found by noting $\\frac{P_i(N)}{P_i(N-1)}=\\frac{(N-m)(N-n)}{N(N-m-n+i)}$ and $P_i(N)‚â•P_i(N-1)$ at the peak, which calculates to be $N_{max}‚â§\\frac{mn}{i}$.\n",
        "\n",
        "- e4.8i: Components are bought in lots of size 10. 3 random samples are inspected from each lot before it is accepted. If 30% of lots have 4 defects and 70% have 1 defect, what proportion of lots are rejected? Let $A$ be lot accepted. $P(A)=P(A|4)P(4)+P(A|1)P(1)=\\frac{\\binom{6}{3}}{\\binom{10}{3}}(0.3)+\\frac{\\binom{9}{3}}{\\binom{10}{3}}(0.7)$\n",
        "\n",
        "- 4.26: One integer is randomly chosen from [1,10]. If you try to guess the number one at a time, how many questions do you expect to ask? Let $X$ be number of questions asked. $P(X=1)=\\frac{1}{10}$, $P(X=2)=(1-\\frac{1}{10})\\frac{1}{9}=\\frac{1}{10}$, $P(X=3)=(1-\\frac{1}{10})(1-\\frac{1}{9})\\frac{1}{8}=\\frac{1}{10}$, ... $\\E[X]=\\suml_{i=1}^{10}i\\frac{1}{10}=5.5$\n",
        "\n",
        "- 4.30: (St Petersburg paradox) A fair coin is tossed until Tails appears on the $n$th toss, and you get paid $2^n$. Let $X$ be payout. $\\E[X]=\\suml_{n=1}^{‚àû}X(n)P(n)=\\suml_{n=1}^{‚àû}2^n(\\frac{1}{2})^{n-1}(\\frac{1}{2})=‚àû$\n",
        "\n",
        "- 4.31: meteorologists reports $p$ chance of rain. He receives score $1-(1-p)^2$ if it rains and $1-p^2$ if it doesn't. If he really believes it will rain with probability $\\hat{p}$, what $p$ should he report to maximize his score? Let $X$ be score.\n",
        "$$\\small\\begin{aligned}\n",
        "\\E[X]&=[1-(1-p)^2]\\hat{p}+(1-p^2)(1-\\hat{p})=\\hat{p}-(1-p)^2\\hat{p}+(1-\\hat{p})-(1-\\hat{p})p^2\\\\\n",
        "\\small{\\frac{‚àÇ\\E[X]}{‚àÇp}}&=0=2\\hat{p}(1-p)-2(1-\\hat{p})p\\quad‚áíp=\\hat{p}\n",
        "\\end{aligned}$$\n",
        "\n",
        "- 4.33: Paperboy buys papers and then sells them. His demand is binomial $n=10$ and $p=1/3$. How many copies should he buy to maximize profits? Let $X$ be profit in copies from buying $b$ copies and selling $s$ copies. $X=\\min(b,s)-b$. $\\E[X]=\\suml_{i=0}^{n}(\\min(b,i)-b)\\binom{n}{i}p^i(1-p)^{n-i}=\\suml_{i=0}^{n}\\min(b,i)\\binom{n}{i}p^i(1-p)^{n-i}-b$\n",
        "\n",
        "- 4.46: It takes ‚â•9 of 12 jury members to convict a defendant. Each juror acts independently, has 0.1 probability to convict innocent and 0.2 probability to absolve guilty. If 65% of defendants are guilty, what is the probability that the jury renders a correct decision? This problem needs to be tackled by finding conditional probabilities of correctness for the jury. Let $E$ be jury correct, $G$ be guilty defendant. $P(E|G)=\\suml_{i=9}^{12}\\binom{12}{i}(0.8)^i(0.2)^{12-i}=0.795$, $P(E|G^C)=\\suml_{i=9}^{12}\\binom{12}{i}(0.9)^i(0.1)^{12-i}=0.999$, and $P(E)=0.866$. It is incorrect to tackle the problem through the total probability of correctness for each juror, because juror independence is conditional. Incorrectly $P(E)=\\suml_{i=9}^{12}\\binom{12}{i}(0.835)^i(0.165)^{12-i}=0.878$\n",
        "\n",
        "- 4.68: 500 ABMs are launched against 10 missiles. Each ABM independently targets a missile with equal probability, and hits its target with probability 0.1. What is the probability that all missiles are hit? ABM $j$ hits missile $k$ with probability $(0.1)^2=0.01$. Missile $k$ is hit by an ABM with probability $1-\\Pois(i=0,Œª=500*0.01)=0.993$. The probability that all missiles are hit is $0.993^{10}=0.935$.\n",
        "\n",
        "- 4.73: In a best of 7, two teams each have a probability 0.5 of winning a game. What's the expected number of games played? nbinom refers to 1 team winning. $\\small{(4)(2)(0.5)^4+(5)(2)\\binom{4}{3}(0.5)^5+(6)(2)\\binom{5}{3}(0.5)^6+(7)(2)\\binom{6}{3}(0.5)^7=5.8}$\n",
        "\n",
        "- 4.84: $n$=10 balls are put in $k$=5 boxes, with each ball independently being placed in box $i$ with probability $p_i$ such that $\\suml_{i=1}^kp_i=1$. Let $I_i$ be  indicator for box $i$ having 0 ball occuring with probability $\\E[I_i]=(1-p_i)^n$. Let $N$ be the number of boxes with 0 ball, then $\\E[N]=\\suml_{i=1}^k\\E[I_i]=\\suml_{i=1}^k(1-p_i)^{n}$. The expected number of boxes with 1 ball is $\\E[N]=\\suml_{i=1}^kp_i(1-p_i)^{n-1}$.\n",
        "\n",
        "- 4.85: $k$ types of coupons. Each coupon is collected as type $i$ independently with probability $p_i$ such that $\\suml_{i=1}^kp_i=1$. What is the expected number of types achieved after $n$ coupons are collected? Let $I_i$ be indicator for type $i$ collected with probability $\\E[I_i]=1-(1-p_j)^n$. Let $K$ be the number of types achieved after $n$ coupons are collected, then $\\E[K]=\\suml_{i=1}^k\\left[1-(1-p_i)^n\\right]=k-\\suml_{i=1}^k(1-p_i)^n$\n",
        "\n",
        "Ross Chapter 7 (Properties of expectation):\n",
        "\n",
        "- e7.2e: Let $X‚àº\\Binom(n,p)$, let $I_i$ be indicator for trial $i$, then\n",
        "$\\E[X]=\\E[I_1]+...+\\E[I_n]=np$.\n",
        "\n",
        "- e7.2f: Let $X‚àº\\t{nbinom}(r,p)$ be the number of trials to reach $r$ successes. Let $X_i$ be the number of trials to reach 1 success then\n",
        "$\\E[X]=\\E[X_1]+...+\\E[X_r]=\\frac{r}{p}$.\n",
        "\n",
        "- e7.2g: let $X‚àº\\HGeom(N,m,n)$ be the number of white balls in $n$-sample. Let $I_i$ be indicator for ball $i$ being white.\n",
        "$\\E[X]=\\E[I_1]+...+\\E[I_n]=n\\frac{m}{N}$.\n",
        "\n",
        "- e7.2j: $k=10$ hunters shoot at a flock of $n=10$ ducks flying overhead, each randomly choosing a target and hitting it with probability $p$. What is the expected number of unhurt ducks?\n",
        "Let $I_i$ be duck $i$ escaping. Duck $i$ escapes hunter $j$ with probability $1-\\frac{p}{n}$ and therefore $\\E[I_i]=(1-\\frac{p}{n})^k$ and $\\E[N]=n(1-\\frac{p}{n})^k$.\n",
        "\n",
        "- 7.9: $n$ balls labeled 1-n are placed into $n$ urns also labeled 1-n such that ball $i$ has equal chance of being placed in urn $1,...,i$. What is the expected number of empty urns?\n",
        "Let $I_i$ be indicator for empty urn $i$.\n",
        "$\\E[I_i]=\\frac{i-1}{i}\\frac{i}{i+1}...\\frac{n-1}{n}$\n",
        "$=\\frac{i-1}{n}$.\n",
        "Then $\\E[N]=\\frac{1}{n}\\suml_{i=1}^n(i-1)$\n",
        "$=\\frac{1}{n}\\suml_{i=1}^{n-1}i$\n",
        "$=\\frac{n-1}{2}$\n",
        "What is the probability that no urn is empty?\n",
        "No urn is empty iff ball $i$ falls into urn $i$ for all balls. The probability of this happening for ball $i$ is $\\frac{1}{i}$. The probability that no urn is empty is therefore $\\frac{1}{1}\\frac{1}{2}...\\frac{1}{n}=\\frac{1}{n!}$.\n",
        "\n",
        "- 7.13: A set of 1000 cards numbered 1-1000 are distributed to 1000 people with each receiving 1 card. What is the expected number of cards distributed to people whose age matches the card number?\n",
        "Let $I_i$ be indicator for person $i$ having the right card, and $N=I_1+...+I_{1000}$. Then $\\E[I_i]=1/1000$ and $\\E[N]=1$.\n",
        "In reverse if we let $I_i$ be indicator for card $i$ being in the hand of the right person, then $\\E[I_i]$ is higher for lower $i$s but they're 0 for higher $i$s. It averages out to the same result.\n",
        "\n",
        "- 7.15: $n$ people throw their hats in a ring and randomly picks up a hat. Find the expected number of matched pairs where $i$ and $j$ pick up each other's hat.\n",
        "Let $I_{ij}$ be indicator that pair $i,j$ is a match.\n",
        "$\\E[I_{ij}]=\\frac{1}{n(n-1)}$.\n",
        "$\\E[N]=\\suml_{i\\lt j}\\E[I_{ij}]$\n",
        "$=\\binom{n}{2}\\frac{1}{n(n-1)}=\\frac{1}{2}$.\n",
        "\n",
        "- 7.19: There are $r$ distinct types of insect each caught with probability $p_1+...+p_r=1$.\n",
        "What is the expected number of insects caught before the first type 1?\n",
        "$\\frac{1}{p_1}$.\n",
        "What is the expected number of types caught before the first type 1?\n",
        "Let $I_i$ be indicator for catching type $i$ before type 1.\n",
        "$\\E[I_i]=\\frac{p_i}{p_1+p_i}$ using the \"race\" reasoning where $i$ needs to appear before 1 in permutations.\n",
        "$\\E[N]=\\suml_{i=2}^r\\frac{p_i}{p_1+p_i}$.\n",
        "\n",
        "- 7.20: $n$ balls each with weight $W(1),...,W(n)$ are being removed from an urn, each with probability equal to its weight over weight of all balls still in the urn. What is the expected number balls removed before ball 1 is removed?\n",
        "Let $I_i$ be indicator that ball $i$ was removed before ball 1. $\\E[I_i]=\\frac{W(i)}{W(i)+W(1)}$.\n",
        "Therefore $\\E[N]=\\suml_{i=2}^n\\frac{W(i)}{W(i)+W(1)}$.\n",
        "\n",
        "- 7.21: for a group of 100 people, compute the number of days of the year that are birthdays of exactly 3 people.\n",
        "Let $I_i$ be indicator for $i$th day of the year being birthdays of exactly 3 people.\n",
        "$\\E[I_i]=\\binom{100}{3}(\\frac{1}{365})^3(\\frac{364}{365})^{97}$.\n",
        "\n",
        "- 7.22: How many times would you expect to roll a fair die before all 6 sides appear at least once?\n",
        "Let $X_i$ be the number of rolls until the $i$th \"new\" side comes up. $\\E[X_i]‚àº\\Geom(\\frac{6-i+1}{6})$.\n",
        "Then $\\E[X_i]=\\frac{6}{6+1-i}$ and $\\E[X]=\\frac{6}{6}+\\frac{6}{5}+...+\\frac{6}{1}$.\n",
        "\n",
        "- 7.23: urn 1 contains 5 white and 6 black balls, urn 2 contains 8 white and 10 black balls. Two balls are selected from urn 1 and placed in urn 2, then three balls are selected from urn 2. What is the expected number of white balls in the three?\n",
        "Let $I_i$ be indicator for white ball $i$ from urn 1 being included, and $J_j$ be indicator for white ball $j$ from urn 2 being included in the trio.\n",
        "$\\E[I_i]=\\frac{2}{11}\\frac{3}{20}$.\n",
        "$\\E[J_j]=\\frac{3}{20}$.\n",
        "$\\E[N]=5\\E[I_i]+8\\E[J_j]$.\n",
        "\n",
        "- 7.24: A bottle initially contains $m$ large pills and $n$ small pills. Each day a pill is randomly chosen from the bottle. If a small pill is chosen it is consumed. If large pill is chosen then half is consumed and the other is returned to the bottle as a small pill. Find $\\E[X]$ where $X$ is the remaining pills after the last large pill had been consumed.\n",
        "Let $I_i$ be indicator that the $i$th of n small pills survives, and $J_j$ be indicator that the $j$th of m half-large pills survives, such that $X=I_1+...+I_n+J_1+...+J_m$. Using the \"race\" reasoning, small pill $i$ is in the final X if it is consumed after $m$ large pills with probability $\\E[I_i]=\\frac{1}{m+1}$. To be in X, half pill $j=1$ needs to survive $m-1$ remaining large pills with probability $\\E[J_1]=\\frac{1}{m}$. Half pill $j=2$ needs to survive $m-2$ large pills with probability $\\E[J_2]=\\frac{1}{m-1}$. Extending this pattern forms a harmonic series. $\\E[X]=\\frac{n}{m+1}+\\suml_{j=1}^m\\frac{1}{j}$.\n",
        "  - The problem can be approached through recurrence as well where $X_{m,n}$ be the number of pills after all large pills have been consumed if we start with $m$ large pills and $n$ small pills, and $A$ be event that large pill was chosen.\n",
        "  $\\E[X_{m,n}]=\\E[X_{m,n}|A]P(A)+\\E[X_{m,n}|A^c]P(A^c)$\n",
        "  $=\\frac{m}{m+n}\\E[X_{m-1,n+1}]+\\frac{n}{m+n}\\E[X_{m,n-1}]$. Boundary coundition $\\E[X_{0,n}]=n$, $\\E[X_{1,0}]=1$. Expanding out the terms would demonstrate the harmonic series, but it's tedious.\n",
        "\n",
        "- 7.35: 52-deck well shuffled. Compute expected number of cards turned face up to obtain 2 aces. $X‚àº\\t{nbinom}(2,\\frac{1}{13})$. $\\E[X]=\\frac{r}{p}=26$."
      ],
      "metadata": {
        "id": "dakWMIH6pK5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blitzstein Chapter 3 (Random variables and their distributions):\n",
        "\n",
        "- e3.7.2 (Random walk): Starting at 0, a particle moves $n$ independent 1-unit left/right steps on a number line with equal probabilities. Let $Y$ be the particle's final position and $X$ be the number of right steps, then $Y$ is the number of right steps minus number of left steps $Y=X-(n-X)=2X-n$. $P(Y=k)=P(X=\\frac{k+n}{2})=\\binom{n}{\\frac{k+n}{2}}\\left(\\frac{1}{2}\\right)^n$\n",
        "  - e3.7.4: Let $D$ be the particle's final distance from 0. Then $D=|Y|=|2X-n|$. $P(D=k)=P(Y=k)+P(Y=-k)=2\\binom{n}{\\frac{k+n}{2}}\\left(\\frac{1}{2}\\right)^n$\n",
        "\n",
        "- 3.25: A fair coin is flipped $n$ times to produce $X‚àº\\Binom(n,0.5)$ Heads. Another fair coin is flipped $n+1$ times to produce $Y‚àº\\Binom(n+1,0.5)$ Heads. What is $P(X\\lt Y)$? Those $n$ flips also produce $n-X‚àº\\Binom(n,0.5)$ Tails, and those $n+1$ flips also produce $n+1-Y‚àº\\Binom(n+1,0.5)$ Tails. Then\n",
        "$$\\begin{aligned}\n",
        "P(X\\lt Y)&=P(n-X\\lt n+1-Y)\n",
        "=P(X+1\\gt Y)\\\\\n",
        "&=P(X‚â•Y)\n",
        "=P((X\\lt Y)^C)\n",
        "\\end{aligned}$$\n",
        "Therefore $P(X\\lt Y)=0.5$.\n",
        "\n",
        "Blitzstein Chapter 4 (Expectation):\n",
        "\n",
        "- e4.3.12: coupons are collected randomly, independently, and equally likely from $k$ types. Let $N$ be the number of coupons collected to achieve complete set, $N_i$ be number of coupons collected until encountering the next uncollected type such that $N=N_1+...+N_k$. Then $N_i‚àº\\Geom(\\frac{k-i+1}{k})$. Then $\\E[N]=\\suml_{i=1}^{k}\\E[N_i]=\\suml_{i=1}^{k}\\frac{k}{k-i+1}=\\suml_{i=1}^{k}\\frac{k}{i}$.\n",
        "\n",
        "- e4.3.14 (St Petersburg paradox): Repeatedly flip a fair coin. The game ends when it lands Heads. Receive $X=2^N$ payoff if the game lasts $N$ rounds. $\\E[X]=\\suml_{n=1}^{‚àû}\\frac{2^n}{2^n}=‚àû$. $N‚àº\\Geom(\\frac{1}{2})$, therefore $\\E[N]=2$ rounds. $\\E[X]‚â†2^{\\E[N]}$. \\E[g(X)]‚â†g(\\E[X]) when g(X) is not linear.\n",
        "\n",
        "- e4.4.4 (Montmort e1.6.4): well shuffled $n$ cards labeled $[1,n]$. A card is a match if its label matches its position. Let $X$ be the number of matches, $I_i$ be indicator that card $i$ is a match. Prior probability $\\E[I_i]=1/n$. Even though $I_i$ and $I_j$ are dependent, linearity still holds that $X=\\sum_iI_i$, and therefore $\\E[X]=\\sum_i\\E[I_i]=1$.\n",
        "\n",
        "- e4.4.5 (birthdays e1.4.10): What is the expected number of distinct birthdays in a group of $n$ people? Let $N$ be the number of distinct birthdays, and $I_i$ be indicator for the $i$th day being represented.\n",
        "$$\\E[I_i]=1-\\left(\\frac{364}{365}\\right)^n \\quad \\E[N]=\\sum_{i=1}^{365}\\E[I_i]=365\\left(1-\\left(\\frac{364}{365}\\right)^n\\right)$$\n",
        "What is the expected number of pairs who share the same birthday? Let $M$ be the number of matches, and $J_j$ be indicator for the $j$th pair having the same birthday.\n",
        "$$\\E[J_j]=\\frac{1}{365} \\quad \\E[M]=\\sum_{j=1}^{\\binom{n}{2}}\\frac{1}{365}=\\frac{\\binom{n}{2}}{365}$$\n",
        "(p36) Let $I_{ij}$ be indicator that person $i$ and $j$ have the same birthday. $I_{12}$ and $I_{13}$ are pairwise independent: knowing that $1$ and $2$ have the same birthday gives no clue whether $1$ and $3$ have the same birthday. However $I_{ij}$ are not independent, because knowing $I_{12}=1$ and $I_{13}=1$ then we know $I_{23}=1$ must also be true.\n",
        "\n",
        "- e4.4.6: A permutation of $1,...,n$ has a local maximum $a_i$ if it is larger than its neighbors $a_{i-1}$ and/or $a_{i+1}$. E.g., 4,2,5,3,6,1 has 3 local maxima. For $n‚â•2$ what is the average number of local maxima of a random and equally likely permutation of $1,...,n$? Let $N$ be the number of local maxima, and let $I_i$ be indicator for local maximum at position $i$. $I_i$ counts 2 of the 3! arrangements of 3 numbers where $a_i$ is the largest (Symmetry of iid, Blitzstein 5.7)\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\E[I_i]=\\BC\n",
        "\\frac{1}{2}, &i=1,n\\\\\n",
        "\\frac{2}{6}, &1\\lt i\\lt n\n",
        "\\EC\n",
        "\\qquad\n",
        "\\E[N]=\\frac{1}{2}+\\frac{n-2}{3}+\\frac{1}{2}=\\frac{n+1}{3}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "- 4.25: (2.50) C and H play a series of games where C has probability of winning $p$. The first player to win 2 games more than his opponent wins. What is the expected number of games played?\n",
        "The probability that someone wins in a pair of games is $p_2=p^2+(1-p)^2$, then the number of games played $Y‚àº2\\Geom(p_2)$. $\\E[Y]=\\frac{2}{p^2+(1-p)^2}$\n",
        "\n",
        "- 4.37: $m=20$ tokens are distributed to $n=20$ students randomly and independently.\n",
        "Find the expected total number of tokens that the firsts three students get. Let $N_i$ be the number of tokens received by student $i$, and $I_j$ be indicator for token $j$ being distributed to student $i$. $\\E[I_j]=\\frac{1}{n}$ and $\\E[N_i]=\\suml_{j=1}^{m}\\E[I_j]=\\frac{m}{n}$, then $3\\E[N_i]=3$.\n",
        "Find the expected number of students who get at least one token. Let $J_i$ be indicator for student $i$ having at least 1 token, $X$ be the number of students having at least 1 token. $\\E[J_i]=1-\\left(\\frac{n-1}{n}\\right)^m$ and $\\E[X]=\\suml_{i=1}^{n}\\E[J_i]=n\\left(1-\\left(\\frac{n-1}{n}\\right)^m\\right)$\n",
        "\n",
        "- 4.38, 4.68: (Ross e2.5m, e3.2f) $n$ people each put their names on a slip of paper and they're shuffled in a hat, then each person randomly draws a slip. What is the expected number of people who draw their own names? Let $N$ be the number of people who draw their own names, and $I_i$ be indicator that person $i$ draws his own name. $\\E[I_i]=\\frac{1}{n}$, and $\\E[N]=1$. By Poisson paradigm, $N‚àº\\Pois(\\E[N])$, and $P(N=0)‚âà\\frac{e^{-1}}{0!}$.\n",
        "\n",
        "- 4.44: There are $n=100$ shoelaces in a box. At each step two random ends are tied together, either resulting in a loop or a longer shoelace. What is the expected number of loops after there are no more ends? At each step the number of ends is reduced by 2 and the number of unlooped pieces is reduced by 1.\n",
        "Let $L$ be the number of loops, and $I_i$ be indicator that a loop was closed when there are $i$ unlooped pieces. Then there are $2i$ ends so $\\E[I_i]=\\frac{i}{\\binom{2i}{2}}=\\frac{1}{2i-1}$ and $\\E[L]=\\suml_{i=1}^{n}\\frac{1}{2i-1}$.\n",
        "\n",
        "- 4.46: A 52 deck is drawn from one card at a time without replacement. What is the expected number of non-Ace cards that appear before the first Ace?\n",
        "Let $N$ be the number of such cards, and $I_i$ be indicator that non-Ace card $i$ appears before the first Ace. $I_i$ is a permutation of non-Ace $i$ and 4 Aces: $\\E[I_i]=\\frac{1}{5}$, therefore $\\E[N]=\\frac{48}{5}$.\n",
        "\n",
        "- 4.50: Ten chords of a circle are chosen randomly independently. A chord is a line between two points on a circle. How many pairs of chords are expected to intersect? For every 4 points on a circle, there are $\\frac{4!}{2!2^2}=3$ possible pairing of chords, and 1 of those pairs intersect the two chords. Let $N$ be number of intersections, $I_i$ be indicator that pair $i$ intersects. $\\E[I_i]=\\frac{1}{3}$ and $\\E[N]=\\sum_{i=1}^{\\binom{10}{2}}\\frac{1}{3}=15$\n",
        "\n",
        "- 4.51: hashtable of size $n$ is used to uniformly randomly store $k$ values. Find the expected number of entries with no value stored. $\\E[I_i]=\\left(\\frac{n-1}{n}\\right)^k$ and $\\E[N]=n\\left(\\frac{n-1}{n}\\right)^k$\n",
        "\n",
        "- 4.52: A coin with probability $p$ of H is flipped $n$ times. Find the number of contiguous runs. Let $I_i$ be indicator that flip $i$ produced a different outcome than $i-1$, then $\\E[I_i]=P(H_i|T_{i-1})P(T_{i-1})+P(T_i|H_{i-1})P(H_{i-1})=2p(1-p)$ and $\\E[N]=1+\\suml_{i=2}^{n}2p(1-p)=1+2(n-1)p(1-p)$\n",
        "\n",
        "- 4.67: (Ch1 p52) $n=100$ students who have two courses in the same room. What is the probability that no one has the same seat for both courses? Let $N$ be the number of students who took same seat. Exact probability is $P(N=0)=\\frac{!n}{n!}=\\suml_{i=1}^n\\frac{(-1)^i}{i!}‚âà\\frac{1}{e}$ Let $I_i$ be student $i$ taking the same seat. $\\E[I_i]=\\frac{1}{n}$. By Poisson paradigm, $N‚àº\\Pois(n\\E[I_i])$, and therefore $P(N=0)‚âà\\frac{e^{-1}}{0!}$.\n",
        "\n",
        "- 4.69: (Ch1 p26) a sample size $k=$ 1000 is chosen with replacement and equal probabilities in a city with $n=$ 1 million people. What is the probability that at least one person will get chosen more than once? Let $N$ be the number of repeats. $P(N=0)=\\frac{n(n-1)(n-2)...(n-k+1)}{n^k}‚âàe^{-\\suml_{i=1}^{k-1}\\frac{i}{n}}=e^{-\\frac{k(k-1)}{2n}}$. Let $I_{ij}$ be sample $i$ and $j$ are the same, then $\\E[I_{ij}]=\\frac{1}{n}$ and $\\E[N]=\\sum_{i=1}^{\\binom{k}{2}}\\E[I_{ij}]=\\frac{\\binom{k}{2}}{n}=\\frac{k(k-1)}{2n}$. Then by Poisson paradigm $N‚àº\\Pois(\\E[N])$."
      ],
      "metadata": {
        "id": "r4FwB1aOlCMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous random variables"
      ],
      "metadata": {
        "id": "wOFd_NGgMHGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculus background\n",
        "- **Fundamental theorem of calculus, part 1**: if $f$ is continuous on $[a,b]$, then function $g=‚à´_a^xf(t)\\ dt,\\ a‚â§x‚â§b$ is continuous and differentiable on $[a,b]$, and $g'(x)=f(x)$. This theorem is written another way using Leibniz notation: $\\frac{d}{dx}‚à´_{a}^{x}f(t)\\ dt=f(x)$ (Stewart Calculus 9ed page 400)\n",
        "  - Leibniz's rule: $\\frac{d}{dt}‚à´_{a(t)}^{b(t)}g(x,t)\\ dx=g(b(t),t)\\frac{d}{dt}b(t)-g(a(t),t)\\frac{d}{dt}a(t)+‚à´_{a(t)}^{b(t)}\\frac{‚àÇ}{‚àÇt}g(x,t)\\ dx$\n",
        "  - Leibniz's rule: $\\frac{d}{dŒ∏}‚à´_{a(Œ∏)}^{b(Œ∏)}f(x,Œ∏)\\ dx=f(b(Œ∏),Œ∏)\\frac{d}{dŒ∏}b(Œ∏)-f(a(Œ∏),Œ∏)\\frac{d}{dŒ∏}a(Œ∏)+‚à´_{a(Œ∏)}^{b(Œ∏)}\\frac{‚àÇ}{‚àÇŒ∏}f(x,Œ∏)\\ dx$\n",
        "- **Integration by parts**: $\\frac{d}{dx}[f(x)g(x)]=f(x)g'(x)+g(x)f'(x)$ then $f(x)g(x)=‚à´f(x)g'(x)\\ dx+‚à´g(x)f'(x)\\ dx$ or $\\int_a^bf(x)g'(x)\\ dx=[f(x)g(x)]_a^b-\\int_a^bg(x)f'(x)\\ dx$(Stewart Calculus 9ed page 486)\n",
        "  - Let $u=f(x)$, $v=g(x)$, $du=f'(x)\\ dx$, and $dv=g'(x)\\ dx$, then $uv=‚à´u\\ dv+‚à´v\\ du$"
      ],
      "metadata": {
        "id": "3dVpb-72_OiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Continuous random variable** $X$: there exists non-negative function $f$ defined for all $x‚àà‚Ñù$ such that for any $B‚äÜ‚Ñù$, $P(X‚ààB)=\\displaystyle‚à´_Bf(x)dx$. $f$ is called the **probability density function** PDF of $X$.\n",
        "- PDF $f$ must satisfy (1) non-negative and (2) $‚à´_{-‚àû}^{‚àû}f(x)dx=1$\n",
        "- CDF $F(b)=P(X‚â§b)=P(X\\lt b)=‚à´_{-‚àû}^bf(x)\\ dx$. Random variable is continuous if its CDF is differentiable with finitely many end points where the CDF is continuous but not differentiable.\n",
        "- $P(a‚â§X‚â§b)=‚à´_a^bf(x)\\ dx=F(b)-F(a)$\n",
        "- $\\E[X]=‚à´_{-‚àû}^{‚àû}xf(x)\\ dx$\n",
        "  - By integrating survival functions (Ross Ch5 t2) $\\E[Y]=‚à´_0^‚àûP(Y>y)\\ dy-‚à´_0^‚àûP(Y\\lt -y)\\ dy$\n",
        "\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "&‚à´_0^‚àûP(Y>y)\\ dy \\\\\n",
        "=&‚à´_{y=0}^‚àû‚à´_{x=y}^‚àûf_Y(x)\\ dx\\ dy \\\\\n",
        "=&‚à´_{x=0}^‚àû‚à´_{y=0}^xdy\\ f_Y(x)\\ dx \\\\\n",
        "=&‚à´_0^‚àûxf_Y(x)\\ dx\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "&‚à´_0^‚àûP(Y\\lt -y)\\ dy \\\\\n",
        "=&‚à´_{y=-‚àû}^0‚à´_{x=-‚àû}^y f_Y(x)\\ dx\\ dy \\\\\n",
        "=&‚à´_{x=-‚àû}^0‚à´_{y=x}^0dy\\ f_Y(x)\\ dx \\\\\n",
        "=&-‚à´_{-‚àû}^0xf_Y(x)\\ dx\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- $\\Var(X)=\\E(X-Œº)^2=\\E[X^2]-\\E[X]^2$\n",
        "  - $\\Var(aX)=a^2\\Var(X)$\n",
        "  - $\\Var(X+Y)=\\Var(X)+\\Var(Y)$ iff $X$ and $Y$ are independent\n",
        "\n",
        "**Function of a random variable**: $Y=g(X),\\ g:‚Ñù‚Üí‚Ñù$\n",
        "- Suppose $g(x)$ is strictly monotonic (increasing or decreasing), differentiable (continuous), then $Y=g(X)$ has CDF $F_Y(y)=F_X(g^{-1}(y))$ and PDF $f_Y(y)=f_X(g^{-1}(y))\\left|\\frac{d}{dx}g^{-1}(y)\\right|$ if $y=g(x)$ for some $x$ (change of variables).\n",
        "  - To find $F_Y$, go to $P(‚ãÖ)$ and arithmetically work backward to $F_X$.  $F_Y(y)=P(Y\\lt y)=P(g(X)\\lt y)=F_X(g^{-1}(y))$ (uniform universality).\n",
        "- LOTUS (Ross Ch5 t3): $\\E[g(X)]=‚à´_{-‚àû}^{‚àû}g(x)f_X(x)\\ dx$\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "&‚à´_0^‚àûP(g(X)>y)\\ dy \\\\\n",
        "=&‚à´_{y=0}^‚àû‚à´_{x:g(x)>y}f_X(x)\\ dx\\ dy \\\\\n",
        "=&‚à´_{x=-‚àû}^‚àû‚à´_{y=0}^‚àûI_{g(x)>y}\\ dy\\ f_X(x)\\ dx \\\\\n",
        "=&‚à´_{-‚àû}^‚àû\\max(g(x),0)\\ f_X(x)\\ dx\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "&‚à´_0^‚àûP(g(X)\\lt -y)\\ dy \\\\\n",
        "=&‚à´_{y=0}^‚àû‚à´_{x:g(x)\\lt -y}f_X(x)\\ dx\\ dy \\\\\n",
        "=&‚à´_{x=-‚àû}^‚àû‚à´_{y=0}^‚àûI_{g(x)\\lt -y}\\ dy\\ f_X(x)\\ dx \\\\\n",
        "=&‚à´_{-‚àû}^‚àû\\min(g(x),0)\\ f_X(x)\\ dx\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "$I_{g(x)>y}=\\BC 1,&g(x)>y \\\\ 0,&g(x)‚â§y\\EC‚áí‚à´_{y=0}^‚àûI_{g(x)>y}\\ dy=\\BC \\int_0^{g(x)}dy,&g(x)>0 \\\\ 0,&g(x)‚â§0\\EC=\\max(g(x),0)$\n"
      ],
      "metadata": {
        "id": "8ZqtFFdyMfUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uniform**: $X‚àº\\Unif(Œ±,Œ≤)$ if its PDF is given by $f(x)=\\BC\n",
        "\\frac{1}{Œ≤-Œ±} &\\ x‚àà[Œ±,Œ≤] \\\\\n",
        "0 &\\t{otherwise}\n",
        "\\EC$ and CDF is given by $F(a)=\\BC\n",
        "0 &a‚â§Œ± \\\\\n",
        "\\frac{a-Œ±}{Œ≤-Œ±} &Œ±\\lt a\\lt Œ≤ \\\\\n",
        "1 &a‚â•Œ≤\n",
        "\\EC$\n",
        "- $\\E[X]=\\frac{Œ±+Œ≤}{2}$, and $\\Var(X)=\\frac{(Œ≤-Œ±)^2}{12}$\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\E[X]&=‚à´_{-‚àû}^‚àûxf(x)\\ dx=\\frac{1}{Œ≤-Œ±}‚à´_{Œ±}^{Œ≤}x\\ dx \\\\\n",
        "&=\\frac{Œ≤^2-Œ±^2}{2(Œ≤-Œ±)}=\\frac{Œ±+Œ≤}{2}\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "\\E[X^2]&=‚à´_{-‚àû}^‚àûx^2f(x)\\ dx=\\frac{1}{Œ≤-Œ±}‚à´_{Œ±}^{Œ≤}x^2\\ dx \\\\\n",
        "&=\\frac{Œ≤^3-Œ±^3}{3(Œ≤-Œ±)}=\\frac{Œ±^2+Œ±Œ≤+Œ≤^2}{3}\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- $U‚àº\\Unif(0,1)$ has support $[0,1]$, PDF $f(x)=1,\\ x‚àà[0,1]$, and CDF $F(a)=a,\\ a‚àà[0,1]$. The probability of an event is the length of its interval on $[0,1]$.\n",
        "\n",
        "**Uniform universality**: Let $F_X$ be a CDF which is continuous and strictly increasing over the support of its distribution such that $F_X^{-1}:[0,1]‚Üí‚Ñù$ exists.\n",
        "1. Let $U‚àº\\Unif(0,1)$, then $F_X^{-1}(U)‚àºX$ is random variable with CDF $F_X$.\n",
        "  - $F_X^{-1}:[0,1]‚Üí‚Ñù$ is a function, then $F_X^{-1}(U)‚àºX$ is a generated random variable with support $‚Ñù$ and CDF $P(F_X^{-1}(U)‚â§a)=P(U‚â§F_X(a))=F_X(a)$\n",
        "  - $\\E[g(X)]=\\E[g(F_X^{-1}(U))]=‚à´_0^1g(F_X^{-1}(u))\\ du$ by LOTUS.\n",
        "2. Let $X$ be random variable with CDF $F_X$, then $F_X(X)‚àº\\Unif(0,1)$.\n",
        "  - $F_X:‚Ñù‚Üí[0,1]$ is a function, then $F_X(X)$ is $\\Unif(0,1)$ with support $(0,1)$ and CDF $P(F_X(X)‚â§a)=P(X‚â§F_X^{-1}(a))=F_X(F_X^{-1}(a))=a$.\n",
        "- Chained together, if $X$ and $Y$ are two continuous random variables then\n",
        "  - $F_Y(Y)=F_X(X)$. E.g., if $Y=e^X$ then $F_Y(y)=F_X(\\ln(y))$\n",
        "  - $Y=F_Y^{-1}(F_X(X))$. E.g., if $F_X=1-e^{-t}$ and $F_Y=1-e^{-Œªt}$ then $Y=\\frac{X}{Œª}$.\n",
        "- C <stdlib.h> `rand()` gives $\\Unif(0,1)$, which can be converted to any distribution CDF using this principle."
      ],
      "metadata": {
        "id": "p2S0VWdYZM2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standard normal**: $Z‚àº\\Normal(0,1)$ has PDF $œï(x)=\\frac{1}{\\sqrt{2œÄ}}e^{-x^2/2}$ and CDF $Œ¶(z)=\\frac{1}{\\sqrt{2œÄ}}\\displaystyle‚à´_{-‚àû}^{z}e^{-x^2/2}\\ dx$\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "I&=\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^{‚àû}e^{-x^2/2}\\ dx \\\\\n",
        "I^2&=\\frac{1}{2œÄ}‚à´_{-‚àû}^{‚àû}e^{-x^2/2}\\ dx‚à´_{-‚àû}^{‚àû}e^{-y^2/2}\\ dy \\\\\n",
        "&=\\frac{1}{2œÄ}‚à´_{x=-‚àû}^{‚àû}‚à´_{y=-‚àû}^{‚àû}e^{-(x^2+y^2)/2}\\ dx\\ dy\\\\\n",
        "x&=r\\t{cos}Œ∏,\\ y=r\\t{sin}Œ∏,\\ dx\\ dy=r\\ dr\\ dŒ∏ \\\\\n",
        "\\end{aligned}\n",
        "\\qquad\n",
        "\\begin{aligned}\n",
        "I^2&=\\frac{1}{2œÄ}‚à´_{r=0}^{‚àû}‚à´_{Œ∏=0}^{2œÄ}re^{-r^2/2}\\ dŒ∏\\ dr\\\\\n",
        "&=\\frac{2œÄ}{2œÄ}‚à´_{0}^{‚àû}re^{-r^2/2}\\ dr \\\\\n",
        "&=\\left[-e^{-r^2/2}\\right]_0^‚àû \\\\\n",
        "I&=1\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- ``scipy.stats.norm``\n",
        "- $\\E[Z]=0$ and $\\Var(Z)=1$\n",
        "  - $\\E[Z]=\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^‚àûxe^{-x^2/2}\\ dx=0$ because integrating odd function.\n",
        "  - $\\E[Z^2]=\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^‚àûx^2e^{-x^2/2}\\ dx$\n",
        "  $=\\frac{2}{\\sqrt{2œÄ}}‚à´_0^‚àûx^2e^{-x^2/2}\\ dx$\n",
        "  $=\\frac{2}{\\sqrt{2œÄ}}\\left(\\overbrace{\\left[-xe^{-x^2/2}\\right]_0^‚àû}^{0}-‚à´_0^‚àû-e^{-x^2/2}\\ dx\\right)$\n",
        "  $=\\frac{2}{\\sqrt{2œÄ}}\\left(0+\\frac{\\sqrt{2œÄ}}{2}\\right)$\n",
        "  Integration by parts $u=x,du=dx,dv=xe^{-x^2/2},v=-e^{-x^2/2}$\n",
        "- $F_X(a)=Œ¶\\left(\\frac{a-Œº}{œÉ}\\right)$.\n",
        "\n",
        "**Normal (Gaussian)**: Let $X=Œº+œÉZ$, then $X‚àº\\Normal(Œº,œÉ^2)$ has PDF\n",
        "$f(x)=\\frac{1}{\\sqrt{2œÄ}œÉ}e^{-\\frac{(x-Œº)^2}{2œÉ^2}} \\ x‚àà‚Ñù$\n",
        "- $f(x)=\\frac{1}{œÉ}œï\\left(\\frac{x-Œº}{œÉ}\\right)$\n",
        "- **Location-scale transformation**: If $Z‚àº\\Normal(0,1)$, then $X=Œº+œÉZ‚àº\\Normal(Œº,œÉ^2)$.\n",
        "  - If $X‚àº\\Normal(Œº,œÉ^2)$ then $Y=aX+b‚àº\\Normal(aŒº+b,a^2œÉ^2)$ and $F_Y(y)=F_X\\left(\\frac{y-b}{a}\\right)$.\n",
        "- If $X_1‚àº\\Normal(Œº_1,œÉ_1^2)$ and $X_2‚àº\\Normal(Œº_2,œÉ_2^2)$ are independent, then $X_1+X_2‚àº\\Normal(Œº_1+Œº_2,œÉ_1^2+œÉ_2^2)$ (Blitzstein e6.6.3)\n",
        "- Binomial-normal approximation: If $X‚àº\\Binom(n,p)$, then $\\liml_{n‚Üí‚àû}X‚àº\\Normal(np,np(1-p))$ is an approximation of binomial distribution. It is a special case of CLT.\n",
        "  - Discrete $P(X=i)$ is transformed to continuous $P(i-\\frac{1}{2}\\lt X\\lt i+\\frac{1}{2})$ and then z-scored.\n",
        "  - Poisson approximation is good when $n$ is large and $p$ is small.\n",
        "  - Normal approximation is good when $n$ is large.\n",
        "- **Normal-normal conjugacy** (e12.1.8): Let $Y|Œ∏‚àº\\Normal(Œ∏,œÉ^2)$ where prior $Œ∏‚àº\\Normal(Œº,œÑ^2)$ is an unknown parameter and $œÉ^2,Œº,œÑ^2$ are known. We want to find posterior distribution $Œ∏|Y$ after observing values of $Y$.\n",
        "  - PDF $f_{Œ∏|Y}(Œ∏|y)=\\frac{f_{Y|Œ∏}(y|Œ∏)f_{Œ∏}(Œ∏)}{f_Y(y)}$\n",
        "  $‚àùf_{Y|Œ∏}(y|Œ∏)f_{Œ∏}(Œ∏)$\n",
        "  $‚àù\\e{-\\frac{(y-Œ∏)^2}{2œÉ^2}}\\e{-\\frac{(Œ∏-Œº)^2}{2œÑ^2}}$. Posterior distribution remains normal. Normal is the conjugate prior of normal. The analytical solution is $(Œ∏|Y=y)‚àº\\Normal(\\frac{y/œÉ^2+Œº/œÑ^2}{1/œÉ^2+1/œÑ^2},\\frac{1}{1/œÉ^2+1/œÑ^2})$\n",
        "  - The mean of posterior $Œ∏|Y=y$ is a certainty-weighted average of observed $y$ and prior $Œº$, while the precision of the posterior is the sum of the precision of observed $y$ and prior $Œº$."
      ],
      "metadata": {
        "id": "3dGX2S5cFKXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exponential**: $X‚àº\\Expo(Œª), \\ Œª>0$ has PDF $f_X(x)=\\BC Œªe^{-Œªx}&x‚â•0 \\\\ 0 &x\\lt0\\EC$ and CDF $F_X(t)=1-e^{-Œªt} \\quad t‚â•0$ models quantity of time until some event arrives, where $Œª$ is the rate of arrival. It is the continuous analog to discrete geometric distribution.\n",
        "- ``scipy.stats.expon``\n",
        "- $\\E[X]=\\frac{1}{Œª}$ and $\\Var(X)=\\frac{1}{Œª^2}$. (Blitzstein Ch6 e6.5.1 proof by MGF)\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\E[X^n]&=‚à´_0^‚àûx^nŒªe^{-Œªx}\\ dx \\\\\n",
        "&=\\left[-x^ne^{-Œªx}\\right]_0^‚àû+\\frac{n}{Œª}‚à´_0^‚àûx^{n-1}Œªe^{-Œªx}\\ dx \\\\\n",
        "&=\\frac{n}{Œª}\\E[X^{n-1}]=\\frac{n!}{Œª^n}\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "u&=x^n \\\\\n",
        "dv&=Œªe^{-Œªx}\\ dx \\\\\n",
        "du&=nx^{n-1}\\ dx \\\\\n",
        "v&=-e^{-Œªx}\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- **Survival function** $1-F(b)=e^{-bŒª}$: the probability that an instrument breaks within $a$ hours is $1-e^{-aŒª}$, and survives more than $b$ hours is $e^{-bŒª}$. Exponential distribution is named after its survival function.\n",
        "  - **Memoryless**: $P(X>s+t|X>t)=P(X>s)$ or $P(X>s+t)=P(X>s)P(X>t)$. The probability that an instrument survives for at least $s+t$ hours given that it has survived $t$ hours is equal to the initial probability that it survives for at least $s$ hours. Exponential is the only continuous distribution with memoryless survival $e^{-Œª(s+t)}=e^{-Œªs}e^{-Œªt}$, because $g(x)=e^{-Œªx}$ is the only right-continuous function that satisfies $g(s+t)=g(s)g(t)$.\n",
        "  - **Memoryless expectation**: $\\E[X|X>t]=t+\\E[X]$. The distribution of $X$ past $t$ also is expon(Œª).\n",
        "- (Blitzstein Ch5 p43) Let $G‚àº\\Geom(p)$ be the number of failures in Bernoulli trials before the first success, and $T$ be the time of the first success when $G$ trials are occurring once per $Œît$: $GŒît=T$. Let $Œª=\\frac{p}{Œît}$ be rate of success per unit time.\n",
        "  - $P(G‚â•n)=\\suml_{i=n}^‚àû(1-p)^ip=(1-p)^n=(1-ŒªŒît)^n$ by geometric series.\n",
        "  - $P(T‚â•t)=P(G‚â•\\frac{t}{Œît})=(1-ŒªŒît)^{\\frac{t}{Œît}}\\xrightarrow{Œît‚Üí0}e^{-Œªt}$ by Taylor series of $e^{-x}$ therefore $T‚àº\\Expo(Œª)$.\n",
        "\n",
        "**Hazard/failure rate function**: $Œª(t)=\\frac{f(t)}{1-F(t)}$ for non-negative continuous random variable $X$ with PDF $f$ and CDF $F$ represents a conditional probability density that an instrument that already survived $t$ units of time will fail in the next $Œît$ units of time.\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "Œª(t)Œît&=\\frac{f(t)Œît}{1-F(t)} \\\\\n",
        "&‚âà\\frac{P(t\\lt X\\lt t+Œît)}{P(X>t)} \\\\\n",
        "&=P(t\\lt X\\lt t+Œît|X>t)\n",
        "\\end{aligned}\n",
        "\\\n",
        "\\begin{aligned}\n",
        "‚à´_0^tŒª(s)\\ ds&=‚à´_0^t\\frac{f(s)}{1-F(s)}\\ ds\\\\\n",
        "&=-\\left[\\ln(1-F(s))\\right]_0^t \\\\\n",
        "&=-\\ln(1-F(t)) \\\\\n",
        "F_X(t)&=1-e^{-‚à´_0^tŒª(s)\\ ds}\\\\\n",
        "f_X(t)&=Œª(t)e^{-‚à´_0^tŒª(s)\\ ds}\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- A positive continuous random variable can be defined (CDF and PDF) through its $Œª(t)$. Exponential has constant $Œª(t)=Œª$. If non-memoryless $Œª(t)=a+bt$, then $F(t)=1-e^{-at-bt^2/2}$ and $f(t)=(a+bt)e^{-at-bt^2/2}$."
      ],
      "metadata": {
        "id": "CTzeMDEPYPGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Poisson process** (Ross Ch4): let $N(t)$ be the number of events occurring in time interval $[0,t]$, then $P(N(t)=k)=e^{-Œªt}\\frac{(Œªt)^k}{k!}$ where $Œª$ is the event occurrence frequency per unit time.\n",
        "- Events that occur at random points of time are said to occur in accordance with a Poisson process having rate $Œª$ if they follow these conditions. Let $h$ be a small time interval, define $o(h)$ as a generic function smaller than $h$ such that $\\liml_{h‚Üí0}\\frac{o(h)}{h}=0$, then.\n",
        "    1. the probability that exactly 1 event occurs in $h$ is $Œªh+o(h)$, and the probability that 2+ events occurs in $h$ is $o(h)$,\n",
        "    2. for $n$ non-overlapping intervals, let $E_i$ be $j_i$ events occurring in the $i$th interval, then $E_1...E_n$ are independent\n",
        "- Time interval $[0,t]$ is divided into $n$ non-overlapping intervals of length $t/n$ each. Then the event $P(N(t)=k)=(1)+(2)$:\n",
        "    1. $k$ intervals have 1 event and other intervals have 0: $\\binom{n}{k}\\left[Œª\\frac{t}{n}+o\\left(\\frac{t}{n}\\right)\\right]^k\\left[1-Œª\\frac{t}{n}-o\\left(\\frac{t}{n}\\right)\\right]^{n-k}$ $‚Üíe^{-Œªt}\\frac{(Œªt)^k}{k!}$.\n",
        "    2. at least 1 interval has 2+ events: $‚â§\\sum_{i}o\\left(\\frac{t}{n}\\right)‚Üí0$\n",
        "\n",
        "**Poisson process** (Blitzstein Ch5): a process of arrivals in continuous time is called a Poisson process with arrival rate $Œª$ if 2 conditions are true:\n",
        "1. Let $N_t$ be number of arrivals in an interval of length $t$, then $N_t‚àº\\Pois(Œªt)$.\n",
        "2. the number of arrivals in disjoint intervals are independent of each other.\n",
        "- **Count-time duality**: Emails arrive at rate of $Œª$ per unit time. Let $N_t$ be the number of arrivals in time interval $t$, and $T_n$ be the time until the $n$th email arrives. Then $P(T_n>t)=P(N_t\\lt n)=\\suml_{k=0}^{n-1}e^{-Œªt}\\frac{(Œªt)^k}{k!}$.\n",
        "  - $T_1$: $P(N_t\\lt 1)=e^{-Œªt}\\frac{(Œªt)^0}{0!}=e^{-Œªt}=P(T_1>t)$\n",
        "  - $X_{(i)}=T_{i}-T_{i-1}‚àº\\Expo(Œª)$ iid.\n",
        "  - $T_n=X_{(1)}+...+X_{(n)}‚àº\\Gamma(n,Œª)$ the sum of independent arrivals. $T_i$ and $T_j$ are not independent.\n",
        "- **First of $n$ parallel arrivals** (e5.6.3): For $n$ independent $X_i‚àº\\Expo(Œª_i)$, then $L=\\min(X_1,...,X_n)$ is the first Poisson process to arrive.\n",
        "$P(L>t)=P(X_1>t,...,X_n>t)$\n",
        "$=\\prodl_{i=1}^n P(X_i>t)$\n",
        "$=e^{-\\left(\\suml_{i=1}^nŒª_i\\right)t}$.\n",
        "Therefore $L‚àº\\Expo(\\suml_{i=1}^nŒª_i)$ combines arrival rates. If $Œª_i$s are the same, then the first of $n$ identical Poisson processes to arrive has $n$ times higher chance to arrive by time $t$.\n",
        "  - if $X_1,...,X_n‚àº\\Expo(Œª)$ iid, then $L=\\min(X_1,...,X_n)‚àº\\Expo(nŒª)‚àº\\frac{1}{n}X_i$.\n",
        "  Proof: $l=\\frac{x}{n}$,\n",
        "  $f_{X_i}(x)=Œªe^{-Œªx}$.\n",
        "  Change of variables\n",
        "  $f_{X_i}(l)=Œªe^{-nŒªl}$\n",
        "  $|\\frac{dx_i}{dl}|=n$\n",
        "  $‚áíf_{L}(l)=nŒªe^{-nŒªl}$.\n",
        "  Therefore $\\frac{X_i}{n}‚àº\\Expo(nŒª)$.\n",
        "  - (Ross t6.8) $X$ and $Y$ are independent with hazard rate functions $Œª_X(t)$ and $Œª_Y(t)$. Let $W=\\min(X,Y)$, then\n",
        "  $P(W>t)=P(X>t,Y>t)$\n",
        "  $=P(X>t)P(Y>t)$\n",
        "  $=e^{-‚à´_0^tŒª_X(s)\\ ds}e^{-‚à´_0^tŒª_Y(s)\\ ds}$\n",
        "  $=e^{-‚à´_0^t(Œª_X(s)+Œª_Y(s))\\ ds}$. Therefore $Œª_W(t)=Œª_X(t)+Œª_Y(t)$.\n",
        "- **Last of $n$ parallel arrivals** (e5.6.4): $n$ independent $X_i‚àº\\Expo(Œª)$, then $T=\\max(X_1,...,X_n)$ is the final arrival. $T$ is not exponential:\n",
        "$P(T‚â§t)=P(X_1‚â§t,...,X_n‚â§t)$\n",
        "$=(1-e^{-Œªt})^n$.\n",
        "Let $T=T_1+...+T_n$ where $T_i$ is the additional time until the $i$th arrival, which is exponential and memoryless.\n",
        "$T_1=\\min(X_1,...,X_n)‚àº\\Expo(nŒª)$.\n",
        "By memoryless, $P(T_2>s+t|T_1=t)=P(T_2>s)$, so the next process would arrive $T_2‚àº\\Expo((n-1)Œª)$.\n",
        "All $n$ processes are expected to arrive by $\\E[T]=\\suml_{i=1}^n\\E[T_i]=\\suml_{i=0}^{n-1}\\frac{1}{(n-i)Œª}$"
      ],
      "metadata": {
        "id": "wwFAguzvMIk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beta**: $X‚àº\\Beta(a,b), \\ a,b>0$ has PDF $f(x)=\\frac{1}{Œ≤(a,b)}x^{a-1}(1-x)^{b-1},\\ 0\\lt x\\lt 1$.\n",
        "- $Œ≤(a,b)=‚à´_0^1x^{a-1}(1-x)^{b-1}\\ dx=\\frac{Œì(a)Œì(b)}{Œì(a+b)}$ is chosen to make the PDF integrate to 1.\n",
        "- Bayes' billiards (Blitzstein S8.3.2): $‚à´_0^1\\binom{n}{k}x^k(1-x)^{n-k}\\ dx=\\frac{1}{n+1}$.\n",
        "  - 1 gray ball and $n$ white balls are placed $\\Unif(0,1)$ onto [0,1]. Let $X$ be the number of white balls placed to the left of the gray ball, and $Y$ the probability that a white ball is placed to the left of the gray ball is also the position of the gray ball.\n",
        "  $P(X=k)=‚à´_0^1P(X=k|Y=p) f_Y(p)\\ dp$\n",
        "  $=‚à´_0^1\\binom{n}{k}p^k(1-p)^{n-k}\\ dp$\n",
        "  - $n+1$ white balls are placed $\\Unif(0,1)$ onto [0,1], and then a random ball is painted gray. $P(X=k)=\\frac{1}{n+1}$.\n",
        "  - let $a-1=k$ and $b-1=n-k$, then\n",
        "  $Œ≤(a,b)=‚à´_0^1x^{a-1}(1-x)^{b-1}\\ dx$\n",
        "  $=\\frac{1}{(a+b-1)\\binom{a+b-2}{a-1}}$\n",
        "  $=\\frac{(a-1)!(b-1)!}{(a+b-1)!}$.\n",
        "- **Beta-binomial conjugacy** (Blitzstein S8.3.3, Ross e7.5l, p7.71): a random misshapen coin lands Heads with unknown probability $p$, which has prior distribution $p‚àº\\Beta(a,b)$. The number of Heads $(X|p)‚àº\\Binom(n,p)$.\n",
        "  - X has beta-binomial distribution with marginal PMF\n",
        "  $P(X=k)=‚à´_0^1P(X=k|p)f(p)\\ dp$\n",
        "  $=\\frac{\\binom{n}{k}}{Œ≤(a,b)}‚à´_0^1p^{k+a-1}q^{n-k+b-1}\\ dp$\n",
        "  $=\\binom{n}{k}\\frac{Œ≤(k+a,n-k+b)}{Œ≤(a,b)}$\n",
        "  - $f(p|X=k)=\\frac{P(X=k|p)f(p)}{P(X=k)}$\n",
        "  $‚àùp^{k+a-1}(1-p)^{n-k+b-1}$.\n",
        "  - Posterior distribution $(p|X=k)‚àº\\Beta(a+k,b+n-k)$: Beta prior distribution on $p$ updated with data that are conditionally binomial given $p$ results in Beta posterior distribution: *Beta is the conjugate prior of Binomial*. Observed success tallies are added to $a$, and failures are added to $b$; while $a-1$ and $b-1$ can be thought of as prior successes/failures. If we start with 0 prior success/failure tallies, that is $\\Beta(1,1)=\\Unif(0,1)$.\n",
        "- CDF (Blitzstein 8.41): $X‚àº\\Binom(n,p)$. Let $U_1,...,U_n‚àº\\Unif(0,1)$ iid and $I_i$ be indicator that $U_i‚â§p$, then $X=I_1+...+I_n$.\n",
        "  - $X‚â•j$ iff at least $j$ elements in the ascending-sorted list is to the left of $p$: $P(X‚â•j)=P(U_{(j)}‚â§p)$.\n",
        "  - By order statistic, $f_{U_{(j)}}(x)=\\frac{n!}{(j-1)!(n-j)!}x^{j-1}(1-x)^{n-j}$, and therefore $U_{(j)}‚àº\\Beta(j,n-j+1)$.\n",
        "- $\\E[X]=\\frac{a}{a+b}$ and $\\Var(X)=\\frac{ab}{(a+b)^2(a+b+1)}$"
      ],
      "metadata": {
        "id": "T8AJb5e2Bh8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gamma**: $X‚àº\\Gamma(n,Œª), \\ n,Œª>0$ has PDF $f(x)=\\frac{e^{-Œªx}(Œªx)^n}{xŒì(n)}, \\ x‚â•0$\n",
        "- $Œì(n+1)=‚à´_0^‚àûx^ne^{-x}\\ dx$\n",
        "$=\\left[-e^{-x}x^n\\right]_0^‚àû+n‚à´_0^‚àûe^{-x}x^{n-1}\\ dx$\n",
        "$=nŒì(n)=n!$\n",
        "- The PDF is derived from the gamma integral:\n",
        "  - $Œì(n)=‚à´_0^‚àûy^{n-1}e^{-y}\\ dy$ - starting from the gamma integral\n",
        "  - $1=‚à´_0^‚àû\\frac{y^{n-1}e^{-y}}{Œì(n)}\\ dy$ - divide both sides by $Œì(n)$ to get gamma(n,1): $f_Y(y)=\\frac{e^{-y}y^{n-1}}{Œì(n)}=\\frac{e^{-y}y^n}{yŒì(n)}$.\n",
        "  - Let $X=Y/Œª$, then $|\\frac{dy}{dx}|=Œª$. Change of variables to get gamma(n,Œª): $f_X(x)=\\frac{e^{-Œªx}(Œªx)^n}{xŒì(n)}$\n",
        "- **Gamma-Poisson conjugacy** (Blitzstein S8.4.5): Buses arrive at a stop according to Poisson process with unknown rate Œª buses per hour, where prior $Œª‚àº\\Gamma(r_0,b_0)$. Let $Y$ be the number of buses in $t$ hours. Then $(Y|Œª)‚àº\\Pois(Œªt)$.\n",
        "  - $f(y,Œª)=\\frac{e^{-Œªt}(Œªt)^y}{y!}\\frac{e^{b_0Œª}(b_0Œª)^{r_0}}{ŒªŒì(r_0)}$.\n",
        "  Match the marginal PMF of $Y$ to $\\Gamma(r_0+y,b_0+t)$: $P(Y=y)=P(Y=y|Œª)f_0(Œª)$\n",
        "  $=‚à´_0^‚àû\\frac{e^{-Œªt}(Œªt)^y}{y!}\\frac{e^{b_0Œª}(b_0Œª)^{r_0}}{Œì(r_0)}\\ \\frac{dŒª}{Œª}$\n",
        "  $=\\frac{t^yb_0^{r_0}}{y!Œì(r_0)}‚à´_0^‚àûe^{-(b_0+t)Œª}Œª^{r_0+y}\\ \\frac{dŒª}{Œª}$\n",
        "  $=\\frac{t^yb_0^{r_0}}{y!Œì(r_0)}\\frac{Œì(r_0+y)}{(b_0+t)^{r_0+y}}‚à´_0^‚àû\\frac{e^{-(b_0+t)Œª}((b_0+t)Œª)^{r_0+y}}{Œì(r_0+y)}\\ \\frac{dŒª}{Œª}$\n",
        "  $=\\frac{t^yb_0^{r_0}}{y!Œì(r_0)}\\frac{Œì(r_0+y)}{(b_0+t)^{r_0+y}}$\n",
        "  $=\\binom{r_0+y-1}{r_0-1}\\left(\\frac{b_0}{b_0+t}\\right)^{r_0}\\left(\\frac{t}{b_0+t}\\right)^y$\n",
        "  $‚àº\\t{nbinom}(r_0,\\frac{b_0}{b_0+t})$.\n",
        "  - $f(Œª|y)=\\frac{P(Y=y|Œª)f_0(Œª)}{P(Y=y)}$\n",
        "  $‚àùe^{-(b_0+t)Œª}Œª^{r_0+y}\\frac{1}{Œª}$\n",
        "  - Posterior $(Œª|y)‚àº\\Gamma(r_0+y,b_0+t)$. Gamma prior distribution on $Œª$ updated with data that are conditionally Poisson distributed given Œª results in gamma posterior distribution. Gamma is the conjugate prior for the Poisson. Observed $y$ arrivals are added to $r_0$ and $t$ hours are added to $b_0$.\n",
        "  - $\\E[Y]$ and $\\Var(Y)$ using Adam and Eve in Blitzstein e9.6.3.\n",
        "- Beta-gamma (Blitzstein S8.5.1): Let $X‚àº\\Gamma(a,Œª)$ and $Y‚àº\\Gamma(b,Œª)$, what is the joint distribution of $T=X+Y$ and $W=\\frac{X}{X+Y}$?\n",
        "  - $X=TW$ and $Y=T(1-W)$.\n",
        "  $|\\frac{‚àÇ(X,Y)}{‚àÇ(t,w)}|=|-tw-t(1-w)|=|-t|$.\n",
        "  $f_{T,W}(t,w)=f_X(x)f_Y(y)|\\frac{‚àÇ(X,Y)}{‚àÇ(t,w)}|$\n",
        "  $=\\frac{e^{-Œªx}(Œªx)^a}{xŒì(a)}\\frac{e^{-Œªy}(Œªy)^b}{yŒì(b)}t$\n",
        "  $=\\left(\\frac{Œì(a+b)}{Œì(a)Œì(b)}w^{a-1}(1-w)^{b-1}\\right)\\left(\\frac{1}{tŒì(a+b)}e^{-Œªt}(Œªt)^{a+b}\\right)$\n",
        "  $=f_W(w)f_T(t)$\n",
        "  where $W‚àº\\Beta(a,b)$ and $T‚àº\\Gamma(a+b,Œª)$\n",
        "  - For $X‚àº\\Gamma(a,Œª)$ and $Y‚àº\\Gamma(b,Œª)$ independent, then $X+Y‚àº\\Gamma(a+b,Œª)$ and $\\frac{X}{X+Y}‚àº\\Beta(a,b)$ are independent.\n",
        "- $\\Gamma(1,Œª)=\\Expo(Œª)$. Let $X_1,...,X_n‚àº\\Expo(Œª)$ iid, then $(X_1+...+X_n)‚àº\\Gamma(n,Œª)$. Proof by MGF $M(t)=(\\frac{Œª}{Œª-t})^n,\\ t\\lt Œª$. (Blitzstein e8.4.3) Where exponential is the continuous analog to geometric, gamma is the continuous analog to negative binomial.\n",
        "- $\\E[X]=\\frac{n}{Œª}$ and $\\Var(X)=\\frac{n}{Œª^2}$. $\\E[X^k]=\\frac{Œì(n+k)}{Œì(n)Œª^k}$.\n",
        "  - Let $Y‚àº\\Gamma(n,1)$.\n",
        "  $\\E[Y]=‚à´_0^‚àûx\\frac{e^{-x}x^{n}}{xŒì(n)}\\ dx$\n",
        "  $=\\frac{Œì(n+1)}{Œì(n)}‚à´_0^‚àû\\frac{e^{-x}x^{n+1}}{xŒì(n+1)}\\ dx$\n",
        "  $=\\frac{Œì(n+1)}{Œì(n)}$\n",
        "  $=\\frac{nŒì(n)}{Œì(n)}=n$.\n",
        "  $\\E[Y^k]=‚à´_0^‚àûx^k\\frac{e^{-x}x^{n}}{xŒì(n)}\\ dx$\n",
        "  $=\\frac{Œì(n+k)}{Œì(n)},\\ k>-n$.\n",
        "  - Let $X=Y/Œª$. Then $\\E[X]=\\frac{n}{Œª}$ and $\\E[X^k]=\\frac{Œì(n+k)}{Œì(n)Œª^k}$."
      ],
      "metadata": {
        "id": "ZJ9sSxnBoBE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chi-square**: Let $V_n=Z_1^2+...+Z_n^2$ where $Z_i‚àº\\Normal(0,1)$, then $V_n‚àºœá_n^2‚àº\\Gamma(\\frac{n}{2},\\frac{1}{2})$ is said to have Chi-square distribution with $n$ degrees of freedom.\n",
        "- Proof: $|\\frac{dz}{dz^2}|=\\frac{1}{\\sqrt{z}}$.\n",
        "$f_{Z_1^2}(x)=\\frac{1}{\\sqrt{x}}œï(\\sqrt{x})$\n",
        "$=\\frac{1}{\\sqrt{2œÄx}}e^{-x/2}$.\n",
        "Let $Y‚àº\\Gamma(\\frac{1}{2},\\frac{1}{2})$.\n",
        "$f_Y(x)=\\frac{e^{-x/2}(x/2)^{1/2}}{xŒì(1/2)}$\n",
        "$=\\frac{1}{\\sqrt{2x}Œì(1/2)}e^{-x/2}$\n",
        "$=\\frac{1}{\\sqrt{2œÄx}}e^{-x/2}$.\n",
        "Therefore $Z_1^2‚àº\\Gamma(\\frac{1}{2},\\frac{1}{2})$ and $V_n‚àº\\Gamma(\\frac{n}{2},\\frac{1}{2})$.\n",
        "- $\\E[V_n]=n$ and $\\Var(V_n)=2n$. MGF $M(t)=(\\frac{1}{1-2t})^{n/2},\\ t\\lt 1/2$\n",
        "- Let $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$ with $S_n^2=\\frac{1}{n-1}\\suml_{i=1}^n(X_i-\\bar{X}_n)^2$, then\n",
        "$\\frac{(n-1)S_n^2}{œÉ^2}‚àºœá_{n-1}^2$. (Blitzstein e10.4.3)\n",
        "  - Proof:\n",
        "  $\\frac{(n-1)S_n^2}{œÉ^2}$\n",
        "  $=\\frac{1}{œÉ^2}\\suml_{j=1}^n(X_j-\\bar{X}_n)^2$\n",
        "  $=\\frac{1}{œÉ^2}\\suml_{j=1}^n(Œº+œÉZ_j-Œº-œÉ\\bar{Z}_n)^2$\n",
        "  $=\\suml_{j=1}^n(Z_j-\\bar{Z}_n)^2$\n",
        "  $=\\suml_{j=1}^nZ_j^2-n\\bar{Z}_n^2$.\n",
        "  Distributions\n",
        "  $\\suml_{j=1}^nZ_j^2‚àºœá_n^2$.\n",
        "  $\\suml_{j=1}^nZ_j‚àº\\Normal(0,n)$\n",
        "  $‚áí\\bar{Z}_n‚àº\\Normal(0,\\frac{1}{n})$\n",
        "  $‚áí\\sqrt{n}\\bar{Z}_n‚àº\\Normal(0,1)$\n",
        "  $‚áín\\bar{Z}_n^2‚àºœá_1^n$.\n",
        "  MGFs\n",
        "  $M_{\\suml_{j=1}^n(Z_j-\\bar{Z}_n)^2}(t)$\n",
        "  $=(\\frac{1}{1-2t})^{n/2}(\\frac{1}{1-2t})^{-1/2}$\n",
        "  $=(\\frac{1}{1-2t})^{(n-1)/2}$.\n",
        "  $‚áí\\suml_{j=1}^n(Z_j-\\bar{Z}_n)^2‚àºœá_{n-1}^2$.\n",
        "  Therefore,\n",
        "  $\\frac{(n-1)S_n^2}{œÉ^2}‚àºœá_{n-1}^2$.\n",
        "- $\\frac{Z^2}{Z^2+V_n}‚àº\\Beta(\\frac{1}{2},\\frac{n}{2})$ (Blitzstein p10.23)\n",
        "\n",
        "**Student-t**: Let $T=\\frac{Z}{\\sqrt{V_n/n}}$ where $Z$ and $V_n$ are independent. Then $T‚àºt_n$ is student-t distribution with $n$ degrees of freedom.\n",
        "- If $T‚àºt_n$ then $-T‚àºt_n$.\n",
        "- $\\E[T]=0,\\ n>1$ and $\\Var(T)=\\frac{n}{n-2},\\ n>2$\n",
        "  - $t_1$ is Cauchy distribution $Z_1/Z_2$ where $Z_1,Z_2‚àº\\Normal(0,1)$. (Blitzstein e7.1.25)\n",
        "  - By independence of $Z$ and $V_n$,\n",
        "  $\\E[T]=\\E[Z]\\E[\\sqrt{\\frac{n}{V_n}}]=0,\\ n>1$.\n",
        "  - $\\E[T^2]=\\E[\\frac{nZ^2}{V_n}]$\n",
        "  $=n\\overbrace{\\E[Z^2]}^{1}\\overbrace{\\E[V_n^{-1}]}^{\\E[X^{k=-1}]}$\n",
        "  $=n\\frac{Œì(n/2-1)}{Œì(n/2)(1/2)^{-1}}$\n",
        "  $=\\frac{nŒì(n/2-1)}{2(n/2-1)Œì(n/2-1)}$\n",
        "  $=\\frac{n}{n-2}$.\n",
        "- $t_n\\dot‚àº\\Normal(0,1)$. Proof: by strong law of large numbers,\n",
        "$\\liml_{n‚Üí‚àû}\\frac{V_n}{n}=\\liml_{n‚Üí‚àû}\\bar{Z}^2_n$\n",
        "$=\\E[Z^2]=1$."
      ],
      "metadata": {
        "id": "8_zWvxeQT31s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ross Chapter 5 (Continuous random variables):\n",
        "\n",
        "- e5.1c: the lifetime in hours of a certain kind of radio tube has pdf $f(x)=\\frac{100}{x^2},\\ x>100$ and 0 otherwise. What is the probability of $A$ that exactly 2 of 5 such tubes need replacement within the first 150 hrs? $p_i=‚à´_{100}^{150}100x^{-2}dx$\n",
        "$=-100x^{-1}\\big\\rvert_{100}^{150}$\n",
        "$=1-\\frac{2}{3}$ and $P(A)=\\binom{5}{2}p_i^2(1-p_i)^3$.\n",
        "\n",
        "- e5.2c: A unit length stick is split into 2 pieces at point U whose position has pdf $f(u)=1,\\ u‚àà(0,1)$. What is the expected length of the piece that contains point $p,\\ 0‚â§p‚â§1$? Let $L_p(U)=\\BC 1-U &\\ p>U \\\\ U &\\ p\\lt U\\EC$ be the length of the piece that contains $p$.\n",
        "$\\E[L_p(U)]=‚à´_{0}^{1}L_p(u)f(u)\\ du$\n",
        "$=‚à´_{0}^{p}1-u\\ du+‚à´_{p}^{1}u\\ du$\n",
        "$=(u-\\frac{1}{2}u^2)\\big|_0^p+(\\frac{1}{2}u^2)\\big|_p^1$\n",
        "$=p-\\frac{p^2}{2}+\\frac{1}{2}-\\frac{p^2}{2}=\\frac{1}{2}+p-p^2$\n",
        "\n",
        "- e5.2d: you're going to an appointment. Being $s$ minutes early incurs cost $cs$, and $s$ minutes late incurs $ks$. If time of travel $X$ has pdf $f$, how many minutes before appointment $t$ should you depart to minimize expected cost? Let $C_t(X)=\\BC c(t-X)&\\ X‚â§t\\\\ k(X-t)&\\ X‚â•t\\EC$ be the cost function.\n",
        "\n",
        "$$\\small\\begin{aligned}\n",
        "\\E[C_t(X)]&=‚à´_0^tc(t-x)f(x)\\ dx+‚à´_t^‚àûk(x-t)f(x)\\ dx \\\\\n",
        "&=ct‚à´_0^tf(x)\\ dx-c‚à´_0^txf(x)\\ dx+k‚à´_t^‚àûxf(x)\\ dx-kt‚à´_t^‚àûf(x)\\ dx \\\\\n",
        "&=ctF(t)-kt[1-F(t)]-c‚à´_0^txf(x)\\ dx+k‚à´_t^‚àûxf(x)\\ dx \\\\\n",
        "\\frac{d}{dt}\\E[C_t(X)]&=\\frac{d}{dt}[(c+k)tF(t)-kt]-c\\frac{d}{dt}‚à´_{-‚àû}^txf(x)\\ dx-k\\frac{d}{dt}‚à´_{t}^{‚àû}xf(x)\\ dx \\\\\n",
        "&=[(c+k)(F(t)+tf(t))-k]-ctf(t)-ktf(t) \\\\\n",
        "0&=(c+k)F(t)-k \\\\\n",
        "F(t)&=\\frac{k}{c+k}\n",
        "\\end{aligned}$$\n",
        "  - Leibniz's rule: $\\frac{d}{dt}‚à´_t^‚àûxf(x)\\ dx$ $=\\left[xf(x)\\frac{dx}{dt}\\right]_{x=t}^{‚àû}+‚à´_t^{‚àû}\\frac{‚àÇ}{‚àÇt}xf(x)\\ dx$ $=[‚àûf(‚àû)\\frac{d‚àû}{dt}]-[tf(t)\\frac{dt}{dt}]+0$ $=-tf(t)$\n",
        "\n",
        "- e5.3d: what is the probability that the length of a random chord on a circle will be longer than the side of the equilateral triangle inscribed within? The length of a side of equilateral triangle inscribed in a circle is $\\sqrt{3}r$. There are two ways to formulate this:\n",
        "  - If the chord is longer than the triangle, its position from the center of the circle is $X\\lt\\frac{r}{2}$. If we assume that distance is uniformly distributed with $f(x)=\\frac{1}{r}$ then $P(X\\lt\\frac{r}{2})=\\frac{1}{2}$\n",
        "  - If the chord is longer than the triangle, then at one end of the chord the angle between its projection and the tangent $60\\lt Y\\lt 120$. If we assume the angle of its projection is uniformly distributed with $f(x)=\\frac{1}{180}$ then $P(60\\lt Y\\lt 120)=\\frac{1}{3}$\n",
        "\n",
        "- e5.5b: the length of a phone call is exponentially distributed with $Œª=1/10$ in minutes. What is the probability that a phone call is between 10 and 20 minutes? $P(10\\lt X\\lt 20)=F(20)-F(10)=e^{-1}-e^{-2}$\n",
        "\n",
        "- e5.7b: $Y=X^2$, then $F_Y(y)=P(X^2\\lt y)=P(-\\sqrt{y}\\lt X\\lt \\sqrt{y})=F_X(\\sqrt{y})-F_X(-\\sqrt{y})$\n",
        "\n",
        "- e5.7c: $Y=|X|$, then $F_Y(y)=P(|X|\\lt y)=P(-y\\lt X\\lt y)=F_X(y)-F_X(-y)$\n",
        "\n",
        "- e5.7e: lognormal $Y=e^X$ where $X‚àº\\Normal(Œº,œÉ^2)$ describes $Y=\\frac{S_n}{S_{n-1}}$ and $X=\\ln\\left(\\frac{S_n}{S_{n-1}}\\right)$. $F_Y(y)=F_X(\\ln(y))$.\n",
        "\n",
        "- 5.2: a system has lifetime given by PDF $f(x)=Cxe^{-x/2},\\ x>0$. What is the probability that the system lasts at least 5 months? By parts $u=x,\\ du=dx,\\ dv=e^{-x/2}\\ dx,\\ v=-2e^{-x/2}$, then $C‚à´_0^‚àûxe^{-x/2}\\ dx$$=[-2Cxe^{-x/2}]_0^‚àû+2C‚à´_0^‚àûe^{-x/2}\\ dx=4C$ and $C=1/4$.  $F_X(a)=-\\frac{1}{2}[xe^{-x/2}]_0^a+\\frac{1}{2}‚à´_0^ae^{-x/2}\\ dx$$=-\\frac{a}{2}e^{-a/2}+[-e^{-x/2}]_0^a$$=1-\\frac{a}{2}e^{-a/2}-e^{-a/2}$$=1-\\frac{a+1}{2}e^{-a/2}$. Survival over 5 months: $1-F(5)=\\frac{7}{2}e^{-5/2}$\n",
        "\n",
        "- 5.8: the lifetime in hours of an instrument has PDF $f(x)=xe^{-x},\\ x‚â•0$. Compute the expected lifetime.\n",
        "$\\E[X]=‚à´_0^‚àûx^2e^{-x}\\ dx=Œì(3)=2$ hours.\n",
        "\n",
        "- 5.11: a line of length L is split into 2 segments at a randomly chosen point. What is the probability that the ratio of the shorter to the longer segment is less than 1/4? Assume $f(x)=\\frac{2}{L},\\ x‚àà[0,\\frac{L}{2}]$ and $F(a)=\\frac{2}{L}[x]_0^a=\\frac{2a}{L}$. $P(x‚àà[0,\\frac{L}{5}])=\\frac{2}{5}$.\n",
        "\n",
        "- 5.14: let $f_X(x)=1,\\ x‚àà[0,1]$, then $F_X(a)=‚à´_0^a\\ dx=a$. By summing survival function $\\E[X]=‚à´_0^1(1-F_X(x))\\ dx$$=‚à´_0^1(1-x)\\ dx$$=[x-\\frac{x^2}{2}]_0^1=\\frac{1}{2}$.\n",
        "\n",
        "- 5.17: If salaries for a profession are normally distributed, 25% earn less than 180k, 25% earn more than 320k. What fraction earn less than 200k? $Œº=250k$, $z=Œ¶^{-1}(0.25)=-0.674=\\frac{180-250}{œÉ}$, $œÉ=\\frac{250-180}{0.674}=104$. Then $z_1=\\frac{200k-250k}{104}=-0.481$. $F(200k)=Œ¶(-0.481)=0.315$.\n",
        "\n",
        "- 5.22: If each independent tennis serve is successful with probability 0.4, What is the probablity that more than 100 serves are needed to have 50 successful serves? That probability is equal to having at most 50 successful serves in 100. By normal approximation $Œ¶\\left(\\frac{50-(100)(0.4)}{\\sqrt{(100)(0.4)(0.6)}}\\right)=0.979$\n",
        "\n",
        "- 5.29: If present price is $s$, after one period it will either go up to $us$ with probability $p$ or down to $ds$, where $u$=1.012, $d$=0.990, $p$=0.52. What is the probability that the price is up more than 30% after $n$=1000 rounds? Let $k$ be the number of up rounds required to meet the +30% target.\n",
        "$u^kd^{n-k}‚â•1.3$,\n",
        "$‚áí k\\ln(u)+(n-k)\\ln(d)‚â•\\ln(1.3)$\n",
        "$‚áík‚â•\\frac{\\ln(1.3)-n\\ln(d)}{\\ln(u)-\\ln(d)}=469$.\n",
        "Then $1-Œ¶(\\frac{469-np}{\\sqrt{np(1-p)}})=0.9994$\n",
        "\n",
        "- 5.31a: A road of length A has a fire station placed at location $a‚àà[0,A]$. Fire occurs at point with location $f(x)=\\frac{1}{A},\\ x‚àà[0,A]$. Find $a$ that minimizes\n",
        "$\\E|X-a|$\n",
        "$=‚à´_0^a\\frac{a-x}{A}\\ dx+‚à´_a^A\\frac{x-a}{A}\\ dx$\n",
        "$=\\frac{1}{A}[ax-\\frac{1}{2}x^2]_0^a+\\frac{1}{A}[\\frac{1}{2}x^2-ax]_a^A$\n",
        "$=\\frac{1}{A}(a^2-\\frac{1}{2}a^2)+\\frac{1}{A}(A^2-aA)-\\frac{1}{A}(\\frac{1}{2}a^2-a^2)$\n",
        "$=\\frac{a^2+A^2-aA}{A}$.\n",
        "Then $\\frac{d}{da}\\E|X-a|$\n",
        "$=\\frac{2a-A}{A}=0$ and $a=\\frac{A}{2}$. Median.\n",
        "\n",
        "- 5.31b: The road is infinite length, but fire occurs at location $f(x)=Œªe^{-Œªx},\\ x‚àà[0,‚àû)$.\n",
        "$\\E|X-a|$\n",
        "$=‚à´_0^a(a-x)Œªe^{-Œªx}\\ dx+‚à´_a^‚àû(x-a)Œªe^{-Œªx}\\ dx$. By Leibniz's rule:\n",
        "$\\frac{d}{da}\\E|X-a|$\n",
        "$=\\frac{d}{da}‚à´_0^a(a-x)Œªe^{-Œªx}\\ dx+\\frac{d}{da}‚à´_a^‚àû(x-a)Œªe^{-Œªx}\\ dx$\n",
        "$=[(a-a)Œªe^{-Œªa}(1)-(a-0)Œªe^{-Œªa}(0)+‚à´_0^aŒªe^{-Œªx}\\ dx]$\n",
        "$+[(‚àû-a)Œªe^{-Œªa}(0)-(a-a)Œªe^{-Œªa}(1)]+‚à´_a^‚àû-Œªe^{-Œªx}\\ dx]$\n",
        "$=‚à´_0^aŒªe^{-Œªx}\\ dx-‚à´_a^‚àûŒªe^{-Œªx}\\ dx$\n",
        "$=[-e^{-Œªx}]_0^a-[-e^{-Œªx}]_a^‚àû$\n",
        "$=1-e^{-Œªa}-e^{-Œªa}=0$.\n",
        "Then $e^{-Œªa}=\\frac{1}{2}$ and $a=\\frac{\\ln2}{Œª}$. Median.\n",
        "\n",
        "- 5.32: time required to reapir a machine is exponentially distributed with $Œª=\\frac{1}{2}$. The probability that repair time exceeds 2 hours is $1-F(2)=e^{-Œª2}$. What is the conditional probability that repair takes at least 10 hours, given that it took more than 9 hours? $P(X>9+1|X>9)=P(X>1)=e^{-Œª}$.\n",
        "\n",
        "- 5.36: suppose the life expectancy of an instrument has hazard rate $Œª(t)=t^3,\\ t>0$. Survival\n",
        "$1-F(a)=e^{-‚à´_0^at^3\\ dt}$\n",
        "$=e^{-\\frac{1}{4}[t^4]_0^a}=e^{-\\frac{a^4}{4}}$. Probability of surviving to age 2 is $e^{-\\frac{2^4}{4}}=0.018$.\n",
        "Probability of lifetime between age 0.4 and 1.4 is $F(1.4)-F(0.4)=0.611$.\n",
        "Conditional probability that an item survives to age 2 given that it has survived age 1 is $e^{-\\frac{1}{4}[t^4]_{1}^{2}}=e^{-\\frac{15}{4}}=0.0235$.\n",
        "Or $P(X>2|X>1)=\\frac{P(X>2,X>1)}{P(X>1)}$\n",
        "$=e^{-\\frac{2^4}{4}}/e^{-\\frac{1^4}{4}}$\n",
        "$=e^{-\\frac{15}{4}}$.\n"
      ],
      "metadata": {
        "id": "ehqng3ifDbnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blitzstein Chapter 5 (Continuous random variables):\n",
        "\n",
        "- e5.1.6: the logistic distribution has CDF $F(x)=\\frac{e^x}{1+e^x},\\ x‚àà‚Ñù$, and PDF $f(x)=\\frac{e^x}{(1+e^x)^2},\\ x‚àà‚Ñù$.\n",
        "\n",
        "- e5.1.7: the Rayleigh distribution has CDF $F(x)=1-e^{-x^2/2},\\ x>0$, and PDF $f(x)=xe^{-x^2/2},\\ x>0$.\n",
        "\n",
        "- e5.3.4: for logistic distribution, $F^{-1}(U)=\\ln(\\frac{U}{1-U})$ is a random variable with CDF $F(a)=\\frac{e^x}{1+e^x}$.\n",
        "\n",
        "- e5.3.5: for Rayleigh distribution, $F^{-1}(U)=\\sqrt{-2\\ln(1-U)}$ is a random variable with CDF $F(a)=1-e^{-x^2/2}$\n",
        "\n",
        "- e5.4.7: for folded normal distribution, $Y=|Z|$ where $Z‚àº\\Normal(0,1)$. Find $\\E[Y]$, $\\Var(Y)$, CDF/PDF of $Y$.\n",
        "$P(Y‚â§a)=P(|Z|‚â§a)$ $=P(-a‚â§Z‚â§a)$.\n",
        "$F_Y(a)=2Œ¶(a)-1$. $f_Y(x)=\\frac{d}{dx}F_Y(x)=2œï(x)$.\n",
        "$\\E|Z|=\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^‚àû|x|e^{-x^2/2}\\ dx$\n",
        "$=\\frac{2}{\\sqrt{2œÄ}}‚à´_0^‚àûxe^{-x^2/2}\\ dx$\n",
        "$=\\frac{2}{\\sqrt{2œÄ}}\\left[-e^{-x^2/2}\\right]_0^‚àû=\\frac{2}{\\sqrt{2œÄ}}$.\n",
        "$\\E[Y^2]=\\E[Z^2]=\\Var(Z)+\\E[Z]^2=1$.\n",
        "$\\Var(Y)=1-\\frac{2}{œÄ}$.\n",
        "\n",
        "- e5.6.5: three students are working on their homework with Exponential time average 6 hours to complete it. On average what is the earliest time all three complete their homework? Let $T=\\max(X_1,X_2,X_3)$ where independent $X_i‚àº\\Expo(Œª=\\frac{1}{6})$. Let $T=T_1+T_2+T_3$ where $T_i$ is the additional time until the $i$th arrival.\n",
        "$T_1=\\min(X_1,X_2,X_3)‚àº\\Expo(3Œª)$.\n",
        "By memoryless, $T_2‚àº\\Expo(2Œª)$, and $T_3‚àº\\Expo(Œª)$.\n",
        "Then $\\E[T]=\\E[T_1]+\\E[T_2]+\\E[T_3]$\n",
        "$=\\frac{1}{3Œª}+\\frac{1}{2Œª}+\\frac{1}{Œª}$.\n",
        "All three students are expected to complete homework after 11 hours.\n",
        "\n",
        "- e5.6.6a: A machine is a cycle of working, breaking down, and getting fixed. It works at time 0 for $\\Expo(Œª)$ days and then breaks down, then would take $\\Expo(Œª)$ days to get fixed, and the cycle continues. Find the number of (working ‚Üí broken) and (broken ‚Üí working) transitions $N_t$ that occur in $(0,t)$. The transitions arrive at iid $\\Expo(Œª)$ in a Poisson process, so $N_t‚àº\\Pois(Œªt)$.\n",
        "\n",
        "- e5.6.6b: The machine is redesigned to have 5 components, each independently works for $\\Expo(Œª)$ days. The machine works at time 0, and breaks down if 2+ components fail. Find the expected time until the machine breaks down. Let $T$ be the time to breakdown, $T_i$ be time to $i$th component failure.\n",
        "$T=T_1+T_2$ and $\\E[T]=\\E[T_1]+\\E[T_2]$ $=\\frac{1}{5Œª}+\\frac{1}{4Œª}$.\n",
        "\n",
        "- e5.7.4: athletes compete one at a time at high jump. Let $X_j$ be continuous iid how high the $j$th jumper scored. $j$th jumper sets a record if $X_j>\\max(X_1,...,X_{j-1})$. Find the expected number of records $N$ among the first $n$ jumpers. Let $I_j$ be indicator that $j$th jumper sets a record. $\\E[I_j]=\\frac{(j-1)!}{j!}=\\frac{1}{j}$. $\\E[N]=\\suml_{j=1}^n\\frac{1}{j}$. $\\liml_{n‚Üí‚àû}\\suml_{j=1}^n\\frac{1}{j}=‚àû$.\n",
        "\n",
        "- 5.10: Let $U‚àº\\Unif(0,8)$. It's newly disclosed that $U‚àà(3,7)$, what is the condition distribution of $U$? $U‚àº\\Unif(3,7)$.\n",
        "\n",
        "- 5.11: Let $U~\\Unif(-1,1)$. What is PDF and CDF of $U^2$?\n",
        "$P(U^2‚â§a)=P(|U|‚â§\\sqrt{a})$\n",
        "$=‚à´_{-\\sqrt{a}}^\\sqrt{a}\\frac{1}{2}\\ dx=\\sqrt{a}$.\n",
        "CDF $F_{U^2}(a)=\\sqrt{a}$ and\n",
        "PDF $f_{U^2}(x)=\\frac{1}{2\\sqrt{x}}$, which is no longer uniform.\n",
        "\n",
        "- 5.14: Let $U_1,...,U_n$ be iid $\\Unif(0,1)$ and $X=\\max(U_1,...,U_n)$.\n",
        "CDF $F_X(a)=P(U_1‚â§a)...P(U_n‚â§a)=a^n$ and\n",
        "PDF $f_X(x)=nx^{n-1}$. $\\E[X]=‚à´_0^1nx^n\\ dx$\n",
        "$=\\frac{n}{n+1}$.\n",
        "\n",
        "- 5.15: Using $U‚àº\\Unif(0,1)$, construct $X‚àº\\Expo(Œª)$. CDF $F_X(a)=1-e^{-Œªa}$, then $X=F_X^{-1}(U)=\\frac{-1}{Œª}ln(1-U)$.\n",
        "$\\E[X]=\\frac{-1}{Œª}\\E[\\ln(1-U)]$\n",
        "$=\\frac{-1}{Œª}‚à´_0^1\\ln(u)\\ du$\n",
        "$=\\frac{-1}{Œª}\\left[u\\ln(u)-u\\right]_0^1=\\frac{1}{Œª}$\n",
        "\n",
        "- 5.16: Let $U‚àº\\Unif(0,1)$ and $X=\\ln\\left(\\frac{U}{1-U}\\right)$. Find $\\E[X^2]$ and $\\E[X]$.\n",
        "By LOTUS, $\\E[X^2]=‚à´_0^1\\ln\\left(\\frac{u}{1-u}\\right)^2 \\ du$.\n",
        "By symmetry of $U$, $\\E[X]=\\E\\left[\\ln\\left(\\frac{U}{1-U}\\right)\\right]$ $=\\E[\\ln(U)]-\\E[\\ln(1-U)]=0$\n",
        "\n",
        "- 5.17: Let $U‚àº\\Unif(0,1)$. Create random variable $X$ as a function of $U$ when $F_X(x)=1-e^{-x^3}$.\n",
        "$y=1-e^{-F_X^{-1}(y)^3}$\n",
        "$‚áí-F_X^{-1}(y)^3=\\ln(1-y)$\n",
        "$‚áíF_X^{-1}(y)=-\\sqrt[3]{\\ln(1-y)}$.\n",
        "\n",
        "- 5.20: Error function $\\t{erf}(z)=\\frac{2}{\\sqrt{œÄ}}‚à´_0^ze^{-x^2}\\ dx$. Let $y=\\frac{x}{\\sqrt{2}}$ then\n",
        "$Œ¶(z)=\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^{z}e^{-x^2/2}\\ dx$\n",
        "$=\\frac{1}{\\sqrt{œÄ}}‚à´_{-‚àû}^{\\frac{z}{\\sqrt{2}}}e^{-y^2}\\ dy$\n",
        "$=\\frac{1}{2}\\left[1+\\frac{2}{\\sqrt{œÄ}}‚à´_0^{\\frac{z}{\\sqrt{2}}}e^{-y^2}\\ dy\\right]$\n",
        "$=\\frac{1}{2}\\left[1+\\frac{2}{\\sqrt{œÄ}}‚à´_0^{\\frac{z}{\\sqrt{2}}}e^{-y^2}\\ dy\\right]$\n",
        "$=\\frac{1}{2}\\left[1+\\t{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right]$\n",
        "\n",
        "- 5.21: Find the points of second derivative inflection of the $\\Normal(0,1)$ PDF\n",
        "$œï(x)=\\frac{1}{\\sqrt{2œÄ}}e^{-x^2/2}$.\n",
        "$\\frac{dœï}{dx}=-x\\frac{1}{\\sqrt{2œÄ}}e^{-x^2/2}$.\n",
        "$\\frac{d^2œï}{dx^2}=-\\frac{1}{\\sqrt{2œÄ}}e^{-x^2/2}+x^2\\frac{1}{\\sqrt{2œÄ}}e^{-x^2/2}$. Then\n",
        "$0=(x^2-1)e^{-x^2/2}=(x-1)(x+1)e^{-x^2/2}$. The points of inflection are at $z=¬±1$ standard deviation.\n",
        "\n",
        "- 5.25: Find $P(X\\lt Y)$ if $X‚àº\\Normal(a,b)$ and $Y‚àº\\Normal(c,d)$ are independent. $X-Y‚àº\\Normal(a-c,b+d)$, and $P(X-Y\\lt0)=Œ¶(\\frac{c-a}{\\sqrt{b+d}})$\n",
        "\n",
        "- 5.34: Let $I_{Z>0}$ be indicator, and $X=ZI_{Z>0}$. Find $\\E[X]$ and $\\Var(X)$.\n",
        "$\\E[X]=\\frac{1}{\\sqrt{2œÄ}}‚à´_0^‚àûxe^{-x^2/2}\\ dx$\n",
        "$=\\frac{1}{\\sqrt{2œÄ}}\\left[-xe^{-x^2/2}\\right]_0^‚àû-\\frac{1}{\\sqrt{2œÄ}}‚à´_0^‚àû-e^{-x^2/2}\\ dx$\n",
        "$=\\frac{1}{\\sqrt{2œÄ}}\\left[-e^{-x^2/2}\\right]_0^‚àû=\\frac{1}{\\sqrt{2œÄ}}$.\n",
        "$\\E[X^2]=\\frac{1}{\\sqrt{2œÄ}}‚à´_0^‚àûx^2e^{-x^2/2}\\ dx$\n",
        "$=\\frac{1}{2}\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^‚àûx^2e^{-x^2/2}\\ dx$\n",
        "$=\\frac{1}{2}\\E[Z^2]=\\frac{1}{2}$\n",
        "\n",
        "- 5.36: a post office has 2 clerks currently serving B and C when A enters. Assuming expon(Œª) time spent serving each customer. What is the probability that A is the last to be done being served? After B or C is done, the probability is 1/2 because expon(Œª) is memoryless. A is expected to spend min(B,C)+A amount of time: $\\frac{1}{2Œª}+\\frac{1}{Œª}$\n",
        "\n",
        "- 5.37: Let $T‚àº\\Expo(Œª)$ be the time until a radioactive particle decays. Time at which the probability of decay is 50% is $e^{-Œªt}=0.5‚áít=\\frac{\\ln(2)}{Œª}$.\n",
        "If $L$ be the time that the first of $n$ iid particles decays,\n",
        "$\\E[L]=\\frac{1}{nŒª}$ and\n",
        "$\\Var(L)=\\frac{1}{(nŒª)^2}$.\n",
        "If $M=\\max(T_1,...,T_n)$, then\n",
        "$\\E[M]=\\suml_{i=1}^n\\frac{1}{iŒª}$ and\n",
        "$\\Var(L)=\\suml_{i=1}^n\\frac{1}{(iŒª)^2}$\n",
        "\n",
        "- 5.38: F wants to sell his car to the first offer over 18k. Offers are iid expon(Œª) with mean 12k. How many offers is he expected to receive before sale?\n",
        "Let $X‚àº\\Expo(\\frac{1}{12})$ be an offer, then $N‚àº\\Geom(P(X>18))$ is the number of offers before sale. $P(X>18)=e^{-18/12}$ then $\\E[N]=e^{18/12}$.\n",
        "By memoryless, $P(X>a|X>18)=P(X>18)P(X>a-18)$. Then $\\E[X|X>18]=18+\\E[X]=30$\n",
        "\n",
        "- 5.39: F wants to sell his car instead to the highest of the first 3 offers that he receives. What is the expected price of sale? $\\E[X]=\\frac{1}{3Œª}+\\frac{1}{2Œª}+\\frac{1}{Œª}=\\frac{12}{3}+\\frac{12}{2}+12=22$\n",
        "\n",
        "- 5.45: Emails are arriving according to Poisson process at 20 per hour. Let $T_3$ be the time at which the 3rd email arrives. Find $P(T_3>0.1)$. According to count-time duality, $P(T_3>0.1)=P(N_{0.1}\\lt3)$ $=e^{-2}+e^{-2}(2)+e^{-2}\\frac{(2)^2}{2}=5e^{-2}$.\n",
        "\n",
        "Blitzstein Chapter 8 (Transformations):\n",
        "\n",
        "- e8.3.4: A new drug clinical trial is to be conducted on $n$ patients to cure conditionitis with probability $p$, which is unknown but has prior distribution $p‚àº\\Unif(0,1)$.\n",
        "  - Find the probability that $k$ of $n$ patients will be cured.\n",
        "  $P(X=k)=‚à´_0^1\\binom{n}{k}p^k(1-p)^{n-k}\\ dp$\n",
        "  $=\\frac{1}{n+1}$.\n",
        "  - Suppose all $n$ patients were cured. Find the probability that $p>\\frac{1}{2}$.\n",
        "  $(p|X=n)‚àº\\Beta(n+1,1)$.\n",
        "  $P(p>\\frac{1}{2}|X=n)=‚à´_{1/2}^1\\frac{1}{Œ≤(n+1,1)}x^n\\ dx$\n",
        "  $=\\frac{(n+1)!}{n!}\\frac{1-(1/2)^{n+1}}{n+1}$\n",
        "  $=1-\\frac{1}{2^{n+1}}$.\n",
        "\n",
        "- e8.3.5: a basketball player shoots $N‚àº\\Pois(Œª)$ free throws in a game. Let $X=I_1+...+I_N$ be number of successful throws where $I_i$ is indicator for making throw $i$ and $\\E[I_i]=p‚àº\\Beta(a,b)$. $I_i$ is conditionally independent given $p$, and $N$ is independent.\n",
        "  - Find $X|N,p$. $(X|N,p)‚àº\\Binom(N,p)$.\n",
        "  - Find $X|p$.\n",
        "  $P(X=k|p)=P(X=k|N=n,p)P(N=n)$\n",
        "  $=\\binom{n}{k}p^kq^{n-k}\\frac{e^{-Œª}Œª^n}{n!}$\n",
        "  $=\\frac{e^{-pŒª}(pŒª)^k}{k!}\\frac{e^{-qŒª}(qŒª)^{n-k}}{(n-k)!}$.\n",
        "  $(X|p)‚àº\\Pois(pŒª)$.\n",
        "  - Find $X|N$ where $a=b=1$.\n",
        "  $P(X=k|N=n)=‚à´_p P(X=k|N,p) f(p)\\ dp$\n",
        "  $=‚à´_0^1\\binom{n}{k}p^k(1-p)^{n-k}\\ dp$\n",
        "  $=\\frac{1}{n+1}$\n",
        "  - Find $p|X,N$.\n",
        "  $(p|X=k,N=n)‚àº\\Beta(a+k,b+n-k)$.\n",
        "\n",
        "- 8.28: $B‚àº\\Beta(a,b)$. Find distribution of $T=1-B$.\n",
        "$f_B(x)=\\frac{Œì(a+b)}{Œì(a)Œì(b)}x^{a-1}(1-x)^{b-1}$.\n",
        "$|\\frac{dx}{dt}|=|-1|$.\n",
        "$f_T(t)=\\frac{Œì(a+b)}{Œì(a)Œì(b)}t^{b-1}(1-t)^{a-1}$.\n",
        "$T‚àº\\Beta(b,a)$.\n",
        "\n",
        "- 8.29: $X‚àº\\Gamma(a,Œª)$ and $Y‚àº\\Gamma(b,Œª)$ independent. Find distribution of $T=X+Y$.\n",
        "$f_X(x)=\\frac{e^{-Œªx}(Œªx)^a}{xŒì(a)}$.\n",
        "$f_T(t)=‚à´_sf_X(t-s)f_Y(s)\\ ds$.\n",
        "Limits of integration: $s>0$ and $t-s>0‚áís\\lt t$. Limits on $t$: $t>0$.\n",
        "$f_T(t)=‚à´_0^t\\frac{e^{-Œª(t-s)}(Œª(t-s))^a}{(t-s)Œì(a)}\\frac{e^{-Œªs}(Œªs)^b}{sŒì(b)}\\ ds$\n",
        "$=\\frac{e^{-Œªt}Œª^{a+b}}{Œì(a)Œì(b)}‚à´_0^t\\frac{(t-s)^as^b}{(t-s)s}\\ ds$\n",
        "$=\\frac{e^{-Œªt}Œª^{a+b}}{Œì(a)Œì(b)}‚à´_0^t(t-s)^{a-1}s^{b-1}\\ ds$\n",
        "$=\\frac{e^{-Œªt}Œª^{a+b}}{Œì(a)Œì(b)}t^{a-1+b-1+1}‚à´_{s=0}^t(1-(\\frac{s}{t}))^{a-1}(\\frac{s}{t})^{b-1}\\ d(\\frac{s}{t})$\n",
        "$=\\frac{e^{-Œªt}Œª^{a+b}}{Œì(a)Œì(b)}t^{a+b-1}‚à´_{u=0}^1(1-u)^{a-1}u^{b-1}\\ du$\n",
        "$=\\frac{e^{-Œªt}Œª^{a+b}}{Œì(a)Œì(b)}t^{a+b-1}\\frac{Œì(a)Œì(b)}{Œì(a+b)}$\n",
        "$=\\frac{e^{-Œªt}(Œªt)^{a+b}}{tŒì(a+b)}$.\n",
        "$T‚àº\\Gamma(a+b,Œª)$.\n",
        "\n",
        "- 8.30: $B‚àº\\Beta(a,b)$. Find $\\E[B^k]$ and $\\Var(B)$.\n",
        "$f_B(x)=\\frac{Œì(a+b)}{Œì(a)Œì(b)}x^{a-1}(1-x)^{b-1}$.\n",
        "$\\E[B^k]=‚à´_0^1\\frac{Œì(a+b)}{Œì(a)Œì(b)}x^kx^{a-1}(1-x)^{b-1}\\ dx$\n",
        "$=\\frac{Œì(a+b)Œì(a+k)}{Œì(a)Œì(a+b+k)}‚à´_0^1\\frac{Œì(a+b+k)}{Œì(a+k)Œì(b)}x^{a+k-1}(1-x)^{b-1}\\ dx$\n",
        "$=\\frac{Œì(a+b)Œì(a+k)}{Œì(a)Œì(a+b+k)}$.\n",
        "$\\E[B^2]=\\frac{(a+b-1)!(a+1)!}{(a-1)!(a+b+1)!}$\n",
        "$=\\frac{(a+1)a}{(a+b+1)(a+b)}$.\n",
        "$\\E[B]=\\frac{(a+b-1)!a!}{(a-1)!(a+b)!}$\n",
        "$=\\frac{a}{a+b}$.\n",
        "$\\Var(B)=\\frac{a(a+b)(a+1)-a^2(a+b+1)}{(a+b+1)(a+b)^2}$\n",
        "$=\\frac{ab}{(a+b+1)(a+b)^2}$.\n",
        "\n",
        "- 8.31: $X‚àº\\Gamma(a,Œª)$ and $Y‚àº\\Gamma(b,Œª)$ independent. Are $X/Y$ and $X+Y$ independent?\n",
        "Let $W=\\frac{X}{X+Y}$, then $1-W=\\frac{Y}{X+Y}$.\n",
        "$\\frac{W}{1-W}=\\frac{X}{Y}$. If $W$ and $Z=X+Y$ are independent, then $g(W)$ and $h(Z)$ are also independent. Therefore $X/Y$ and $X+Y$ are independent.\n",
        "\n",
        "- 8.32 (F-test): Let $X‚àº\\Gamma(\\frac{m}{2},\\frac{1}{2})$, $Y‚àº\\Gamma(\\frac{n}{2},\\frac{1}{2})$, and $V=\\frac{X/m}{Y/n}$. Find the distribution of $\\frac{mV}{n+mV}$.\n",
        "$\\frac{mV}{n+mV}=\\frac{nX/Y}{n+nX/Y}$\n",
        "$=\\frac{X}{X+Y}$\n",
        "$‚àº\\Beta(\\frac{m}{2},\\frac{n}{2})$.\n",
        "\n",
        "- 8.33: Customers arrive according to a Poisson process with unknown rate guessed to be $Œª‚àº\\Expo(3)$ customers per hour. Let $X$ be the number of customers who arrive between 1pm and 3pm. Given that $X=2$ was observed, find the posterior PDF of $Œª$.\n",
        "$f_1(Œª|x)=\\frac{P(X=x|Œª)f_0(Œª)}{P(X=x)}$\n",
        "$‚àù\\frac{e^{-2Œª}(2Œª)^x3e^{-3Œª}}{x!}$.\n",
        "$f_1(Œª|2)‚àù\\frac{e^{-2Œª}(2Œª)^23e^{-3Œª}}{2}$\n",
        "$‚àùe^{-5Œª}Œª^2$\n",
        "$‚àù\\frac{e^{-5Œª}(5Œª)^3}{Œª}$.\n",
        "Prior $Œª‚àº\\Gamma(1,3)$.\n",
        "Posterior $(Œª|X)‚àº\\Gamma(3,5)$\n",
        "\n",
        "Blitzstein Chapter 10: Inequalities and limit theorems\n",
        "\n",
        "- 10.23: Let $V_n‚àºœá_n^2$ and $T_n‚àºt_n$.\n",
        "  - Find $a_n,b_n$ such that $a_n(V_n-b_n)$ converges to $\\Normal(0,1)$.\n",
        "  Location-scale: If $Z‚àº\\Normal(0,1)$ then $b+aZ‚àº\\Normal(b,a^2)$. Chi-square $V_n\\dot‚àº\\Normal(b_n,\\frac{1}{a_n^2})\\dot‚àº\\Normal(n,2n)$. Therefore $b_n=n,a_n=\\frac{1}{\\sqrt{2n}}$.\n",
        "  - Find distribution of $\\frac{T_n^2}{n+T_n^2}$.\n",
        "  $T_n=\\frac{Z}{\\sqrt{V_n/n}}$\n",
        "  $\\frac{T_n^2}{n+T_n^2}=\\frac{Z^2}{Z^2+V_n}$.\n",
        "  $Z^2‚àº\\Gamma(\\frac{1}{2},\\frac{1}{2})$,\n",
        "  $V_n‚àº\\Gamma(\\frac{n}{2},\\frac{1}{2})$.\n",
        "  Therefore $\\frac{V_1}{V_1+V_n}‚àº\\Beta(\\frac{1}{2},\\frac{n}{2})$.\n",
        "\n",
        "- 10.24: let $T_1,...,T_n‚àºt_m,\\ m‚â•3$ iid. Find $a_n,b_n$ such that $a_n(T_1+...+T_n-b_n)$ converges to $\\Normal(0,1)$.\n",
        "$T_i‚àº(0,\\frac{m}{m-2})$.\n",
        "By location-scale, $T_1+...+T_n\\dot‚àº\\Normal(b_n,\\frac{1}{a_n^2})\\dot‚àº\\Normal(0,\\frac{nm}{m-2})$. Therefore $a_n=0,b_n=\\sqrt{\\frac{m-2}{nm}}$."
      ],
      "metadata": {
        "id": "OUMxaSKZo2y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Joint distributions"
      ],
      "metadata": {
        "id": "09BvKQcdHIkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Joint PMF, PDF, and CDF**:\n",
        "- For discrete $X$ and $Y$ has joint PMF $\\suml_{x,y}P(X=x,Y=y)=1$. Marginal PMF $P(X=x)=\\suml_yP(X=x,Y=y)$ is the sum of PMFs for each $x$ along all values of $y$.\n",
        "  - Marginal CDF: $F_X(a)=\\liml_{y‚Üí‚àû}F_{X,Y}(a,y)$.\n",
        "- $X$ and $Y$ are **jointly continuous** if there exists a function $f_{X,Y}(x,y)$ such that (1) $f_{X,Y}(x,y)‚â•0$ and $\\iint_{x,y}f_{X,Y}(x,y)\\ dx\\ dy=1$, and (2) for every set in $C=((x,y):x‚àà‚Ñù,y‚àà‚Ñù)$ pairs, $P((X,Y)‚ààC)=\\iint_{(x,y)‚ààC}f_{X,Y}(x,y)\\ dx\\ dy$.\n",
        "  - Using joint PDF $f$ we can find $P(X‚ààA,Y‚ààB)=‚à´_B‚à´_Af(x,y)\\ dx\\ dy$.\n",
        "  - $f(x,y)=\\frac{‚àÇ^2}{‚àÇx‚àÇy}F(x,y)$\n",
        "  - $f_X(x)=‚à´_{-‚àû}^‚àûf(x,y)\\ dy$\n",
        "- **LOTUS**: $\\E[g(X,Y)]=\\suml_{y}\\suml_{x}g(x,y)P(X=x,Y=y)$ and $\\E[g(X,Y)]=\\iint_{x,y} g(x,y)f(x,y)\\ dx\\ dy$\n",
        "  - $\\E[X]=\\iint_{x,y}xf_{X,Y}(x,y)\\ dy\\ dx=‚à´_xxf_X(x)\\ dx$\n",
        "- Conditional PMF $P(X=x|Y=y)=\\frac{P(X=x,Y=y)}{P(Y=y)}$\n",
        "- Conditional PDF $f_{X|Y}(x|y)=\\frac{f(x,y)}{f_Y(y)}$.\n",
        "  - Baye's theorem: $f_{X|Y}(x|y)=\\frac{f_{Y|X}(y|x)f_X(x)}{f_Y(y)}$\n",
        "\n",
        "$X$ and $Y$ are **independent** iff\n",
        "- $P(X‚ààA,Y‚ààB)=P(X‚ààA)P(Y‚ààB)$ for all $A,B$\n",
        "- $P(X=x,Y=y)=P(X=x)P(Y=y)$ for all $x,y$\n",
        "- $F_{X,Y}(a,b)=F_X(a)F_Y(b)$ for all $a,b$\n",
        "- $f_{X,Y}(x,y)=f_X(x)f_Y(y)$ for all $x,y$.\n",
        "- $f_{X,Y}(x,y)=h(x)g(y),\\ x‚àà(a,b), y‚àà(c,d)$ the joint PDF or PMF can be factored into two terms with independent supports. I.e., $0\\lt x+y\\lt 1$ means dependency.\n",
        "- $\\E[g(X)h(Y)]=\\E[g(X)]\\E[h(Y)]$ for **all** $g$ and $h$\n",
        "  - $\\E[XY]=\\E[X]\\E[Y]$ (Cov(X,Y)=0) does not imply $X$ and $Y$ are independent (see MVN)\n",
        "- $M_{X+Y}(t)=M_X(t)M_Y(t)$ for MGF and characteristic functions.\n",
        "- $M_{X,Y}(s,t)=M_X(s)M_Y(t)$ for joint MGF and joint characteristic functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "P0d59HrbHP4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change of variables**: Given $f_{X_1,X_2}$, $Y_1=g_1(X_1,X_2)$, and $Y_2=g_2(X_1,X_2)$ for functions $g_1$ and $g_2$ such that (1) $x_1$ and $x_2$ can be expressed as $x_1=h_1(y_1,y_2)$ and $x_2=h_2(y_1,y_2)$ and (2) Jacobian $J(x_1,x_2)=\\left|\\frac{‚àÇ(g_1,g_2)}{‚àÇ(x_1,x_2)}\\right|\\neq0$, then $f_{Y_1,Y_2}(y_1,y_2)=f_{X_1,X_2}(x_1,x_2)|J(x_1,x_2)|^{-1}$ where $x_1=h_1(y_1,y_2)$ and $x_2=h_2(y_1,y_2)$.\n",
        "- Alternatively $J(y_1,y_2)=\\left|\\frac{‚àÇ(x_1,x_2)}{‚àÇ(y_1,y_2)}\\right|$ and $f_{Y_1,Y_2}(y_1,y_2)=f_{X_1,X_2}(x_1,x_2)|J(y_1,y_2)|$  where $x_1=h_1(y_1,y_2)$ and $x_2=h_2(y_1,y_2)$.\n",
        "- Let $X$ be continuous random variable and let $Y=g(X)$ where $g$ is differentiable and monotonous, then $f_Y(y)=f_X(x)|\\frac{dx}{dy}|$ where $x=g^{-1}(y)$ and $Y$ has support in all $g(x)$ for all support of $X$.\n",
        "  - Proof: $F_Y(y)=F_X(g^{-1}(y))=F_X(x)$. Differentiate $f_Y(y)=\\frac{d}{dy}F_X(x)=f_X(x)\\frac{dx}{dy}$.\n",
        "  - If $g$ is monotonous decreasing then $\\frac{dx}{dy}\\lt 0$, hence $|\\frac{dx}{dy}|$ is used.\n",
        "  - $|\\frac{dy}{dx}|^{-1}$ fraction reciprocal works as well: $\\frac{dy}{dx}\\frac{dx}{dy}=1$.\n",
        "  - Can be written as $f_Y(y)\\ dy=f_X(x)\\ dx$ where $f_X(x)Œîx$ and $f_Y(y)Œîy$ are probability centered at $x$ and $y$.\n",
        "  - location-scale transformation (e8.1.6): If $Y=a+bX$, then $|\\frac{dx}{dy}|=\\frac{1}{|b|}$ and $f_Y(x)=\\frac{1}{|b|}f_X(\\frac{y-a}{b})$.\n",
        "- Let $\\v{X}=(X_1,...,X_n)$ be continuous random vector, and let $g:A_0‚ÜíB_0$ be invertible function on open subsets of $‚Ñù^n$. Let $\\v{Y}=g(\\v{X})$ and $\\v{y}=g(\\v{x})$, then $\\v{X}=g^{-1}(\\v{Y})$ and $\\v{x}=g^{-1}(\\v{y})$. Suppose all $\\frac{‚àÇx_i}{‚àÇy_i}$ exist and continuous, then PDF\n",
        "$f_{\\v{Y}}(\\v{y})=f_{\\v{X}}(g^{-1}(\\v{y}))\\lVert\\frac{‚àÇ\\v{x}}{‚àÇ\\v{y}}\\rVert,\\ \\v{x}‚ààA_0,\\v{y}‚ààB_0$ using Jacobian matrix\n",
        "$$\\small J=\\frac{‚àÇ\\v{x}}{‚àÇ\\v{y}}=\\BPM\n",
        "\\frac{‚àÇx_1}{‚àÇy_1} & ... & \\frac{‚àÇx_1}{‚àÇy_n} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{‚àÇx_n}{‚àÇy_1} & ... & \\frac{‚àÇx_n}{‚àÇy_n}\n",
        "\\EPM$$\n",
        "  - $\\left|\\frac{‚àÇ\\v{x}}{‚àÇ\\v{y}}\\right|=\\left|\\frac{‚àÇ\\v{y}}{‚àÇ\\v{x}}\\right|^{-1}$ just like the 1-D case. Use whichever is easier to compute.\n",
        "  \n"
      ],
      "metadata": {
        "id": "aJ02h2eel6gZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sums of independent distributions**: let $X$ and $Y$ be independent, then PDF $f_{X+Y}$ is the convolution $f_X‚äóf_Y$.\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "F_{X+Y}(t)&=P(X+Y‚â§t) \\\\\n",
        "&=\\iint_{x+y‚â§t}f_X(x)f_Y(y)\\ dx\\ dy \\\\\n",
        "&=‚à´_{-‚àû}^‚àûF_X(t-y)f_Y(y)\\ dy\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "f_{X+Y}(t)&=\\frac{d}{dt}‚à´_{-‚àû}^‚àûF_X(t-s)f_Y(s)\\ ds \\\\\n",
        "&=‚à´_{-‚àû}^‚àûf_X(t-s)f_Y(s)\\ ds \\\\\n",
        "&=f_X \\otimes f_Y\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- **How to compute convolution**: The convolution is segmented by $t$, because we only integrate over $s$-domain where the integrant $f_X(t-s)f_Y(s)$ is nonzero. The support of $X$ and $Y$ place max/min constraints on the limits of integration, which means the limits of integration varies with different values of $t$. Or use Laplace/Fourier transform.\n",
        "- Alternate proof/route use $U=X+Y$, $W=X$, find $f_{U,W}(u,w)$, then integrate away W to get $f_U(u)$.\n",
        "- Discrete: $P(X+Y=t)=\\suml_{x}P(Y=t-x)P(X=x)$.\n",
        "- For $X‚àº\\Gamma(s,Œª)$ and $Y‚àº\\Gamma(t,Œª)$ independent, then $(X+Y)‚àº\\Gamma(s+t,Œª)$ and $\\frac{X}{X+Y}‚àº\\Beta(s,t)$\n",
        "  - (Ross Ch6 e3b, convolution Blitzstein p8.29) for $X_1,...,X_n$ where $X_i‚àº\\Expo(Œª)$, $\\suml_{i=1}^nX_i‚àº\\Gamma(n,Œª)$.\n",
        "- For $X_1,...,X_n$ independent where $X_i‚àº\\Normal(Œº_i,œÉ_i^2)$, then $\\suml_{i=1}^nX_i‚àº\\Normal\\left(\\suml_{i=1}^nŒº_i,\\suml_{i=1}^nœÉ_i^2\\right)$ (Ross S6.3.3, Blitzstein e6.6.3)\n",
        "  - $(X-Y)‚àº\\Normal(Œº_X-Œº_Y,œÉ_X^2+œÉ_Y^2)$ because $-Y‚àº\\Normal(-Œº_Y,œÉ_Y^2)$\n",
        "- For $X_1‚àº\\Pois(Œª_1)$ and $X_2‚àº\\Pois(Œª_2)$ independent, $(X_1+X_2)‚àº\\Pois(Œª_1+Œª_2)$ (Blitzstein e6.5.1) and $(X_1|X_1+X_2=n)‚àº\\Binom(n,\\frac{Œª_1}{Œª_1+Œª_2})$.\n",
        "- For $X_1‚àº\\Binom(m,p)$ and $X_2‚àº\\Binom(n,p)$ independent, $(X_1+X_2)‚àº\\Binom(m+n,p)$."
      ],
      "metadata": {
        "id": "RzvFBh5s4x3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Order statistics**: let $X_1,...,X_n$ be continuous iid with PDF $f$ and CDF $F$, then the sorted list $X_{(1)}‚â§...‚â§X_{(n)}$ are random variables called *order statistics*. $X_{(i)}$ are not independent even if $X_i$ are.\n",
        "- (Ross) Let $i_1,...,i_n$ be the indices of $X_1,...,X_n$ in the sorted list such that $X_{(i_j)}=X_j$, then $X_{(1)}=x_1‚â§\\ ...\\ ‚â§X_{(n)}=x_n$ iff there exists a permutation $i_1,...,i_n$ such that $X_1=x_{i_1},\\ ...\\ ,X_n=x_{i_n}$.\n",
        "  - $f_{X_{(1)},...,X_{(n)}}(x_1,...,x_n)\\ dx_1\\ ...\\ dx_n$ is the probability that $X_{(1)}‚âàx_1,...,X_{(n)}‚âàx_n$.\n",
        "  - There are n choices for $X_{(1)}$, n-1 choices for $X_{(2)}$, and so on, therefore the $n!$ constant.\n",
        "  - The $X_{(i)}$s are not independent, but the $X_i$s are independent and their marginals are factored out.\n",
        "\n",
        "$$\\small\\begin{aligned}\n",
        "f_{X_{(1)},...,X_{(n)}}(x_1,...,x_n)(Œîx)^n=&P\\left(X_{(1)}‚àà(x_1¬±\\frac{Œîx}{2}),\\ ...\\ ,X_{(n)}‚àà(x_n¬±\\frac{Œîx}{2})\\right) \\\\\n",
        "=&n!P\\left(X_1‚àà(x_{i_1}¬±\\frac{Œîx}{2}),\\ ...\\ ,X_n‚àà(x_{i_n}¬±\\frac{Œîx}{2})\\right) \\\\\n",
        "f_{X_{(1)},...,X_{(n)}}(x_1,...,x_n)=&n!f_{X_1,...,X_n}(x_{i_1},...,x_{i_n}) \\\\\n",
        "=&n!f(x_1)\\ ...\\ f(x_n)\n",
        "\\end{aligned}$$\n",
        "- (Blitzstein) For $X_{(n)}=\\max(X_1,...,X_n)$, CDF $F_{X_{(n)}}(x)=F(x)^n$. Similarly for $X_{(1)}=\\min(X_1,...,X_n)$, CDF $F_{X_{(1)}}(x)=(1-F(x))^n$.\n",
        "- For $F_{X_{(j)}}(x)$, j or more of the $X_i$s need to fall to the left of $x$. Let $N$ be the number of $X_i$s to the left of $x$, then $N‚àº\\Binom(n,F(x))$. Therefore $F_{X_{(j)}}(x)=P(N‚â•j)=\\suml_{k=j}^n\\binom{n}{k}F(x)^k(1-F(x))^{n-k}$.\n",
        "- For probability $f_{X_{(j)}}(x)\\ Œîx$, an element is placed in $(x¬±\\frac{Œîx}{2})$  with probability $nf(x)\\ Œîx$, j-1 elements are placed before $x$ with probability $\\binom{n-1}{j-1}F(x)^{j-1}$, and n-j elements are placed to the right of x with probability $(1-(F(x))^{n-j}$. The constants are $n\\binom{n-1}{j-1}$\n",
        "$=\\frac{n(n-1)!}{(j-1)!(n-j)!}$\n",
        "$=\\frac{n!}{(j-1)!(n-j)!}$.\n",
        "Therefore\n",
        "$f_{X_{(j)}}(x)=\\frac{n!}{(j-1)!(n-j)!}F(x)^{j-1}(1-F(x))^{n-j}f(x)$.\n",
        "  - For $U_1,...,U_n‚àº\\Unif(0,1)$, $F_{U_i}(x)=x$. Therefore $f_{U_{(j)}}=\\frac{n!}{(j-1)!(n-j)!}x^{j-1}(1-x)^{n-j}$ and $U_{(j)}‚àº\\Beta(j,n-j+1)$. $\\E[U_{(j)}]=\\frac{j}{n+1}$. This setup relates beta CDF with binomial CDF (Blitzstein 8.41).\n",
        "  - Same setup for joint PDF $f_{X_{(i)},X_{(j)}}(x_i,x_j)$ (Blitzstein 8.48)."
      ],
      "metadata": {
        "id": "w7w_WS2zHezs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ross Chapter 6 (Jointly distributed random variables):\n",
        "- e6.1c: let $f(x,y)=2e^{-x}e^{-2y},\\ x>0,y>0$.\n",
        "$P(X\\lt Y)=\\iint_{x\\lt y}f(x,y)\\ dx\\ dy$\n",
        "$=2‚à´_0^‚àû‚à´_x^‚àûe^{-2y}\\ dy\\ e^{-x}\\ dx$\n",
        "$=2‚à´_0^‚àû\\left[\\frac{-1}{2}e^{-2y}\\right]_x^‚àûe^{-x}\\ dx$\n",
        "$=\\left[\\frac{-1}{3}e^{-3y}\\right]_0^‚àû=\\frac{1}{3}$\n",
        "\n",
        "- e6.1d: A point $(x,y)$ is randomly chosen from a circular area of radius R with uniform distribution $f(x,y)=c,\\ x^2+y^2‚â§R^2$.\n",
        "  - Determine $c$.\n",
        "  $\\iint_{x^2+y^2‚â§R^2}c\\ dx\\ dy=1$ then $dx\\ dy=r\\ dr\\ dŒ∏$ and $\\frac{1}{c}=‚à´_0^R‚à´_0^{2œÄ}r\\ dr\\ dŒ∏=œÄR^2$. Therefore $c=\\frac{1}{œÄR^2}$\n",
        "  - Find $F_X$. $F_X(x)=‚à´_{x^2+y^2‚â§R^2}\\frac{1}{œÄR^2}\\ dy$ $=\\frac{1}{œÄR^2}‚à´_{-\\sqrt{R^2-x^2}}^{\\sqrt{R^2-x^2}}\\ dy$\n",
        "  $=\\frac{2}{œÄR^2}\\sqrt{R^2-x^2}$\n",
        "  - Find CDF of $D=\\sqrt{X^2+Y^2}$. $F(D‚â§a)=\\iint_{\\sqrt{X^2+Y^2}‚â§a}f(x,y)\\ dx\\ dy$\n",
        "  $=\\frac{1}{œÄR^2}‚à´_0^{2œÄ}‚à´_0^ar\\ dr\\ dŒ∏$\n",
        "  $=\\frac{2}{R^2}\\left[\\frac{1}{2}r^2\\right]_0^a$\n",
        "  $=\\frac{a^2}{R^2}$\n",
        "  - Find $\\E[D]$.\n",
        "  $\\E[D]=‚à´_0^R(1-F_D(r))\\ dr$\n",
        "  $=‚à´_0^R\\ dr-‚à´_0^R\\frac{r^2}{R^2} dr$\n",
        "  $=R-\\frac{1}{3R^2}\\left[r^3\\right]_0^R=\\frac{2R}{3}$.\n",
        "  Or $f_D(r)=\\frac{2r}{R^2}$ and\n",
        "  $\\E[D]=‚à´_0^Rrf_D(r)\\ dr$\n",
        "  $=‚à´_0^R\\frac{2r^2}{R^2}\\ dr$\n",
        "  $=\\left[\\frac{2r^3}{3R^2}\\right]_0^R=\\frac{2R}{3}$.\n",
        "\n",
        "- e6.1e: let $f(x,y)=e^{-(x+y)},\\ x>0,y>0$. Find the PDF of $X/Y$.\n",
        "$F_{X/Y}(a)=\\iint_{x/y‚â§a}e^{-(x+y)}\\ dx\\ dy$\n",
        "$=‚à´_{y=0}^‚àû‚à´_{x=0}^{ay}e^{-(x+y)}\\ dx\\ dy$\n",
        "$=‚à´_{y=0}^‚àû\\left[-e^{-(x+y)}\\right]_0^{ay}\\ dy$\n",
        "$=‚à´_{y=0}^‚àû(e^{-y}-e^{-(a+1)y})\\ dy$\n",
        "$=\\left[-e^{-y}\\right]_0^‚àû-\\left[\\frac{-1}{a+1}e^{-(a+1)y}\\right]_0^‚àû$\n",
        "$=1-\\frac{1}{a+1}=\\frac{a}{a+1}$.\n",
        "Therefore $f_{X/Y}(a)=\\frac{1}{a+1}-\\frac{a}{(a+1)^2}$\n",
        "$=\\frac{1}{(a+1)^2}$.\n",
        "\n",
        "- e6.2b: the number of people who enter a post office on a given day follows $\\Pois(Œª)$ and is male with probability $p$. Find the number of males entering a day.\n",
        "  - $P(M+F=i+j)=e^{-Œª}\\frac{Œª^{i+j}}{(i+j)!}$\n",
        "  - $P(M=i,F=j|M+F=i+j)=\\binom{i+j}{i}p^i(1-p)^j=\\frac{(i+j)!}{i!j!}p^i(1-p)^j$\n",
        "  - $P(M=i,F=j)=P(M=i,F=j|M+F=i+j)P(M+F=i+j)$\n",
        "  $=e^{-Œª}\\frac{Œª^{i+j}}{(i+j)!}\\frac{(i+j)!}{i!j!}p^i(1-p)^j$\n",
        "  $=e^{-pŒª}\\frac{Œª^ip^i}{i!}e^{-(1-p)Œª}\\frac{Œª^j(1-p)^j}{j!}$. By independence $P(M=i,F=j)=P(M=i)P(M=j)$ where $P(M=i)=e^{-pŒª}\\frac{(pŒª)^i}{i!}‚àº\\Pois(pŒª)$.\n",
        "\n",
        "- e6.2c: two people decide to meet at a place at a time uniformly distributed between 1pm and 2pm. Find the probability that the first to arrive has to wait longer than 10 minutes.\n",
        "Because $X$ and $Y$ iid, $f(x,y)=f_X(x)f_Y(y)=\\frac{1}{60^2}$, and assume 2x $Y>X$ case.\n",
        "Then $P(X‚â•Y+10)+P(Y‚â•X+10)=2P(Y‚â•X+10)$\n",
        "$=2\\iint_{x‚â•y+10}f(x,y)\\ dx\\ dy$\n",
        "$=2‚à´_0^{50}‚à´_{x=y+10}^{60}\\frac{1}{60^2}\\ dx\\ dy$\n",
        "$=\\frac{2}{60^2}\\int_0^{50}\\left[x\\right]_{y+10}^{60}\\ dy$\n",
        "$=\\frac{2}{60^2}\\left[50y-\\frac{1}{2}y^2\\right]_0^{50}$\n",
        "$=2\\frac{25}{36}-\\frac{25}{36}=\\frac{25}{36}$\n",
        "\n",
        "- e6.2d: (Buffon's needle) A needle of length $L$ is thrown onto a strip whose width is $D>L$ such that the middle of the needle is on the strip. What is the probability that the needle will intersect an edge of the strip?\n",
        "Let $X$ be the length of the normal from edge of the strip to the center of the needle, and $Œò$ be the acute angle formed between the needle and that normal, then the needle intersects the edge of the strip if $\\frac{L}{2}\\cosŒò>X$. Let $f_X(x)=\\frac{2}{D},\\ x=[0,\\frac{D}{2}]$,\n",
        "and $f_Œò(Œ∏)=\\frac{2}{œÄ},\\ Œ∏=[0,\\frac{œÄ}{2}]$.\n",
        "Then $P(\\frac{L}{2}\\cosŒò>X)=\\iint_{\\frac{L}{2}\\cosŒ∏>x}f(x,Œ∏)\\ dx\\ dŒ∏$\n",
        "$=\\frac{4}{œÄD}‚à´_0^{œÄ/2}‚à´_0^{\\frac{L}{2}\\cosŒ∏}\\ dx\\ dŒ∏$\n",
        "$=\\frac{2L}{œÄD}‚à´_0^{œÄ/2}\\cosŒ∏\\ dŒ∏$\n",
        "$=\\frac{2L}{œÄD}$\n",
        "\n",
        "- e6.3a: let $X$ and $Y$ be iid $\\Unif(0,1)$ What is the PDF of $X+Y$?  \n",
        "  - By convolution $f_{X+Y}(t)=‚à´_0^1f_X(t-s)f_Y(s)\\ ds$.\n",
        "  - We need to find the boundaries of integration.\n",
        "  From $f_Y(s)>0$: $s‚àà[0,1]$.\n",
        "  From $f_X(t-s)>0$: $t-s‚àà[0,1]$ or $s‚àà[t-1,t]$.\n",
        "  Together $s‚àà[\\max(0,t-1), \\min(1,t)]$.\n",
        "  For $t‚àà[0,1]$, $s‚àà[0,t]$: $f_{X+Y}(t)=‚à´_0^t\\ ds=t$.\n",
        "  For $t‚àà(1,2]$, $s‚àà[t-1,1]$: $f_{X+Y}(t)=‚à´_{t-1}^1\\ ds=2-t$.\n",
        "  Therefore $f_{X+Y}(t)=\\BC t,&t‚àà[0,1]\\\\ 2-t,&t‚àà(1,2]\\EC$\n",
        "  - Laplace transform: $\\mathcal{L}(f_X(t))(s)=‚à´_0^1e^{-st}\\ dt=\\frac{1-e^s}{s}$,\n",
        "  convolution $\\mathcal{L}(f_X(t)‚äóf_Y(t))(s)=\\frac{1-2e^s+e^{2s}}{s^2}$. Therefore $f_X(t)‚äóf_Y(t)=t-2(t-1)u(t-1)+(t-2)u(t-2), t‚â•0$\n",
        "  $=\\BC t&t‚àà[0,1] \\\\ t-2(t-1)&t‚àà(1,2]\\EC$\n",
        "\n",
        "- e6.3c: A basketball team will play 26 games against class A teams with win probability 0.4 and 18 against class B teams with win probability 0.7. What is the probability that the team wins 25 games or more?\n",
        "By binomial approximation,\n",
        "$X_A‚àº\\Normal(n_Ap_A,n_Ap_A(1-p_A))$ and\n",
        "$X_B‚àº\\Normal(n_Bp_B,n_Bp_B(1-p_B))$.\n",
        "Then $(X_A+X_B)‚àº\\Normal(Œº_A+Œº_B,œÉ_A^2+œÉ_B^2)$ and\n",
        "$P(X_A+X_B‚â•25)=1-Œ¶(\\frac{24.5-23}{\\sqrt{10.02}})=.3178$.\n",
        "What is the probability that the team wins more games against class A than B?\n",
        "$(X_A-X_B)‚àº\\Normal(Œº_A-Œº_B,œÉ_A^2+œÉ_B^2)$\n",
        "  \n",
        "- e6.3d: Assume security price ratios $S_n/S_{n-1}$ are iid lognormal with $Œº=0.0165$ and $œÉ=0.0730$, what is the probability that prices are up during both of the next two sessions? Let $X_1=\\ln(S_1/S_0)$ and $X_2=\\ln(S_2/S_1)$.\n",
        "$P(X_1>0)=Œ¶(\\frac{-Œº}{œÉ})$ then\n",
        "$P(X_1>0, X_2>0)=Œ¶(\\frac{-Œº}{œÉ})^2=.3474$.\n",
        "What is the probability that prices are up after two sessions?\n",
        "$(X_1+X_2)‚àº\\Normal(Œº_1+Œº_2,œÉ_1^2+œÉ_2^2)$ therefore\n",
        "$P(X_1+X_2>0)=Œ¶(\\frac{-2Œº}{\\sqrt{2œÉ^2}})=.6254$\n",
        "\n",
        "- e6.4b: Let $X‚àº\\Pois(Œª_1)$ and $Y‚àº\\Pois(Œª_2)$ be independent. what is $P(X=k|X+Y=n)$?\n",
        "$P(X=k|X+Y=n)=\\frac{P(X=k,Y=n-k)}{P(X+Y=n)}$\n",
        "$=\\binom{n}{k}\\frac{Œª_1^kŒª_2^{n-k}}{(Œª_1+Œª_2)^n}$\n",
        "$=\\binom{n}{k}\\left(\\frac{Œª_1}{Œª_1+Œª_2}\\right)^k\\left(\\frac{Œª_2}{Œª_1+Œª_2}\\right)^{n-k}$.\n",
        "Therefore $(X|X+Y=n)‚àº\\Binom(n,\\frac{Œª_1}{Œª_1+Œª_2})$\n",
        "\n",
        "- e6.5a: Let $f(x,y)=\\frac{12}{5}x(2-x-y),\\ 0\\lt x,y\\lt 1$. Find $f_{X|Y}(x|y)$.\n",
        "First $f_Y(y)=‚à´_0^1f(x,y)\\ dx$\n",
        "$=\\frac{12}{5}\\left[x^2-\\frac{1}{3}x^3-\\frac{y}{2}x^2\\right]_0^1$\n",
        "$=\\frac{8}{5}-\\frac{6}{5}y$.\n",
        "Then $f_{X|Y}(x|y)=\\frac{6x(2-x-y)}{4-3y}$\n",
        "\n",
        "- e6.5b: Let $f(x,y)=\\frac{e^{-x/y}e^{-y}}{y},\\ x>0,y>0$. Find $P(X>1|Y=y)$.\n",
        "First $f_Y(y)=e^{-y}‚à´_0^‚àû\\frac{1}{y}e^{-x/y}\\ dx$\n",
        "$=e^{-y}[-e^{-x/y}]_0^‚àû$\n",
        "$=e^{-y}$.\n",
        "Then $f_{X|Y}(x|y)=\\frac{1}{y}e^{-x/y}$.\n",
        "Finally $P(X>1|Y=y)=‚à´_1^‚àû\\frac{1}{y}e^{-x/y}$\n",
        "$=[-e^{-x/y}]_1^‚àû=e^{-1/y}$\n",
        "\n",
        "- e6.6a: Along a unit distance, 3 markings are placed at iid $\\Unif(0,1)$. What is the probability that no two markings are less than $d$ distance away from each other where $d‚â§1/2$?\n",
        "$f_{X_{(1)},X_{(2)},X_{(3)}}(x_1,x_2,x_3)=3!$. Then\n",
        "$P(X_{(i)}-X{(i-1)}>d)=\\iiint_{x_i-x_{i-1}>d}f_{X_{(1)},X_{(2)},X_{(3)}}(x_1,x_2,x_3)\\ dx_1\\ dx_2\\ dx_3$\n",
        "$=3!‚à´_{x_1=0}^{1-2d}‚à´_{x_2=x_1+d}^{1-d}‚à´_{x_3=x_2+d}^1dx_3\\ dx_2\\ dx_1$\n",
        "$=3!‚à´_{x_1=0}^{1-2d}‚à´_{x_2=x_1+d}^{1-d}(1-d-x_2)dx_2\\ dx_1$\n",
        "$=3!‚à´_{x_1=0}^{1-2d}‚à´_{y_2=1-2d-x_1}^0-y_2\\ dy_2\\ dx_1$\n",
        "$=3!‚à´_{x_1=0}^{1-2d}\\frac{1}{2}(1-2d-x_1)^2\\ dx_1$\n",
        "$=3!‚à´_{y_1=1-2d}^0-\\frac{1}{2}y^2\\ dy_1$\n",
        "$=(1-2d)^3$\n",
        "\n",
        "- e6.6b: In a sample of size 3 from $\\Unif(0,1)$ is observed, what is the probability that the sample median is between 1/4 and 3/4?\n",
        "$f_{(2)}(x)=3!x(1-x)$.\n",
        "$P(\\frac{1}{4}\\lt X_{(2)}\\lt \\frac{3}{4})=3!‚à´_{1/4}^{3/4}x(1-x)\\ dx$\n",
        "$=3!\\left[\\frac{1}{2}x^2-\\frac{1}{3}x^3\\right]_{1/4}^{3/4}=\\frac{11}{16}$\n",
        "\n",
        "- e6.6c: With $n$ iid samples each with PDF $f$ and CDF $F$, find range $X_{(n)}-X_{(1)}$ CDF.\n",
        "$P(X_{(n)}-X_{(1)}‚â§a)=\\iint_{x_n-x_1‚â§a}f_{X_{(1)},X_{(n)}}(x_1,x_n)\\ dx_1\\ dx_n$\n",
        "$=‚à´_{x_1=-‚àû}^‚àû‚à´_{x_n=x_1}^{a+x_1}\\frac{n!}{(n-2)!}(F(x_n)-F(x_1))^{n-2}f(x_1)f(x_n)\\ dx_n\\ dx_1$\n",
        "$=\\frac{n!}{(n-2)!}‚à´_{x_1=-‚àû}^‚àû‚à´_{y=0}^{F(x_1+a)-F(x_1)}y^{n-2}f(x_1)\\ dy\\ dx_1$\n",
        "$=\\frac{n!}{(n-2)!}‚à´_{-‚àû}^‚àû\\left[\\frac{1}{n-1}y^{n-1}\\right]_0^{F(x_1+a)-F(x_1)}f(x_1)\\ dx_1$\n",
        "$=n‚à´_{-‚àû}^‚àû(F(x_1+a)-F(x_1))^{n-1}f(x_1)\\ dx_1$\n",
        "\n",
        "- e6.7a: let $Y_1=X_1+X_2$ and $Y_2=X_1-X_2$, find joint density. $J(x_1,x_2)=\\BVM1&1\\\\1&-1\\EVM=-2$. Then $f_{Y_1,Y_2}=\\frac{1}{2}f_{X_1,X_2}\\left(\\frac{y_1+y_2}{2},\\frac{y_1-y_2}{2}\\right)$.\n",
        "\n",
        "- e6.7b: let $X,Y‚àº\\Normal(0,1)$ represent coordinates to a point. Find $f_{R,Œò}(r,Œ∏)$.\n",
        "$R(x,y)=\\sqrt{x^2+y^2}$, $Œò(x,y)=\\arctan(y/x)$,\n",
        "$X=R\\cosŒò$, $Y=R\\sinŒò$.\n",
        "$\\frac{‚àÇR}{‚àÇx}=\\frac{x}{\\sqrt{x^2+y^2}}$,\n",
        "$\\frac{‚àÇR}{‚àÇy}=\\frac{y}{\\sqrt{x^2+y^2}}$,\n",
        "$\\frac{‚àÇŒò}{‚àÇx}=\\frac{-y/x^2}{y^2/x^2+1}=\\frac{-y}{x^2+y^2}$,\n",
        "$\\frac{‚àÇŒò}{‚àÇy}=\\frac{1/x}{y^2/x^2+1}=\\frac{x}{x^2+y^2}$.\n",
        "$J(x,y)=\\left|\\frac{‚àÇ(R,Œò)}{‚àÇ(x,y)}\\right|$\n",
        "$=\\frac{x^2}{(x^2+y^2)^{3/2}}-\\frac{-y^2}{(x^2+y^2)^{3/2}}$\n",
        "$=\\frac{1}{\\sqrt{x^2+y^2}}=\\frac{1}{r}$.\n",
        "$f_{X,Y}(x,y)=f_X(x)f_Y(y)$\n",
        "$=\\frac{1}{2œÄ}e^{-(x^2+y^2)/2}=\\frac{1}{2œÄ}e^{-r^2/2}$.\n",
        "$f_{R,Œò}(r,Œ∏)=\\frac{1}{2œÄ}re^{-r^2/2}$\n",
        "  - $f_Œò(Œ∏)=\\frac{1}{2œÄ}$ uniform distribution. $f_R(r)=re^{-r^2/2}$ Rayleigh distribution.\n",
        "\n",
        "- e6.7c: $X‚àº\\Gamma(a,Œª)$ and $Y‚àº\\Gamma(b,Œª)$. Compute the joint density of $U=X+Y$ and $V=X/(X+Y)$.\n",
        "$x=uv$, $y=u(1-v)$.\n",
        "$f_X(x)=\\frac{Œªe^{-Œªx}(Œªx)^{a-1}}{Œì(a)}$,\n",
        "$f_Y(y)=\\frac{Œªe^{-Œªy}(Œªy)^{b-1}}{Œì(b)}$.\n",
        "$f_{X,Y}(x,y)=\\frac{Œª^{a+b}}{Œì(a)Œì(b)}e^{-Œª(x+y)}x^{a-1}y^{b-1}$.\n",
        "$\\frac{‚àÇU}{‚àÇx}=1$, $\\frac{‚àÇU}{‚àÇy}=1$,\n",
        "$\\frac{‚àÇV}{‚àÇx}=\\frac{x+y-x}{(x+y)^2}$,\n",
        "$\\frac{‚àÇV}{‚àÇy}=\\frac{-x}{(x+y)^2}$.\n",
        "$J(x,y)=\\left|\\frac{‚àÇ(U,V)}{‚àÇ(x,y)}\\right|=\\frac{-1}{x+y}$.\n",
        "$f_{U,V}(u,v)=f_{X,Y}(uv,u(1-v))u$\n",
        "\n",
        "- 6.7: sequence of Bernoulli trials with success probability $p$. Let $X_1$ be number of failures before the first success, and $X_2$ be the number of failures between the first two successes. Find the joint PMF.\n",
        "Because $X_1$ and $X_2$ are independent,\n",
        "$P(X_1=x_1,X_2=x_2)=P(X_1=x_1)P(X_2=x_2)$\n",
        "$=(1-p)^{x_1+x_2}$.\n",
        "\n",
        "- 6.8: $f(x,y)=c(y^2-x^2)e^{-y}$ for $x‚àà[-y,y],y‚àà(0,‚àû)$. Find $c$, $f_X$, $f_Y$, and $\\E[X]$.\n",
        "$‚à´_{y=0}^‚àû‚à´_{x=-y}^yc(y^2-x^2)e^{-y}\\ dx\\ dy$\n",
        "$=‚à´_{y=0}^‚àûce^{-y}\\left[y^2x-\\frac{1}{3}x^3\\right]_{-y}^y\\ dy$\n",
        "$=‚à´_{y=0}^‚àû\\frac{4}{3}cy^3e^{-y}\\ dy$\n",
        "$=\\frac{4c}{3}Œì(4)$\n",
        "$=8c$. Therefore $c=\\frac{1}{8}$.\n",
        "$f_Y(y)=‚à´_{-y}^yf(x,y)\\ dx=\\frac{1}{6}y^3e^{-y}$.\n",
        "$f_X(x)=\\frac{1}{8}‚à´_0^‚àû(y^2e^{-y}-x^2e^{-y})\\ dy$\n",
        "$=\\frac{1}{8}(Œì(3)+[x^2e^{-y}]_0^‚àû)$\n",
        "$=\\frac{1}{8}(2-x^2)$.\n",
        "$\\E[X]=\\frac{1}{8}‚à´_{-y}^y(2x-x^3)\\ dx=0$ because odd integrant.\n",
        "\n",
        "- 6.10: given $f(x,y)=e^{-(x+y)},\\ x,y‚â•0$ find $P(X\\lt Y)$ and $P(X\\lt a)$.\n",
        "$P(X\\lt Y)=‚à´_{x=0}^‚àû‚à´_{y=x}^‚àû e^{-(x+y)}\\ dy\\ dx$\n",
        "$=‚à´_0^‚àû[-e^{-(x+y)}]_{y=x}^‚àû\\ dx$\n",
        "$=‚à´_0^‚àûe^{-2x}\\ dx=\\frac{1}{2}$.\n",
        "$P(X\\lt a)=‚à´_0^ae^{-2x}\\ dx=\\frac{1}{2}(1-e^{-2a})$\n",
        "\n",
        "- 6.11: 45% of customers will buy an ordinary television, 15% of will buy a plasma television. If 5 customers enter the store on a day, what is the probability that there are exactly 2 ordinary televisions and 1 plasma television sold on that day?\n",
        "$P(X_1=2,X_2=1,X_3=2)=\\frac{5!}{2!2!}p_1^2,p_2^1,p_3^2$\n",
        "$=(30)(0.45^2)(0.15)(0.4^2)=.1458$\n",
        "\n",
        "- 6.12: number of people who enter a store in an hour follows poisson(Œª=10). Given that 10 women entered in that hour, what is the probability that at most 3 men entered?\n",
        "Let $X$ be number of men and $Y$ be women.\n",
        "$P(X+Y=k)=e^{-Œª}\\frac{Œª^k}{k!}$.\n",
        "$P(X=x,Y=y|X+Y=x+y)=\\binom{x+y}{x}p^x(1-p)^y$ where $p$ is the probability of male.\n",
        "$P(X=x,Y=y)=\\binom{x+y}{x}e^{-pŒª}e^{-qŒª}\\frac{(pŒª)^x(qŒª)^y}{(x+y)!}$\n",
        "$=e^{-pŒª}\\frac{(pŒª)^x}{x!}e^{-qŒª}\\frac{(qŒª)^y}{y!}$.\n",
        "I.e., $X‚àº\\Pois(pŒª)$ and $Y‚àº\\Pois(qŒª)$ are independent.\n",
        "Therefore $P(X‚àà[0,3])=e^{-pŒª}(1+pŒª+\\frac{(pŒª)^2}{2}+\\frac{(pŒª)^3}{6})$\n",
        "\n",
        "- 6.13: two people independently arrive at minutes $X$‚àºuniform(15,45) and $Y$‚àºuniform(0,60). What is the probability that the first to arrive waits longer than 5 minutes?\n",
        "Let $c=f(x,y)=\\frac{1}{(30)(60)}$.\n",
        "$P(X>Y+5)+P(Y>X+5)=\\iint_{y\\lt x-5}c\\ dy\\ dx+\\iint_{y>x+5}c\\ dy\\ dx$\n",
        "$=‚à´_{x=15}^{45}‚à´_{y=0}^{x-5}c\\ dy\\ dx+‚à´_{x=15}^{45}‚à´_{y=x+5}^{60}c\\ dy\\ dx$\n",
        "$=c[\\frac{1}{2}x^2-5x+55x-\\frac{1}{2}x^2]_{15}^{45}$\n",
        "$=\\frac{(50)(30)}{(30)(60)}=\\frac{5}{6}$.\n",
        "\n",
        "- 6.14: Ambulance and accident occur uniform(0,L) on a road. What is the distribution of the distance from ambulance to accident? Because $X$ and $Y$ iid, $f(x,y)=\\frac{1}{L^2}$. Assume 2x $X\\lt Y$ case, then\n",
        "$P(|Y-X|\\lt a)=\\frac{2}{L^2}\\iint_{y-x\\lt a}\\ dx\\ dy$\n",
        "$=\\frac{2}{L^2}\\left[‚à´_{y=0}^a‚à´_{x=0}^{y}\\ dx\\ dy+‚à´_{y=a}^{L}‚à´_{x=y-a}^y\\ dx\\ dy\\right]$\n",
        "$=\\frac{2}{L^2}\\left[‚à´_{y=0}^ay\\ dy+‚à´_{y=a}^{L}a\\ dy\\right]$\n",
        "$=\\frac{2}{L^2}\\left[\\frac{1}{2}a^2+a(L-a)\\right]$\n",
        "$=\\frac{a(2L-a)}{L^2}$\n",
        "\n",
        "- 6.15: (X,Y) is uniformly distributed over a square region $f(x,y)=\\frac{1}{4},\\ x,y‚àà(-1,1)$. What is the probability that $(X,Y)$ lies in a circle of radius 1 centered at (0,0)? $\\frac{œÄ(1)^2}{(2)(2)}=\\frac{œÄ}{4}$.\n",
        "$R(x,y)=\\sqrt{x^2+y^2}$,\n",
        "$Œò(x,y)=\\arctan(y/x)$,\n",
        "$J(x,y)=\\BVM\n",
        "\\frac{x}{\\sqrt{x^2+y^2}} &\n",
        "\\frac{y}{\\sqrt{x^2+y^2}} \\\\\n",
        "\\frac{-y}{x^2+y^2} &\n",
        "\\frac{x}{x^2+y^2}\n",
        "\\EVM$\n",
        "$=\\frac{1}{\\sqrt{x^2+y^2}}=\\frac{1}{r}$.\n",
        "$f(r,Œ∏)=\\frac{r}{4}$.\n",
        "$P(x^2+y^2‚â§1)=\\frac{1}{4}\\iint_{x^2+y^2‚â§1}\\ dx\\ dy$\n",
        "$=\\iint_{r‚â§1}\\frac{r}{4}\\ dr\\ dŒ∏$\n",
        "$=‚à´_{r=0}^1\\frac{2œÄr}{4}\\ dr=\\frac{œÄ}{4}$.\n",
        "\n",
        "- 6.16: $n$ points are iid uniformly chosen on the circumference of a circle. What is the probability of $A$ that all points lie on one side of a line through the center of the circle?\n",
        "If points are labeled $1,...,n$ by CW ordering, let $A_i$ be all $(n-1)$ points being in a semicircle CW from point $i$. Each point has probability $\\frac{1}{2}$ of being in the semicircle, therefore $P(A_i)=\\left(\\frac{1}{2}\\right)^{n-1}$. $A_i$ and $A_j$ are disjoint events therefore $P(A)=P(\\bigcup\\limits_{i=1}^nA_i)=n\\left(\\frac{1}{2}\\right)^{n-1}$.\n",
        "\n",
        "- 6.17: Three points $X_1$, $X_2$, and $X_3$ are randomly selected on a line $L$. What is the probability that $X_2$ lies in between $X_1$ and $X_3$? The probability is $\\frac{2!}{3!}=\\frac{1}{3}$\n",
        "\n",
        "- 6.18: two points are selected uniformly on a line such that $X‚àà(0,L/2)$ and $Y‚àà(L/2,L)$. Find the probability that $Y-X>L/3$. Let $c=f(x,y)=\\frac{4}{L^2}$\n",
        "$P(Y-X>\\frac{L}{3})=\\iint_{y>x+L/3}c\\ dy\\ dx$\n",
        "$=c‚à´_{x=0}^{L/6}‚à´_{y=L/2}^L\\ dy\\ dx+c‚à´_{x=L/6}^{L/2}‚à´_{y=x+L/3}^{L}\\ dy\\ dx$\n",
        "$=c‚à´_{x=0}^{L/6}[L/2]\\ dx+c‚à´_{x=L/6}^{L/2}[\\frac{2L}{3}-x]\\ dx$\n",
        "$=c\\frac{L^2}{12}+c\\frac{2L^2}{9}-c\\frac{1}{2}((\\frac{L}{2})^2-(\\frac{L}{6})^2)$\n",
        "$=c\\frac{11L^2}{36}-c\\frac{L^2}{8}+c\\frac{L^2}{72}=c\\frac{7L^2}{36}=\\frac{7}{9}$\n",
        "\n",
        "- 6.19: $f(x,y)=1/x,\\ 0\\lt y\\lt x\\lt 1$ then $f_Y(y)=[\\ln x]_y^1=-\\ln y$.\n",
        "\n",
        "- 6.27: $X_1$‚àºexpon(Œª1) and $X_2$‚àºexpon(Œª2) are independent. Find distribution of $Z=X_1/X_2$.\n",
        "$P(Z\\lt a)=\\iint_{x_1/x_2\\lt a}Œª_1e^{-Œª_1x_1}Œª_2e^{-Œª_2x_2}\\ dx_1\\ dx_2$\n",
        "$=Œª_1Œª_2‚à´_{x_1=0}^‚àûe^{-Œª_1x_1}‚à´_{x_2=x_1/a}^‚àûe^{-Œª_2x_2}\\ dx_2\\ dx_1$\n",
        "$=Œª_1Œª_2‚à´_{x_1=0}^‚àûe^{-Œª_1x_1}[-\\frac{1}{Œª_2}e^{-Œª_2x_2}]_{x_1/a}^‚àû\\ dx_1$\n",
        "$=Œª_1‚à´_{x_1=0}^‚àûe^{-(Œª_1+\\frac{Œª_2}{a})x_1}\\ dx_1$\n",
        "$=\\frac{aŒª_1}{aŒª_1+Œª_2}$.\n",
        "$f_Z(z)=\\frac{Œª_1}{zŒª_1+Œª_2}\\frac{-zŒª_1^2}{(zŒª_1+Œª_2)^2}$\n",
        "$=\\frac{Œª_1Œª_2}{(zŒª_1+Œª_2)^2}$\n",
        "  - $Z=X_1/X_2$, $W=X_2$, $X_1=WZ$, $X_2=W$,\n",
        "  $J(w,z)=\\BVM\n",
        "  z & w \\\\\n",
        "  1 & 0\n",
        "  \\EVM=w$.\n",
        "  $f(z)=Œª_1Œª_2‚à´_{w=0}^‚àûwe^{-(zŒª_1+Œª_2)w}\\ dw$\n",
        "  $=\\frac{Œª_1Œª_2}{(zŒª_1+Œª_2)^2}$.\n",
        "\n",
        "- 6.28: $A$ and $M$ are both expon(1). $A$ starts at time 0 and $M$ starts at time $t$. What is the probability that $M+t\\lt A$?\n",
        "$\\iint_{m+t\\lt a,t\\lt a}e^{-m-a}\\ dm\\ da$\n",
        "$=‚à´_{a=t}^‚àû‚à´_{m=0}^{a-t}e^{-m-a}\\ dm\\ da$\n",
        "$=‚à´_{a=t}^‚àûe^{-a}-‚à´_{a=t}^‚àûe^{-2a+t}\\ da$\n",
        "$=e^{-t}-\\frac{1}{2}e^{-t}$\n",
        "$=\\frac{1}{2}e^{-t}$.\n",
        "If they start sequentially with $A$ first followed by $M$, what is the probability that $A+M\\lt 2$?\n",
        "$f_{A+M}(t)=‚à´_{-‚àû}^‚àûf_A(s)f_M(t-s)\\ ds$\n",
        "$=‚à´_0^te^{-s}e^{-(t-s)}\\ ds$\n",
        "$=te^{-t}$. Then\n",
        "$P(A+M\\lt 2)=‚à´_0^{2}te^{-t}\\ dt$\n",
        "$=[-te^{-t}-e^{-t}]_0^2$\n",
        "$=1-3e^2$\n",
        "\n",
        "- 6.29: If weekly income is $X_i‚àº\\Normal(2200,230^2)$, what is the probability that total income over next 2 weeks exceeds 5000? $(X_1+X_2)‚àº\\Normal(Œº_1+Œº_2,œÉ_1^2+œÉ_2^2)$. Therefore $1-Œ¶(\\frac{5000-4400}{230\\sqrt{2}})=0.0325$\n",
        "\n",
        "- 6.30: $X‚àº\\Normal(170,20^2)$, and $Y‚àº\\Normal(160,15^2)$. What is the probability that $Y>X$? $(Y-X)‚àº\\Normal(-10,20^2+15^2)$, therefore $P(Y-X>0)=1-Œ¶(\\frac{10}{\\sqrt{20^2+15^2}})=0.345$\n",
        "\n",
        "- 6.31: 25.2% of men and 23.6% of women never eat breakfast. Among 200 men and 200 women, what is the probability that at least 110 never eat breakfast?\n",
        "$X‚àº\\Normal(50.4,37.7)$ and $Y‚àº\\Normal(47.2,36.1)$ then $(X+Y)‚àº\\Normal(97.6,73.8)$.\n",
        "\n",
        "- 6.33: The expected number of typos on a page is 0.2. What is the probability that a 10-page magazine contains 0 errors? $Œª_i=0.2$ and $X=X_1+...+X_n‚àº\\Pois(nŒª_i)$. $P(X=0)=e^{-2}$.\n",
        "\n",
        "- 6.38: $X$ is randomly chosen from $1,2,3,4,5$, then $Y$ is randomly chosen from $1,2,...,X$. What is the joint PMF of $X$ and $Y$?\n",
        "$P(X=i,Y=j)=P(Y=j|X=i)P(X=i)$\n",
        "$=\\frac{1}{5i}$.\n",
        "Are $X$ and $Y$ independent?\n",
        "$P(Y=j)=\\suml_{i=1}^nP(X=i,Y=j)$\n",
        "$=\\suml_{i=1}^n\\frac{1}{5i}$.\n",
        "$P(X=i)P(Y=j)=\\frac{1}{5}\\suml_{i=1}^n\\frac{1}{5i}$\n",
        "$\\neq P(X=i,Y=j)$\n",
        "\n",
        "- 6.41: $f(x,y)=xe^{-x(y+1)},\\ x,y>0$. What is $f_{X|Y}(x|y)$?\n",
        "$f_Y(y)=‚à´_0^‚àûxe^{-x(y+1)}\\ dx$\n",
        "$=\\frac{1}{(y+1)^2}$.\n",
        "$f_{X|Y}(x|y)=\\frac{f(x,y)}{f_Y(y)}$\n",
        "$=x(y+1)^2e^{-x(y+1)}$\n",
        "\n",
        "- 6.44: if $X_1,X_2,X_3$ are iid uniform(0,1), what is the probability that the largest is greater than the sum of the lesser two?\n",
        "$P(X_{(3)}>X_{(1)}+X_{(2)})=\\iiint_{x_3>x_1+x_2,1>x_1+x_2}f(x_1,x_2,x_3)\\ dx_1\\ dx_2\\ dx_3$\n",
        "$=n!‚à´_{x_1=0}^{\\frac{1}{2}}‚à´_{x_2=x_1}^{1-x_1}‚à´_{x_3=x_1+x_2}^1\\ dx_3\\ dx_2\\ dx_1$\n",
        "$=n!‚à´_{x_1=0}^{\\frac{1}{2}}‚à´_{x_2=x_1}^{1-x_1}(1-x_1-x_2)\\ dx_2\\ dx_1$\n",
        "$=n!‚à´_{x_1=0}^{\\frac{1}{2}}‚à´_{y_2=1-2x_1}^0-y_2\\ dy_2\\ dx_1$\n",
        "$=n!‚à´_{x_1=0}^{\\frac{1}{2}}\\frac{1}{2}(1-2x_1)^2\\ dx_1$\n",
        "$=n!‚à´_{y_1=1}^0-\\frac{1}{4}y_1^2\\ dy_1$\n",
        "$=\\frac{n!}{12}=\\frac{1}{2}$\n",
        "\n",
        "- 6.45: a machine operates when at least 3 of 5 motors are working. Each motor works iid $f(x)=xe^{-x},\\ x>0$, what is the density function of the machine working?\n",
        "$F(x)=‚à´_0^xye^{-y}\\ dy$\n",
        "$=[-ye^{-y}-e^{-y}]_0^x$\n",
        "$=1-(x+1)e^{-x}$.\n",
        "The machine working is equivalent to\n",
        "$f_{X_{(3)}}(x)=\\frac{5!}{2!2!}F(x)^2(1-(F(x))^2f(x)$\n",
        "\n",
        "- 6.46: 3 markings are made along a L-unit distance at iid uniform(0,L). What is the probability that no 2 markings are within $d‚â§L/2$ distance of each other?\n",
        "$P(X_{(2)}-X_{(1)}>d,X_{(3)}-X_{(2)}>d)$\n",
        "$=\\frac{3!}{L^3}\\iiint_{x_2-x_1>d,x_3-x_2>d}\\ dx_3\\ dx_2\\ dx_1$\n",
        "$=\\frac{3!}{L^3}‚à´_{x_1=0}^{L-2d}‚à´_{x_2=x_1+d}^{L-d}‚à´_{x_3=x_2+d}^L\\ dx_3\\ dx_2\\ dx_1$\n",
        "$=\\frac{3!}{L^3}‚à´_{x_1=0}^{L-2d}‚à´_{x_2=x_1+d}^{L-d}(L-d-x_2)\\ dx_2\\ dx_1$\n",
        "$=\\frac{3!}{L^3}‚à´_{x_1=0}^{L-2d}‚à´_{y_2=L-2d-x_1}^0-y_2\\ dy_2\\ dx_1$\n",
        "$=\\frac{3!}{L^3}‚à´_{x_1=0}^{L-2d}[\\frac{1}{2}y_2^2]_0^{L-2d-x_1}\\ dx_1$\n",
        "$=\\frac{3!}{L^3}‚à´_{x_1=0}^{L-2d}\\frac{1}{2}(L-2d-x_1)^2\\ dx_1$\n",
        "$=\\frac{3!}{L^3}‚à´_{y_1=L-2d}^0-\\frac{1}{2}y_1^2\\ dx_1$\n",
        "$=\\frac{3!}{L^3}\\frac{1}{6}[y_1^3]_0^{L-2d}=\\frac{(L-2d)^3}{L^3}$. (Ross Ch6 e6a)\n",
        "\n",
        "- 6.47: 5 samples iid uniform(0,1). What's the probability that the median is in $(\\frac{1}{4},\\frac{3}{4})$?\n",
        "$f_3(x)=\\frac{5!}{2!2!}x^2(1-x)^2$.\n",
        "$P(\\frac{1}{4}\\lt X\\lt \\frac{3}{4})$\n",
        "$=30‚à´_{1/4}^{3/4}x^2-2x^3+x^4\\ dx$\n",
        "$=30[\\frac{x^3}{3}-\\frac{x^4}{2}+\\frac{x^5}{5}]_{1/4}^{3/4}$\n",
        "$=\\frac{203}{256}$\n",
        "\n",
        "- 6.49: let $X_{(1)},....,X_{(n)}$ be the order statistics on iid uniform(0,1). Find conditional distribution of $X_{(n)}$ given $X_{(1)}=s1,\\ ...\\ ,X_{(n-1)}=s_{n-1}$.\n",
        "$f_{X_{(n)}|X_{(1)}=s1,...,X_{(n-1)}=s_{n-1}}(x)=\\frac{f_{X_{(1)},...,X_{(n)}}(s_1,...,s_{n-1},x)}{f_{X_{(1)},...,X_{(n-1)}}(s_1,...,s_{n-1})}$\n",
        "$=\\frac{n!}{n!‚à´_{s_n=s_{n-1}}^1\\ ds_n}$\n",
        "$=\\frac{n!}{n!(1-s_{n-1})}$\n",
        "$=\\frac{1}{1-s_{n-1}}$\n",
        "$‚àº\\Unif(s_{n-1},1)$. The denominator is not $(n-1)!$ because there are $n$ underlying items. The denominator is the marginal PDF, which integrates over $s_{n-1}$.\n",
        "\n",
        "- 6.50: $Z_1,Z_2‚àº\\Normal(0,1)$ iid. $X=Z_1$, $Y=Z_1+Z_2$. $Z_1=X$, $Z_2=-X+Y$. $J(x,y)=\\BVM1 & 0 \\\\ -1 & 1\\EVM=1$.\n",
        "$f(x,y)=\\frac{1}{2œÄ}e^{-(z_1^2+z_2^2)/2}$\n",
        "$=\\frac{1}{2œÄ}e^{-(x^2+(y-x)^2)/2}$\n",
        "$=\\frac{1}{2œÄ}e^{-(2x^2-2xy+y^2)/2}$\n",
        "\n",
        "- 6.52: let $X,Y$ be coordinates of a point uniformly chosen in a circle of radius 1 centered at the origin i.e., $f(x,y)=\\frac{1}{œÄr^2}=\\frac{1}{œÄ},\\ x^2+y^2‚â§1$. What is the joint density function in polar coordinates? $X=R\\cosŒò$ and $Y=R\\sinŒò$. $J(r,Œ∏)=\\BVM\\cosŒ∏ & -r\\sinŒ∏ \\\\ \\sinŒ∏ & r\\cosŒ∏\\EVM=r$. Therefore $f(r,Œ∏)=f(x,y)|J(r,Œ∏)|=\\frac{r}{œÄ}$.\n",
        "\n",
        "- 6.53: let $X,Y‚àº\\Unif(0,1)$ iid. Find joint density in polar coordinates $R,Œò$.\n",
        "$X=R\\cosŒò$, $Y=R\\sinŒò$, $J(r,Œ∏)=r$.\n",
        "$f(r,Œ∏)=r,$\n",
        "$\\ Œ∏‚àà(0,\\frac{œÄ}{2}),r‚àà(0,\\min(\\frac{1}{\\cosŒ∏},\\frac{1}{\\sinŒ∏},\\sqrt{2}))$.\n",
        "The restraint on $r$ comes from $0\\lt x\\lt 1‚áí\\frac{0}{\\cosŒ∏}\\lt r\\lt \\frac{1}{\\cosŒ∏}$\n",
        "\n",
        "- 6.54: $U‚àº\\Unif(0,2œÄ)$ and $Z‚àº\\Expo(1)$. $X=\\sqrt{2Z}\\cos U$ and $Y=\\sqrt{2Z}\\sin U$. What is the joint PDF of X and Y?\n",
        "$J(u,z)=\\BVM\n",
        "\\frac{\\cos U}{\\sqrt{2Z}} & -\\sqrt{2Z}\\sin U \\\\\n",
        "\\frac{\\sin U}{\\sqrt{2Z}} & \\sqrt{2Z}\\cos U\n",
        "\\EVM=1$.\n",
        "$Z=\\frac{1}{2}(X^2+Y^2)$.\n",
        "$f(x,y)=\\frac{1}{2œÄ}e^{-(x^2+y^2)/2}$. Therefore $X,Y‚àº\\Normal(0,1)$.\n",
        "\n",
        "- 6.58: $X_1,X_2‚àº\\Expo(Œª)$ iid, find the joint density of $Y_1=X_1+X_2$ and $Y_2=e^{X_1}$.\n",
        "$J(y_1,y_2)=\\BVM\n",
        "0 & \\frac{1}{y_2} \\\\\n",
        "1 & -\\frac{1}{y_2}\n",
        "\\EVM=\\frac{1}{y_2}$.\n",
        "$f(x_1,x_2)=Œª^2e^{-Œª(x_1+x_2)}$ then\n",
        "$f(y_1,y_2)=\\frac{Œª^2}{y_2}e^{-Œªy_1}$\n",
        "\n",
        "- t6.6: if $X$ and $Y$ are jointly continuous with PDF $f_{X,Y}(x,y)$, then\n",
        "$P(X+Y\\lt t)=\\iint_{x+y\\lt t}f_{X,Y}(x,y)\\ dx\\ dy$\n",
        "$=‚à´_{x=-‚àû}^‚àû‚à´_{y=-‚àû}^{t-x}f_{X,Y}(x,y)\\ dy\\ dx$.\n",
        "$f_{X+Y}(t)=\\frac{‚àÇ}{‚àÇt}‚à´_{x=-‚àû}^‚àû‚à´_{y=-‚àû}^{t-x}f_{X,Y}(x,y)\\ dy\\ dx$\n",
        "$=‚à´_{-‚àû}^‚àû\\frac{‚àÇ}{‚àÇt}‚à´_{y=-‚àû}^{t-x}f_{X,Y}(x,y)\\ dy\\ dx$\n",
        "$=‚à´_{-‚àû}^‚àûf_{X,Y}(x,t-x)\\ dy\\ dx$ by Leibniz's rule\n",
        "\n",
        "Ross Chapter 7 (Properties of expectation):\n",
        "\n",
        "- e7.2a: an accident occurs at $X$‚àºuniform(0,L), an ambulance is at $Y$‚àºuniform(0,L). Find the expected distance between ambulance and the accident.\n",
        "$\\E|X-Y|=\\iint_{y>x}\\frac{1}{L^2}(y-x)\\ dx\\ dy$\n",
        "$+\\iint_{x>y}\\frac{1}{L^2}(x-y)\\ dx\\ dy$\n",
        "$=\\frac{1}{L^2}‚à´_{x=0}^L‚à´_{y=0}^x(x-y)\\ dy\\ dx$\n",
        "$+\\frac{1}{L^2}‚à´_{x=0}^L‚à´_{y=x}^L(y-x)\\ dy\\ dx$\n",
        "$=\\frac{1}{L^2}‚à´_{x=0}^L(x^2-\\frac{1}{2}x^2)\\ dx$\n",
        "$+\\frac{1}{L^2}‚à´_{x=0}^L(\\frac{1}{2}L^2-\\frac{1}{2}x^2-Lx+x^2)\\ dx$\n",
        "$=\\frac{1}{L^2}‚à´_{x=0}^L(x^2-Lx+\\frac{1}{2}L^2)\\ dx$\n",
        "$=\\frac{1}{L^2}(\\frac{L^3}{3}-\\frac{L^3}{2}+\\frac{L^3}{2})=\\frac{L}{3}$.\n",
        "Cannot apply linearity because $\\E|X-Y|\\neq|\\E[X]-\\E[Y]|$.\n",
        "\n",
        "- e7.2l: A particle begins at origin and each step moves 1 unit distance at an angle $Œò_i‚àº\\Unif(0,2œÄ)$. What's the expected square distance from origin after $n$ steps?\n",
        "Let $X,Y$ be the final coordinate, and $X_i,Y_i$ be stepwise displacement.\n",
        "Square distance\n",
        "$X^2+Y^2=\\left(\\suml_{i=1}^nX_i\\right)^2$\n",
        "$+\\left(\\suml_{i=1}^nY_i\\right)^2$\n",
        "$=\\suml_{i=1}^n(X_i^2+Y_i^2)$\n",
        "$+\\suml_{i\\neq j}(X_iX_j+Y_iY_j)$\n",
        "$=\\suml_{i=1}^n(\\cos^2Œ∏_i+\\sin^2Œ∏_i)$\n",
        "$+\\suml_{i\\neq j}(\\cosŒ∏_i\\cosŒ∏_j+\\sinŒ∏_i\\sinŒ∏_j)$\n",
        "$=n+\\suml_{i\\neq j}\\cos(Œ∏_i-Œ∏_j)$.\n",
        "$\\E[X^2+Y^2]=n+\\suml_{i\\neq j}\\E[\\cos(Œ∏_i-Œ∏_j)]$\n",
        "$=n+\\suml_{i\\neq j}‚à´_0^{2œÄ}‚à´_0^{2œÄ}\\frac{1}{4œÄ^2}\\cos(Œ∏_i-Œ∏_j)\\ dŒ∏_i\\ dŒ∏_j$\n",
        "$=n$\n",
        "\n",
        "- e7.2n: Let $A_1,...,A_n$ be events, and $I_i$ be indicators such that $P(A_i)=\\E[I_i]$. Then\n",
        "$\\bigcup\\limits_{i=1}^nA_i$\n",
        "$=\\left(\\bigcap\\limits_{i=1}^nA_i^c\\right)^c$\n",
        "$‚áíI\\left(\\bigcup\\limits_{i=1}^nA_i\\right)$\n",
        "$=1-I\\left(\\bigcap\\limits_{i=1}^nA_i^c\\right)$\n",
        "$=1-\\prodl_{i=1}^n(1-I_i)$\n",
        "$‚áíP\\left(\\bigcup\\limits_{i=1}^nA_i\\right)$\n",
        "$=\\E\\left[1-\\prodl_{i=1}^n(1-I_i)\\right]$\n",
        "$=\\E\\left[\\suml_{i=1}^nI_i-\\suml_{i\\lt j}I_iI_j+...+(-1)^{n+1}I_1...I_n\\right]$\n",
        "$=\\suml_{i=1}^nP(A_i)-\\suml_{i\\lt j}P(A_i,A_j)+...+(-1)^{n+1}P(A_i,...,A_n)$\n",
        "\n",
        "- 7.4: $f_{X,Y}(x,y)=1/y,\\ y‚àà(0,1),x‚àà(0,y)$. What is $\\E[XY]$?\n",
        "$\\E[XY]=‚à´_{y=0}^1‚à´_{x=0}^y\\frac{xy}{y}\\ dx\\ dy$\n",
        "$=‚à´_{y=0}^1\\frac{1}{2}y^2\\ dy$\n",
        "$=\\frac{1}{6}$.\n",
        "\n",
        "- 7.5: A hospital is located at $(0,0)$ center of a 3x3 square. An accident occurs at $(x,y)$ uniformly distributed within this square, what is the expected $|x|+|y|$?\n",
        "$\\E[|X|+|Y|]=\\iint_{x,y‚àà(0,3/2)}(x+y)\\frac{4}{9}\\ dx\\ dy$\n",
        "$=\\frac{4}{9}‚à´_0^{3/2}‚à´_0^{3/2}(x+y)\\ dy\\ dx$\n",
        "$=\\frac{4}{9}‚à´_0^{3/2}[\\frac{3}{2}x+\\frac{1}{2}(\\frac{3}{2})^2]\\ dx$\n",
        "$=\\frac{4}{9}[\\frac{1}{2}(\\frac{3}{2})^3+\\frac{1}{2}(\\frac{3}{2})^3]$\n",
        "$=\\frac{3}{4}+\\frac{3}{4}=\\frac{3}{2}$.\n",
        "\n",
        "- 7.14: an urn has $m$ black balls. At each stage a black ball is removed and with probability $p$ another black ball is added. Find the expected number of stages until there are no more black balls.\n",
        "Let $N_m$ be the number of stages with $m$ black balls, and let $A$ be event that a black ball is added back in.\n",
        "$\\E[N_m]=1+\\E[N_m|A]P(A)+\\E[N_m|A^c]P(A^c)$\n",
        "$=1+\\E[N_m]p+\\E[N_{m-1}](1-p)$\n",
        "$=\\frac{1}{1-p}+\\E[N_{m-1}]$.\n",
        "$\\E[N_0]=0$.\n",
        "$\\E[N_1]=\\frac{1}{1-p}$.\n",
        "$\\E[N_2]=\\frac{2}{1-p}$.\n",
        "$\\E[N_m]=\\frac{m}{1-p}$.\n",
        "\n",
        "- 7.25: Let $X_1,...,X_N$ be sequence of iid. Let $N‚â•2$ such that $X_1‚â•X_2‚â•...‚â•X_{N-1}\\lt X_N$. Find $\\E[N]$.\n",
        "$P(N>n)=\\frac{1}{n!}$.\n",
        "$P(N>n-1)=\\frac{1}{(n-1)!}$.\n",
        "$P(N=n)=\\frac{n-1}{n!}$.\n",
        "$\\E[N]=\\suml_{n=2}^‚àû\\frac{n(n-1)}{n!}$\n",
        "$=\\suml_{n=2}^‚àû\\frac{1}{(n-2)!}$\n",
        "$=1+\\frac{1}{1!}+\\frac{1}{2!}+...=e$\n",
        "\n",
        "- 7.26: If $X_1,...,X_n‚àº\\Unif(0,1)$ iid, find $\\E[\\max(X_1,...,X_n)]$ and $\\E[\\min(X_1,...,X_n)]$.\n",
        "First find CDF\n",
        "$P(\\max(X_1,...,X_n)\\lt a)=P(X_1\\lt a,...,X_n\\lt a)$\n",
        "$=P(X_i\\lt a)^n=a^n$.\n",
        "$\\E[\\max(X_1,...,X_n)]=‚à´_0^1(1-x^n)\\ dx$\n",
        "$=\\frac{n}{n+1}$.\n",
        "$P(\\min(X_1,...,X_n)>a)=P(X_1>a,...,X_n>a)$\n",
        "$=P(X_i>a)^n=(1-a)^n$.\n",
        "$\\E[\\min(X_1,...,X_n)]=‚à´_0^1(1-x)^n\\ dx$\n",
        "$=‚à´_1^0-y^n\\ dy$\n",
        "$=\\frac{1}{n+1}$.\n"
      ],
      "metadata": {
        "id": "GSLk5cqy_Lpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blitzstein Chapter 7 (Joint distributions):\n",
        "\n",
        "- 7.1.9 (Chicken-egg): A chicken lays $N‚àº\\Pois(Œª)$ eggs. Each egg hatches with probability $p$. The number of eggs that hatch $(X|N=n)‚àº\\Binom(n,p)$ and eggs that didn't hatch $(Y|N=n)‚àº\\Binom(n,q)$.\n",
        "$P(X=i,Y=j)=P(X=i|N=n)P(N=n)$\n",
        "$=\\binom{n}{i}p^iq^j\\frac{e^{-Œª}Œª^{i+j}}{(i+j)!}$\n",
        "$=\\frac{e^{-Œªp}(Œªp)^i}{i!}\\frac{e^{-Œªq}(Œªq)^j}{j!}$.\n",
        "  - $X‚àº\\Pois(Œªp)$, $Y‚àº\\Pois(Œªq)$, and $X$ and $Y$ are independent iff $N=X+Y‚àº\\Pois(Œª)$ and $(X|N=n)‚àº\\Binom(n,p)$.\n",
        "\n",
        "- 7.1.25 (Cauchy): Let $X,Y‚àº\\Normal(0,1)$ iid and let $T=X/Y$. Find PDF of $f_T(t)$.\n",
        "$P(T‚â§t)=P(X/Y‚â§t)$\n",
        "$=\\iint_{y;x‚â§t|y|}\\frac{1}{2œÄ}e^{-\\frac{x^2}{2}}e^{-\\frac{y^2}{2}}\\ dy\\ dx$\n",
        "$=‚à´_{y=-‚àû}^{‚àû}\\frac{1}{\\sqrt{2œÄ}}e^{-\\frac{y^2}{2}}Œ¶(t|y|)\\ dy$\n",
        "$=‚à´_{y=0}^{‚àû}\\frac{2}{\\sqrt{2œÄ}}e^{-\\frac{y^2}{2}}Œ¶(ty)\\ dy$.\n",
        "$f_T(t)=\\frac{d}{dt}‚à´_{y=0}^{‚àû}\\frac{2}{\\sqrt{2œÄ}}e^{-\\frac{y^2}{2}}Œ¶(ty)\\ dy$\n",
        "$=‚à´_{y=0}^{‚àû}\\frac{2}{\\sqrt{2œÄ}}e^{-\\frac{y^2}{2}}\\frac{d}{dt}Œ¶(ty)\\ dy$\n",
        "$=‚à´_{y=0}^{‚àû}\\frac{2}{\\sqrt{2œÄ}}e^{-\\frac{y^2}{2}}(y\\frac{1}{\\sqrt{2œÄ}}e^{-\\frac{(ty)^2}{2}})\\ dy$\n",
        "$=\\frac{1}{œÄ}‚à´_{y=0}^{‚àû}ye^{-\\frac{y^2(t^2+1)}{2}}\\ dy$\n",
        "$=\\frac{1}{œÄ(1+t^2)}$.\n",
        "\n",
        "- 7.2.3 (distance between normals): For $X,Y‚àº\\Normal(0,1)$ iid, what is $\\E|X-Y|$?\n",
        "By addition of independent normals and location-scale transformation $X-Y‚àº\\Normal(0,2)$\n",
        "$=\\sqrt{2}Z$.\n",
        "$\\E|X-Y|=\\E|\\sqrt{2}Z|$\n",
        "$=\\frac{2}{\\sqrt{œÄ}}$ folded normal (Blitzsein e5.4.7).\n",
        "\n",
        "- 7.1: A and B arrange to meet at noon, but both arrive at uniform(0,60) minutes late. Each is willing to wait 15 minutes. What is the probability that they end up meeting?\n",
        "$\\iint_{|x-y|‚â§0.25}\\ dx\\ dy$\n",
        "$=2\\iint_{x>y,x-y‚â§0.25}\\ dx\\ dy$\n",
        "$=2‚à´_{y=0}^{0.75}‚à´_{x=y}^{y+0.25}\\ dx\\ dy$\n",
        "$+2‚à´_{y=0.75}^1‚à´_{x=y}^1\\ dx\\ dy$\n",
        "$=2\\frac{3}{4}\\frac{1}{4}+2\\frac{1}{4}-2\\frac{1}{2}(1-\\frac{9}{16})$\n",
        "$=\\frac{3}{8}+\\frac{1}{2}+\\frac{9}{16}-1$\n",
        "$=\\frac{7}{16}$.\n",
        "\n",
        "- 7.5: A fair die is rolled $X$, then a coin with $p$ Heads is rolled $X$ times to achieve $Y$ heads.\n",
        "  - What is the joint PMF of $X$ and $Y$?\n",
        "  $P(X=x)=\\frac{1}{6}$.\n",
        "  $P(Y=y|X=x)=\\binom{x}{y}p^yq^{x-y}$.\n",
        "  $P(X=x,Y=y)=\\frac{1}{6}\\binom{x}{y}p^yq^{x-y}$.\n",
        "  - What is the marginal PMF of Y?\n",
        "  $P(Y=y)=\\suml_{x=1}^6\\frac{1}{6}\\binom{x}{y}p^yq^{x-y}$\n",
        "  - What is the conditional PMF of X given Y=y?\n",
        "  $P(X=x|Y=y)=\\frac{P(X=x,Y=y)}{P(Y=y)}$\n",
        "\n",
        "- 7.9: $X,Y‚àº\\Geom(p)$ iid, and $N=X+Y$. What is the PMF of $(X|N=n)?$\n",
        "$P(X=x,Y=y,N=n)=q^{x+y}p^2=q^np^2$.\n",
        "$P(N=n)=\\suml_{x=0}^nq^np^2$\n",
        "$=(n+1)q^np^2$.\n",
        "$P(X=x|N=n)=\\frac{1}{n+1}$.\n",
        "\n",
        "- 7.10: $X,Y‚àº\\Expo(Œª)$ iid, and $T=X+Y$.\n",
        "  - What is the CDF $F_{T|X}(t|x)$?\n",
        "  $P(T\\lt t|X=x)=P(Y\\lt t-x|X=x)$\n",
        "  $=‚à´_{y=0}^{t-x}Œªe^{-Œªy}\\ dy$\n",
        "  $=1-e^{-Œª(t-x)},\\ t>x$.\n",
        "  - What is the PDF $f_{T|X}(t|x)$?\n",
        "  $f_{T|X}(t|x)=Œªe^{-Œª(t-x)},\\ t>x$.\n",
        "  - What is the PDF $f_{X|T}(x|t)$?\n",
        "  $f_{X,T}(x,t)=f_{T|X}(t|x)f_X(x)$\n",
        "  $=Œªe^{-Œª(t-x)}Œªe^{-Œªx}$\n",
        "  $=Œª^2e^{-Œªt},\\ t>x$.\n",
        "  $f_{X|T}(x|t)=\\frac{Œª^2e^{-Œªt}}{f_T(t)}$ the numerator does not depend on $x$. $f_T(t)=‚à´_x f_{X,T}(x,t)\\ dx$ also does not depend on $x$. Therefore $f_{X|T}(x|t)$ must be constant and $(X|T)‚àº\\Unif(0,t)$.\n",
        "  $‚à´_0^tf_{X|T}(x|t)\\ dx=1$\n",
        "  $‚áíf_{X|T}(x|t)=\\frac{1}{t}$.\n",
        "  - What is the PDF $f_T(t)$?\n",
        "  $f_T(t)=\\frac{Œª^2e^{-Œªt}}{f_{X|T}(x|t)}$\n",
        "  $=Œª^2te^{-Œªt}$.\n",
        "\n",
        "- 7.11: $X‚àº\\Normal(0,1)$ and $(Y,Z|X=x)‚àº\\Normal(x,1)$ iid. Find joint PDF of $X,Y,Z$.\n",
        "$f_{X,Y,Z}(x,y,z)=f_{Y|X}(y|x)f_{Z|X}(z|x)f_{X}(x)$\n",
        "$=\\frac{1}{(2œÄ)^{3/2}}e^{-\\frac{x^2+(y-x)^2+(z-x)^2}{2}}$\n",
        "\n",
        "- 7.12: $X‚àº\\Expo(Œª)$. Find $F_X(x|X>c)$ and $f_X(x|X>c)$.\n",
        "$P(X\\lt x|X>c)$\n",
        "$=\\frac{P(c\\lt X\\lt x)}{P(X>c)}$\n",
        "$=\\frac{e^{-Œªc}-e^{-Œªx}}{e^{-Œªc}}$\n",
        "$=1-e^{-Œª(x-c)}$\n",
        "$=P(X+c\\lt x)$\n",
        "thus proving memoryless.\n",
        "\n",
        "- 7.13: $X,Y‚àº\\Expo(Œª)$ iid. Find $(X|X\\lt Y)$.\n",
        "$P(X\\lt a|X\\lt Y)=1-P(X>a|X\\lt Y)$\n",
        "$=1-\\frac{P(a\\lt X\\lt Y)}{P(X\\lt Y)}$.\n",
        "$P(a\\lt X\\lt Y)$\n",
        "$=‚à´_{y=a}^‚àû‚à´_{x=a}^yŒª^2e^{-Œª(x+y)}\\ dx\\ dy$\n",
        "$=‚à´_a^‚àûŒª(e^{-Œª(a+y)}-e^{-Œª2y})\\ dy$\n",
        "$=e^{-Œª2a}-\\frac{1}{2}e^{-Œª2a}$\n",
        "$=\\frac{1}{2}e^{-Œª2a}$.\n",
        "$P(X\\lt Y)=\\frac{1}{2}$.\n",
        "$P(X\\lt a|X\\lt Y)=1-e^{-Œª2a}$.\n",
        "It is the same as $P(\\min(X,Y)\\lt a)$.\n",
        "\n",
        "- 7.14a: a stick is broken into 3 pieces by picking 2 points uniform randomly. What is the probability that the 3 pieces can assemble a triangle (no piece > L/2)?\n",
        "$P=\\iint_{x\\lt\\frac{L}{2},y>x,y\\lt x+\\frac{L}{2},y>\\frac{L}{2}}f(x,y)\\ dx\\ dy$\n",
        "$=\\frac{1}{L^2}‚à´_{x=0}^\\frac{L}{2}‚à´_{y=\\frac{L}{2}}^{x+\\frac{L}{2}}\\ dy\\ dx$\n",
        "$=\\frac{1}{L^2}‚à´_{x=0}^\\frac{L}{2}x\\ dx$\n",
        "$=\\frac{1}{4}$.\n",
        "\n",
        "- 7.14b: a round table has 3 legs positioned uniform randomly on its perimeter. What is the probability that the table stands?\n",
        "The table falls if there is no chord through the center that has all 3 legs on one side (event $A$). This happens if there exists point $i$ on the perimeter from where all other points sit within $œÄ$ degrees CW.\n",
        "$P(A)=3(\\frac{1}{2})^2$.\n",
        "$P(A^c)=\\frac{1}{4}$.\n",
        "\n",
        "- 7.18: $(X,Y)$ is a uniformly random point in the area (0,0), (0,1), and (1,0).\n",
        "  - Find $f_{X|Y}(x|y)$.\n",
        "  $f(x,y)=2,\\ x‚àà(0,1),y‚àà(0,1-x)$.\n",
        "  $‚à´_{x=0}^1‚à´_{y=0}^{1-x}2\\ dy\\ dx$\n",
        "  $=2‚à´_0^1(1-x)\\ dx$\n",
        "  $=1$ valid.\n",
        "  $y\\lt 1-x‚áíx\\lt 1-y$.\n",
        "  $f_Y(y)=‚à´_{x=0}^{1-y}2\\ dx$\n",
        "  $=2(1-y)$.\n",
        "  $f_{X|Y}(x,y)=\\frac{2}{2(1-y)}$\n",
        "  $=\\frac{1}{1-y}$.\n",
        "  $(X|Y=y)‚àº\\Unif(0,1-y)$.\n",
        "  - 7.34: Find $\\Cov(X,Y)$.\n",
        "  $\\E[XY]=‚à´_{x=0}^1‚à´_{y=0}^{1-x}2xy\\ dy\\ dx$\n",
        "  $=\\frac{1}{12}$.\n",
        "  $\\E[X]=‚à´_{x=0}^12x(1-x)\\ dx$\n",
        "  $=\\frac{1}{3}$.\n",
        "  $\\E[Y]=‚à´_{y=0}^12y(1-y)\\ dy$\n",
        "  $=\\frac{1}{3}$.\n",
        "  $\\Cov(X,Y)=\\E[XY]-\\E[X]\\E[Y]=-\\frac{1}{36}$.\n",
        "\n",
        "- 7.20: $U_1,U_2,U_3‚àº\\Unif(0,1)$ iid, $L=\\min(U_1,U_2,U_3)$, and $M=\\max(U_1,U_2,U_3)$. Find $f_{M|L}(m|l)$.\n",
        "  - $F_M(m)=P(M\\lt m)$\n",
        "  $=P(U_i\\lt m)^3$\n",
        "  $=m^3$.\n",
        "  $F_L(l)=1-P(L>l)$\n",
        "  $=1-P(U_i>l)^3$\n",
        "  $=1-(1-l)^3$.\n",
        "  $f_L(l)=3(1-l)^2$.\n",
        "  - $P(L‚â•l,M‚â§m)=P(M‚â§m)-P(L‚â§l,M‚â§m)$.\n",
        "  $P(L‚â•l,M‚â§m)=P(U_i‚àà[l,m])^3$\n",
        "  $=(m-l)^3$.\n",
        "  $F_{L,M}(l,m)=m^3-(m-l)^3$.\n",
        "  $f_{L,M}(l,m)=\\frac{‚àÇ^2}{‚àÇl‚àÇm}F_{L,M}(l,m)$\n",
        "  $=6(m-l)$.\n",
        "  - $f_{M|L}(m|l)=\\frac{f_{M,L}(l,m)}{f_{L}(l)}$\n",
        "  $=\\frac{2(m-l)}{(1-l)^2}$\n",
        "\n",
        "- 7.24: A takes $Y_1‚àº\\Expo(Œª_1)$ hours to finish homework while B takes $Y_2‚àº\\Expo(Œª_2)$.\n",
        "  - Find CDF and PDF of $Y_1/Y_2$.\n",
        "  $P(Y_1/Y_2‚â§a)=P(Y_1‚â§aY_2)$\n",
        "  $=\\iint_{y_1‚â§ay_2}f(y_1,y_2)\\ dy_1\\ dy_2$\n",
        "  $=‚à´_{y_2=0}^‚àûŒª_2e^{-Œª_2y_2}‚à´_{y_1=0}^{ay_2}Œª_1e^{-Œª_1y_1}\\ dy_1\\ dy_2$\n",
        "  $=‚à´_{y_2=0}^‚àû(Œª_2e^{-Œª_2y_2}-e^{-(Œª_2+Œª_1a)y_2})\\ dy_2$\n",
        "  $=1-\\frac{Œª_2}{Œª_2+Œª_1a}$\n",
        "  $=\\frac{Œª_1a}{Œª_2+Œª_1a}$\n",
        "  - What's the probability that A finishes before B?\n",
        "  $P(Y_1\\lt Y_2)=\\frac{Œª_1}{Œª_1+Œª_2}$.\n",
        "\n",
        "- 7.31: $X,Y‚àº\\Unif(0,1)$ iid. Find variance of distance between them.\n",
        "$\\E|X-Y|=‚à´_0^1‚à´_0^1|x-y|\\ dy\\ dx$\n",
        "$=2‚à´_{y=0}^1‚à´_{x=y}^1x-y\\ dx\\ dy$\n",
        "$=‚à´_{y=0}^1[1-2y+y^2]\\ dy$\n",
        "$=\\frac{1}{3}$.\n",
        "$\\E(X-Y)^2=‚à´_0^1‚à´_0^1(x-y)^2\\ dy\\ dx$\n",
        "$=\\frac{1}{6}$.\n",
        "$\\Var(X-Y)=\\frac{1}{6}-\\frac{1}{9}=\\frac{1}{18}$.\n",
        "\n",
        "- 7.32: $X,Y‚àº\\Expo(Œª)$ iid. Find $\\E|X-Y|$.\n",
        "Let $M=\\max(X,Y)$ and $L=\\min(X,Y)$.\n",
        "$|X-Y|=M-L‚àº\\Expo(Œª)$ due to memoryless property.\n",
        "$\\E|X-Y|=\\frac{1}{Œª}$.\n",
        "\n",
        "- 7.60: A poisson(Œª) number of people vote in certain election. Voters vote for A with probability $p$ and B with $q$. Let $V=A-B$. Find $\\E[V]$ and $\\Var(V)$.\n",
        "$A‚àº\\Pois(pŒª)$ and $B‚àº\\Pois(qŒª)$.\n",
        "$\\E[V]=\\E[A]-\\E[B]=(2p-1)Œª$.\n",
        "$\\Var(V)=\\Var(A-B)$\n",
        "$=\\Var(A)+\\Var(-B)+2\\Cov(A,B)$\n",
        "$=Œª$.\n",
        "\n",
        "- 7.63: A chicken produces $N‚àº\\Binom(n,p)$ hatchlings, each surviving with probability $s$. Let $X+Y=N$ be hatchings that survive/not survive. Find marginal PMF of $X$ and joint PMF of $X,Y$.\n",
        "Marginal $X‚àº\\Binom(n,ps)$.\n",
        "Joint $P(X=x,Y=y)=P(X=x|N=m)P(N=m)$\n",
        "$=\\binom{m}{x}s^x(1-s)^{m-x}\\binom{n}{m}p^m(1-p)^{n-m}$\n",
        "$=\\frac{m!n!}{x!(m-x)!m!(n-m)!}s^x(1-s)^yp^m(1-p)^{n-m}$\n",
        "$=\\frac{n!}{x!y!(n-x-y)!}(ps)^x(p(1-s))^y(1-p)^{n-x-y}$ multinomial.\n",
        "\n",
        "Blitzstein Chapter 8 (Transformations):\n",
        "\n",
        "- e8.1.3 (lognormal): Let $X‚àº\\Normal(0,1), Y=e^X$. Find PDF of $Y$.\n",
        "$f_Y(y)=f_X(x)|\\frac{dy}{dx}|^{-1}$\n",
        "$=œï(\\ln(y))\\frac{1}{e^{\\ln(y)}}$\n",
        "$=œï(\\ln(y))\\frac{1}{y},\\ y>0$.\n",
        "  - $f_Y(y)=f_X(x)|\\frac{dx}{dy}|=œï(\\ln(y))\\frac{1}{y}$.\n",
        "\n",
        "- e8.1.4 (Chi-square): Let $X‚àº\\Normal(0,1), Y=X^2$. Find PDF of $Y$. Cannot use change of variable because $g(x)=x^2$ is not monotonous. However $X^2‚â§y$ is equivalent to $-\\sqrt{y}‚â§X‚â§\\sqrt{y}$ therefore $F_Y(y)=2Œ¶(\\sqrt{y})-1$ and $f_Y(y)=\\frac{2œï(\\sqrt{y})}{\\sqrt{y}},\\ y>0$.\n",
        "\n",
        "- e8.1.5 (Cauchy): A lighthouse is shining light toward the ocean at a random angle $U‚àº\\Unif(-\\frac{œÄ}{2},\\frac{œÄ}{2})$ measured against the shoreline orthogonal. Light is projected onto a parallel line 1 mile away from the shoreline to a point $X$ miles away from the orthogonal. Find the distribution of $X$.\n",
        "$X=\\tan(U)$.\n",
        "$\\frac{du}{dx}=\\frac{1}{1+x^2}$.\n",
        "$f_X(x)=f_U(u)\\frac{du}{dx}$\n",
        "$=\\frac{1}{œÄ(1+x^2)}$.\n",
        "\n",
        "- e8.1.9 (Box-Muller normal Ross 6.54): Let $U‚àº\\Unif(0,2œÄ),T‚àº\\Expo(1),X=\\sqrt{2T}\\cos U,Y=\\sqrt{2T}\\sin U$.\n",
        "$\\left|\\frac{‚àÇ(x,y)}{‚àÇ(t,u)}\\right|=\\BVM\n",
        "\\frac{\\cos U}{\\sqrt{2T}} & -\\sqrt{2T}\\sin U \\\\\n",
        "\\frac{\\sin U}{\\sqrt{2T}} & \\sqrt{2T}\\cos U\n",
        "\\EVM=1$.\n",
        "$f_{X,Y}(x,y)=\\frac{1}{2œÄ}\\e{-\\frac{x^2+y^2}{2}}$.\n",
        "\n",
        "- e8.1.10 (bivariate normal e7.5.10): Let $X,Y‚àº\\Normal(0,1)$ iid. Construct $(Z,W)$ BVN with $\\Normal(0,1)$ marginals correlated with $œÅ$, and find its joint PDF.\n",
        "$Z=aX+bY$,\n",
        "$W=cX+dY$,\n",
        "let $Z=X$,\n",
        "$1=c^2+d^2$,\n",
        "$œÅ=\\Cov(Z,W)=c$,\n",
        "$d=\\sqrt{1-œÅ^2}=œÑ$,\n",
        "then $W=œÅX+œÑY$.\n",
        "$X=Z,Y=\\frac{W-œÅZ}{œÑ}$.\n",
        "$\\left|\\frac{‚àÇ(X,Y)}{‚àÇ(z,w)}\\right|=\\BVM\n",
        "1 & 0 \\\\\n",
        "-\\frac{œÅ}{œÑ} & \\frac{1}{œÑ}\n",
        "\\EVM=\\frac{1}{œÑ}$.\n",
        "$f_{Z,W}(z,w)=\\frac{1}{2œÄœÑ}\\e{-\\frac{z^2}{2}}\\e{-\\frac{(w-œÅz)^2}{2œÑ^2}}$\n",
        "$=\\frac{1}{2œÄœÑ}\\e{-\\frac{1}{2œÑ^2}\\left[z^2œÑ^2+w^2-2œÅwz+œÅ^2z^2\\right]}$\n",
        "$=\\frac{1}{2œÄœÑ}\\e{-\\frac{1}{2œÑ^2}\\left[z^2+w^2-2œÅwz\\right]}$\n",
        "\n",
        "- e8.2.4 (exponential convolution): let $X,Y‚àº\\Expo(Œª)$ iid, find distribution of $T=X+Y$.\n",
        "$f_T(t)=‚à´_{s>0,t-s>0}f_X(t-s)f_Y(s)\\ ds$\n",
        "$=Œª^2‚à´_0^te^{-Œª(t-s)}e^{-Œªs}\\ ds$\n",
        "$=Œª^2e^{-Œªt}‚à´_0^t\\ ds$\n",
        "$=Œªte^{-Œªt}$ Gamma(2,Œª) distribution.\n",
        "\n",
        "- e8.2.5 (uniform convolution Ross e6.3a): let $X,Y‚àº\\Unif(0,1)$ iid, find distribution of $T=X+Y$.\n",
        "Domain of integration:\n",
        "$0\\lt s\\lt 1$,\n",
        "$0\\lt t-s\\lt 1‚áít-1\\lt s\\lt t$,\n",
        "therefore $\\max(t-1,0)\\lt s\\lt\\min(1,t)$ and $0\\lt t,t-1\\lt 1$.\n",
        "$f_T(t)=‚à´_{\\max(t-1,0)\\lt s\\lt\\min(1,t)}\\ ds$\n",
        "$=\\BC\n",
        "‚à´_0^t\\ ds=t&0\\lt t\\lt 1 \\\\\n",
        "‚à´_{t-1}^1\\ ds=2-t&1\\lt t\\lt 2\n",
        "\\EC$.\n",
        "\n",
        "- 8.1: Find PDF of $Y=e^{-X}$ where $X‚àº\\Expo(1)$. $x=-\\ln y$. $|\\frac{dx}{dy}|=\\frac{1}{y}$. $f_Y(y)=e^{\\ln y}/y=1$.\n",
        "\n",
        "- 8.8: Find distribution of $Y=X^2$ where $X‚àº\\DUnif(0,1,2,...,n)$.\n",
        "$Y‚àº\\DUnif(0,1,4,...,n^2)$.\n",
        "\n",
        "- 8.10: Let $X‚àº\\Pois(Œª)$, and $Y$ be indicator that $X$ is odd. Find PMF of $Y$.\n",
        "$P(Y=0)=\\suml_{k \\t{ even}}\\frac{e^{-Œª}Œª^k}{k!}$ and\n",
        "$P(Y=1)=\\suml_{k \\t{ odd}}\\frac{e^{-Œª}Œª^k}{k!}$.\n",
        "Taylor series $P(Y=0)-P(Y=1)$\n",
        "$=\\suml_{k=0}^‚àû(-1)^k\\frac{e^{-Œª}Œª^k}{k!}$\n",
        "$=e^{-Œª}\\suml_{k=0}^‚àû\\frac{(-Œª)^k}{k!}$\n",
        "$=e^{-Œª}e^{-Œª}$\n",
        "$=e^{-2Œª}$.\n",
        "$P(Y=0)+P(Y=1)=1$. Solve 2 equations with 2 unknowns.\n",
        "$P(Y=0)=\\frac{1+e^{-2Œª}}{2}$ and\n",
        "$P(Y=1)=\\frac{1-e^{-2Œª}}{2}$\n",
        "\n",
        "- 8.13: $X,Y‚àº\\Expo(Œª)$ iid, $T=\\ln(X/Y)$. Find distribution of T.\n",
        "$P(T\\lt t)=P(X/Y\\lt e^t)$\n",
        "$=‚à´_{y=0}^‚àû‚à´_{x=0}^{ye^t}Œª^2e^{-Œªx}e^{-Œªy}\\ dx\\ dy$\n",
        "$=‚à´_{y=0}^‚àûŒªe^{-Œªy}[1-e^{-Œªye^t}]\\ dy$\n",
        "$=1-‚à´_{y=0}^‚àûŒªe^{-Œªy(1+e^t)}\\ dy$\n",
        "$=\\frac{e^t}{1+e^t}$.\n",
        "$f_T(t)=\\frac{e^t}{1+e^t}-\\frac{e^{2t}}{(1+e^t)^2}$\n",
        "$=\\frac{e^t}{(1+e^t)^2}$.\n",
        "\n",
        "- 8.18: $X$ and $Y$ independent with $f_X$ and $f_Y$. Let $T=X/Y$. Find $f_T$ using Jacobian.\n",
        "$X=W,Y=W/T$.\n",
        "$|\\frac{‚àÇ(x,y)}{‚àÇ(w,t)}|=|\\frac{-w}{t^2}|$.\n",
        "$f_{W,T}(w,t)=f_X(w)f_Y(\\frac{w}{t})\\frac{w}{t^2}$.\n",
        "$f_T(t)=‚à´_{w=0}^‚àûf_X(w)f_Y(\\frac{w}{t})\\frac{w}{t^2}\\ dw$\n",
        "\n",
        "- 8.20: $U‚àº\\Unif(0,1)$ and $X‚àº\\Expo(1)$ independent. Find PDF of $U+X$.\n",
        "Support $1>t-s>0‚áít>s>t-1$ and $s>0$.\n",
        "$\\max(0,t-1)\\lt s\\lt t$\n",
        "$‚áí0\\lt t\\lt 1$ and $t>1$.\n",
        "$f_{U+X}(t)=f_U(t)‚äóf_X(t)$\n",
        "$=‚à´f_U(t-s)f_X(s)\\ ds$\n",
        "$=‚à´e^{-s}\\ ds$\n",
        "$=\\BC\n",
        "‚à´_0^te^{-s}=1-e^{-t}\\ ds &0\\lt t\\lt 1 \\\\\n",
        "‚à´_{t-1}^te^{-s}\\ ds=e^{-(t-1)}-e^{-t} &t>1\n",
        "\\EC$\n",
        "\n",
        "- 8.21 (Laplace): $X,Y‚àº\\Expo(1)$ iid. Find PDF of $L=X-Y$.\n",
        "$P(L\\lt t)=P(X-Y\\lt t)$\n",
        "$=‚à´_{y=0}^‚àû‚à´_{x=0}^{y+t}f(x)f(y)\\ dx\\ dy$\n",
        "$‚áíf_L(t)=‚à´_sf_X(s+t)f_Y(s)\\ ds$.\n",
        "Support $s>\\max(0,-t)‚áít\\lt 0$ and $t>0$.\n",
        "$f_L(t)=\\BC\n",
        "‚à´_0^‚àûe^{-2s-t}\\ ds=\\frac{1}{2}e^{-t} &t‚â•0 \\\\\n",
        "‚à´_{-t}^‚àûe^{-2s-t}\\ ds=e^{-t}\\frac{1}{2}e^{2t}=\\frac{1}{2}e^{t} &t\\lt 0\n",
        "\\EC$\n",
        "$=\\frac{1}{2}e^{-|t|}$.\n",
        "\n",
        "- 8.41, 8.43 (beta CDF): $X‚àº\\Binom(n,p)$ and $B‚àº\\Beta(j,n-j+1)$. Show that $P(B‚â§p)=P(X‚â•j)$.\n",
        "  - Let $U_1,...,U_n‚àº\\Unif(0,1)$ iid and $I_i$ be indicator that $U_i‚â§p$, such that $X=I_1+...+I_n$.\n",
        "  $X‚â•j$ iff at least $j$ elements in the ascending-sorted list is to the left of $p$: $P(X‚â•j)=P(U_{(j)}‚â§p)$.\n",
        "  - By order statistic,\n",
        "  $f_{U_{(j)}}(x)=n\\binom{n-1}{j-1}x^{j-1}(1-x)^{n-j}$, and therefore $U_{(j)}‚àº\\Beta(j,n-j+1)$.\n",
        "\n",
        "- 8.45: $X,Y‚àº\\Expo(Œª)$ iid, and $M=\\max(X,Y)$. Find the distribution of $M$.\n",
        "  - $P(M\\lt t)=(1-e^{-Œªt})^2$\n",
        "  $‚áíf_M(t)=2Œªe^{-Œª}(1-e^{-Œªt})$.\n",
        "  - Let $L=\\min(X,Y)$ then $L‚àº\\Expo(2Œª)‚àº\\frac{1}{2}Y$. $M-L‚àº\\Expo(Œª)$. Therefore $M‚àº\\Expo(2Œª)+\\Expo(Œª)$.\n",
        "  Convolution with limits of integration $s>0, s\\lt t$.\n",
        "  $f_M(t)=‚à´_0^tŒªe^{-Œª(t-s)}2Œªe^{-2Œªs}\\ ds$\n",
        "  $=2Œªe^{-Œªt}‚à´_0^tŒªe^{-Œªs}\\ ds$\n",
        "  $=2Œªe^{-Œªt}(1-e^{-Œªt})$.\n",
        "\n",
        "- 8.47: $X_1,X_2,...$ be iid with CDF $F$, $M_n=\\max(X_1,...,X_n)$. Find joint distribution of $M_n$ and $M_{n+1}$. If $a>b$ then $b$ is the maximum for all $X_1,...,X_n,X_{n+1}$.\n",
        "$P(M_n‚â§a, M_{n+1}‚â§b)=\\BC\n",
        "F(a)^nF(b) &a‚â§b \\\\\n",
        "F(b)^{n+1} &a>b\n",
        "\\EC$.\n",
        "\n",
        "- 8.48: $X_1,...,X_n$ be iid with $F$ and $f$. Find the joint PDF $f_{X_{(i)},X_{(j)}}(x_i,x_j)$.\n",
        "The probability $f_{X_{(i)},X_{(j)}}(x_i,x_j)\\ dx_i\\ dx_j$ has an element placed at $x_i$ with probability $nf(x_i)\\ dx_i$, another element placed at $x_j$ with probability $(n-1)f(x_j)\\ dx_j$, (i-1) elements placed before $x_i$ with probability $\\binom{n-2}{i-1}F(x_i)^{i-1}$, (j-i-1) elements placed between $x_i$ and $x_j$ with probability $\\binom{n-i-1}{j-i-1}(F(x_j)-F(x_i))^{j-i-1}$, and (n-j) elements after $x_j$: $(1-F(x_j))^{n-j}$. The constants are\n",
        "$\\frac{n(n-1)(n-2)!(n-i-1)!}{(i-1)!(n-2-i+1)!(j-i-1)!(n-j)!}$\n",
        "$=\\frac{n!}{(i-1)!(j-i-1)!(n-j)!}$.\n",
        "Therefore the bivariate order statistic PDF\n",
        "$f_{X_{(i)},X_{(j)}}(x_i,x_j)=\\frac{n!}{(i-1)!(j-i-1)!(n-j)!}F(x_i)^{i-1}(F(x_j)-F(x_i))^{j-i-1}(1-F(x_j))^{n-j}f(x_i)f(x_j)$."
      ],
      "metadata": {
        "id": "_kP2pathzwra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Covariance and Conditional Expectation\n",
        "\n",
        "- $\\left(\\suml_{i=1}^na_i\\right)^2=\\suml_{i=1}^na_i^2+\\suml_{i\\neq j}a_ia_j$\n",
        "- $\\min(a,b)=-\\max(-a,-b)$"
      ],
      "metadata": {
        "id": "KTXGM8LqQ5rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covariance** between $X$ and $Y$ is defined by $\\Cov(X,Y)=\\E\\left[(X-\\E[X])(Y-\\E[Y])\\right]$.\n",
        "- $\\Cov(X,Y)=\\E[XY]-\\E[X]\\E[Y]$\n",
        "- If $X$ and $Y$ are independent, then $\\Cov(X,Y)=0$. However $\\Cov(X,Y)=0$ does not imply independence.\n",
        "  - Let $P(X)=\\BC\\frac{1}{3}&X=-1,0,1\\\\0&\\t{else}\\EC$ and $I_{X=0}$ be indicator. Then $\\Cov(X,I_{X=0})=0$ even though they're dependent.\n",
        "  - Let $X‚àº\\Normal(0,1)$ and $Y=X^2$. $\\Cov(X,Y)=0$ even though they're dependent.\n",
        "  - $\\E[XY]=\\E[X]\\E[Y]$ if $\\Cov(X,Y)=0$. However, $\\E[g(X)h(Y)]=\\E[g(X)]\\E[h(Y)]$ iff $X$ and $Y$ are independent.\n",
        "- $\\Cov(X,X)=\\Var(X)$\n",
        "- $\\Cov(aX,Y)=a\\Cov(X,Y)$\n",
        "- $\\Cov\\left(\\suml_{i=1}^nX_i,\\suml_{j=1}^mY_j\\right)=\\suml_{i=1}^n\\suml_{j=1}^m\\Cov(X_i,Y_j)$\n",
        "  - $\\Cov(X+Y,Z)=\\Cov(X,Z)+\\Cov(Y,Z)$\n",
        "- $\\Var\\left(\\suml_{i=1}^nX_i\\right)=\\suml_{i=1}^n\\Var(X_i)+2\\suml_{i\\lt j}\\Cov(X_i,X_j)$\n",
        "  - $\\Var\\left(\\suml_{i=1}^nX_i\\right)=\\suml_{i=1}^n\\suml_{j=1}^n\\Cov(X_i,X_j)$ - summation of the covariance matrix\n",
        "  - $\\Var(X+Y)=\\Var(X)+\\Var(Y)+2\\Cov(X,Y)$\n",
        "  - $\\Var\\left(\\suml_{i=1}^nX_i\\right)=\\suml_{i=1}^n\\Var(X_i)$ if $X_i,X_j$ are pairwise independent.\n",
        "  - $\\Var(X+Y)=\\Var(X)+\\Var(Y)$ if $X,Y$ are independent.\n",
        "\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "&\\Cov\\left(\\suml_{i=1}^nX_i,\\suml_{j=1}^mY_j\\right) \\\\\n",
        "=&\\E\\left[\\left(\\sum_{i=1}^nX_i-\\sum_{i=1}^nŒº_{X_i}\\right)\\left(\\sum_{j=1}^mY_j-\\sum_{j=1}^mŒº_{Y_j}\\right)\\right] \\\\\n",
        "=&\\E\\left[\\sum_{i=1}^n(X_i-Œº_{X_i})\\sum_{j=1}^m(Y_j-Œº_{Y_j})\\right] \\\\\n",
        "=&\\sum_{i=1}^n\\sum_{j=1}^m\\E[(X_i-Œº_{X_i})(Y_j-Œº_{Y_j})] \\\\\n",
        "=&\\sum_{i=1}^n\\sum_{j=1}^m\\Cov(X_i,Y_j)\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "&\\Var\\left(\\sum_{i=1}^nX_i\\right) \\\\\n",
        "=&\\Cov\\left(\\sum_{i=1}^nX_i,\\sum_{j=1}^nX_j\\right) \\\\\n",
        "=&\\sum_{i=1}^n\\sum_{j=1}^n\\Cov(X_i,X_j) \\\\\n",
        "=&\\sum_{i=1}^n\\Var(X_i)+\\sum_{i\\neq j}\\Cov(X_i,X_j) \\\\\n",
        "=&\\sum_{i=1}^n\\Var(X_i)+2\\sum_{i\\lt j}\\Cov(X_i,X_j)\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- $\\Corr(X,Y)=\\frac{\\Cov(X,Y)}{\\sqrt{\\Var(X)\\Var(Y)}}‚àà[-1,1]$. (Proof: Cauchy-Schwarz)\n",
        "\n"
      ],
      "metadata": {
        "id": "aHfPItfIQ_9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Moments from indicators**: for events $A_1,...,A_n$ where $I_i$ is indicator such that $\\E[I_i]=P(A_i)$. Let $X=I_1+...+I_n$ be the number of successes in $n$ trials then $\\E[X]=\\suml_{i=1}^n\\E[I_i]$ and\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\binom{X}{2}&=\\sum_{i\\lt j}I_iI_j \\\\\n",
        "E\\binom{X}{2}&=\\sum_{i\\lt j}\\E[I_iI_j] \\\\\n",
        "\\E[X(X-1)]&=2\\sum_{i\\lt j}\\E[I_iI_j] \\\\\n",
        "\\E[X^2]&=\\E[X]+2\\sum_{i\\lt j}\\E[I_iI_j] \\\\\n",
        "\\Var(X)&=\\E[X]-\\E[X]^2+2\\sum_{i\\lt j}\\E[I_iI_j]\n",
        "\\end{aligned}\n",
        "\\\n",
        "\\begin{aligned}\n",
        "\\Var(X)&=\\Var\\left(\\sum_{i=1}^nI_i\\right) \\\\\n",
        "&=\\sum_{i=1}^n\\Var(I_i)+2\\sum_{i\\lt j}\\Cov(I_i,I_j) \\\\\n",
        "&=\\sum_{i=1}^n\\E[I_i](1-\\E[I_i])+2\\sum_{i\\lt j}(\\E[I_iI_j]-\\E[I_i]\\E[I_j]) \\\\\n",
        "&=\\E[X]-\\sum_{i=1}^n\\sum_{j=1}^n\\E[I_i]\\E[I_j]+2\\sum_{i\\lt j}\\E[I_iI_j] \\\\\n",
        "&=\\E[X]-\\E[X]^2+2\\sum_{i\\lt j}\\E[I_iI_j]\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- $\\E[I_iI_j]=P(A_i)P(A_j|A_i)$\n",
        "- $\\E[I_iI_j]=1-P(A_i^c‚à™A_j^c)$\n",
        "$=1-P(A_i^c)-P(A_j^c)+P(A_i^c,A_j^c)$.\n",
        "- Higher moments of $X$ can be recursively derived from $E\\binom{X}{k}$\n"
      ],
      "metadata": {
        "id": "s0vqp45tREWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multivariate normal distribution (MVN)**: random vector $\\v{X}_n=X_1,...,X_n$ is said to have MVN distribution if every linear combination of $\\v{X}$ has a normal distribution. $\\v{X}_n$ is MVN iff $(t_1X_1+...+t_nX_n)‚àº\\Normal$ for all $\\v{t}_n‚àà‚Ñù^n$.\n",
        "- $\\v{X}_n‚àº\\Normal(\\v{Œº}_n,\\Cov(\\v{X}_n))$.\n",
        "- Let $\\v{Z}_n=\\{Z_1,...,Z_n\\}$ be iid $\\Normal(0,1)$ and $\\v{Œº}_m=\\{Œº_1,...,Œº_m\\}$, then for matrix $A_{mn}$, $\\v{X}_m=A_{mn}\\v{Z}_n+\\v{Œº}_m‚àº\\Normal(\\v{Œº}_m,AA^T)$ are MVN.\n",
        "  - Blitzstein e7.5.3: $Z_1,Z_2‚àº\\Normal(0,1)$ iid. $(Z_1+2Z_2,3Z_1+5Z_2)$ are bivariate normal because $t_1(Z_1+2Z_2)+t_2(3Z_1+5Z_2)$ can be rewritten as $(t_1+3t_2)Z_1+(2t_1+5t_2)Z_2$.\n",
        "- (Substring and concatenation) If $(X_1,X_2,X_3)$ are MVN, then subvector $(X_1,X_2)$ are also MVN. If $\\v{X}=(X_1,...,X_n)$ and $\\v{Y}=(Y_1,...,Y_m)$ are MVN, then the concatenation $\\v{W}=(X_1,...,X_n,Y_1,...,Y_m)$ are also MVN.\n",
        "- Joint MGF $M(\\v{t})=\\E[\\e{t_1X_1+...+t_nX_n}]$ for $\\v{t}_n‚àà‚Ñù^n$ is a lognormal expectation. If $W‚àº\\Normal$ then $\\E[e^W]=\\e{\\E[W]+\\frac{1}{2}\\Var(W)}$ therefore $M(\\v{t})=\\e{t_1\\E[X_1]+...+t_n\\E[X_n]+\\frac{1}{2}\\Var(t_1X_1+...+t_nX_n)}$.\n",
        "- If $\\v{X}=(\\v{X}_1,\\v{X}_2)$ is MVN and $\\v{X}_1$ and $\\v{X}_2$ are subvectors and every random variable of $\\v{X}_1$ is uncorrelated with every component of $\\v{X}_2$, then $\\v{X}_1$ and $\\v{X}_2$ are independent. (Blitzstein 7.5.7)\n",
        "  - $\\Corr(X,Y)=0$ does not generally imply independence. However if $(X,Y)$ is MVN then $\\Corr(X,Y)=0$ implies independence.\n",
        "  - Proof: let $X‚àº\\Normal(Œº_1,œÉ_1^2)$ and $Y‚àº\\Normal(Œº_2,œÉ_2^2)$ be BVN.\n",
        "  Then $M_{X,Y}(s,t)=\\e{sX+tY}$\n",
        "  $=\\e{sŒº_1+tŒº_2+\\frac{1}{2}\\Var(sX+tY)}$\n",
        "  $=\\e{sŒº_1+tŒº_2+\\frac{1}{2}[\\Var(sX)+\\Var(tY)+2\\Cov(sX,tY)]}$.\n",
        "  If $\\Cov(X,Y)=0$ then\n",
        "  $M_{X,Y}(s,t)=\\e{sŒº_1+tŒº_2+\\frac{1}{2}[\\Var(sX)+\\Var(tY)]}$\n",
        "  $=M_X(s)M_Y(t)$. Joint MGF $M_{X,Y}(s,t)=M_X(s)M_Y(t)$ iff $X,Y$ are independent (proof in MGF section).\n",
        "- Let $X,Y$ be iid, then $\\Cov(X+Y,X-Y)=0$ is true for all distributions. However only for $X,Y‚àº\\Normal$ are $X+Y$ and $X-Y$ independent because they are bivariate normal. (Blitzstein 7.5.8)\n",
        "\n",
        "**Bivariate normal distribution**: for $X‚àº\\Normal(Œº_x,œÉ_x^2)$ and $Y‚àº\\Normal(Œº_y,œÉ_y^2)$ with $œÅ$,\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\v{z}&=\\BPM\n",
        "x-Œº_x \\\\\n",
        "y-Œº_y\n",
        "\\EPM \\\\\n",
        "Œ£&=\\Cov(X,Y)=\\BPM\n",
        "œÉ_x^2 & œÅœÉ_xœÉ_y \\\\\n",
        "œÅœÉ_xœÉ_y & œÉ_y^2 \\EPM\n",
        "\\end{aligned}\n",
        "\\\n",
        "\\begin{aligned}\n",
        "\\det(Œ£)&=œÉ_x^2œÉ_y^2(1-œÅ^2) \\\\\n",
        "Œ£^{-1}&=\\frac{1}{\\det(Œ£)}\\BPM\n",
        "œÉ_y^2 & -œÅœÉ_xœÉ_y \\\\\n",
        "-œÅœÉ_xœÉ_y & œÉ_x^2\n",
        "\\EPM\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "$$\\small\\begin{aligned}\n",
        "f_{X,Y}(x,y)&=\\frac{1}{2œÄ\\sqrt{\\det(Œ£)}}\\e{-\\frac{1}{2}\\v{z}^TŒ£^{-1}\\v{z}} \\\\\n",
        "&=\\frac{1}{2œÄœÉ_xœÉ_y\\sqrt{1-œÅ^2}}\\e{-\\frac{1}{2(1-œÅ)^2}\\left[\\frac{(x-Œº_x)^2}{œÉ_x^2}-\\frac{2œÅ(x-Œº_x)(y-Œº_y)}{œÉ_xœÉ_y}+\\frac{(y-Œº_y)^2}{œÉ_y^2}\\right]}\n",
        "\\end{aligned}$$"
      ],
      "metadata": {
        "id": "nmWT60b9RIVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multinomial distribution**: $n$ iid experiments each with $r$ possible outcomes at probabilities $p_1+...+p_r=1$. Let $X_i$ be the number of experiments that result in outcome $i$, then $P(X_1=n_1,\\ ...\\ ,X_r=n_r)=\\frac{n!}{n_1!...n_r!}p_1^{n_1}...p_r^{n_r}$.\n",
        "- $\\v{X}=\\Mult_r(n,\\v{p})$.\n",
        "- Marginal $X_i‚àº\\Binom(n,p_i)$.\n",
        "  - $(X_i|X_j)‚àº\\Binom(n-X_j,\\frac{p_i}{1-p_j})$\n",
        "- Lumped marginal $X_i+X_j‚àº\\Binom(n,p_i+p_j)$.\n",
        "- Conditional $P(X_2=n_2,\\ ...\\ ,X_r=n_r|X_1=n_1)=\\frac{P(X_1=n_1,\\ ...\\ ,X_r=n_r)}{P(X_1=n_1)}$\n",
        "- $(X_2,...,X_r|X_1=n_1)‚àº\\Mult_{r-1}(n-n_1,\\v{p}')$ where $p_i'=\\frac{p_i}{p_2+...+p_r}$.\n",
        "- $\\E[X_iX_j]=n(n-1)p_ip_j$ and $\\Cov(X_i,X_j)=-np_ip_j$\n",
        "  - Let $I_u$ be indicator that $u$th trial outcome $i$ and $J_v$ be indicator that $v$th trial outcome $j$.\n",
        "\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "X_i&=\\sum_{u=1}^nI_u\\quad X_j=\\sum_{v=1}^nJ_v\\\\\n",
        "X_iX_j&=\\left(\\sum_{u=1}^nI_u\\right)\\left(\\sum_{v=1}^nJ_v\\right) \\\\\n",
        "&=\\sum_{u=1}^n\\sum_{v=1}^nI_uJ_v\\\\\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "\\E[X_iX_j]&=\\sum_{u=1}^n\\sum_{v=1}^n\\E[I_uJ_v]\\\\\n",
        "&=\\sum_{u=1}^n\\E[I_uJ_u]+\\sum_{u\\neq v}\\E[I_uJ_v]\\\\\n",
        "&=0+n(n-1)\\E[I_u]\\E[J_v]\\\\\n",
        "&=n(n-1)p_ip_j\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- The $n$ trials are independent, but $\\v{X}$ are all dependent and negatively correlated."
      ],
      "metadata": {
        "id": "dYe7itJLRMnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditional expectation**:\n",
        "- $\\E[X|Y=y]=\\suml_{x}xP(X=x|Y=y)$\n",
        "- $\\E[X|Y=y]=‚à´_xxf_{X|Y}(x|y)\\ dx$ - function of $y$\n",
        "- $\\E[h(X)Y|X]=h(X)\\E[Y|X]$ - conditional constant\n",
        "  - $\\E[Y\\E[Y|X]|X]=\\E[Y|X]^2$\n",
        "- **Law of total probability**:\n",
        "  - $P(X=x)=\\suml_yP(X=x|Y=y)P(Y=y)=\\E[P(X=x|Y=y)]$\n",
        "  - $f_X(x)=‚à´_yf_{X|Y}(x|y)f_Y(y)\\ dy=\\E[f(x|y)]$ marginal PDF\n",
        "- **Law of total expectation**: $\\E[X]=\\E[\\E[X|Y]]$ (Adam's law).\n",
        "  - $\\E[X]=\\sum_{y}\\E[X|Y=y]P(Y=y)$\n",
        "  - $\\E[X]=‚à´_y\\E[X|Y=y]f_Y(y)\\ dy$\n",
        "  - $\\E[X|Z]=\\E[\\E[X|Y,Z]|Z]$.\n",
        "  - $\\E[XY]=\\E[X\\E[Y|X]]$: used for solving $\\Cov(X,Y)$.\n",
        "\n",
        "$$\\small\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\E[XY]&=\\sum_x\\sum_yxyP(X=x,Y=y) \\\\\n",
        "&=\\sum_xx\\sum_yyP(X=x,Y=y) \\\\\n",
        "&=\\sum_xx\\sum_yyP(Y=y|X=x)P(X=x) \\\\\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "&=\\sum_xxP(X=x)\\sum_yyP(Y=y|X=x) \\\\\n",
        "&=\\sum_xx\\E[Y|X=x]P(X=x) \\\\\n",
        "&=\\E[X\\E[Y|X]] \\\\\n",
        "&\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "\n",
        "**Conditional variance**: $\\Var(X|Y)=\\E[(X-\\E[X|Y])^2|Y]$ and $\\Var(X|Y)=\\E[X^2|Y]-\\E[X|Y]^2$\n",
        "- Proof (Blitzstein 9.18): Recognize $\\E[‚ãÖ|X]$ is a function of $X$, and that $\\E[h(X)Y|X]=h(X)\\E[Y|X]$.\n",
        "$\\E[(Y-\\E[Y|X])^2|X]$\n",
        "$=\\E[Y^2|X]-2\\E[Y\\E[Y|X]|X]+\\E[\\E[Y|X]^2|X]$\n",
        "$=\\E[Y^2|X]-2\\E[Y|X]\\E[Y|X]+\\E[Y|X]^2$\n",
        "$=\\E[Y^2|X]-\\E[Y|X]^2$.\n",
        "- $\\Var(X)=\\E[\\Var(X|Y)]+\\Var(\\E[X|Y])$ (Eve's Law - **Law of total variance**)\n",
        "  - $\\E[\\Var(X|Y)]=\\E[X^2]-\\E[\\E[X|Y]^2]$ - average within-group variations (unexplained variations)\n",
        "  - $\\Var(\\E[X|Y])=\\E[\\E[X|Y]^2]-\\E[X]^2$ - variation of group means (explained variations)\n",
        "  $\\Var(Y)\\neq\\Var(Y|A)P(A)+\\Var(Y|A^c)P(A^c)$\n",
        "- $\\Var(h(Z)|Z)=0$ - conditional constant\n",
        "- Conditional on event (Blitzstein 9.32): A customer makes purchase with probability $p$. Given that a purchase is made, $(Œº,œÉ^2)$ is spent. Find mean and variance of spending $X$.\n",
        "  - *LOTP/LOTUS*: Let event $A$ be purchase is made.\n",
        "  $\\E[X]=\\E[X|A]P(A)+\\E[X|A^c]P(A^c)$\n",
        "  $=Œºp$.\n",
        "  $\\E[X^2]=\\E[X^2|A]P(A)+\\E[X^2|A^c]P(A^c)$\n",
        "  $=(œÉ^2+Œº^2)p$.\n",
        "  $\\Var(X)=(œÉ^2+Œº^2)p-Œº^2p^2=Œº^2p(1-p)+œÉ^2p$.\n",
        "  - *Adam and Eve*: Let $I$ be indicator for purchase.\n",
        "  $X|I‚àº(ŒºI,œÉ^2I)$.\n",
        "  $\\E[X|I]=ŒºI$.\n",
        "  $\\E[X]=Œº\\E[I]=Œºp$.\n",
        "  $\\Var(\\E[X|I])=Œº^2\\Var(I)=Œº^2p(1-p)$.\n",
        "  $\\E[\\Var(X|I)]=œÉ^2\\E[I]=œÉ^2p$.\n",
        "  $\\Var(X)=Œº^2p(1-p)+œÉ^2p$.\n"
      ],
      "metadata": {
        "id": "0eBN11Hxf_kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geometric interpretation**: a probability space is mapped to a vector space where each vector/point represents a random variable.\n",
        "- The inner product $‚ü®X,Y‚ü©=\\E[XY]$, square magnitude $\\norm{X}^2=‚ü®X,X‚ü©=\\E[X^2]$, and square distance $\\norm{X-Y}^2=\\E(X-Y)^2$.\n",
        "  - If $\\E[X]=0$ and $\\E[Y]=0$ then $\\norm{X}^2=\\Var(X)$, $‚ü®X,Y‚ü©=\\Cov(X,Y)$, and $\\cos(Œ∏_{XY})=\\Corr(X,Y)$. Most importantly, $‚ü®X,Y‚ü©=0$ iff $\\Corr(X,Y)=0$\n",
        "- Projection: $\\Cov(Y-\\E[Y|X],h(X))=0$. Equivalently $\\E[(Y-\\E[Y|X])h(X)]=0$.\n",
        "  - $\\E[(Y-\\E[Y|X])h(X)]$\n",
        "  $=\\E[h(X)Y]-\\E[h(X)\\E[Y|X]]$\n",
        "  $=\\E[h(X)Y]-\\E[\\E[h(X)Y|X]]$\n",
        "  $=0$.\n",
        "  - **Best predictor**: If $g(X)$ is a predictor of $Y$ from observed value of $X$, then $g(X)=\\E[Y|X]$ is the predictor with the least $\\E(Y-g(X))^2$."
      ],
      "metadata": {
        "id": "72coBpyGwUxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blitzstein Chapter 7 (Joint distributions):\n",
        "\n",
        "- e7.5.9 (independence of sample mean and variance): Let $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$ iid. Show $\\bar{X}_n$ and $S_n^2$ are independent from the random vector $(\\bar{X}_n,X_1-\\bar{X}_n,...,X_n-\\bar{X}_n)$.\n",
        "The random vector is MVN.\n",
        "$\\Cov(\\bar{X}_n,X_j-\\bar{X}_n)$\n",
        "$=\\Cov(\\bar{X}_n,X_j)-\\Cov(\\bar{X}_n,\\bar{X}_n)$\n",
        "$=\\frac{1}{n}\\suml_{i=1}^n\\Cov(X_i,X_j)-\\frac{œÉ^2}{n}$\n",
        "$=\\frac{1}{n}\\suml_{i\\neq j}\\Cov(X_i,X_j)+\\frac{1}{n}\\Cov(X_j,X_j)-\\frac{œÉ^2}{n}$\n",
        "$=0$.\n",
        "$\\bar{X}_n$ is independent of the random vector. $S_n^2=\\frac{1}{n-1}\\suml_{j=1}^n(X_j-\\bar{X}_n)^2$ is a linear combination of the random vector.\n",
        "\n",
        "- e7.5.10 (bivariate normal generation): Given $X,Y‚àº\\Normal(0,1)$ iid, generate bivariate normal vector $(Z,W)$ with $œÅ$ and $Z,W‚àº\\Normal(0,1)$ marginally.\n",
        "$\\BC Z=aX+bY\\\\W=cX+dY\\EC$ are bivariate normal.\n",
        "$\\BC a^2+b^2=1\\\\c^2+d^2=1\\EC$ sets the marginal variances to 1.\n",
        "$\\Cov(aX+bY,cX+bY)$\n",
        "$=\\Cov(aX,cX)+\\Cov(aX,dY)$\n",
        "$+\\Cov(bY,cX)+\\Cov(bY,dY)$\n",
        "$=ac\\Var(X)+bd\\Var(Y)$\n",
        "$‚áíœÅ=ac+bd$.\n",
        "One solution uses $b=0,a=1$ and we get\n",
        "$\\BC Z=X\\\\W=œÅX+\\sqrt{1-œÅ^2}Y\\EC$.\n",
        "\n",
        "- 7.37: $X,Y$ iid continuous with $f$, $Œº$, and $œÉ^2$. Find $\\E(X-Y)^2$.\n",
        "$\\E(X-Y)^2=\\iint_{x,y}(x-y)^2f(x)f(y)\\ dy\\ dx$\n",
        "$=\\iint_{x,y}(x^2-2xy+y^2)f(x)f(y)\\ dy\\ dx$\n",
        "$=‚à´_xx^2f(x)\\ dx$\n",
        "$+‚à´_yy^2f(y)\\ dy$\n",
        "$-\\iint_{x,y}2xyf(x)f(y)\\ dy\\ dx$\n",
        "$=\\E[X^2]+\\E[Y^2]-2\\E[XY]$\n",
        "$=œÉ^2+Œº^2+œÉ^2+Œº^2-2\\Cov(X,Y)-2Œº^2$\n",
        "$=2œÉ^2$.\n",
        "\n",
        "- 7.41: let $X,Y$ be standardized with $Œº=0,œÉ^2=1,œÅ$. Find standardized $Z=aX+bY$ and $W=cX+dY$ with 0 correlation.\n",
        "$\\Var(aX+bY)$\n",
        "$=\\Var(aX)+\\Var(bY)-2\\Cov(aX,bY)$\n",
        "$=a^2+b^2-2abœÅ$.\n",
        "$\\Var(cX+dY)$\n",
        "$=c^2+d^2-2cdœÅ$.\n",
        "$\\Cov(aX+bY,cX+dY)$\n",
        "$=ac+(bc+ad)œÅ+bd$.\n",
        "Let $Z=W$, then\n",
        "$a=1,b=0$,\n",
        "$c+dœÅ=0$.\n",
        "$d^2-(dœÅ)^2=1$.\n",
        "$d=\\frac{1}{\\sqrt{1-œÅ^2}}$,\n",
        "$c=-\\frac{œÅ}{\\sqrt{1-œÅ^2}}$.\n",
        "\n",
        "- 7.42 (e4.4.5): $X$ be the number of distinct birthdays in a group of n=110 people. Find the mean and variance of $X$.\n",
        "Let $I_i/A_i$ be $i$th day being represented in the group.\n",
        "$\\E[I_i]=1-(\\frac{364}{365})^n$.\n",
        "$P(A_i^c‚à™A_j^c)=P(A_i^c)+P(A_j^c)$\n",
        "$-P(A_i^c,A_j^c)$\n",
        "$=2(\\frac{364}{365})^n-(\\frac{363}{365})^n$.\n",
        "$\\E[I_iI_j]=1-2(\\frac{364}{365})^n+(\\frac{363}{365})^n$.\n",
        "$\\E[X]=365(1-(\\frac{364}{365})^n$\n",
        "$=95.08$.\n",
        "$E\\binom{X}{2}=\\binom{365}{2}\\E[I_iI_j]$\n",
        "$‚áí\\Var(X)=\\E[X]+(365)(364)$\n",
        "$(1-2(\\frac{364}{365})^n+(\\frac{363}{365})^n)$\n",
        "$-\\E[X]^2$\n",
        "$=10.62$.\n",
        "\n",
        "- 7.44 (4.3.12): Coupons are being collected from $k$ types randomly with equal likelihood until all types are collected after $N$ coupons. What is the variance of $N$?\n",
        "$N=N_1+...+N_k$, where\n",
        "$N_i‚àº\\Geom(\\frac{k-i+1}{k})$ independent.\n",
        "$\\Var(N)$\n",
        "$=\\suml_{i=1}^k\\Var(N_i)+2\\suml_{i\\lt j}\\Cov(N_i,N_j)$\n",
        "$=\\suml_{i=2}^k\\frac{i-1}{k}/(\\frac{k-i+1}{k})^2$\n",
        "$=k\\suml_{i=2}^k\\frac{i-1}{(k-i+1)^2}$\n",
        "$=k\\suml_{j=1}^{k-1}\\frac{k-j}{j^2}$.\n",
        "\n",
        "- 7.45: a random triangle has the same joint distribution for all 3 pairs of angles and each angle has non-zero variance. What is the correlation between two angles?\n",
        "Let $X,Y,Z$ denote the angles.\n",
        "$0=\\Var(X+Y+Z)$\n",
        "$=3\\Var(X)+2\\binom{3}{2}\\Cov(X,Y)$\n",
        "$‚áí\\Cov(X,Y)=\\frac{-\\Var(X)}{2}$.\n",
        "$œÅ=-\\frac{1}{2}$.\n",
        "\n",
        "- 7.46 (4.38, 4.68): $n$ people write their names on slips and shuffle them in a hat, then each person randomly draws a slip. What is the variance of number of people who draw their own names $X$?\n",
        "Let $I_i$ be indicator person $i$ draws their own name.\n",
        "$\\E[I_i]=\\frac{1}{n}$.\n",
        "$\\E[I_iI_j]=\\frac{1}{n}\\frac{1}{n-1}$.\n",
        "$\\Var(X)=\\E[X]-\\E[X]^2+2\\binom{n}{2}\\E[I_iI_j]$\n",
        "$=1$.\n",
        "\n",
        "- 7.47 (e4.4.7): Balls are withdrawn without replacement from an urn containing $w$ white balls and $b$ black balls until $X$ black balls and the $r$th white ball are drawn. Find $\\Var(X)$.\n",
        "Let $I_i$ be indicator for the $i$th black ball drawn before the $r$th white ball.\n",
        "$\\E[I_i]=\\frac{1+(r-1)}{w+1}=\\frac{r}{w+1}$.\n",
        "$\\E[X]=\\frac{rb}{w+1}$.\n",
        "$\\E[I_iI_j]=\\frac{r}{w+1}\\frac{r+1}{w+2}$ the conditional probability that the $j$th black ball is placed in the $r+1$ slots before the $r$th white ball given that the $i$th black ball is already placed before the $r$th white ball is $\\frac{r+1}{w+2}$.\n",
        "$\\Var(X)=\\E[X]-\\E[X]^2+2\\binom{b}{2}\\E[I_iI_j]$\n",
        "$=\\frac{rb(w+1)}{(w+1)^2}$\n",
        "$-\\frac{r^2b^2}{(w+1)^2}$\n",
        "$+\\frac{r(r+1)b(b-1)}{(w+1)(w+2)}$\n",
        "$=\\frac{rb(w+b+1)(w-r+1)}{(w+1)^2(w+2)}$.\n",
        "\n",
        "- 7.48 (e5.7.4): athletes compete one at a time at high jump. Let $X_j$ be continuous iid how high the $j$th jumper scored. $j$th jumper sets a record if $X_j>\\max(X_1,...,X_{j-1})$. What is the variance of the number of records $N$ in the first $n$ jumps?\n",
        "Let $I_i$ be indicator for $i$th jumper setting record.\n",
        "$\\E[I_i]=\\frac{1}{i}$.\n",
        "$\\E[I_iI_j]=\\frac{1}{ij}$ because independent. $X_j|X_i$ needs to account for $X_1,...,X_{j-1}$ and not limited to $X_i,...,X_{j-1}$, otherwise the probability of record setting increases with higher $i$.\n",
        "Using Monte Python intuition $P(I_{1001}=1|I_{1000}=1)\\neq \\frac{1}{2}$.\n",
        "$\\Var(N)=\\E[N]-\\E[N]^2+2\\suml_{i\\lt j}\\E[I_iI_j]$\n",
        "$=\\suml_{i=1}^n\\frac{1}{i}$\n",
        "$-\\left(\\suml_{i=1}^n\\frac{1}{i}\\right)^2$\n",
        "$+2\\suml_{i\\lt j}\\frac{1}{ij}$\n",
        "$=\\suml_{i=1}^n(\\frac{1}{i}-\\frac{1}{i^2})$.\n",
        "\n",
        "- 7.49 (e7.1.9): A chicken lays $N‚àº\\Pois(Œª)$ eggs, each hatching with probability $p$. Let $X$ be the number of hatchlings such that $(X|N=n)‚àº\\Binom(n,p)$. Find the correlation between $N$ and $X$.\n",
        "  - $\\Cov(N,X)=\\E[NX]-\\E[N]\\E[X]$\n",
        "  $=\\E[N\\E[X|N]]-\\E[N]\\E[\\E[X|N]]$\n",
        "  $=\\E[NNp]-Œª\\E[Np]$\n",
        "  $=p\\E[N^2]-Œªp\\E[N]$\n",
        "  $=p(\\Var(N)+\\E[N]^2)-pŒª^2$\n",
        "  $=pŒª$.\n",
        "  $\\Var(X)=\\Var(\\E[X|N])+\\E[\\Var(X|N)]$\n",
        "  $=\\Var(Np)+\\E[Np(1-p)]$\n",
        "  $=p^2Œª+p(1-p)Œª$\n",
        "  $=pŒª$.\n",
        "  $œÅ=\\frac{pŒª}{\\sqrt{pŒª^2}}=\\sqrt{p}$.\n",
        "  - $X‚àº\\Pois(pŒª)$ and $N=X+Y$.\n",
        "  $\\Cov(N,X)=\\Cov(X+Y,X)$\n",
        "  $=\\Cov(X,X)+\\Cov(X,Y)$\n",
        "  $=\\Var(X)$\n",
        "  $=pŒª$.\n",
        "  $œÅ=\\frac{pŒª}{\\sqrt{pŒª^2}}=\\sqrt{p}$.\n",
        "\n",
        "- 7.51: $X$ and $Y$ are independent. Find $\\Var(XY)$.\n",
        "$\\Var(XY)$\n",
        "$=\\E[X^2Y^2]-\\E[XY]^2$\n",
        "$=\\E[X^2]\\E[Y^2]-\\E[X]^2\\E[Y]^2$\n",
        "$=(\\Var(X)+\\E[X]^2)(\\Var(Y)+\\E[Y]^2)$\n",
        "$-\\E[X]^2\\E[Y]^2$\n",
        "$=\\Var(X)\\Var(Y)$\n",
        "$+\\E[X]^2\\Var(Y)+\\E[Y]^2\\Var(X)$.\n",
        "\n",
        "- 7.52: $3n$ students, $n$ of whom wear each of 3 sizes. $3n$ shirts, $n$ shirts of each size, were randomly handed to the students. Let $X$ be the number of students who get their right size shirt. What is the variance of X?\n",
        "Let $I_i/A_i$ be indicator that student $i$ gets the right size shirt.\n",
        "$\\E[I_i]=\\frac{1}{3}$.\n",
        "$\\E[I_iI_j]=\\BC\n",
        "\\frac{n}{3n}\\frac{n-1}{3n-1}=\\frac{n-1}{3(3n-1)}&3\\binom{n}{2} \\t{pairs who wear same size} \\\\\n",
        "\\frac{n}{3n}\\frac{n}{3n-1}=\\frac{n}{3(3n-1)}&\\binom{3n}{2}-3\\binom{n}{2}\\t{pairs who wear diff sizes}\n",
        "\\EC$.\n",
        "$\\Var(X)=\\E[X]-\\E[X]^2$\n",
        "$+2(3)\\binom{n}{2}\\frac{n-1}{3(3n-1)}$\n",
        "$+2(3)n^2\\frac{n}{3(3n-1)}$\n",
        "$=n-n^2+\\frac{n(n-1)^2}{3n-1}+\\frac{2n^3}{3n-1}$\n",
        "$=\\frac{2n^2}{3n-1}$.\n",
        "\n",
        "- 7.53: A person starts at (0,0), and each step either moves N, W, S, or E. After $n$ steps he end up at $(X_n,Y_n)$. Find $\\Cov(X_n,Y_n)$.\n",
        "Let $X_i,Y_i$ be movement at step $i$.\n",
        "$\\Cov(\\suml_{i=1}^nX_i, \\suml_{j=1}^nY_j)$\n",
        "$=\\suml_{i=1}^n\\suml_{j=1}^n\\Cov(X_i,Y_j)$\n",
        "$=\\suml_{i=1}^n\\Cov(X_i,Y_i)$\n",
        "$+\\suml_{i\\neq j}\\Cov(X_i,Y_j)$\n",
        "$=\\suml_{i=1}^n\\E[X_iY_i]$\n",
        "$-\\suml_{i=1}^n\\E[X_i]\\E[Y_i]$\n",
        "$+\\suml_{i\\neq j}\\E[X_i,Y_j]$\n",
        "$-\\suml_{i\\neq j}\\E[X_i]\\E[Y_j]$\n",
        "$=0$.\n",
        "\n",
        "- 7.54 (max and min of normals): $X,Y‚àº\\Normal(0,1)$ iid. Let $L=\\min(X,Y)$ and $M=\\max(X,Y)$. Find $\\Corr(L,M)$.\n",
        "  - (1) $M+L=X+Y$, (2) $M-L=|X-Y|$, (3) $ML=XY$, (4) $\\min(X,Y)=-\\max(-X,-Y)‚àº-\\max(X,Y)$\n",
        "  - $\\E[M]+\\E[L]=\\E[X]+\\E[Y]=0$ and\n",
        "  $\\E[M]-\\E[L]=\\E|Y-X|=\\frac{2}{\\sqrt{œÄ}}$\n",
        "  therefore $\\E[M]=\\frac{1}{\\sqrt{œÄ}}$\n",
        "  and $\\E[L]=-\\frac{1}{\\sqrt{œÄ}}$.\n",
        "  - $\\Cov(L,M)$\n",
        "  $=\\E[LM]-\\E[L]\\E[M]$\n",
        "  $=\\E[X]\\E[Y]+\\frac{1}{œÄ}$\n",
        "  $=\\frac{1}{œÄ}$.\n",
        "  - $(X-Y)‚àº\\Normal(0,2)$, therefore $\\E(X-Y)^2=2$.\n",
        "  - $\\E(X-Y)^2=\\E(M-L)^2$\n",
        "  $=\\E[M^2]-2\\E[ML]+\\E[L^2]$\n",
        "  $=\\E[M^2]+\\E[L^2]-2\\E[X]\\E[Y]$\n",
        "  $=\\Var(M)+\\E[M]^2+\\Var(L)+\\E[L]^2$.\n",
        "  Because $M‚àº-L$,\n",
        "  $\\Var(M)=\\Var(L)$\n",
        "  $=1-\\frac{1}{œÄ}$.\n",
        "  - $\\Corr(L,M)=\\frac{1/œÄ}{1-1/œÄ}=\\frac{1}{œÄ-1}$\n",
        "\n",
        "- 7.55: Let $U‚àº\\Unif(-1,1)$ and $V=2|U|-1$. Find distribution of $V$ and show $U$ and $V$ are uncorrelated.\n",
        "The support of $V$ is $(-1,1)$.\n",
        "$P(V\\lt v)=P(|U|\\lt \\frac{v+1}{2})$\n",
        "$=2‚à´_0^{(v+1)/2}\\frac{1}{2}\\ du$\n",
        "$=\\frac{v+1}{2}$.\n",
        "$f_V(v)=\\frac{1}{2},\\ v‚àà(-1,1)$\n",
        "therefore $V‚àº\\Unif(-1,1)$.\n",
        "$\\Cov(U,V)=\\E[U|U|]=0$ since $U|U|$ is odd on $(-1,1)$.\n",
        "\n",
        "- 7.65: Let $(X_1,...,X_k)‚àº\\Mult_k(p_1,...,p_k)$, find $\\Cov(X_i,X_j)$.\n",
        "Let $I_u,J_v$ be indicators for $i,j$th outcome on trial $u,v$.\n",
        "  - $\\Cov(\\suml_{u=1}^nI_u,\\suml_{v=1}^nJ_v)$\n",
        "  $=\\suml_{u=1}^n\\suml_{v=1}^n\\Cov(I_u,J_v)$\n",
        "  $=\\suml_{u=1}^n\\suml_{v=1}^n(\\E[I_uJ_v]-\\E[I_u]\\E[J_v])$\n",
        "  $=\\suml_{u\\neq v}\\E[I_uJ_v]$\n",
        "  $-n^2p_ip_j$\n",
        "  $=(n^2-n)p_ip_j-n^2p_ip_j$\n",
        "  $=-np_ip_j$.\n",
        "  - $\\Cov(\\suml_{u=1}^nI_u,\\suml_{v=1}^nJ_v)$\n",
        "  $=\\suml_{u=1}^n\\suml_{v=1}^n\\Cov(I_u,J_v)$\n",
        "  $=\\suml_{u=1}^n\\Cov(I_u,J_u)$\n",
        "  $=\\suml_{u=1}^n(\\E[I_uJ_u]-\\E[I_u]\\E[J_u])$\n",
        "  $=-np_ip_j$.\n",
        "\n",
        "- 7.72: Let $(X,Y)$ be bivariate normal and marginally $\\Normal(0,1)$ with $œÅ$. Show $(X+Y,X-Y)$ is bivariate and find joint PDF.\n",
        "$s(X+Y)+t(X-Y)=(s+t)X+(s-t)Y$ is linear combination of $(X,Y)$.\n",
        "$\\Var(X+Y)=\\Var(X)+\\Var(Y)+2\\Cov(X,Y)$\n",
        "$=2+2œÅ$.\n",
        "$\\Var(X-Y)=\\Var(X)+\\Var(-Y)+2\\Cov(X,-Y)$\n",
        "$=2-2œÅ$.\n",
        "$(X+Y)‚àº\\Normal(0,2+2œÅ)$, and $(X-Y)‚àº\\Normal(0,2-2œÅ)$.\n",
        "$Œ£=\\Cov(X+Y,X-Y)=\\BPM\n",
        "2+2œÅ & 0 \\\\\n",
        "0 & 2-2œÅ\n",
        "\\EPM$\n",
        "$Œ£^{-1}=\\begin{pmatrix}\n",
        "\\frac{1}{2+2œÅ} & 0 \\\\\n",
        "0 & \\frac{1}{2-2œÅ}\n",
        "\\EPM$.\n",
        "$ùê≥=\\BPM\n",
        "s \\\\\n",
        "t\n",
        "\\EPM$.\n",
        "$f_{X+Y,X-Y}(s,t)=\\frac{1}{2œÄ\\sqrt{\\det(Œ£)}}\\e{-\\frac{1}{2}ùê≥^TŒ£^{-1}ùê≥}$\n",
        "$=\\frac{1}{4œÄ\\sqrt{1-œÅ^2}}\\e{-\\frac{1}{4}(\\frac{s^2}{1+œÅ}+\\frac{t^2}{1-œÅ})}$.\n",
        "\n",
        "- 7.73: $f_{X,Y}(x,y)=c\\e{-\\frac{x^2}{2}-\\frac{y^2}{2}},\\ x,y‚àà‚Ñù$.\n",
        "  - Find c.\n",
        "  $1=c‚à´_{-‚àû}^{‚àû}‚à´_{-‚àû}^{‚àû}\\e{-\\frac{x^2}{2}-\\frac{y^2}{2}}\\ dx\\ dy$\n",
        "  $=c‚à´_{-‚àû}^{‚àû}\\e{-\\frac{x^2}{2}}\\ dx‚à´_{-‚àû}^{‚àû}\\e{-\\frac{y^2}{2}}\\ dy$.\n",
        "  Therefore $c=\\frac{1}{2œÄ}$.\n",
        "  - Find marginal $X$ and $Y$. $X,Y‚àº\\Normal(0,1)$ iid.\n",
        "\n",
        "- 7.74: $f_{X,Y}(x,y)=c\\e{-\\frac{x^2}{2}-\\frac{y^2}{2}},\\ xy>0$. The support of $f(x,y)$ is 2 of 4 quadrants on the $x,y$-plane.\n",
        "  - Find c.\n",
        "  $1=c\\iint_{xy>0}\\e{-\\frac{x^2}{2}}\\e{-\\frac{y^2}{2}}\\ dx\\ dy$.\n",
        "  $=c‚à´_0^{‚àû}\\e{-\\frac{x^2}{2}}\\ dx‚à´_0^{‚àû}\\e{-\\frac{y^2}{2}}\\ dy$\n",
        "  $+c‚à´_{-‚àû}^0\\e{-\\frac{x^2}{2}}\\ dx‚à´_{-‚àû}^0\\e{-\\frac{y^2}{2}}\\ dy$.\n",
        "  Therefore $c=\\frac{1}{œÄ}$.\n",
        "  - Find marginal $X$ and $Y$. $X‚àº\\Normal(0,1)$ on $x\\lt 0$, and separately $X‚àº\\Normal(0,1)$ on $x>0$. Therefore $X,Y‚àº\\Normal(0,1)$. $X,Y$ are not independent.\n",
        "  - $(X,Y)$ is not bivariate normal because $f(x,y)=0$ in 2 of 4 quadrants.\n",
        "\n",
        "- 7.75: $X,Y,Z‚àº\\Normal(0,1)$ iid. Find the joint MGF of $(X+2Y,3X+4Z,5Y+6Z)$.\n",
        "$t_1(X+2Y)+t_2(3X+4Z)+t_3(5Y+6Z)$\n",
        "$=(t_1+3t_2)X+(2t_1+5t_3)Y+(4t_2+6t_3)Z$.\n",
        "$M(t_1,t_2,t_3)=\\E[\\e{(t_1+3t_2)X+(2t_1+5t_3)Y+(4t_2+6t_3)Z}]$\n",
        "$=\\e{\\frac{1}{2}\\Var((t_1+3t_2)X+(2t_1+5t_3)Y+(4t_2+6t_3)Z)}$\n",
        "$=\\e{\\frac{1}{2}((t_1+3t_2)^2+(2t_1+5t_3)^2+(4t_2+6t_3)^2)}$.\n",
        "\n",
        "- 7.77: $(X,Y)$ be bivariate normal $X‚àº\\Normal(0,œÉ_1^2)$ and $Y‚àº\\Normal(0,œÉ_2^2)$ with $œÅ$. Find $c$ such that $Y-cX$ and $X$ are independent.\n",
        "Because $(Y-cX,X)$ are bivariate, $\\Corr(Y-cX,X)=0$ implies independence.\n",
        "$\\Cov(Y-cX,X)=\\Cov(X,Y)-c\\Var(X)=0$\n",
        "$‚áíc=\\frac{\\Cov(X,Y)}{\\Var(X)}$\n",
        "$=\\frac{œÅœÉ_1œÉ_2}{œÉ_1^2}$\n",
        "$=\\frac{œÅœÉ_2}{œÉ_1}$.\n",
        "\n",
        "Blitzstein Chapter 9 (Conditional expectation):\n",
        "\n",
        "- e9.2.4: a stick of length 1 is broken at point $X‚àº\\Unif(0,1)$. Given $X=x$, we choose $Y‚àº\\Unif(0,x)$. Find $\\E[Y|X]$, its mean and variance.\n",
        "$\\E[Y|X=x]=‚à´_{y=0}^x\\frac{y}{x}\\ dy$\n",
        "$=\\frac{x}{2}$.\n",
        "$\\E[\\E[Y|X]]=\\E[\\frac{X}{2}]$\n",
        "$=\\frac{1}{4}$.\n",
        "$\\Var(\\E[Y|X])=\\Var(\\frac{X}{2})=\\frac{1}{4}\\frac{1}{12}$.\n",
        "\n",
        "- e9.2.5: $X,Y‚àº\\Expo(Œª)$ iid. Find $\\E[\\max(X,Y)|\\min(X,Y)]$.\n",
        "$\\E[M|L=l]=l+\\E[M-L]=l+\\frac{1}{Œª}$.\n",
        "\n",
        "- e9.3.10 (linear regression): $\\E[Y|X]=a+bX$. $Y=a+bX+œµ$ where $\\E[œµ|X]=0$. Solve $a,b$ in terms of $\\E[X]$, $\\E[Y]$, $\\Cov(X,Y)$, and $\\Var(X)$.\n",
        "$\\E[\\E[Y|X]]=a+b\\E[X]$\n",
        "$‚áía=\\E[Y]-b\\E[X]$.\n",
        "$\\Cov(X,Y)=\\Cov(X,a+bX+œµ)$\n",
        "$=b\\Var(X)$.\n",
        "$b=\\frac{\\Cov(X,Y)}{\\Var(X)}$ and\n",
        "$a=\\E[Y]-\\E[X]\\frac{\\Cov(X,Y)}{\\Var(X)}$.\n",
        "\n",
        "- e9.5.3: let $Z‚àº\\Normal(0,1)$, $Y=Z^2$. Find $\\Var(Y|Z)$ and $\\Var(Z|Y)$.\n",
        "$\\Var(Y|Z=z)=\\Var(h(Z)|Z)=0$. Given $Z^2=a$, $Z=¬±\\sqrt{a}$ with equal probability - non-zero variance.\n",
        "$\\Var(Z|Y)=\\Var(Z|Z^2)=\\E[Z^2|Z^2]-\\E[Z|Z^2]^2=Z^2-0=Y$.\n",
        "\n",
        "- e9.5.4 (BVN Blitzstein e7.5.10): Let $(Z,W)$ be BVN with $Z,W‚àº\\Normal(0,1)$ and $\\Corr(Z,W)=œÅ$. Find $\\E[W|Z]$ and $\\Var(W|Z)$.\n",
        "Let $X,Y‚àº\\Normal(0,1)$ iid. $Z=X$, and $W=aX+bY$. $\\Corr(Z,W)=a=œÅ$. $\\Var(W)=1=œÅ^2+b^2$. Therefore $W=œÅX+\\sqrt{1-œÅ^2}Y$.\n",
        "$\\E[W|Z]=\\E[W|X]=œÅX$.\n",
        "$\\Var(W|Z)=\\Var(W|X)=1-œÅ^2$.\n",
        "\n",
        "- e9.6.1: A store receives $N$ customers in a day. Independently let $X_j‚àº(Œº,œÉ^2)$ be amount spent by $j$th customer. Find mean and variance of $X=\\suml_{i=1}^N\\E[X_i]$.\n",
        "$\\E[X]=\\E[\\E[X|N]]$\n",
        "$=\\E[\\suml_{i=1}^N\\E[X_i]]$\n",
        "$=Œº\\E[N]$.\n",
        "$\\Var(\\E[X|N])$\n",
        "$=\\Var(\\suml_{i=1}^N\\E[X_i])$\n",
        "$=\\Var(NŒº)$\n",
        "$=Œº^2\\Var(N)$.\n",
        "$\\E[\\Var(X|N)]$\n",
        "$=\\E[\\Var(\\suml_{i=1}^NX_i)]$\n",
        "$=\\E[œÉ^2N]$\n",
        "$=œÉ^2\\E[N]$.\n",
        "$\\Var(X)=Œº^2\\Var(N)+œÉ^2\\E[N]$.\n",
        "\n",
        "- e9.6.2 (cluster sampling): A city is randomly chosen, and then a sample of $n$ people is randomly drawn. Let $Q$ be the prevalence and $X$ be the number of a diseased people in a sample. Suppose $Q‚àº\\Unif(0,1)$ also represents the probability each individual has the disease. Find $\\E[X]$ and $\\Var(X)$.\n",
        "$X|Q‚àº\\Binom(n,Q)$.\n",
        "$\\E[X]=\\E[\\E[X|Q]]$\n",
        "$=\\E[nQ]$\n",
        "$=\\frac{n}{2}$.\n",
        "$\\E[\\Var(X|Q)]$\n",
        "$=\\E[nQ(1-Q)]$\n",
        "$=n(\\E[Q]-\\E[Q^2])$\n",
        "$=n(\\frac{1}{2}-\\frac{1}{12}-\\frac{1}{4})$\n",
        "$=\\frac{n}{6}$.\n",
        "$\\Var(\\E[X|Q])$\n",
        "$=\\Var(nQ)$\n",
        "$=\\frac{n^2}{12}$.\n",
        "$\\Var(X)=\\frac{2n+n^2}{12}$.\n",
        "\n",
        "- e9.6.3 (Blitzstein S8.4.5): Counting the number of buses at bus stop for $t$ hours $Y|Œª‚àº\\Pois(Œªt)$ to update $Œª‚àº\\Gamma(r_0,b_0)$. Find $\\E[Y]$ and $\\Var(Y)$.\n",
        "$\\E[Y]=\\E[\\E[Y|Œª]]$\n",
        "$=\\E[Œªt]=\\frac{r_0t}{b_0}$.\n",
        "$\\E[\\Var(Y|Œª)]$\n",
        "$=\\E[Œªt]=\\frac{r_0t}{b_0}$.\n",
        "$\\Var(\\E[Y|Œª])$\n",
        "$=\\Var(Œªt)=\\frac{r_0t^2}{b_0^2}$.\n",
        "$\\Var(Y)=\\frac{r_0t}{b_0}+\\frac{r_0t^2}{b_0^2}$\n",
        "$=\\frac{r_0t(b_0+t)}{b_0^2}$.\n",
        "\n",
        "- 9.2: $X‚àº\\Pois(10)$ and $Y‚àº\\Pois(40)$ are independent. Given $X+Y=30$, what is $\\E[X]$?\n",
        "$(X|X+Y=n)‚àº\\Binom(n,\\frac{Œª_X}{Œª_X+Œª_Y})$.\n",
        "$\\E[X|X+Y=30]=30\\frac{10}{10+40}=6$.\n",
        "\n",
        "- 9.3: $X‚àº\\Binom(21,p)$ and $Y‚àº\\Binom(14,p)$ are independent. Given $X+Y=5$, what is $\\E[X]$?\n",
        "$(X|X+Y=k)‚àº\\HGeom(m+n,n,k)$.\n",
        "$\\E[X|X+Y=5]=5\\frac{21}{21+14}=3$.\n",
        "\n",
        "- 9.4 (truncated Poisson): Let $X‚àº\\Pois(Œª)$ be the number of times a random person had been arrested over ten years. A police database holds the records of arrest over ten years, which holds $X|X‚â•1$. Find $\\E[X|X‚â•1]$ and $\\Var(X|X‚â•1)$.\n",
        "$P(X=k|X‚â•1)=\\frac{P(X=k,X‚â•1)}{P(X‚â•1)}$\n",
        "$=\\frac{e^{-Œª}Œª^k}{k!(1-e^{-Œª})}$.\n",
        "$\\E[X|X‚â•1]=\\suml_{k=1}^‚àûk\\frac{e^{-Œª}Œª^k}{k!(1-e^{-Œª})}$\n",
        "$=\\frac{Œª}{1-e^{-Œª}}\\suml_{k=1}^‚àû\\frac{e^{-Œª}Œª^{k-1}}{(k-1)!}$\n",
        "$=\\frac{Œª}{1-e^{-Œª}}$.\n",
        "$\\E[X^2|X‚â•1]=\\suml_{k=1}^‚àûk^2\\frac{e^{-Œª}Œª^k}{k!(1-e^{-Œª})}$\n",
        "$=\\frac{Œª}{1-e^{-Œª}}[\\suml_{j=0}^‚àûj\\frac{e^{-Œª}Œª^j}{j!}+\\suml_{j=0}^‚àû\\frac{e^{-Œª}Œª^j}{j!}]$\n",
        "$=\\frac{Œª(Œª+1)}{1-e^{-Œª}}$.\n",
        "$\\Var(X|X‚â•1)=\\frac{Œª(Œª+1)}{1-e^{-Œª}}-\\frac{Œª^2}{(1-e^{-Œª})^2}$\n",
        "\n",
        "- 9.6: Let $X‚àº\\Expo(Œª)$. Find $\\E[X|X\\lt 1]$.\n",
        "$\\E[X|X‚â•1]=1+\\E[X]=1+\\frac{1}{Œª}$ by memoryless.\n",
        "$\\E[X]=\\E[X|X\\lt 1]P(X\\lt 1)$\n",
        "$+\\E[X|X‚â•1]P(X‚â•1)‚áí$\n",
        "$\\E[X|X\\lt 1]=\\frac{\\E[X]-\\E[X|X‚â•1]P(X‚â•1)}{P(X\\lt 1)}$\n",
        "$=\\frac{\\frac{1}{Œª}-(1+\\frac{1}{Œª})e^{-Œª}}{1-e^{-Œª}}$\n",
        "$=\\frac{1}{Œª}-\\frac{1}{e^{-Œª}-1}$.\n",
        "\n",
        "- 9.7: A box contains $V‚àº\\Unif(0,1)$ prize. You bid $b$. $b‚â•\\frac{V}{4}$ is accepted, or else rejected. Find expected payoff $\\E[V-b]$.\n",
        "$\\E[V-b]=\\E[V-b|V‚â§4b]P(V‚â§4b)+0$\n",
        "$=(\\E[V]-b)4b$\n",
        "$=2b-4b^2$.\n",
        "$\\frac{d}{db}\\E[V-b]=2-8b$\n",
        "$‚áíb=\\frac{1}{4}$.\n",
        "\n",
        "- 9.11 (e9.1.9 first HT or HH): A coin comes up Heads with probability $p$ is flipped repeatedly.\n",
        "  - What is the expected number of flips until HT appears?\n",
        "  Let $N$ be flips until HT, $N_1‚àº\\Geom(p)$ be flips until the first H, then $N_2‚àº\\Geom(q)$ be flips until the first T after first H.\n",
        "  $\\E[N]=\\E[N_1]+\\E[N_2]$\n",
        "  $=\\frac{1}{p}+\\frac{1}{q}$.\n",
        "  - What is the expected number of flips until HH appears? Let $N$ be flips until HH. Use time-forward recursion from the first toss.\n",
        "  $\\E[N]=\\E[N|H_1]p+\\E[N|T_1]q$.\n",
        "  $\\E[N|H_1]=\\E[N|H_1H_2]p+\\E[N|H_1T_2]q$\n",
        "  $=2p+(2+\\E[N])q$.\n",
        "  $\\E[N|T_1]=1+\\E[N]$.\n",
        "  $\\E[N]=(2p+(2+\\E[N])q)p+(1+\\E[N])q$\n",
        "  $=2p^2+2pq+pq\\E[N]+q+q\\E[N]$\n",
        "  $=\\frac{2p^2+2pq+q}{1-pq-q}$\n",
        "  $=\\frac{p+1}{p^2}$\n",
        "  $=\\frac{1}{p}+\\frac{1}{p^2}$.\n",
        "  - If $p‚àº\\t{beta}(a,b),\\ a>2,b>2$ is unknown.\n",
        "  $\\E[N]=\\E[\\E[N|p]]$\n",
        "  $=\\frac{Œì(a+b)}{Œì(a)Œì(b)}‚à´_0^1[\\frac{1}{p}+\\frac{1}{p^2}]p^{a-1}(1-p)^{b-1}\\ dp$\n",
        "\n",
        "- 9.12: A coin comes up with Heads with probability $p$ is flipped repeatedly.\n",
        "  - Find the expected length of the first run $N_1$.\n",
        "  $\\E[N_1]=\\E[N_1|H_1]p+\\E[N_1|T_1]q$\n",
        "  $=\\frac{p}{q}+\\frac{q}{p}$.\n",
        "  - Find the expected length of the second run $N_2$.\n",
        "  $\\E[N_2]=\\E[N_2|H_1]p+\\E[N_2|T_1]q$\n",
        "  $=\\frac{p}{p}+\\frac{q}{q}$\n",
        "  $=2$.\n",
        "\n",
        "- 9.14: A fair die is rolled repeatedly. Find the expected number of rolls $N$ needed to get a 1 immediately followed by 2.\n",
        "$\\E[N]=\\E[N|R_1]\\frac{1}{6}+\\E[N|R_{\\bar{1}}]\\frac{5}{6}$.\n",
        "$\\E[N|R_1]=\\E[N|R_{1,2}]\\frac{1}{6}+\\E[N|R_{1,1}]\\frac{1}{6}+\\E[N|R_{1,\\bar{12}}]\\frac{4}{6}$\n",
        "$=\\frac{2}{6}+\\frac{1+\\E[N|R_1]}{6}+\\frac{4(2+\\E[N])}{6}$\n",
        "$=\\frac{11}{6}+\\frac{1}{6}\\E[N|R_1]+\\frac{4}{6}\\E[N]$\n",
        "$=\\frac{11+4\\E[N]}{5}$\n",
        "$\\E[N]=\\frac{11+4\\E[N]+25+25\\E[N]}{30}$\n",
        "$=36$.\n",
        "\n",
        "- 9.15: Let $X_1,X_2$ be iid and let $\\bar{X}=\\frac{1}{2}(X_1+X_2)$. Find $\\E[w_1X_1+w_2X_2|\\bar{X}]$ where $w_1+w_2=1$.\n",
        "$\\E[w_1X_1+w_2X_2|\\bar{X}]$\n",
        "$=w_1\\E[X_1|\\bar{X}]+w_2\\E[X_2|\\bar{X}]$\n",
        "$=\\bar{X}$.\n",
        "\n",
        "- 9.17: $n$ pairs of roommates. Each of the $2n$ students independently decides whether to take a certain course with probability $p$. Let $N$ be number of students who take the course, and $X$ be number of roommate pairs where both take the course. Find $\\E[X]$ and $\\E[X|N]$.\n",
        "  - $X‚àº\\Binom(n,p^2)‚áí\\E[X]=np^2$.\n",
        "  - Let $I_{ij}$ be indicator that student $i$ and $j$ are roommates, then the number of roommate pairs amongst $N$ students is\n",
        "  $X|N=\\suml_{i\\lt j}^NI_{ij}$.\n",
        "  $\\E[I_{ij}]=\\frac{1}{2n-1}$.\n",
        "  $\\E[X|N]=\\binom{N}{2}\\frac{1}{2n-1}$.\n",
        "  - (Hypergeometric) Let $I_i$ be indicator that roommates $A,B$ in pair $i$ both take the course, then\n",
        "  $\\E[I_i]=P(A)P(B|A)$\n",
        "  $=\\frac{N}{2n}\\frac{N-1}{2n-1}$.\n",
        "  $\\E[X|N]=\\suml_{i=0}^n\\E[I_i]$\n",
        "  $=n\\frac{N}{2n}\\frac{N-1}{2n-1}$\n",
        "  $=\\binom{N}{2}\\frac{1}{2n-1}$.\n",
        "\n",
        "- 9.20: Let $\\v{X}‚àº\\Mult_5(n,\\v{p})$.\n",
        "  - Find $\\E[X_1|X_2]$ and $\\Var(X_1|X_2)$.\n",
        "  $X_1|X_2‚àº\\Binom(n-X_2,\\frac{p_1}{1-p_2})$.\n",
        "  - Find $\\E[X_1|X_2+X_3]$.\n",
        "  $X_1|X_2+X_3‚àº\\Binom(n-X_2-X_3,\\frac{p_1}{1-p_2-p_3})$.\n",
        "\n",
        "- 9.23: Let $W=Y-\\E[Y|X]$. Compute $\\E[W]$ and $\\E[W|X]$.\n",
        "$\\E[W]=\\E[Y-\\E[Y|X]]$\n",
        "$=\\E[Y]-\\E[Y]=0$.\n",
        "$\\E[W|X]=\\E[Y-\\E[Y|X]|X]$\n",
        "$=\\E[Y|X]-\\E[\\E[Y|X]|X]$\n",
        "$=\\E[Y|X]-\\E[Y|X]=0$.\n",
        "\n",
        "- 9.25 (Kelly criterion): $n$ bets, each independently has winning probability $p$. Starting with $x_0$, each bet $j$ risks a fraction of principal $r‚àà(0,1)$ and results in $X_j$. Find $X_n$.\n",
        "$\\E[X_{j+1}|X_j]=X_j[(1+r)p+(1-r)q]$\n",
        "$=X_j(2pr+1-r)$.\n",
        "$\\E[X_{j+1}]=\\E[\\E[X_{j+1}|X_j]]$\n",
        "$=\\E[X_j](2pr+1-r)$.\n",
        "Therefore $\\E[X_n]=x_0(2pr+1-r)^n$.\n",
        "\n",
        "- 9.26: $N‚àº\\Pois(Œª_1)$ movies are released next year. Independently, $X_i‚àº\\Pois(Œª_2)$ tickets are sold for each movie. What is the expected number and variance of tickets sold next year $X$?\n",
        "$\\E[X|N]=N\\E[X_i]=NŒª_2$.\n",
        "$\\E[X]=\\E[\\E[X|N]]$\n",
        "$=Œª_2\\E[N]=Œª_1Œª_2$.\n",
        "$\\Var(\\E[X|N])=Œª_2^2\\Var(N)$\n",
        "$=Œª_2^2Œª_1$.\n",
        "$\\E[\\Var(X|N)]=\\E[N\\Var(X_i)]$\n",
        "$=\\E[NŒª_2]=Œª_1Œª_2$.\n",
        "$\\Var(X)=Œª_1Œª_2(1+Œª_2)$.\n",
        "\n",
        "- 9.27: $N‚àº\\Pois(Œª)$ people show up to a party at $X_i‚àº\\Unif(0,4)$ hours after 8pm.\n",
        "  - Given $N‚â•1$ find the expected first arrival time.\n",
        "  Truncated Poisson\n",
        "  $P(N=k|N‚â•1)=\\frac{P(N=k,N‚â•1)}{P(N‚â•1)}$\n",
        "  $=\\frac{e^{-Œª}Œª^k/k!}{1-e^{-Œª}}$\n",
        "  $(X_{(j)}|N)‚àºŒ≤(j,N-j+1)$.\n",
        "  $\\E[X_{(1)}|N]=\\frac{1}{N+1}$\n",
        "  $\\E[X_{(1)}]=\\E[\\frac{1}{N+1}]$\n",
        "  $=\\suml_{k=1}^‚àû\\frac{1}{k+1}\\frac{e^{-Œª}Œª^k/k!}{1-e^{-Œª}}$\n",
        "  $=\\frac{e^{-Œª}}{Œª(1-e^{-Œª})}\\suml_{k=1}^‚àû\\frac{Œª^{k+1}}{(k+1)!}$\n",
        "  $=\\frac{e^{-Œª}}{Œª(1-e^{-Œª})}\\suml_{j=2}^‚àû\\frac{Œª^j}{j!}$\n",
        "  $=\\frac{e^{-Œª}(e^{Œª}-Œª-1)}{Œª(1-e^{-Œª})}$\n",
        "  $=\\frac{1}{Œª}-\\frac{1}{e^{Œª}-1}$.\n",
        "  Expected arrival time is $240\\E[X_{(1)}]$ minutes after 8pm.\n",
        "  - Given $N‚â•1$ find the expected last arrival time.\n",
        "  $\\E[X_{(N)}|N]=\\frac{N}{N+1}$\n",
        "  $=1-\\E[X_{(1)}|N]$.\n",
        "  $\\E[X_{(N)}]=1-\\E[X_{(1)}]$\n",
        "  $=1-\\frac{1}{Œª}+\\frac{1}{e^{Œª}-1}$.\n",
        "\n",
        "- 9.28 (Baye's procedure): We want to estimate parameter $Œ∏$ based on observed values of $X$. Let $\\hat{Œ∏}$ be the estimator as a function of $X$, which is unbiased if $\\E[\\hat{Œ∏}|Œ∏]=Œ∏$ and Baye's procedure if $\\E[Œ∏|X]=\\hat{Œ∏}$.\n",
        "  - Let $\\hat{Œ∏}$ be unbiased. Find $\\E(\\hat{Œ∏}-Œ∏)^2$.\n",
        "  $\\E[(\\hat{Œ∏}-Œ∏)^2|Œ∏]$\n",
        "  $=\\E[\\hat{Œ∏}^2-2\\hat{Œ∏}Œ∏+Œ∏^2|Œ∏]$\n",
        "  $=\\E[\\hat{Œ∏}^2|Œ∏]-2Œ∏\\E[\\hat{Œ∏}|Œ∏]+Œ∏^2$\n",
        "  $=\\E[\\hat{Œ∏}^2|Œ∏]-Œ∏^2$.\n",
        "  $\\E(\\hat{Œ∏}-Œ∏)^2=\\E[\\hat{Œ∏}^2]-\\E[Œ∏^2]$.\n",
        "  - Let $\\hat{Œ∏}$ be Baye's procedure. Find $\\E(\\hat{Œ∏}-Œ∏)^2$.\n",
        "  $\\E[(\\hat{Œ∏}-Œ∏)^2|X]$\n",
        "  $=\\E[\\hat{Œ∏}^2-2\\hat{Œ∏}Œ∏+Œ∏^2|X]$\n",
        "  $=\\E[\\hat{Œ∏}^2|X]-2\\hat{Œ∏}\\E[Œ∏|X]+\\E[Œ∏^2|X]$.\n",
        "  $\\E(\\hat{Œ∏}-Œ∏)^2$\n",
        "  $=\\E[Œ∏^2]-\\E[\\hat{Œ∏}^2]$.\n",
        "  - $\\E(\\hat{Œ∏}_{\\t{unbiased}}-Œ∏)^2=-\\E(\\hat{Œ∏}_{\\t{Baye's procedure}}-Œ∏)^2$. Baye's procedure cannot be unbiased.\n",
        "\n",
        "- 9.29: Show that if $\\E[Y|X]=c$ is a constant, then $\\Corr(X,Y)=0$.\n",
        "$\\E[XY|X]=cX$, $\\E[XY]=c\\E[X]$, $\\E[Y]=c$.\n",
        "$\\Cov(X,Y)=\\E[XY]-\\E[X]\\E[Y]$\n",
        "$=c\\E[X]-c\\E[X]=0$.\n",
        "\n",
        "- 9.31 (Geometric-gamma): Emails arrive one at a time. Let $T_j$ be time of arrival for $j$th email, and $T_{j+1}-T{j}‚àº\\Expo(Œª)$ iid. Each email is non-spam with probability $p$. Let $X$ be the arrival time for the first non-spam email.\n",
        "  - Find mean and variance of $X$.\n",
        "  Let $N$ be the first non-spam email.\n",
        "  $N‚àº\\Geom(p)$.\n",
        "  $X|N‚àº\\Gamma(N,Œª)$.\n",
        "  $\\E[X|N]=\\frac{N}{Œª}$.\n",
        "  $\\E[X]=\\frac{\\E[N]}{Œª}$\n",
        "  $=\\frac{1}{pŒª}$.\n",
        "  $\\Var(\\E[X|N])=\\frac{\\Var(N)}{Œª^2}$\n",
        "  $=\\frac{1-p}{p^2Œª^2}$.\n",
        "  $\\E[\\Var(X|N)]=\\E[\\frac{N}{Œª^2}]$\n",
        "  $=\\frac{1}{pŒª^2}$.\n",
        "  $\\Var(X)=\\frac{1-p}{p^2Œª^2}+\\frac{1}{pŒª^2}$\n",
        "  $=\\frac{1}{p^2Œª^2}$.\n",
        "  - Find MGF of $X$.\n",
        "  $\\E[e^{tX}|N]=‚à´_0^‚àûe^{tx}\\frac{e^{-Œªx}(Œªx)^N}{xŒì(N)}\\ dx$\n",
        "  $=\\frac{Œª^N}{(Œª-t)^N}‚à´_0^‚àû\\frac{e^{-(Œª-t)x}((Œª-t)x)^N}{xŒì(N)}\\ dx$\n",
        "  $=\\frac{Œª^N}{(Œª-t)^N}$.\n",
        "  $M_X(t)=\\E[\\E[e^{tX}|N]]$\n",
        "  $=\\E[\\frac{Œª^N}{(Œª-t)^N}]$\n",
        "  $=\\suml_{k=1}^‚àû(\\frac{Œª}{Œª-t})^kq^{k-1}p$\n",
        "  $=\\frac{pŒª}{Œª-t}\\suml_{j=0}^‚àû(\\frac{qŒª}{Œª-t})^j$\n",
        "  $=\\frac{pŒª/(Œª-t)}{1-qŒª/(Œª-t)}$\n",
        "  $=\\frac{pŒª}{Œª-t-(1-p)Œª}$\n",
        "  $=\\frac{pŒª}{pŒª-t}$.\n",
        "  $X‚àº\\Expo(pŒª)$.\n",
        "\n",
        "- 9.32 (Adam and Eve): Customers arrive by Poisson process of Œª per hour. Each independenly makes a purchase $I_i$ with probability $p$ for spending $(Œº,œÉ^2)$.\n",
        "  - Find mean and variance of how much a customer spends. Let $X_i$ be customer spending.\n",
        "  $X_i|I_i‚àº(ŒºI_i,œÉ^2I_i)$.\n",
        "  $\\E[X_i|I_i]=ŒºI_i$.\n",
        "  $\\E[X_i]=pŒº$.\n",
        "  $\\Var(\\E[X_i|I_i])=pqŒº^2$.\n",
        "  $\\E[\\Var(X_i|I_i)]=pœÉ^2$.\n",
        "  $\\Var(X_i)=pqŒº^2+pœÉ^2$.\n",
        "  - Find mean and variance of store revenue in 8 hours.\n",
        "  Let $N$ be number of customers, then $N‚àº\\Pois(8Œª)$.\n",
        "  $X|N=\\suml_{i=1}^NX_i$.\n",
        "  $\\E[X|N]=N\\E[X_i]$.\n",
        "  $\\E[X]=\\E[N]\\E[X_i]=8ŒªpŒº$.\n",
        "  $\\Var(\\E[X|N])=\\E[X_i]^2\\Var(N)$\n",
        "  $=8Œªp^2Œº^2$.\n",
        "  $\\E[\\Var(X|N)]=\\E[N\\Var(X_i)]$\n",
        "  $=8Œª(pqŒº^2+pœÉ^2)$.\n",
        "  $\\Var(X)=8Œª(pqŒº^2+pœÉ^2+p^2Œº^2)$\n",
        "  $=8Œªp(Œº^2+œÉ^2)$.\n",
        "  - Find mean and variance of store revenue in 8 hours using Poisson-binomial.\n",
        "  Let $\\hat{X}_i‚àº(Œº,œÉ^2)$ be a purchase, $N‚àº\\Pois(8Œª)$ be number of customers, $N_X‚àº\\Pois(8Œªp)$ be number of purchases.\n",
        "  $X|N_X=\\suml_{i=1}^{N_X}\\hat{X}_i$.\n",
        "  $\\E[X|N_X]=ŒºN_X$.\n",
        "  $\\E[X]=8ŒªpŒº$.\n",
        "  $\\Var(\\E[X|N_X])=Œº^2\\Var(N_X)=8ŒªpŒº^2$.\n",
        "  $\\E[\\Var(X|N_X)]=\\E[N_XœÉ^2]=8ŒªpœÉ^2$.\n",
        "  $\\Var(X)=8Œªp(Œº^2+œÉ^2)$.\n",
        "\n",
        "- 9.33: A computer runs for $\\Expo(Œª)$ before breaking down. With probability $p$ it is fixed good as new. The cycle continues until the computer can no longer be fixed and is replaced. Find the expected lifespan of this computer.\n",
        "Let $N‚àº\\Geom(1-p)$ be the number of working cycles, $T$ be the total lifespan.\n",
        "$T|N‚àº\\Gamma(N,Œª)$.\n",
        "$\\E[T|N]=\\frac{N}{Œª}$.\n",
        "$\\E[T]=\\frac{\\E[N]}{Œª}=\\frac{1}{(1-p)Œª}$.\n",
        "\n",
        "- 9.35 (geometric-binomial): Judith plays $N‚àº\\QGeom(s)$ tournaments in her career, winning each with independent probability $p$. Let $T$ be the number of tournaments she wins.\n",
        "  - Find mean and variance of $T$.\n",
        "  $T|N‚àº\\Binom(N,p)$.\n",
        "  $\\E[T|N]=Np$.\n",
        "  $\\E[T]=\\frac{p(1-s)}{s}$.\n",
        "  $\\Var(\\E[T|N])=\\frac{p^2(1-s)}{s^2}$.\n",
        "  $\\E[\\Var(T|N)]=\\frac{p(1-p)}{s}$.\n",
        "  $\\Var(T)=\\frac{p^2(1-s)+ps(1-p)}{s^2}$.\n",
        "  - Find MGF of $T$.\n",
        "  $\\E[e^{tT}|N]=(pe^t+q)^N$.\n",
        "  $M_T(t)=\\suml_{k=0}^‚àû(pe^t+q)^k(1-s)^ks$\n",
        "  $=s\\suml_{k=0}^‚àû((1-s)(pe^t+q))^k$\n",
        "  $=\\frac{s}{1-(1-s)(pe^t+1-p)}$\n"
      ],
      "metadata": {
        "id": "UxDbjKXsRRMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ross Chapter 7 (Properties of expectation):\n",
        "\n",
        "- e7.3a: $X‚àº\\Binom(n,p)$. Let $I_i$ be indicator such that $\\E[I_i]=P(A_i)=p$, then $\\E[I_iI_j]=p^2$.\n",
        "Then $E\\binom{X}{2}=\\suml_{i\\lt j}\\E[I_iI_j]=\\binom{n}{2}p^2$ and\n",
        "$\\E[X(X-1)]=n(n-1)p^2$ or $\\E[X^2]-\\E[X]=n(n-1)p^2$.\n",
        "In general, $E\\binom{X}{k}=\\binom{n}{k}p^k$ and\n",
        "$\\E[X(X-1)...(X-k+1)]=n(n-1)...(n-k+1)p^k$.\n",
        "\n",
        "- e7.3b: $X‚àº\\HGeom(N,m,n)$. Let $I_i$ be indicator such that $\\E[I_i]=\\frac{m}{N}$ and $\\E[I_iI_j]=\\frac{m(m-1)}{N(N-1)}$.\n",
        "Then $E\\binom{X}{2}=\\suml_{i\\lt j}\\E[I_iI_j]$\n",
        "$=\\binom{n}{2}\\frac{m(m-1)}{N(N-1)}$ and\n",
        "$\\E[X^2]-\\E[X]=n(n-1)\\frac{m(m-1)}{N(N-1)}$.\n",
        "\n",
        "- e7.3c: $N$ men throw their hat in a ring and each then randomly picks up a hat. Let $A_i$ be person $i$ picks up his own hat, and $I_i$ be indicator such that $\\E[I_i]=\\frac{1}{N}$ and $\\E[I_iI_j]=\\frac{1}{N(N-1)}$.\n",
        "Then $E\\binom{X}{2}=\\suml_{i\\lt j}\\E[I_iI_j]$\n",
        "$=\\binom{N}{2}\\frac{1}{N(N-1)}$ and\n",
        "$\\E[X(X-1)]=1$. In fact $\\E[X(X-1)...(X-k+1)]=1$.\n",
        "\n",
        "- e7.3d: $k$ types of coupons, each obtained with probability $p_1+...+p_k=1$. What are the expected value and variance of types collected after $n$ coupons?\n",
        "Let $I_i$ be indicator for obtaining type $i$ such that $\\E[I_i]=1-(1-p_i)^n$ and $\\E[I_iI_j]=(1-(1-p_i)^n)(1-(1-p_j)^n)$. Then $\\E[X]=\\suml_{i=1}^k(1-q_i^n)$.\n",
        "$E\\binom{X}{2}=\\suml_{i\\lt j}\\E[I_iI_j]$ and\n",
        "$\\E[X^2]-\\E[X]=2\\suml_{i\\lt j}(1-q_i^n)(1-q_j^n)$\n",
        "\n",
        "- e7.4c: A population of $N$ people, from which $n$ random samples are taken. Let $v_1,...,v_N,\\ v_i‚àà[0,1]$ be the opinion values with regard to certain subject where $\\bar{v}=\\suml_{i=1}^N\\frac{v_i}{N}$ is the proportion of the population in favor. Let $S$ be the sum of the $n$ samples where $\\frac{S}{n}$ is an estimate of $\\bar{v}$, what are its mean and variance?\n",
        "Let $I_1,...,I_n$ be indicator that person $i$ is among the $n$ samples.\n",
        "  - $\\E\\left[\\frac{S}{n}\\right]$\n",
        "  $=\\E\\left[\\suml_{i=1}^N\\frac{v_iI_i}{n}\\right]$\n",
        "  $=\\suml_{i=1}^N\\frac{v_i}{n}\\E[I_i]$\n",
        "  $=\\frac{N\\bar{v}}{n}\\frac{n}{N}$\n",
        "  $=\\bar{v}$.\n",
        "  - $\\Var\\left(\\frac{S}{n}\\right)$\n",
        "  $=\\Var\\left(\\suml_{i=1}^N\\frac{v_iI_i}{n}\\right)$\n",
        "  $=\\suml_{i=1}^N\\Var\\left(\\frac{v_iI_i}{n}\\right)+2\\suml_{i\\lt j}\\Cov\\left(\\frac{v_iI_i}{n},\\frac{v_jI_j}{n}\\right)$\n",
        "  $=\\suml_{i=1}^N\\frac{v_i^2}{n^2}\\Var(I_i)+2\\suml_{i\\lt j}\\frac{v_iv_j}{n^2}\\Cov(I_i,I_j)$\n",
        "  $=...=\\frac{N-n}{n(N-1)}\\bar{v}(1-\\bar{v})$\n",
        "  - $v_i$ are assumed constants that do not change between measurements. They are like numbers labeling cards in a deck.\n",
        "\n",
        "- e7.4e: $X_1,...,X_n$ are iid with $œÉ^2$. Find $\\Cov(X_i-\\bar{X},\\bar{X})$.\n",
        "$\\Cov(X_i-\\bar{X},\\bar{X})=\\Cov(X_i,\\bar{X})-\\Cov(\\bar{X},\\bar{X})$\n",
        "$=\\Cov\\left(X_i,\\frac{1}{n}\\suml_{j=1}^nX_j\\right)-\\Var(\\bar{X})$\n",
        "$=\\frac{1}{n}\\suml_{j=1}^n\\Cov(X_i,X_j)-\\Var(\\bar{X})$\n",
        "$=\\frac{1}{n}\\Var(X_i)+0-\\Var(\\bar{X})$\n",
        "$=\\frac{œÉ^2}{n}-\\frac{œÉ^2}{n}=0$.\n",
        "\n",
        "- e7.4f: $m$ independent trials with $r$ possible outcomes at $p_1+...+p_r=1$. What is the covariance of the outcome $N_1,...,N_r$?\n",
        "$N_i=\\suml_{u=1}^mI_{i}(u)$ and\n",
        "$N_j=\\suml_{v=1}^mI_{j}(v)$.\n",
        "$\\Cov(N_i,N_j)$\n",
        "$=\\Cov\\left(\\suml_{u=1}^mI_{i}(u), \\suml_{v=1}^mI_{j}(v)\\right)$\n",
        "$=\\suml_{u=1}^m\\suml_{v=1}^m\\Cov(I_{i}(u),I_{j}(v))$\n",
        "$=\\suml_{u=1}^m\\Cov(I_i(u),I_j(u))+2\\suml_{u\\lt v}\\Cov(I_i(u),I_j(v))$\n",
        "$=\\suml_{u=1}^m(\\E[I_i(u)I_j(u)]-\\E[I_i(u)]\\E[I_j(v)])+0$\n",
        "$=\\suml_{u=1}^m(0-p_ip_j)$\n",
        "$=-mp_ip_j$.\n",
        "\n",
        "- e7.5a: $X,Y‚àº\\Binom(n,p)$ iid, calculate $\\E[X|X+Y=m]$.\n",
        "$P(X|X+Y=m)=\\frac{P(X=x)P(Y=m-x)}{P(X+Y=m)}$\n",
        "$=\\frac{\\binom{n}{x}p^xq^{n-x}\\binom{n}{m-x}p^{m-x}q^{n-m+x}}{\\binom{2n}{m}p^mq^{2n-m}}$\n",
        "$=\\frac{\\binom{n}{x}\\binom{n}{m-x}}{\\binom{2n}{m}}$\n",
        "$‚àº\\HGeom(2n,n,m)$.\n",
        "$\\E[X|X+Y=m]=m\\frac{n}{2n}=\\frac{m}{2}$.\n",
        "\n",
        "- e7.5b: $X,Y$ are given by $f(x,y)=\\frac{e^{-x/y}e^{-y}}{y},\\ x,y>0$. Compute $\\E[X|Y=y]$.\n",
        "$f_Y(y)=‚à´_0^‚àû\\frac{e^{-x/y}e^{-y}}{y}\\ dx$\n",
        "$=\\frac{e^{-y}}{y}[-ye^{-x/y}]_0^‚àû=e^{-y}$.\n",
        "$f_{X|Y}(x,y)=\\frac{1}{y}e^{-x/y}$.\n",
        "$\\E[X|Y=y]=‚à´_0^‚àû\\frac{x}{y}e^{-x/y}\\ dx$\n",
        "$=[-xe^{-x/y}-ye^{-x/y}]_{x=0}^‚àû$\n",
        "$=y$.\n",
        "\n",
        "- e7.5d: the number of people entering a store is a random variable with mean 5, and the amounts of money spent by visitors are independent random variables having mean 10 and are also independent of the number of visitors. What's the expected amount of money spent in the store?\n",
        "Let $N$ be the number of customers, and $X_i$ be the amount spent.\n",
        "$\\E[X]=\\E\\left[\\suml_{i=1}^NX_i\\right]$ is what we're looking for, which has 2 random variables in it. For constant $n$, $\\E\\left[\\suml_{i=1}^nX_i|N=n\\right]=10n$. Use LOTE\n",
        "$\\E[X]=\\E\\left[\\suml_{i=1}^NX_i\\right]=\\E\\left[\\E\\left[\\suml_{i=1}^NX_i|N\\right]\\right]$\n",
        "$=\\E[N\\E[X_i]]=\\E[N]\\E[X_i]$ due to independence.\n",
        "\n",
        "- e7.5g: $n$ independent trials with $N_1,...,N_k$ trials producing outcomes $1,...,k$ with respective probabilities $p_1+...+p_k=1$. Find $\\E[N_j|N_i>0]$ and $\\E[N_j|N_i>1]$.\n",
        "  - $\\E[N_j]=\\E[N_j|N_i>0]P(N_i>0)+\\E[N_j|N_i=0]P(N_i=0)$ LOTE.\n",
        "  $\\E[N_j]=np_j$.\n",
        "  $\\E[N_j|N_i=0]=n\\frac{p_j}{1-p_i}$.\n",
        "  $P(N_i=0)=(1-p_i)^n$.\n",
        "  $\\E[N_j|N_i>0]=\\frac{np_j-\\frac{np_j}{1-p_i}(1-p_i)^n}{1-(1-p_i)^n}$\n",
        "  $=\\frac{np_j(1-(1-p_i)^{n-1})}{1-(1-p_i)^n}$.\n",
        "  - $\\E[N_j]=\\E[N_j|N_i>1]P(N_i>1)+\\E[N_j|N_i‚â§1]P(N_i‚â§1)$.\n",
        "  $\\E[N_j|N_i‚â§1]=\\E[N_j|N_i=0]+\\E[N_j|N_i=1]$\n",
        "  $=n\\frac{p_j}{1-p_i}+(n-1)\\frac{p_j}{1-p_i}$.\n",
        "  $P(N_i‚â§1)=(1-p_i)^n+p_i(1-p_i)^{n-1}$.\n",
        "\n",
        "- e7.5h: $N‚àº\\Geom(p)$. Find variance of $N$.\n",
        "Let $I$ be indicator of success on first trial. We have a recursive relationship\n",
        "$\\E[N^2|I=1]=1$, $\\E[N^2|I=0]=\\E[(1+N)^2]$, and\n",
        "$\\E[N^2]=\\E[\\E[N^2|I]]$\n",
        "$=\\E[N^2|I=1]P(I=1)+\\E[N^2|I=0]P(I=0)$\n",
        "$=p+(1-p)\\E[(N+1)^2]$\n",
        "$=1+(1-p)\\E[N^2]+2(1-p)\\E[N]$.\n",
        "Then $\\E[N^2]=\\frac{1+2(1-p)\\E[N]}{p}$\n",
        "$=\\frac{2-p}{p^2}$.\n",
        "$\\Var(N)=\\frac{1-p}{p^2}$.\n",
        "\n",
        "- e7.5i (gambler's ruin): $r$ players begin with $n_1+...+n_r=n$ units. At each independent stage 2 players are chosen with the winner receiving 1 unit from the loser with probability $p=\\frac{1}{2}$. A player with 0 unit is eliminated. Find the average number of stages until a single player wins all $n$ units.\n",
        "  - In the $r=2$ case where one player starts with $j$ units and the other starts with $n-j$ units. Let $X_j$ be the number of stages played starting with $j$ units.\n",
        "  $\\E[X_j]=\\E[X_j|win]P(win)+\\E[X_j|lose]P(lose)$.\n",
        "  $EX_j=1+EX_{j+1}p+EX_{j-1}q$ or\n",
        "  $EX_{j+1}=2EX_j-EX_{j-1}-2$.\n",
        "  Recursion with 2 boundary conditions $EX_0=0$ and $EX_n=0$, we have $EX_2=2EX_1-2$, $EX_3=3EX_1-6$, $EX_4=4EX_1-12$, we get the pattern $EX_j=j(EX_1-j+1)$.\n",
        "  Using the other boundary condition $EX_n=0$ we get $EX_1=n-1$ and $\\E[X_j]=j(n-j)$, which is symmetrical for $X_j$ and $X_{n-j}$.\n",
        "  - In the general $r$ case, player $i$ facing $r-1$ opponents is analogous to facing 1 opponent with $n-n_i$ units: $\\E[X_{n_i}]=n_i(n-n_i)$. Let $Y_i$ be the number of stages involving player $i$ so that the total number of stages is $Y=\\suml_{i=1}^r\\frac{Y_i}{2}$, then\n",
        "  $\\E[Y]=\\suml_{i=1}^r\\frac{n_i(n-n_i)}{2}$\n",
        "  $=\\frac{1}{2}\\left(n^2-\\suml_{i=1}^rn_i^2\\right)$.\n",
        "\n",
        "- e7.5j: let $U_1,U_2,...$ be sequence of iid uniform(0,1) and $N(x)=\\min\\left(n:\\suml_{i=1}^nU_i>x\\right),\\ x‚àà[0,1]$. Find $\\E[N(1)]$.\n",
        "We establish a recursion based only on $U_1$.\n",
        "$\\E[N(x)]=‚à´_0^1\\E[N(x)|U_1=y]\\ dy$\n",
        "where $\\E[N(x)|U_1=y]=\\BC 1&y>x\\\\1+\\E[N(x-y)]&y‚â§x\\EC$.\n",
        "$\\E[N(x)]=1+‚à´_0^x\\E[N(x-y)]\\ dy$\n",
        "$=1+‚à´_0^x\\E[N(u)]\\ du$.\n",
        "Differentiating both sides using Leibniz rule\n",
        "$\\frac{d}{dx}\\E[N(x)]=\\frac{d}{dx}‚à´_0^x\\E[N(u)]\\ du$\n",
        "$=\\E[N(x)]$.\n",
        "Therefore $\\E[N(x)]=e^x$ and $\\E[N(1)]=e$.\n",
        "\n",
        "- e7.5l: $U‚àº\\Unif(0,1)$, and $(X|U=p)‚àº\\Binom(n,p)$. Find the PMF of $X$.\n",
        "$P(X=k)=\\binom{n}{k}‚à´_0^1u^k(1-u)^{n-k}\\ du$\n",
        "$=\\binom{n}{k}\\frac{k!(n-k)!}{(n+1)!}$\n",
        "$=\\frac{1}{n+1}$.\n",
        "  - If a coin whose probability of Heads is uniform(0,1) is flipped n times, then the number of heads occurring is equally likely to be any of 0,1,...,n.\n",
        "\n",
        "- e7.5o: At any time $t$ the number of people who arrive at a train line is $N(t)‚àº\\Pois(Œªt)$. If initial train arrives at $Y‚àº\\Unif(0,T)$, what is the mean and variance of number of passengers who enter the train?\n",
        "$\\E[N(Y)|Y=t]=\\E[N(t)]=Œªt$\n",
        "$‚áí\\E[N(Y)|Y]=ŒªY$.\n",
        "$\\E[N(Y)]=\\E[ŒªY]=Œª\\frac{T}{2}$.\n",
        "$\\Var(N(Y)|Y=t)=\\Var(N(t))=Œªt$.\n",
        "$‚áí\\Var(N(Y)|Y)=ŒªY$.\n",
        "$\\Var(N(Y))=\\E[ŒªY]+\\Var(ŒªY)$\n",
        "$=Œª\\frac{T}{2}+Œª^2\\frac{T^2}{12}$.\n",
        "\n",
        "- e7.5p: Let $X_1,X_2,...$ be iid, and $N$ be independent positive integer. Compute $\\Var\\left(\\suml_{i=1}^NX_i\\right)$.\n",
        "$\\E\\left[\\suml_{i=1}^NX_i|N\\right]=N\\E[X_i]$.\n",
        "$\\Var\\left(\\suml_{i=1}^NX_i|N\\right)=N\\Var(X_i)$.\n",
        "$\\Var\\left(\\suml_{i=1}^NX_i\\right)=\\E[X_i]^2\\Var(N)+\\E[N]\\Var(X_i)$\n",
        "\n",
        "- e7.7i: compute the MGF of Chi-squared with n degrees of freedom $Z_1^2+...+Z_n^2$.\n",
        "$\\E[e^{tZ^2}]=\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^‚àûe^{tx^2}e^{-x^2/2}\\ dx$\n",
        "$=\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^‚àûe^{-(1-2t)x^2/2}\\ dx$\n",
        "$=\\frac{1}{\\sqrt{1-2t}}$.\n",
        "Therefore $M(t)=(1-2t)^{-n/2}$.\n",
        "\n",
        "- e7.7j (e7.5d, e7.5p): Let $X_1,X_2,...$ be iid, and $N$ be independent positive integer. What is the MGF of $Y=\\suml_{i=1}^NX_i$?\n",
        "$\\E[e^{tY}|N]=M_{X_i}(t)^N$.\n",
        "$M_Y(t)=\\E[M_{X_i}(t)^N]=G_N(M_{X_i}(t))$.\n",
        "\n",
        "- e7.7k (e.7.5k): $Y‚àº\\Unif(0,1)$ and $(X|Y=p)‚àº\\Binom(n,p)$. Show that all values of $X$ have equal probability.\n",
        "$\\E[e^{tX}|Y=p]=M_{X_i|Y}(t)^n$\n",
        "$=(e^tp+1-p)^n$.\n",
        "$\\E[e^{tX}]=\\E[(e^tp+1-p)^n]$\n",
        "$=\\frac{1}{e^t-1}‚à´_1^{e^t}y^n\\ dy$\n",
        "$=\\frac{1}{n+1}\\frac{e^{t(n+1)}-1}{e^t-1}$\n",
        "$=\\frac{1}{n+1}(1+e^t+e^{2t}+...+e^{nt})$, which is a PGF with equal coefficient for all $e^{tx}$ terms $x‚àà[0,n]$.\n",
        "\n",
        "- 7.30: $X,Y$ are iid $(Œº,œÉ^2)$. What is $\\E[(X-Y)^2]$?\n",
        "$\\E[(X-Y)^2]=\\E[X^2]-2\\E[XY]+\\E[Y^2]$\n",
        "$=2(œÉ^2+Œº^2)-2(\\Cov(X,Y)+Œº^2)$\n",
        "$=2œÉ^2$.\n",
        "\n",
        "- 7.34: if 10 married couples are randomly seated at a round table, what is the expected number and variance of wives who are seated next to their husbands?\n",
        "Let $I_i$ be couple $i$ sitting together.\n",
        "$\\E[I_i]=\\frac{2}{19}$: fix 1 spouse with 2 adjacent spots for the other spouse.\n",
        "$\\E[I_iI_j]=\\frac{17!(2)(2)}{19!}=\\frac{2}{(19)(9)}$: 2 pairs + 16 individuals = 18 blocks, fix 1 and permute the other 17, with consideration paid to elative wife-husband orientation of the 2 pairs.\n",
        "$\\E[X]=\\frac{20}{19}$.\n",
        "$E\\binom{X}{2}=\\binom{10}{2}\\E[I_iI_j]$\n",
        "$‚áí\\E[X^2]-\\E[X]=\\frac{(10)(9)(2)}{(19)(9)}$\n",
        "$=\\frac{40}{19}$.\n",
        "$\\Var(X)=\\E[X^2]-\\E[X]^2$\n",
        "$=\\frac{40}{19}-\\frac{20^2}{19^2}=\\frac{360}{361}$\n",
        "\n",
        "- 7.36: let $X$ be the number of 1's and $Y$ be the number of 2's that occur in $n$ rolls of a fair die. What is Cov(X,Y)?\n",
        "Multinomial $\\E[XY]=n(n-1)\\frac{1}{6}\\frac{1}{6}$.\n",
        "$\\Cov(X,Y)=\\E[XY]-\\E[X]\\E[Y]$\n",
        "$=\\frac{n(n-1)}{36}-\\frac{n}{6}\\frac{n}{6}$\n",
        "$=-\\frac{n}{36}$.\n",
        "  - Let $I_i$ be indicator that roll $i$ produced 1, and $J_j$ be indicator that roll $j$ produced 2.\n",
        "  $X=I_1+...+I_n$ and $Y=J_1+...+J_n$.\n",
        "  $\\Cov(X,Y)=\\Cov\\left(\\suml_{i=1}^nI_i,\\suml_{j=1}^nJ_j\\right)$\n",
        "  $=\\suml_{i=1}^n\\suml_{j=1}^n\\Cov(I_i,J_j)$\n",
        "  $=\\suml_{i=1}^n\\Cov(I_i,J_i)$\n",
        "  $+\\suml_{i\\neq j}\\Cov(I_i,J_j)$\n",
        "  $=\\suml_{i=1}^n(\\E[I_i,J_i]-\\E[I_i]\\E[J_i])+0$\n",
        "  $=-\\suml_{i=1}^n\\E[I_i]\\E[J_i]$\n",
        "  $=-\\frac{n}{36}$.\n",
        "\n",
        "- 7.37: A die is rolled twice $X_1,X_2$. Let $X=X_1+X_2$ and $Y=X_1-X_2$. What is Cov(X,Y)?\n",
        "$\\Cov(X,Y)=\\Cov(X_1+X_2,X_1-X_2)$\n",
        "$=\\Cov(X_1,X_1)+\\Cov(X_1,X_2)$\n",
        "$-\\Cov(X_1,X_2)-\\Cov(X_2,X_2)$\n",
        "$=\\Var(X_1)-\\Var(X_2)$\n",
        "$=0$.\n",
        "\n",
        "- 7.38: $f(x,y)=\\frac{2}{x}e^{-2x},\\ x‚â•0, y‚àà[0,x]$. Find Cov(X,Y).\n",
        "  - $f_X(x)=‚à´_0^x\\frac{2}{x}e^{-2x}\\ dy$\n",
        "  $=2e^{-2x},\\ x‚â•0$.\n",
        "  $\\E[X]=‚à´_0^‚àû2xe^{-2x}\\ dx$\n",
        "  $=\\frac{1}{2}$.\n",
        "  - $f_Y(y)=‚à´_y^‚àû\\frac{2}{x}e^{-2x}\\ dx$ does not have a closed form expression.\n",
        "  We must evalute $\\E[Y]$ via conditional route $\\E[Y|X]$.\n",
        "  $f_{Y|X}(y|x)=\\frac{f(x,y)}{f_X(x)}$\n",
        "  $=\\frac{1}{x},\\ y‚àà[0,x]$.\n",
        "  $\\E[Y|X]=‚à´_0^x\\frac{y}{x}\\ dy$\n",
        "  $=\\frac{x}{2}$.\n",
        "  $\\E[Y]=\\E[\\E[Y|X]]$\n",
        "  $=‚à´_0^‚àûxe^{-2x}\\ dx$\n",
        "  $=\\frac{1}{4}$.\n",
        "  - $\\E[XY]=\\E[X\\E[Y|X]]=\\E[\\frac{X^2}{2}]$\n",
        "  $=‚à´_0^‚àûx^2e^{-2x}\\ dx$\n",
        "  $=\\frac{1}{4}$.\n",
        "  - $\\Cov(X,Y)=\\E[XY]-\\E[X]\\E[Y]=\\frac{1}{8}$.\n",
        "\n",
        "- 7.39: let $X_1,X_2,..$ be iid $(Œº,œÉ^2)$. Let $Y_n=X_n+X_{n+1}+X_{n+2}$. Find $\\Cov(Y_n,Y_{n+j})$.\n",
        "$\\Cov(Y_n,Y_{n+j})$\n",
        "$=\\Cov(\\suml_{i=n}^{n+2}X_i,\\suml_{i=n+j}^{n+2+j}X_i)$\n",
        "$=\\suml_{i=n}^{n+2-j}\\Var(X_i)$\n",
        "$+\\suml_{i\\neq j}\\Cov(X_i,X_j)$\n",
        "$=\\max(n-j,0)œÉ^2$.\n",
        "\n",
        "- 7.40: $f(x,y)=\\frac{1}{y}e^{-(y+\\frac{x}{y})},\\ x,y>0$. Find \\E[X], \\E[Y], and show Cov(X,Y)=1.\n",
        "  - $f_Y(y)=‚à´_{x=0}^‚àû\\frac{1}{y}e^{-(y+\\frac{x}{y})}\\ dx$\n",
        "  $=[-e^{-(y+\\frac{x}{y})}]_{x=0}^‚àû=e^{-y}$.\n",
        "  $\\E[Y]=‚à´_{y=0}^‚àûye^{-y}\\ dy=Œì(2)=1$\n",
        "  - $f_{X|Y}(x|y)=\\frac{f(x,y)}{f_Y(y)}$\n",
        "  $=\\frac{1}{y}e^{-\\frac{x}{y}}$.\n",
        "  $\\E[X|Y]=‚à´_{x=0}^‚àû\\frac{x}{y}e^{-\\frac{x}{y}}\\ dx$\n",
        "  $=y‚à´_{u=0}^‚àû ue^{-u}\\ du=yŒì(2)=y$.\n",
        "  $\\E[X]=\\E[\\E[X|Y]]$\n",
        "  $=‚à´_{y=0}^‚àûye^{-y}\\ dy=Œì(2)=1$.\n",
        "  - $\\E[XY]=\\E[Y\\E[X|Y]]=\\E[Y^2]$\n",
        "  $=‚à´_{y=0}^‚àûy^2e^{-y}\\ dy=Œì(3)=2$.\n",
        "  $\\Cov(X,Y)=\\E[XY]-\\E[X]\\E[Y]=1$.\n",
        "\n",
        "- 7.41: A pond contains 100 fish, of which 30 are carp. If 20 fish are caught, what are the mean and variance of number of carp $X$?\n",
        "Hypergeometric. Let $I_i$ be $i$th fish (in 20) is carp.\n",
        "$\\E[I_i]=\\frac{30}{100}$.\n",
        "$\\E[I_iI_j]=\\frac{30}{100}\\frac{29}{99}$.\n",
        "$\\E[X]=\\frac{(20)(30)}{100}=6$.\n",
        "$E\\binom{X}{2}=\\binom{20}{2}\\E[I_iI_j]$\n",
        "$‚áí\\E[X^2]-\\E[X]=(20)(19)\\frac{30}{100}\\frac{29}{99}=33.4$\n",
        "$‚áí\\E[X^2]=39.4$\n",
        "$‚áí\\Var(X)=3.4$.\n",
        "\n",
        "- 7.42: A group of 10 men and 10 women are randomly arranged into 10 pairs. What is the expectation and variance of pairs consisting of a man and a woman?\n",
        "Let $I_i$ be indicator for pair $i$ consisting of man and woman.\n",
        "$\\E[I_i]=\\frac{10}{19}$.\n",
        "$\\E[X]=\\frac{100}{19}$.\n",
        "$\\E[I_iI_j]=\\frac{10}{19}\\frac{9}{17}$.\n",
        "$E\\binom{X}{2}=\\binom{10}{2}\\E[I_iI_j]$\n",
        "$‚áí\\E[X^2]=\\frac{100}{19}+(10)(9)\\frac{10}{19}\\frac{9}{17}$\n",
        "$=30.34$\n",
        "$‚áí\\Var(X)=2.64$.\n",
        "\n",
        "- 7.49: Two misshapen coins lands on Heads with probabilities $p_A=0.4$ and $p_B=0.7$. One is randomly chosen and flipped 10 times. The first 3 flips produced 2 heads ($F$), what is the conditional expectation of heads in the 10 flips?\n",
        "$P(A|F)=\\frac{\\frac{1}{2}\\binom{3}{2}p_A^2q_A}{\\frac{1}{2}\\binom{3}{2}p_A^2q_A+\\frac{1}{2}\\binom{3}{2}p_B^2q_B}$\n",
        "$=0.395$.\n",
        "$P(B|F)=\\frac{\\frac{1}{2}\\binom{3}{2}p_B^2q_B}{\\frac{1}{2}\\binom{3}{2}p_A^2q_A+\\frac{1}{2}\\binom{3}{2}p_B^2q_B}$\n",
        "$=0.605$.\n",
        "$\\E[X|F]=\\E[X|F,A]P(A|F)+\\E[X|F,B]P(B|F)$\n",
        "$=2+7(0.395p_A+0.605p_B)$\n",
        "$=6.07$.\n",
        "\n",
        "- 7.50: $f(x,y)=\\frac{1}{y}e^{-(y+\\frac{x}{y})},\\ x,y>0$. Compute $\\E[X^2|Y=y]$.\n",
        "  - $f_Y(y)=‚à´_0^‚àû\\frac{1}{y}e^{-(y+\\frac{x}{y})}\\ dx$\n",
        "  $=e^{-y}$.\n",
        "  $f_{X|Y}(x|y)=\\frac{1}{y}e^{-\\frac{x}{y}}$.\n",
        "  - $\\E[X^2|Y=y]=‚à´_{x=0}^‚àû\\frac{x^2}{y}e^{-\\frac{x}{y}}\\ dx$\n",
        "  $=[-x^2e^{-\\frac{x}{y}}-2xye^{-\\frac{x}{y}}-2y^2e^{-\\frac{x}{y}}]_{x=0}^‚àû$\n",
        "  $=2y^2$\n",
        "\n",
        "- 7.51: $f(x,y)=\\frac{1}{y}e^{-y},\\ x‚àà(0,y), y>0$. Find $\\E[X^3|Y=y]$.\n",
        "$f_Y(y)=‚à´_{x=0}^y\\frac{1}{y}e^{-y}\\ dx=e^{-y}$.\n",
        "$f_{X|Y}(x|y)=\\frac{1}{y}$.\n",
        "$\\E[X^3|Y=y]=‚à´_0^y\\frac{x^3}{y}\\ dx$\n",
        "$=\\frac{y^3}{4}$.\n",
        "\n",
        "- 7.53: a prisoner is trapped in a cell with 3 doors, and he chooses door 1, 2, 3 with probability 0.5, 0.3, 0.2. The first door leads back to the cell after 2 days. The second leads back to the cell after 4 days. The third leads to freedom after 1 day. What is the expected number of days until he reaches freedom?\n",
        "$\\E[X]=\\E[X|D=1]P(D=1)$\n",
        "$+\\E[X|D=2]P(D=2)+\\E[X|D=3]P(D=3)$\n",
        "$=(2+\\E[X])(0.5)+(4+\\E[X])(0.3)+(0.2)$\n",
        "$‚áí\\E[X]=\\frac{1+1.2+0.2}{0.2}=12$\n",
        "\n",
        "- 7.55: $k=10$ hunters shoot at ducks flying overhead at the same time, each independently picking a random target and hits with probability $p=0.6$. Compute the number of ducks hit $X$ if the number of ducks $N‚àº\\Pois(Œª=6)$.\n",
        "Let $I_i$ be indicator that duck $i$ was hit.\n",
        "$\\E[I_i|N=n]=1-(1-\\frac{p}{n})^k$.\n",
        "$\\E[X|N=n]=n\\left(1-(1-\\frac{p}{n})^k\\right)$.\n",
        "$\\E[X]=\\E[\\E[X|N=n]]$\n",
        "$=\\suml_{n=1}^‚àûn\\left(1-(1-\\frac{p}{n})^k\\right)e^{-Œª}\\frac{Œª^n}{n!}$\n",
        "\n",
        "- 7.56: $X‚àº\\Pois(Œª=10)$ passengers enter an elevator on ground floor and independent and equally likely to exit on any one of $N$ floors. Compute the expected number of stops $S$ before discharging all passengers.\n",
        "Let $I_i$ be indicator that elevator stops at floor $i$.\n",
        "$\\E[I_i|X=x]=1-(1-\\frac{1}{N})^x$.\n",
        "$\\E[S|X=x]=N\\left(1-(\\frac{N-1}{N})^x\\right)$.\n",
        "$\\E[S]=\\suml_{x=1}^‚àûN\\left(1-(\\frac{N-1}{N})^x\\right)e^{-Œª}\\frac{Œª^x}{x!}$.\n",
        "\n",
        "- 7.61: let $X_1,...,X_N$ be iid with CDF $F$, and $N‚àº\\Geom(p)$. Let $M=\\max(X_1,...,X_N)$.\n",
        "$P(M‚â§x|N)=P(X_1‚â§x,...,X_N‚â§x|N)$\n",
        "$=P(X_1‚â§x)...P(X_N‚â§x)$\n",
        "$=F(x)^N$\n",
        "\n",
        "- 7.63: an urn contains $N=30$ balls, of which 10 red and 8 blue. $n=12$ balls were withdrawn, $X$ red and $Y$ blue. Find Cov(X,Y).\n",
        "Let $I_i$ be indicator $i$th ball red drawn. $J_j$ be $j$th ball blue drawn.\n",
        "$\\E[I_i]=\\binom{29}{11}/\\binom{30}{12}=\\frac{12}{30}$.\n",
        "$\\E[I_iJ_j]=\\binom{28}{10}/\\binom{30}{12}=\\frac{12}{30}\\frac{11}{29}$.\n",
        "$\\Cov(X,Y)=\\Cov(\\suml_{i=1}^{10}I_i,\\suml_{j=1}^{8}J_j)$\n",
        "$=\\suml_{i=1}^{10}\\suml_{j=1}^{8}\\Cov(I_i,J_j)$\n",
        "$=\\suml_{i=1}^{10}\\suml_{j=1}^{8}(\\E[I_iJ_j]-\\E[I_i]\\E[J_j])$\n",
        "$=80(\\frac{12}{30}\\frac{11}{29}-\\frac{12}{30}\\frac{12}{30})$\n",
        "$=-0.662$.\n",
        "\n",
        "- 7.64: Type $i$ bulb has lifetime ($Œº_i,œÉ_i^2$). A lightbulb is randomly chosen type 1 with probability $p$ and type 2 with $1-p$. What is $\\E[X]$ and $\\Var(X)$ for its lifetime?\n",
        "$\\E[X]=\\E[\\E[X|T]]$\n",
        "$=\\E[X|T=1]P(T=1)$\n",
        "$+\\E[X|T=2]P(T=2)$\n",
        "$=Œº_1p+Œº_2(1-p)$.\n",
        "$\\E[\\Var(X|T)]=\\Var(X|T=1)P(T=1)$\n",
        "$+\\Var(X|T=2)P(T=2)$\n",
        "$=œÉ_1^2p+œÉ_2^2(1-p)$.\n",
        "$\\Var(\\E[X|T])=\\E[\\E[X|T]^2]-\\E[X]^2$\n",
        "$=Œº_1^2p+Œº_2^2(1-p)-(Œº_1p+Œº_2(1-p))^2$\n",
        "$=p(1-p)(Œº_1-Œº_2)^2$.\n",
        "$\\Var(X)=pœÉ_1^2+(1-p)œÉ_2^2+p(1-p)(Œº_1-Œº_2)^2$.\n",
        "\n",
        "- 7.65: The number of winter storms in a good year is Poisson(Œª=3) in a bad year is Poisson(Œª=5). Next year is good year with probability 0.4 and bad year 0.6. What is the expected value and variance of number of storms next year?\n",
        "$\\E[X]=\\E[\\E[X|Y]]=0.4(3)+0.6(5)=4.2$.\n",
        "$\\Var(\\E[X|Y])=\\E[\\E[X|Y]^2]-\\E[X]^2$\n",
        "$=0.4(9)+0.6(25)-4.2^2=0.946$.\n",
        "$\\E[\\Var(X|Y)]=0.4(3)+0.6(5)=4.2$.\n",
        "$\\Var(X)=5.15$.\n",
        "\n",
        "- 7.68: Number of accidents a person has is Poisson(Œª) where Œª=2 for 0.6 of the population and Œª=3 for 0.4. What is the probability that a randomly chosen person has B=3 accidents in a given year given that he had A=0 accidents in the preceding year?\n",
        "$P(L=2|A=0)=\\frac{P(A=0,L=2)}{P(A=0,L=2)+P(A=0,L=3)}$\n",
        "$=\\frac{0.6e^{-2}}{0.6e^{-2}+0.4e^{-3}}$\n",
        "$=0.803$.\n",
        "$P(L=3|A=0)=0.197$.\n",
        "$P(B=3|A=0)=0.803e^{-2}\\frac{2^3}{3!}+0.197e^{-3}\\frac{3^3}{3!}$\n",
        "$=0.189$.\n",
        "\n",
        "- 7.70: A coin is randomly selected from an urn containing a large number of coins with different probabilities of turning up heads $Y‚àº\\Unif(0,1)$. What is the probability that the first flip turns up heads?\n",
        "$P(H|Y=y)=y$.\n",
        "$P(H)=‚à´_0^1P(H|Y=y)f_Y(y)\\ dy$\n",
        "$=‚à´_0^1y\\ dy=\\frac{1}{2}$.\n",
        "  - 7.71: The selected coin is tossed $n$ times. Let $X$ denote the number of heads that occur. Show $P(X=i)=\\frac{1}{n+1}$.\n",
        "  $P(X=i|Y=y)=\\binom{n}{i}y^i(1-y)^{n-i}$.\n",
        "  $P(X=i)=‚à´_yP(X=i|Y=y)f_Y(y)\\ dy$\n",
        "  $=‚à´_0^1\\binom{n}{i}y^i(1-y)^{n-i}\\ dy$\n",
        "  $=\\binom{n}{i}\\frac{i!(n-i)!}{(n+1)!}$\n",
        "  $=\\frac{1}{n+1}$.\n"
      ],
      "metadata": {
        "id": "wmjEX2e0RVNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Moments and Moment Generating Functions\n",
        "- Geometric series:\n",
        "  - $a\\suml_{i=0}^{n-1}r^i=\\BC a\\frac{1-r^n}{1-r} &r\\neq1 \\\\ ai &r=1\\EC$\n",
        "  - $a\\suml_{i=0}^‚àûr^i=\\frac{a}{1-r}$ if $r\\lt 1$"
      ],
      "metadata": {
        "id": "oLo9cgDDWnqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summaries of \"middle\":\n",
        "- **Median** of random variable $X$ is $c$ such that $P(X‚â§c)‚â•1/2$ and $P(X‚â•c)‚â•1/2$. Usually happens where CDF hits 1/2.\n",
        "- **Mode** of discrete $X$ is $c$ such that PMF $P(X=c)‚â•P(X=x),\\ x‚àà‚Ñù$. Mode of continuous $X$ is $c$ such that PDF $f(c)‚â•f(x),\\ x‚àà‚Ñù$.\n",
        "- Let $X$ have mean $Œº$ and median $m$. The value that minimizes **mean squared error** $\\E(X-c)^2$ is $c=\\E[X]$, when the MSE becomes $\\Var(X)$. The value that minimizes **mean absolute error** $\\E|X-c|$ is $c=m$.\n",
        "  - In the proof for MSE, $\\E(X-c)^2$ is minimized when $c=\\E[X]$. In the proof for MAE, assume $a>m$ (setting $a\\lt m$ comes to the same conclusion), we want to show $\\E|X-a|‚â•\\E|X-m|$ or $\\E[|X-a|-|X-m|]‚â•0$. Casella 2.18 has the continuous version of $\\E |X-m|$, which is more intuitive.\n",
        "\n",
        "$$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\Var(X)&=\\Var(X-c)\\\\\n",
        "&=\\E(X-c)^2-(\\E(X-c))^2\\\\\n",
        "&=\\E(X-c)^2-(Œº-c)^2 \\\\\n",
        "\\E(X-c)^2&=\\Var(X)+(Œº-c)^2\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "&\\E[|X-a|-|X-m|] \\\\\n",
        "=&\\E[a-X-(m-X)]=\\E|a-m| \\\\\n",
        "=&(a-m)P(X‚â§m)+(m-a)P(X>m) \\\\\n",
        "=&(a-m)(2P(X‚â§m)-1) \\\\\n",
        "‚â•&0\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "\n",
        "**Moments**: Let $X$ be a random variable with mean $Œº$ and variance $œÉ^2$, the following may or may not exist.\n",
        "- $\\E[X^n]$ is the **nth moment** of X\n",
        "- $\\E(X-Œº)^n$ is the **nth central moment** of X\n",
        "- $\\E\\left(\\frac{X-Œº}{œÉ}\\right)^n$ is the **nth standardized moment** of X\n",
        "- Mean is the first moment, variance is the second central moment, **skew** $\\E\\left(\\frac{X-Œº}{œÉ}\\right)^3$ is the third standardized moment.\n",
        "- $X$ is **symmetric** about the mean if $X-Œº$ has same distribution as $Œº-X$, i.e., mirror image about the mean.\n",
        "  - If $X$ is continuous with PDF $f$, then $X$ is symmetric about $Œº$ iff $f(x)=f(2Œº-x)$.\n",
        "  - **(Odd central moments)** If $X$ is symmetric about $Œº$, then for any odd $m$ the $m$th central moment $\\E(X-Œº)^m=0$. Since $m=1$ moment is always 0 symmetric or not, we use $m=3$ skew.\n",
        "\n",
        "$$\\begin{equation}\\begin{aligned}\n",
        "F(x)&=P(X-Œº‚â§x-Œº)\\\\\n",
        "&=P(Œº-X‚â§x-Œº)\\\\\n",
        "&=P(X‚â•2Œº-x) \\\\\n",
        "&=1-F(2Œº-x) \\\\\n",
        "f(x)&=f(2Œº-x)\n",
        "\\end{aligned}\\quad\\begin{aligned}\n",
        "&\\E(X-Œº)^m \\\\\n",
        "=&\\E(Œº-X)^m \\\\\n",
        "=&-\\E(X-Œº)^m \\\\\n",
        "=&0\n",
        "\\end{aligned}\\end{equation}$$\n",
        "- **Kurtosis** $\\E\\left(\\frac{X-Œº}{œÉ}\\right)^4-3$ is adjusted because $\\E[Z^4]=3$ (Blitzstein e6.5.2).\n",
        "    "
      ],
      "metadata": {
        "id": "jQ4rAYGla-dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample moments**: Let $X_1,...,X_n$ be iid random variables each with $Œº$ and $œÉ^2$. The $k$th sample moment is $\\E[\\bar{X}_n^k]=\\frac{1}{n}\\suml_{j=1}^nX_j^k$ is an unbiased estimate for the $k$th moment.\n",
        "$$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\E[\\bar{X}_n^k]&=\\E\\left[\\frac{1}{n}\\sum_{j=1}^nX_j^k\\right]\\\\\n",
        "&=\\frac{1}{n}\\sum_{j=1}^n\\E[X_j^k] \\\\\n",
        "&=\\E[X_1^k] \\\\\n",
        "\\\\\n",
        "\\Var(\\bar{X}_n)&=\\Var\\left(\\frac{1}{n}\\sum_{j=1}^nX_j\\right)\\\\\n",
        "&=\\frac{1}{n^2}\\sum_{j=1}^n\\Var(X_j) \\\\\n",
        "&=\\frac{œÉ^2}{n}\n",
        "\\end{aligned}\n",
        "\\quad\n",
        "\\begin{aligned}\n",
        "\\sum_{j=1}^n(X_j-\\bar{X}_n)^2\n",
        "&=\\sum_{j=1}^nX_j^2-2\\bar{X}_n\\sum_{j=1}^nX_j+\\sum_{j=1}^n\\bar{X}_n^2 \\\\\n",
        "&=\\sum_{j=1}^nX_j^2-2\\bar{X}_n(n\\bar{X}_n)+n\\bar{X}_n^2 \\\\\n",
        "\\E\\left[\\sum_{j=1}^n(X_j-\\bar{X}_n)^2\\right]\n",
        "&=\\sum_{j=1}^n\\E[X_j^2]-n\\E[\\bar{X}_n^2] \\\\\n",
        "&=\\sum_{j=1}^n\\Var(X_j)+\\sum_{j=1}^n\\E[X_j]^2-n\\Var(\\bar{X})-n\\E[\\bar{X}]^2 \\\\\n",
        "&=nœÉ^2+nŒº^2-n\\frac{œÉ^2}{n}-nŒº^2 \\\\\n",
        "&=(n-1)œÉ^2 \\\\\n",
        "\\E[S_n^2]&=œÉ^2\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "- **Sample mean** $\\bar{X}_n=\\frac{1}{n}\\suml_{j=1}^nX_j$ is unbiased estimate for the mean $\\E[\\bar{X}_n]=Œº$ with variance $\\Var(\\bar{X}_n)=\\frac{œÉ^2}{n}$.\n",
        "  - $\\E[\\bar{X}]=\\E[X_i]=Œº$. Both are unbiased estimators for $Œº$. However, $\\bar{X}$ is $n$-times more precise than $X_i$ as an estimator for $Œº$ because it has $n$-times smaller variance.\n",
        "- **Sample variance** $S_n^2=\\frac{1}{n-1}\\suml_{j=1}^n(X_j-\\bar{X}_n)^2$ is an unbiased estimate for $œÉ^2$ However, $S_n$ is *not* an unbiased estimate for $œÉ$."
      ],
      "metadata": {
        "id": "M3F8B_qq1bDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Moment generating function**: the MGF of $X$ is $M_X(t)=\\E[e^{tX}]$ if it exists as a finite function on some open interval containing 0 $t‚àà(-a,a),\\ a>0$ is an infinite series of all moments of $X$.\n",
        "$$\\begin{aligned}\n",
        "M_X(t)&=1+t\\E[X]+\\frac{t^2}{2!}\\E[X^2]+\\frac{t^3}{3!}\\E[X^3]+...\\\\\n",
        "M_X''(t)&=0+0+\\frac{2!}{2!}\\E[X^2]+\\frac{(3)(2)t}{3!}\\E[X^3]+...\\\\\n",
        "M_X''(0)&=0+0+\\E[X^2]+0+...\n",
        "\\end{aligned}$$\n",
        "- $M_X^{(n)}(0)=\\E[X^n]$ the $n$th derivative evaluated at $t=0$ gives the $n$th moment.\n",
        "  - $M(t)=\\suml_{n=0}^‚àûM^{(n)}(0)\\frac{t^n}{n!}$ by Taylor expansion of $M(t)$ centered at $t=0$.\n",
        "  - $M(t)=\\E[e^{tX}]=\\E\\left[\\suml_{n=0}^‚àû\\frac{(tX)^n}{n!}\\right]=\\suml_{n=0}^‚àû\\E[X^n]\\frac{t^n}{n!}$ by Taylor series of $e^x$.\n",
        "- (Ross Chapter 7) let $M(t)=\\E[e^{tX}]=\\BC\n",
        "  \\suml_xe^{tx}P(X=x)&\\\\\n",
        "  ‚à´_xe^{tx}f(x)\\ dx&\n",
        "  \\EC$ then\n",
        "  - $M'(t)=\\E[\\frac{d}{dt}e^{tX}]=\\E[Xe^{tX}]‚áíM'(0)=\\E[X]$\n",
        "  - $M''(t)=\\E[\\frac{d}{dt}Xe^{tX}]=\\E[X^2e^{tX}]‚áíM''(0)=\\E[X^2]$\n",
        "- Joint MGF for multivariate random vector $(X_1,...,X_n)$: $M_{X_1,...,X_n}(t_1,...,t_n)=\\E[e^{t_1X_1+...+t_nX_n}]$.\n",
        "  - Bivarate random vector joint MGF: $M_{X,Y}(s,t)=\\E[e^{sX+tY}]$.\n",
        "- $M(t)=\\E[e^{tX}]$ doesn't always exist, but $œà(t)=\\E[e^{itX}]$ where $i=\\sqrt{-1}$ always exists, and it is the characteristic function (Fourier transform).\n",
        "\n",
        "**MGF properties**: If $M_X(t)=M_Y(t)$ for all $t‚àà(-a,a),\\ a>0$ then $X$ and $Y$ are identically distributed. The MGF uniquely determines the distribution if it exists for all $t$ in an open interval around 0.\n",
        "- $M(0)=1$\n",
        "- $M_{X+Y}(t)=M_X(t)M_Y(t)$ iff $X$ and $Y$ are independent. (Blitzstein T6.4.7)\n",
        "  - $\\E[e^{tX}e^{tY}]=\\E[e^{tX}]\\E[e^{tY}]$ iff $X$ and $Y$ are independent.\n",
        "  - Proof (independence ‚áí factorialization): Let $X,Y$ be independent ‚áí $\\E[g(X)h(Y)]=\\E[g(X)]\\E[h(Y)]$ for all $g,h$. This implies\n",
        "  $M_{X+Y}(t)=\\E[e^{tX}e^{tY}]$\n",
        "  $=\\E[e^{tX}]\\E[e^{tY}]$\n",
        "  $=M_X(t)M_Y(t)$.\n",
        "  - Proof (factorialization ‚áí independence): Let $M_{X+Y}(t)=\\E[e^{tX}]\\E[e^{tY}]$. Let $Z,W$ be independent, then it was previously proven\n",
        "  $M_{Z+W}(t)=\\E[e^{tZ}]\\E[e^{tW}]$.\n",
        "  Two random variables sharing the same MGF are the same random variable, then it follows $X‚àºZ$ and $Y‚àºW$ and $X,Y$ must also be independent.\n",
        "- $M_{X,Y}(s,t)=M_X(s)M_Y(t)$ iff $X$ and $Y$ are independent. (Blitzstein T7.5.7)\n",
        "  - Proof (independence ‚áí factorialization): Let $X,Y$ be independent ‚áí $\\E[g(X)h(Y)]=\\E[g(X)]\\E[h(Y)]$ for all $g,h$.\n",
        "  This implies\n",
        "  $M_{X,Y}(s,t)=\\E[e^{sX}e^{tY}]$\n",
        "  $=\\E[e^{sX}]\\E[e^{tY}]$\n",
        "  $=M_X(s)M_Y(t)$.\n",
        "  - Proof (factorialization ‚áí independence): Let $M_{X,Y}(s,t)=\\E[e^{sX}]\\E[e^{tY}]$. Let $Z,W$ be independent, then it was previously proven\n",
        "  $M_{Z,W}(s,t)=\\E[e^{sX}]\\E[e^{tY}]$.\n",
        "  Two random variables sharing the same joint MGF are the same random variable, then it follows $X‚àºZ$ and $Y‚àºW$ and $X,Y$ must also be independent.\n",
        "- $M_{a+bX}(t)=\\E[e^{t(a+bX)}]=e^{at}M_X(bt)$.\n"
      ],
      "metadata": {
        "id": "iH64ZYyP1ge-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Probability generating function**: the PGF of a non-negative discrete $X$ is $G(t)=\\E[t^X]=\\suml_{k=0}^‚àût^kP(X=k)$ for $|t|\\lt1$ is an infinite series of the PMF of $X$.\n",
        "$$\\begin{aligned}\n",
        "G_X(t)&=P(X=0)+tP(X=1)+t^2P(X=2)+t^3P(X=3)+... \\\\\n",
        "G_X''(t)&=0+0+2!P(X=2)+(3)(2)tP(X=3)+... \\\\\n",
        "\\frac{1}{2!}G_X''(0)&=0+0+P(X=2)+0+...\n",
        "\\end{aligned}$$\n",
        "- $P(X=k)=\\frac{G_X^{(k)}(0)}{k!}$ the $k$th derivative evaluated at $t=0$ gives $P(X=k)$.\n",
        "- If $G_X(t)=G_Y(t)$ for all $t‚àà(-a,a),\\ 0\\lt a\\lt 1$ then $X$ and $Y$ are identically distributed.\n",
        "- PGF always exists. If MGF also exists then $G_X(t)=\\E[t^X]=\\E[e^{X\\ln(t)}]=M_X(\\ln(t))$ for $t>0$.\n"
      ],
      "metadata": {
        "id": "wkIqTvv91mte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blitzstein Chapter 6 (Moments):\n",
        "- e6.4.2: for Bernoulli $X‚àºI(p)$, $M_X(t)=pe^{t}+q$ for $t‚àà‚Ñù$.\n",
        "\n",
        "- e6.4.3: for $X‚àº\\Geom(p)$, $M_X(t)=\\suml_{k=0}^‚àûe^{tk}q^kp=p\\suml_{k=0}^‚àû(qe^t)^k=\\frac{p}{1-qe^t}$ for $qe^t‚àà(0,1)$.\n",
        "\n",
        "- e6.4.4: for $U‚àº\\Unif(a,b)$, $M_U(t)=\\frac{1}{b-a}‚à´_a^be^{xt}\\ dx=\\frac{1}{t(b-a)}\\left[e^{xt}\\right]_a^b=\\frac{e^{bt}-e^{at}}{t(b-a)}$\n",
        "\n",
        "- e6.4.8: for $X‚àº\\Binom(n,p)$, $M_X(t)=(pe^{t}+q)^n$\n",
        "\n",
        "- e6.4.9: for $X‚àº\\t{nbinom}(r,p)$, $M_X(t)=\\left(\\frac{p}{1-qe^t}\\right)^r$ for $qe^t‚àà(0,1)$.\n",
        "\n",
        "- e6.4.12: for $Z‚àº\\Normal(0,1)$,\n",
        "$M_Z(t)=\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^‚àûe^{tx-x^2/2}\\ dx$\n",
        "$=e^{t^2/2}\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^‚àûe^{-(t^2-2tx+x^2)/2}\\ dx$\n",
        "$=e^{t^2/2}\\frac{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^‚àûe^{-(x-t)^2/2}\\ dx$ $=e^{t^2/2}$.\n",
        "For $X‚àº\\Normal(Œº,œÉ^2)$, $M_X(t)=e^{Œºt}e^{(œÉt)^2/2}$ $=e^{Œºt+\\frac{1}{2}œÉ^2t^2}$.\n",
        "\n",
        "- e6.4.13: for $X‚àº\\Expo(1)$, MGF\n",
        "$M_X(t)=‚à´_0^‚àûe^{tx}e^{-x}\\ dx$\n",
        "$=‚à´_0^‚àûe^{-(1-t)x}\\ dx$\n",
        "$=\\frac{-1}{1-t}\\left[e^{-(1-t)x}\\right]_0^‚àû=\\frac{1}{1-t}$ for $t\\lt1$. For $Y‚àº\\Expo(Œª)$,\n",
        "$Y=F_Y^{-1}(F_X(X))=\\frac{X}{Œª}$ and\n",
        "$M_Y(t)=M_X(\\frac{t}{Œª})=\\frac{Œª}{Œª-t}$.\n",
        "\n",
        "- e6.5.1: for $X‚àº\\Expo(1)$, MGF $M_X(t)=\\frac{1}{1-t}=\\suml_{n=0}^‚àût^n$ by geometric series. It can be rewritten as $M_X(t)=\\suml_{n=0}^‚àûn!\\frac{t^n}{n!}$, and therefore $\\E[X^n]=n!$ For $Y‚àº\\Expo(Œª)=\\frac{X}{Œª}$, $\\E[Y^n]=\\E\\left[\\frac{X^n}{Œª^n}\\right]=\\frac{n!}{Œª^n}$\n",
        "\n",
        "- e6.5.2: for $Z‚àº\\Normal(0,1)$, $M_Z(t)=e^{t^2/2}=\\suml_{n=0}^‚àû\\frac{(t^2/2)^n}{n!}$ by Taylor series. It can be rewritten as $M_Z(t)=\\suml_{n=0}^‚àû\\frac{t^{2n}}{2^nn!}=\\suml_{n=0}^‚àû\\frac{(2n)!}{2^nn!}\\frac{t^{2n}}{(2n)!}$, and therefore $\\E[Z^{2n}]=\\frac{(2n)!}{2^nn!}=(2n-1)(2n-3)...$ (partnering 2n people into n teams). $\\E[Z^4]=3$, which is why kurtosis has a $-3$ shift.\n",
        "\n",
        "- e6.5.3: for lognormal $Y=e^Z$ where $Z‚àº\\Normal(0,1)$, $M_Y(t)=\\E[e^{te^Z}]$ $=‚à´_{-‚àû}^‚àû \\frac{1}{\\sqrt{2œÄ}}e^{te^z-z^2/2}\\ dz$ diverges to ‚àû because $te^z-z^2/2>0$ for any $t>0$. Therefore $M_Y(t)$ does not exist. However $M_Z(t)$ does and we can generate moments from $\\E[Y^n]=\\E[e^{nZ}]=M_Z(n)$. For general lognormal $Y=e^X$ where $X‚àº\\Normal(Œº,œÉ^2)$, $\\E[Y^n]=e^{Œºn+\\frac{1}{2}œÉ^2n^2}$.\n",
        "\n",
        "- e6.5.5: for Weibull $T=X^{1/Œ≥}$ where $X‚àº\\Expo(Œª)$, then by uniform universality\n",
        "$F_T(T)=F_X(T^Œ≥)=1-e^{-Œªt^Œ≥}$ and\n",
        "$f(t)=Œ≥Œªt^{Œ≥-1}e^{-Œªt^Œ≥}$. For\n",
        "$\\t{Weibull}(Œª=1,Œ≥=\\frac{1}{3})$, $X‚àº\\Expo(1)$,\n",
        "$M_X(t)=\\frac{1}{1-t}$, $\\E[X^n]=n!$, $T=X^3$,\n",
        "$\\E[T^n]=\\E[X^{3n}]=(3n)!$ $\\E[T]=6$, $\\E[T^2]-\\E[T]^2=6!-6^2$.\n",
        "$\\E[e^{tT}]=\\E[e^{tX^3}]=‚à´_0^‚àûe^{tx^3}e^{-x}\\ dt$ diverges to ‚àû.\n",
        "\n",
        "- e6.6.1: for $X‚àº\\Pois(Œª)$, $M_X(t)=\\E[e^{tX}]$\n",
        "$=\\suml_{k=0}^‚àûe^{tk}e^{-Œª}\\frac{Œª^k}{k!}$\n",
        "$=e^{-Œª}\\suml_{k=0}^‚àû\\frac{(e^tŒª)^k}{k!}$\n",
        "$=e^{-Œª}e^{Œªe^t}=e^{Œª(e^t-1)}$. If $X_1‚àº\\Pois(Œª_1)$ and $X_2‚àº\\Pois(Œª_2)$ are independent, then\n",
        "$M_{X_1+X_2}(t)=M_{X_1}(t)M_{X_2}(t)=e^{(Œª_1+Œª_2)(e^t-1)}$, which shows $(X_1+X_2)‚àº\\Pois(Œª_1+Œª_2)$.\n",
        "\n",
        "- e6.6.3 (sums of independent normals): $X_1‚àº\\Normal(Œº_1,œÉ_1^2)$ and $X_2‚àº\\Normal(Œº_2,œÉ_2^2)$ are independent, then\n",
        "$M_{X_1+X_2}(t)=e^{Œº_1t+\\frac{1}{2}œÉ_1^2t^2}e^{Œº_2t+\\frac{1}{2}œÉ_2^2t^2}$\n",
        "$=e^{(Œº_1+Œº_2)t+\\frac{1}{2}(œÉ_1^2+œÉ_2^2)t^2}$. Therefore $(X_1+X_2)‚àº\\Normal(Œº_1+Œº_2,œÉ_1^2+œÉ_2^2)$.\n",
        "\n",
        "- e6.7.2: let $X=X_1+...+X_6$ be the total from rolling 6 fair dice. What is $P(X=18)$?\n",
        "  - $G_{X_1}(t)=\\E[t^{X_1}]=\\suml_{k=1}^6t^kP(X_1=k)$\n",
        "  $=\\frac{1}{6}(t+t^2+...+t^6)$.\n",
        "  - $G_X(t)=\\E[t^{X_1+...+X_6}]=\\E[t^{X_1}]...\\E[t^{X_6}]$\n",
        "  $=\\frac{t^6}{6^6}(1+t+...+t^5)^6$. $P(X=18)$ is the coefficient of $t^{18}$.\n",
        "\n",
        "- e6.7.3: let $X‚àº\\Binom(n,p)$, then $\\E[t^{X_i}]=pt+q$.\n",
        "  - $G_X(t)=\\E[t^X]=(pt+q)^n$\n",
        "  $=\\suml_{k=0}^n\\binom{n}{k}(pt)^kq^{n-k}$ $=\\suml_{k=0}^nP(X=k)t^k$. The PMF of $X=k$ is the coefficient of $t^k$.\n",
        "  - $G_X'(t)=np(pt+q)^{n-1}=\\suml_{k=1}^nkP(X=k)t^{k-1}$ and $G_X'(1)=\\E[X]=np$.\n",
        "  - $G_X''(t)=n(n-1)p^2(pt+q)^{n-2}=\\suml_{k=2}^nk(k-1)P(X=k)t^{k-2}$ and $G_X''(1)=\\E[X(X-1)]=n(n-1)p^2$\n",
        "  - $G_X^{(k)}(1)=\\E\\left[\\frac{X!}{(X-k)!}\\right]=\\frac{n!}{(n-k)!}p^k=k!\\binom{n}{k}p^k$. Then $E\\binom{X}{k}=\\binom{n}{k}p^k$.\n",
        "\n",
        "- 6.14: let $U_1,...,U_{60}$ be iid $\\Unif(0,1)$ and $X=U_1+...+U_{60}$. $M_{U_i}(t)=\\int_0^1e^{tx}\\ dx=\\frac{1}{t}\\left[e^{tx}\\right]_0^1=\\frac{e^t-1}{t}$ and $M_X(t)=\\left(\\frac{e^t-1}{t}\\right)^{60}$.\n",
        "\n",
        "- 6.18: let $X‚àº\\Geom(p)$.\n",
        "$M_X(t)=\\suml_{k=0}^‚àûe^{tk}q^kp=\\frac{p}{1-qe^t}$. $M_X'(t)=\\frac{pqe^t}{(1-qe^t)^2}$ and\n",
        "$\\E[X]=\\frac{pq}{(1-q)^2}=\\frac{q}{p}$.\n",
        "$M_X''(t)=\\frac{pqe^t}{(1-qe^t)^2}+\\frac{2p(qe^t)^2}{(1-qe^t)^3}$\n",
        "$=\\frac{pqe^t(1-qe^t)+2p(qe^t)^2}{(1-qe^t)^3}$\n",
        "$=\\frac{pqe^t+p(qe^t)^2}{(1-qe^t)^3}$. $\\E[X^2]=\\frac{q+q^2}{p^2}$ and $Var(X)=\\frac{q}{p^2}$\n",
        "\n",
        "- 6.19: let $X$ and $Y$ be iid $\\Pois(Œª)$.\n",
        "$M_X(t)=\\suml_{k=0}^‚àûe^{tk}e^{-Œª}\\frac{Œª^k}{k!}$\n",
        "$=e^{-Œª}\\suml_{k=0}^‚àû\\frac{(Œªe^t)^k}{k!}$\n",
        "$=e^{-Œª+Œªe^t}=e^{Œª(e^t-1)}$.\n",
        "$M_{X+2Y}(t)=M_X(t)M_Y(2t)$ is not poisson."
      ],
      "metadata": {
        "id": "ywMcOH4V1scH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inequalities and Limit Theorems"
      ],
      "metadata": {
        "id": "RHhB4IDEOtOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cauchy-Schwarz inequality**: $|\\E[XY]|‚â§\\sqrt{\\E[X^2]\\E[Y^2]}$\n",
        "- Proof: $0‚â§\\E(Y-tX)^2=\\E[Y^2]-2t\\E[XY]+t^2\\E[X^2]$. We want the smallest right hand side (tightest bound).\n",
        "  - $\\frac{d}{dt}(\\E[Y^2]-2t\\E[XY]+t^2\\E[X^2])$\n",
        "  $=-2\\E[XY]+2t\\E[X^2]$\n",
        "  $‚áít=\\E[XY]/\\E[X^2]$\n",
        "  - $0‚â§\\E[Y^2]-\\E[XY]^2/\\E[X^2]$\n",
        "- Finding $\\E[XY]$ requires joint distribution of $X$ and $Y$, which is hard unless $\\Cov(X,Y)=0$ then $\\E[XY]=\\E[X]\\E[Y]$ using marginal expectations. Cauchy-Schwarz places upperbound using marginal second moments.\n",
        "- $|œÅ|‚â§1$ (Blitzstein 10.1.2)\n",
        "- $\\E[X^2]‚â•\\E[X]^2$ (set Y=1; although variance‚â•0 also proof)\n",
        "- $P(X>0)‚â•\\frac{\\E[X]^2}{\\E[X^2]}$ and $P(X=0)‚â§\\frac{\\Var(X)}{\\E[X^2]}$ for nonnegative $X$ (Blitzstein 10.1.3)\n",
        "- If marginal MGFs exist then joint MGF exists. (Blitzstein 10.1.4)\n",
        "\n",
        "**Jensen inequality**. If $g''(x)‚â•0$ (convex) then $\\E[g(X)]‚â•g(\\E[X])$. If $g''(x)‚â§0$ (concave) then $\\E[g(X)]‚â§g(\\E[X])$. $\\E[g(X)]=g(\\E[X])$ if $g(X)=a+bX$ for constants $a,b$.\n",
        "- Proof: Let $a+bX$ be tangent line at point $(X,g(X))$. If $g$ is convex then $g(X)‚â•a+bX$ for all values of $X$. $\\E[g(X)]‚â•a+b\\E[X]=g(\\E[X])$.\n",
        "- $\\E|X|‚â•|\\E[X]|$,\n",
        "$\\E[\\frac{1}{X}]‚â•\\frac{1}{\\E[X]}$,\n",
        "$\\E[\\ln X]‚â§\\ln \\E[X]$\n",
        "- $\\E[S_n]‚â§œÉ$ (Blitzstein 10.1.6)\n",
        "- Entropy $H(X)$ is maximized when $X‚àº\\DUnif$. (Blitzstein 10.1.7)\n",
        "- Kullback-Leibler divergence $D(\\v{p},\\v{r})‚â•0$. (Blitzstein 10.1.8)\n",
        "\n",
        "**Markov inequality**: $P(|X|‚â•a)‚â§\\frac{Œº_{|X|}}{a},\\ a>0$\n",
        "- Proof: Let $Y=\\frac{|X|}{a}$ then inequality becomes $P(Y‚â•1)‚â§\\E[Y]$. Indicator $I_{Y‚â•1}‚â§Y‚áí\\E[I_{Y‚â•1}]‚â§\\E[Y]$.\n",
        "- Let $X$ is the income of a random individual, $a=2\\E[X]$, then $P(X‚â•2\\E[X])‚â§\\frac{1}{2}$. No more than half the population can have income at least twice the mean.\n",
        "- Does not apply to Cauchy distribution.\n",
        "\n",
        "**Chebyshev inequality**: $P(|X-Œº_X|‚â•a)‚â§\\frac{œÉ_X^2}{a^2},\\ X‚àº(Œº_X,œÉ_X^2),a>0$\n",
        "- Proof: By Markov, $P(|X-Œº|‚â•a)=P((X-Œº)^2‚â•a^2)‚â§\\frac{\\E(X-Œº)^2}{a^2}=\\frac{œÉ^2}{a^2}$.\n",
        "- For $a=zœÉ$ z-score, $P(|X-Œº|‚â•zœÉ)‚â§\\frac{1}{z^2}$ gives an upperbound on probability that $X$ is $z$ sigmas away from the mean.\n",
        "- **One-sided Chebyshev**: $P(X-Œº‚â•a)‚â§\\frac{œÉ^2}{œÉ^2+a^2},\\ a>0$\n",
        "\n",
        "**Chernoff inequality**: $P(X‚â•a)‚â§\\frac{\\E[e^{tX}]}{e^{ta}}$\n",
        "- Proof: By Markov, $P(X‚â•a)=P(e^{tX}‚â•e^{ta})‚â§\\frac{\\E[e^{tX}]}{e^{ta}}$.\n"
      ],
      "metadata": {
        "id": "kmLu2cVAO2WN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Law of large numbers**: let $X_1,...X_n‚àº(Œº,œÉ^2)$ be iid, and let $\\bar{X}_n=\\frac{X_1+...+X_n}{n}‚àº(Œº,\\frac{œÉ^2}{2})$ be sample mean.\n",
        "- **Strong law of large numbers**: $P\\left(\\liml_{n‚Üí‚àû}\\bar{X}_n(s)=Œº\\right)=1$ for each $s‚ààS$.\n",
        "  - $\\bar{X}_n$ converges to $Œº$ pointwise with probability 1.\n",
        "  - $s$ is one outcome sequence of $n$ coin tosses amongst all possible permutations in $S$. $X_i(s)$ is the result of the $i$th toss, and $\\bar{X}_n(s)$ is the running average of the first $n$ tosses. As $n=|s|‚Üí‚àû$, $\\bar{X}_n‚ÜíŒº$.\n",
        "- **Weak law of large numbers**: $\\liml_{n‚Üí‚àû}P(|\\bar{X}_n-Œº|>œµ)=0$ for all $œµ>0$.\n",
        "  - the probability that there is any gap between the running average $\\bar{X}_n$ and $Œº$ converges to 0.\n",
        "  - Proof: Chebyshev: $P(|\\bar{X}_n-Œº|>œµ)‚â§\\frac{œÉ^2/n}{œµ^2}$\n",
        "- **Central limit theorem**: $\\liml_{n‚Üí‚àû}\\left(\\frac{\\bar{X}_n-Œº}{\\sqrt{œÉ^2/n}}\\right)‚àº\\Normal(0,1)$.\n",
        "  - Proof: the MGF of $\\sqrt{n}\\bar{X}_n=(X_1+...+X_n)/\\sqrt{n}$ converges to $e^{t^2/2}$.\n",
        "  - The formal CLT says standardized $\\bar{X}_n$ approaches standard normal as $n‚Üí‚àû$. By location-scale transformation of normal, CLT is rewritten as $\\liml_{n‚Üí‚àû}\\bar{X}_n‚àº\\Normal(Œº,\\frac{œÉ^2}{n})$ or $\\bar{X}_n\\arr{d}\\Normal(Œº,\\frac{œÉ^2}{n})$.\n",
        "  - CLT is expanded via location-scale transformation to $W_n=X_1+...+X_n=n\\bar{X}_n\\arr{d}\\Normal(nŒº_{X_i},nœÉ_{X_i}^2)$, while $X_i‚àº(Œº_{X_i},œÉ_{X_i}^2)$ can be *any* distribution as long as $Œº_{X_i},œÉ_{X_i}^2$ are finite.\n",
        "    - **Poisson**: Let $Y_1,...,Y_n‚àº\\Pois(1)$ iid, then $Y‚àº\\Pois(n)\\arr{d}\\Normal(n,n)$.\n",
        "    - **Binomial**: Let $Y_1,...,Y_n‚àº\\t{bern}(p)$ iid, then $Y‚àº\\Binom(n,p)\\arr{d}\\Normal(np,np(1-p))$. With continuity correction, $P(Y=k)=Œ¶(\\frac{k+0.5-np}{\\sqrt{np(1-p)}})-Œ¶(\\frac{k-0.5-np}{\\sqrt{np(1-p)}})$.\n",
        "    - **Gamma**: Let $Y_1,...,Y_n‚àº\\Expo(Œª)$ iid, then $Y‚àº\\Gamma(n,Œª)\\arr{d}\\Normal(\\frac{n}{Œª},\\frac{n}{Œª^2})$.\n",
        "    - **Chi-square**: Let $Z_1^2,...,Z_n^2‚àºœá_1^2$ iid, then $Y‚àºœá_n^2\\arr{d}\\Normal(n,2n)$\n",
        "  - Let $X_1,X_2,...‚àº\\t{bern}(\\frac{1}{2})$ iid as indicators of heads in fair coin tosses. Let $\\bar{X}_n$ be proportion of Heads after $n$ tosses, then the strong LLN says $\\bar{X}_1,\\bar{X}_2,...$ converges to $\\frac{1}{2}$. The weak LLN says the probability that $\\bar{X}_n$ is more than $œµ$ away from $\\frac{1}{2}$ converges to 0. The CLT says $\\bar{X}_n‚àº\\Normal(\\frac{1}{2},\\frac{1}{4n})$. (Blitzstein e10.2.3/e10.3.3)\n",
        "- LLNs do not apply to Cauchy distribution."
      ],
      "metadata": {
        "id": "awDAo3HPmOaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blitzstein Chapter 10: Inequalities and limit theorems\n",
        "\n",
        "- 10.1.2 (correlation): Cauchy-Schwarz inequality is rewritten $\\frac{|\\E[XY]|}{\\sqrt{\\E[X^2]\\E[Y^2]}}‚â§1$. If $\\E[X]=\\E[Y]=0$, or apply to $X-\\E[X]$ and $Y-\\E[Y]$, then $\\frac{|\\Cov(X,Y)|}{\\sqrt{\\Var(X)\\Var(Y)}}‚â§1$.\n",
        "\n",
        "- 10.1.3 (X=0): Let $X=XI_{X>0}$ (nonnegative). Then $\\E[XI_{X>0}]‚â§\\sqrt{\\E[X^2]\\E[I_{X>0}^2]}$\n",
        "$‚áí\\E[X]^2‚â§\\E[X^2]P(X>0)$.\n",
        "$P(X>0)‚â•\\frac{\\E[X]^2}{\\E[X^2]}$.\n",
        "$P(X=0)‚â§1-\\frac{\\E[X]^2}{\\E[X^2]}$\n",
        "$=\\frac{\\Var(X)}{\\E[X^2]}$.\n",
        "  - Second moment method: let $X=I_1+...+I_n$ where $I_i$ are uncorrelated indicators and $\\E[I_i]=p_i$.\n",
        "  Then\n",
        "  $\\Var(X)=\\suml_{i=1}^np_j(1-p_j)$\n",
        "  $=Œº_X-\\suml_{i=1}^np_j^2$.\n",
        "  $\\E[X^2]=\\Var(X)+\\E[X]^2$.\n",
        "  $P(X=0)‚â§\\frac{Œº_X-c}{Œº_X^2+Œº_X-c}$\n",
        "  $‚â§\\frac{1}{\\E[X]+1}$.\n",
        "  - n=14 people in a room. What is the probability that no two people with the same birthday or have birthdays 1 day apart? Let $I_{ij}$ be indicator for person $i$ and $j$ have the same birthday or 1 day apart.\n",
        "  $\\E[I_{ij}]=\\frac{3}{365}$.\n",
        "  $\\E[N]=\\binom{n}{2}\\E[I_{ij}]=0.370$.\n",
        "  Poisson approximation\n",
        "  $P(N=0)=e^{-\\E[N]}=0.473$.\n",
        "  Precise answer is $\\frac{365√ó362√ó359√ó...√ó326}{365^{14}}=0.460$.\n",
        "  By second moment we know\n",
        "  $P(N=0)‚â§\\frac{1}{\\E[N]+1}=0.572$.\n",
        "\n",
        "- 10.1.4 (Joint MGF): Let $X_1,X_2$ be jointly distributed. show that if the marginal MGFs exist then the joint MGF exists.\n",
        "If $M_{X_1}(s)=\\E[e^{sX_1}]$ and $M_{X_2}(t)=\\E[e^{tX_2}]$ are finite in an open interval around $s=0,t=0$, then by Cauchy-Schwarz,\n",
        "$\\E[e^{sX_1}e^{tX_2}]‚â§\\sqrt{\\E[e^{2sX_1}]\\E[e^{2tX_2}]}$ right-hand side must also be finite.\n",
        "\n",
        "- 10.1.6 (Sample stdev bias): Let $X_1,...,X_n$ be iid with $œÉ^2$. Sample variance $\\E[S_n^2]=œÉ^2$. The square root function is convex. Jensen's inequality\n",
        "$\\E[\\sqrt{S_n^2}]‚â§\\sqrt{\\E[S_n^2]}$\n",
        "$‚áí\\E[S_n]‚â§œÉ$. The magnitude of the bias varies with distribution, so there is no way to fix the bias. The bias tends to be small with large $n$.\n",
        "\n",
        "- 10.1.7 (Surprise and entropy): **Surprise** that an event with probability $p$ happened is defined to be $\\log_2(\\frac{1}{p})$ bits. H(A) + H(B) = H(A‚à©B). If p=0.5 then H=1. Let $X$ be discrete with possible values $a_1,...,a_n$ with probabilities $p_1,...,p_n$. **Entropy** is defined to be the average surprise of $X$:\n",
        "$H(X)=\\suml_{i=1}^np_i\\log_2(\\frac{1}{p_j})$.\n",
        "Prove that $H(X)$ is maximized when $X‚àº\\DUnif(1,n)$.\n",
        "$H(X)=\\suml_{i=1}^n\\frac{1}{n}\\log_2(n)$\n",
        "$=\\log_2(n)$.\n",
        "Let $Y$ take on values $\\frac{1}{p_1},...,\\frac{1}{p_n}$ with probabilities $p_1,...,p_n$, then\n",
        "$\\E[Y]=n$ and\n",
        "$H(Y)=\\suml_{i=1}^np_i\\log_2(y_i)$.\n",
        "By Jensen inequality\n",
        "$H(Y)=\\E[\\log_2(Y)]‚â§\\log_2(\\E[Y])$\n",
        "$=\\log_2(n)=H(X)$.\n",
        "Now reassign $Y$ to any possible values and $H(Y)‚â§H(X)$ still stands, because entropy does not depend on values of $Y$.\n",
        "\n",
        "- 10.1.8 (Kullback-Leibler divergence): Let $\\v{p}=(p_1,...,p_n)$ and $\\v{r}=(r_1,...,r_n)$ be probability vectors that each sum to 1, which represent PMFs. The **Kullback-Leibler divergence** between $\\v{p}$ and $\\v{r}$ is defined as\n",
        "$D(\\v{p},\\v{r})=\\suml_{j=1}^np_j\\log_2(\\frac{1}{r_j})-\\suml_{j=1}^np_j\\log_2(\\frac{1}{p_j})$\n",
        "$=-\\suml_{j=1}^np_j\\log_2(\\frac{r_j}{p_j})$.\n",
        "If $\\v{p}=\\v{r}$ then $D(\\v{p},\\v{r})=0$.\n",
        "Prove $D(\\v{p},\\v{r})‚â•0$.\n",
        "Let $Y$ take on values\n",
        "$\\frac{r_1}{p_1},...,\\frac{r_n}{p_n}$\n",
        "with probabilities $p_1,...,p_n$,\n",
        "then $\\E[Y]=r_1+...+r_n=1$.\n",
        "Then $D(\\v{p},\\v{r})=-\\E[\\log_2(Y)]$\n",
        "$‚â•-\\log_2(\\E[Y])=0$.\n",
        "\n",
        "- 10.1.13: Let $Z‚àº\\Normal(0,1)$. Find upperbound for $P(|Z|>3)=0.003$.\n",
        "  - $\\E|Z|=‚à´_{-‚àû}^‚àû\\overbrace{|z|œï(z)}^{\\t{even}}\\ dz$\n",
        "  $=\\frac{2}{\\sqrt{2œÄ}}‚à´_0^‚àûze^{-z^2/2}\\ dz$\n",
        "  $=\\frac{2}{\\sqrt{2œÄ}}[e^{-z^2/2}]_0^‚àû$\n",
        "  $=\\sqrt{\\frac{2}{œÄ}}$.\n",
        "  - Markov: $P(|Z|>3)‚â§\\frac{\\E|Z|}{3}=0.266$\n",
        "  - Chebyshev: $P(|Z|>3)‚â§\\frac{1}{3^2}=0.111$\n",
        "  - Chernoff: $P(|Z|>3)=2P(Z>3)‚â§2\\frac{\\E[e^{tZ}]}{e^{3t}}$\n",
        "  $=2\\e{\\frac{t^2}{2}-3t}$.\n",
        "  Optimize $\\frac{d}{dt}2\\e{\\frac{t^2}{2}-3t}$\n",
        "  $=2(t-3)\\e{\\frac{t^2}{2}-3t}$\n",
        "  $‚áít=3$.\n",
        "  Therefore $P(|Z|>3)‚â§2\\e{\\frac{9}{2}-9}$\n",
        "  $=2e^{-9/2}=0.022$\n",
        "\n",
        "- 10.2.5 (Monte Carlo integration): Find $‚à´_a^bf(x)\\ dx$ provided $f(x)‚â§c$.\n",
        "Let $(X_1,Y_1),...,(X_n,Y_n)$ be iid $X_i‚àº\\Unif(a,b),Y_i‚àº\\Unif(0,c)$.\n",
        "Then $‚à´_a^bf(x)\\ dx‚âàc(b-a)\\frac{1}{n}\\suml_{i=1}^nI_{Y_i‚â§f(X_i)}$.\n",
        "\n",
        "- 10.2.6 (Empirical CDF): Let $X_1,...,X_n$ be iid with CDF $F$. The empirical CDF $\\hat{F}_n(x)=\\frac{1}{n}\\suml_{i=1}^nI_{X_i‚â§x}$ is calculated on observed values of $X_1,...,X_n$, which are treated as discrete random variables and each observed value has probability $\\frac{1}{n}$.\n",
        "\n",
        "- 10.3.7: Each day a volatile stock price either rises 70% or drops 50% independently with equal probabilities. Let $Y_n$ be stock price after $n$ days starting from $Y_0=100$. Find $\\ln Y_n$.\n",
        "Let $U_n‚àº\\Binom(n,\\frac{1}{2})$ be the number of up days, then\n",
        "$U_n\\arr{d}\\Normal(\\frac{n}{2},\\frac{n}{4})$.\n",
        "$Y_n=Y_0(1.7)^{U_n}(0.5)^{n-U_n}$\n",
        "$=Y_0(1.7√ó2)^{U_n}(0.5)^n$\n",
        "$‚áí\\ln Y_n=\\ln Y_0+U_n\\ln 3.4-n\\ln 2$\n",
        "$\\arr{d}\\Normal(n(\\frac{1}{2}-\\ln 2+\\frac{\\ln 3.4}{2})+\\ln Y_0,\\frac{n(\\ln 3.4)^2}{4})$\n",
        "\n",
        "- 10.1: Survey with replacement sample size $n$. Let $\\hat{p},p$ be survey/population proportion in support. Show $P(|\\hat{p}-p|>c)‚â§\\frac{1}{4nc^2}$ for every $c>0$.\n",
        "$\\hat{p}‚àº(p,\\frac{p(1-p)}{n})$.\n",
        "By Chebyshev $P(|\\hat{p}-p|>c)‚â§\\frac{\\Var(\\hat{p})}{c^2}$\n",
        "$=\\frac{p(1-p)}{nc^2}$.\n",
        "$\\frac{d}{dp}\\frac{p-p^2}{nc^2}=\\frac{1-2p}{nc^2}=0$\n",
        "$‚áíp=\\frac{1}{2}$ gives maximum bound.\n",
        "\n",
        "- 10.2: $X_1,...,X_n‚àº(Œº,œÉ^2)$ iid. Find $n$ such that $\\bar{X}_n$ is within 2 stdev of $Œº$ with 99% chance.\n",
        "$\\bar{X}_n‚àº(Œº,\\frac{œÉ^2}{n})$.\n",
        "By Chebyshev $P(|\\bar{X}_n-Œº_{\\bar{X}_n}|>c)‚â§\\frac{œÉ_{\\bar{X}_n}^2}{c^2}$.\n",
        "Let $c=2œÉ$.\n",
        "$P(|\\bar{X}_n-Œº|>2œÉ)‚â§\\frac{œÉ_{\\bar{X}_n}^2}{4œÉ^2}$\n",
        "$=\\frac{1}{4n}‚â§\\frac{1}{100}$\n",
        "$‚áín‚â•25$.\n",
        "\n",
        "- 10.3: Let $X,Y$ be positive with neither a constant multiple of the other. Show $\\E[\\frac{X}{Y}]\\E[\\frac{Y}{X}]>1$.\n",
        "  - Cauchy-Schwarz: $\\E[\\sqrt{\\frac{X}{Y}}\\sqrt{\\frac{Y}{X}}]‚â§\\sqrt{\\E[\\frac{X}{Y}]\\E[\\frac{Y}{X}]}$.\n",
        "  - Jensen: Let $W=\\frac{X}{Y}$ and $g(x)=\\frac{1}{x}$.\n",
        "  $\\E[g(W)]>g(\\E[W])$\n",
        "  $‚áí\\E[\\frac{Y}{X}]>\\frac{1}{\\E[X/Y]}$\n",
        "\n",
        "- 10.4: Prove $\\frac{a_1+...+a_n}{n}‚â•(a_1...a_n)^{1/n}$.\n",
        "$\\ln \\frac{1}{n}\\suml_{i=1}^na_i‚â•\\frac{1}{n}\\suml_{i=1}^n\\ln a_i$\n",
        "Let $X=\\{a_1,...,a_n\\}$ with equal probability, then that is equivalent to\n",
        "$\\ln \\E[X]‚â•\\E[\\ln X]$, which is true according to Jensen's inequality for concave function $\\ln x$.\n",
        "\n",
        "- 10.5: Entropy $H(X)=\\suml_{k=0}^‚àûp_k\\log_2(\\frac{1}{p_k})$ where $p_k=P(X=x_k)$. Find $H(X)$ for $X‚àº\\QGeom(p)$. PMF $p_k=q^kp$.\n",
        "Then $H(X)=-\\suml_{k=0}^‚àûq^kp(k\\log_2(q)+\\log_2(p))$\n",
        "$=-\\log_2(q)\\suml_{k=0}^‚àûkq^kp$\n",
        "$-\\log_2(p)\\suml_{k=0}^‚àûq^kp$\n",
        "$=-\\frac{1-p}{p}\\log_2(1-p)-log_2(p)$\n",
        "\n",
        "- 10.22: Let $X=U_1+...+U_n$ where $U_i‚àº\\Unif(0,1)$ iid.\n",
        "$X\\arr{d}\\Normal(\\frac{n}{2},\\frac{n}{12})$.\n",
        "\n",
        "- 10.25: Let $X‚àº\\Expo(3),Y=e^X$.\n",
        "  - Find mean and variance of $Y$.\n",
        "  $|\\frac{dx}{dy}|=\\frac{1}{y}$.\n",
        "  $f_Y(y)=\\frac{1}{y}Œªe^{-Œª\\ln y}$\n",
        "  $=Œªy^{-(Œª+1)},\\ y‚â•1$.\n",
        "  $\\E[Y]=Œª‚à´_1^‚àûy^{-Œª}\\ dy$\n",
        "  $=\\frac{Œª}{1-Œª}[y^{-(Œª-1)}]_1^‚àû$\n",
        "  $=\\frac{Œª}{Œª-1}$.\n",
        "  $\\E[Y^2]=Œª‚à´_1^‚àûy^{-(Œª-1)}\\ dy$\n",
        "  $=\\frac{Œª}{Œª-2}$.\n",
        "  $\\Var(Y)=\\E[Y^2]-\\E[Y]^2$\n",
        "  $=\\frac{Œª}{(Œª-2)(Œª-1)^2}$\n",
        "  - For $Y_1,...,Y_n‚àºY$ iid, what is the distribution of $\\bar{Y}_n$?\n",
        "  $\\bar{Y}_n\\arr{d}\\Normal(\\frac{3}{2},\\frac{3}{4n})$\n",
        "\n",
        "- 10.26 (Stirling's formula): Let $Y=Y_1+...+Y_n$ where $Y_i‚àº\\Pois(1)$ iid.\n",
        "By CLT approximation $P(Y=n)‚âà\\frac{1}{\\sqrt{2œÄn}}\\overbrace{‚à´_{n-1/2}^{n+1/2} e^{-\\frac{(x-n)^2}{2n}}\\ dx}^{‚âà1}$.\n",
        "By Poisson $P(Y=n)=\\frac{e^{-n}(n)^n}{n!}$.\n",
        "Therefore $n!‚âà\\sqrt{2œÄn}(\\frac{n}{e})^n$."
      ],
      "metadata": {
        "id": "t4ht-DwPNYr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Chain Monte Carlo"
      ],
      "metadata": {
        "id": "pO_So0ejnd3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markov chain**: a sequence of random variables $X_0,X_1,X_2,...$ taking values in the **state space** $\\{1,2,...,M\\}$ is called a Markov chain if for all $n‚â•0$,\n",
        "$P(X_{n+1}=j|X_n=i,...,X_0=i_0)$ $=P(X_{n+1}=j|X_n=i)$. The Markov property says only the most recent $X_n$ term with transition probability from state $i$ to $j$ matters for predicting $X_{n+1}$.\n",
        "- Let $q_{ij}$ be the **transition probability** from state $i$ to $j$. Then matrix $Q_{M√óM}=(q_{ij})$ is called the **transition matrix**. $q_{ij}$ is found on row $i$ column $j$. Each row in $Q$ sums to 1 (**stochastic**). Row $i$ is the conditional PMF of $X_1|X_0=i$.\n",
        "  - Let PMF vector $\\v{t}:t_i=P(X_0=i)$ be initial PMF of $X_0$. Then PMF vector $\\v{t}Q$ is the marginal PMF of $X_1$ where $P(X_1=j)=\\suml_{i=1}^Mt_iq_{ij}$.\n",
        "- The **$n$-step transition probability** is the probability $q_{ij}^{(n)}=P(X_n=j|X_0=i)$.\n",
        "  - $q_{ij}^{(2)}=\\suml_{k}q_{ik}q_{kj}$ is the $(i,j)$ entry of $Q^2$.\n",
        "  - $q_{ij}^{(n)}$ is the $(i,j)$ entry of $Q^n$.\n",
        "  - The marginal PMF of $X_n$ is given by $\\v{t}Q^n$.\n",
        "- State $i$ is **recurrent** if starting from $i$ the chain will eventually return to $i$. State $i$ is *transient* if there is probability $p>0$ the chain never returns to $i$. Then the number of times the chain returns to $i$ before leaving forever is $\\QGeom(p)$.\n",
        "  - A chain is **irreducible** if you can go from any $i$ to any $j$ in finite steps with positive probability, which means there is a positive $n$ such that $(i,j)$ entry of $Q^n$ is positive. Irreducible chain implies all states are recurrent.\n",
        "  - the **period** of state $i$ is the gcd of the steps to start at $i$ and return to $i$, or the gcd of all $n$ such that $(i,i)$ entry of $Q^n$ is positive. The state is **periodic** if period>1. The chain is periodic if at least 1 state is periodic.\n",
        "  - In an irreducible chain, all states have the same period.\n",
        "  - A chain is irreducible and aperiodic iff $Q^m$ is positive in all entries for some $m$.\n",
        "- A PMF row vector $\\v{s}$ for transition matrix $Q$ is a **stationary distribution** if $s_j=\\suml_{i=1}s_iq_{ij}$ or $\\v{s}=\\v{s}Q$ meaning $X_0,X_1,...$ have the same PMF row vector. $\\v{s}$ is characteristic of $Q$ such that if the initial PMF is $\\v{s}$ then all subsequent PMFs are also $\\v{s}$.\n",
        "  - Given $n$ particles moving in a chain, then $\\v{s}$ is stationary distribution iff\n",
        "  $s_j=\\suml_is_iq_{ij}$\n",
        "  $=s_jq_{jj}+\\suml_{i\\neq j}s_iq_{ij}$\n",
        "  $‚áíns_j(1-q_{jj})=\\suml_{i\\neq j}ns_iq_{ij}$.\n",
        "  At each step particles exiting $j$ is equal to particles entering $j$ and the system as a whole appears stationary.\n",
        "  - $\\v{s}Q=Œª\\v{s}$ - here $\\v{s}$ is the left eigenvector of $Q$ with eigenvalue Œª=1. Or in the familiar form $Q^T\\v{s}^T=Œª\\v{s}^T$\n",
        "  - For 2√ó2, $\\BPM s & 1-s\\EPM\\BPM 1-a & a \\\\ b & 1-b\\EPM=\\BPM s & 1-s\\EPM$ gives $\\v{s}=\\BPM\\frac{a}{a+b} & \\frac{b}{a+b}\\EPM$.\n",
        "  - Perron-Frobenius theorem: For any irreducible chain, there exists one unique stationary distribution where every state has positive probability.\n",
        "  - For an irreducible aperiodic chain, $Q^n$ converges to a matrix where every row is its stationary distribution $\\v{s}$ as $n‚Üí‚àû$. After a large number of steps, $P(X_n=i)=s_i$.\n",
        "  - For an irreducible chain starting at $i$, the expected time it takes to return to $i$ is $r_i=\\frac{1}{s_i}$.\n",
        "- For transition matrix $Q$, if there exists PMF vector $\\v{s}$ such that the detailed balance condition $s_iq_{ij}=s_jq_{ji}$ holds for all $i,j$, then the chain is **reversible** with respect to $\\v{s}$.\n",
        "  - Given $n$ particles moving in a chain, the detailed balance condition requires that $ns_jq_{ji}=ns_iq_{ij}$. At each step particles leaving from $j$ to $i$ is equal to particles entering from $i$ to $j$.\n",
        "  - If $Q$ is reversible with respect to PMF vector $\\v{s}$ then $\\v{s}$ is a stationary distribution.\n",
        "  1. If $Q$ is **doubly stochastic** where each column also sums to 1 (e.g., symmetric matrix), then $Q$ is reversible with respect to stationary distribution $\\v{s}=\\{\\frac{1}{M},...,\\frac{1}{M}\\}$.\n",
        "  2. If the chain is on an undirected graph, then $Q$ is stochastically normalized adjacency matrix, and $\\v{d}$ has the degrees of nodes. Then $q_{ij}=\\frac{1}{d_i}$ and $d_iq_{ij}=d_jq_{ji}$. Normalize with $D=\\suml_{i}d_i$ then $\\v{s}=\\{\\frac{d_1}{D},...,\\frac{d_M}{D}\\}$ is a stationary distribution.\n",
        "  3. **Birth-death chain**: If the states are sequentially laid out in a line, and each step can only move 1 step left or 1 step right such that $q_{ij}>0$ only if $|i-j|=1$, then it is reversible: $s_1q_{12}=s_2q_{21}$ $‚áís_2=s_1\\frac{q_{12}}{q_{21}}$, then for each neighboring pair $s_j=s_{j-1}\\frac{q_{j-1,j}}{q_{j,j-1}}$ $=s_1\\frac{q_{12}...q_{j-1,j}}{q_{j,j-1}...q_{21}}$. Finally choose $s_1$ to stochastically normalize $\\v{s}$."
      ],
      "metadata": {
        "id": "5TXMFtBYng6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PageRank**: Let $M$ be the number of pages. Construct $Q_{M√óM}$ from links between pages. If a page has no links it has uniformly distributed row $\\frac{1}{M}$. The stationary distribution $\\v{s}$ is the measure of importance of all pages. To ensure irreducible aperiodic chain, the Google transition matrix $G=Œ±Q+(a-Œ±)\\frac{J}{M}$, where $J_{M√óM}$ is filled with 1, guarantees the existence of a unique stationary distribution $\\v{s}$ called PageRank.\n",
        "  - Higher $Œ±$ respects the original structure, lower $Œ±$ make the chain faster to converge. The original Google PageRank uses 0.85.\n",
        "  - To calculate $\\v{s}$ without solving eigenvector, use starting distribution $\\v{t}$: $\\v{t}G^n‚Üí\\v{s}$ as $n‚Üí‚àû$. Iterate $\\v{t}G=Œ±(\\v{t}Q)+\\frac{1-Œ±}{M}(\\v{t}J)$."
      ],
      "metadata": {
        "id": "xqaTTsqxOu4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metropolis-Hastings MCMC**: Let $\\v{s}$ be a desired stationary distribution where $s_i>0$ for all $i$ in the state space. We initially choose a proposal transition matrix $P=(p_{ij})$ to traverse the state space.\n",
        "- Simulation (MCMC): at each step $X_n=i$, make a proposal for $X_{n+1}$ based on $p_{ij}$, and accept it with probability $a_{ij}=\\min(\\frac{s_jp_{ji}}{s_ip_{ij}}, 1)$. If accepted then $X_{n+1}=j$ otherwise $X_{n+1}=i$.\n",
        "  - The proposal distribution $p_{ij}$ represents probability density for proposing $j$ given $i$.\n",
        "  - The final chain $X_0,X_1,...$ would take on states at frequencies that are distributed according to $\\v{s}$ as $n‚Üí‚àû$.\n",
        "- For smaller state spaces, $Q$ can be deterministically computed without simulation by assigning $q_{ij}=p_{ij}a_{ij}$ so that $Q$ is reversible with respect to the desired $\\v{s}$.\n",
        "  - $P$ and $Q$ have the same directed graph structure. $a_{ij}>0$ (and therefore $q_{ij}>0$ and $q_{ji}>0$) iff $p_{ij}>0$ and $p_{ji}>0$. The Metropolis-Hastings algorithm only assigns in $Q$ the non-zero entries of $P$.\n",
        "  - If $s_jp_{ji}\\lt s_ip_{ij}$ more particles are moving to $j$ than coming back, then $a_{ij}=\\frac{s_jp_{ji}}{s_ip_{ij}}\\lt 1$ and $a_{ji}=1$. The acceptance probability biases against direction of particle flow defined in $P$ to ensure $Q$ observes detailed balance: $s_iq_{ij}=s_ip_{ij}\\frac{s_jp_{ji}}{s_ip_{ij}}$ $=s_jp_{ji}=s_j\\frac{q_{ji}}{a_{ji}}=s_jq_{ji}$.\n",
        "- Simulation is required in larger state spaces, computationally infeasible $Q$, or when direct sampling of $\\v{s}$ is hard. The proposal distribution $P$ determines acceptance rates. If acceptance rates are too low or too high then it will be slow to converge.\n",
        "- **Independence sampler**: continuous target stationary distribution and infinite state space drawn from $\\Unif(0,1)$. Because there are infinite number of states, there are no $P$, $Q$, and $\\v{s}$.\n",
        "  - Start $X_0$ with any value. At each step $X_n=w$, make a proposal for $X_{n+1}$ with a freshly drawn $u‚àº\\Unif(0,1)$ value. $p_{ij}$ is replaced by $p(w,u)=f_{U|X_n}(u|X_n=w)=f_U(u)=1$, which is probability density for proposing $w‚Üíu$ and is independent of $w$. Similarly $p(u,w)=1$. Target stationary distribution densities are $s(u)=f_s(u)$ and $s(w)=f_s(w)$. Acceptance probability is $a(w,u)=\\min(\\frac{s(u)p(u,w)}{s(w)p(w,u)},1)=\\min(\\frac{f_s(u)}{f_s(w)},1)$. If accepted $X_{n+1}=u$, otherwise $X_{n+1}=w$.\n",
        "  - The target stationary distribution is the PDF $f_s$. As $n‚Üí‚àû$, values held by the chain $X_0,...,X_n$ converge to stationary distribution. Acceptance depends on current state, therefore $X_0,...,X_n$ are correlated and not independent. A chain can get stuck in certain areas of the state space and cause **autocorrelation at lag k**, where $X_n$ and $X_{n+k}$ are correlated.\n",
        "  - The key feature of independence sampler is the proposal distribution $p(w,u)$ is independent of $w$. In practice beta and truncated normal are often used instead of uniform.\n",
        "- **Normal-normal conjugacy** (e12.1.8): Let $Y|Œ∏‚àº\\Normal(Œ∏,œÉ^2)$ where prior $Œ∏‚àº\\Normal(Œº,œÑ^2)$ is an unknown parameter and $œÉ^2,Œº,œÑ^2$ are known. We want to find posterior distribution $Œ∏|Y$ after observing values of $Y$.\n",
        "  - PDF $f_{Œ∏|Y}(Œ∏|y)=\\frac{f_{Y|Œ∏}(y|Œ∏)f_{Œ∏}(Œ∏)}{f_Y(y)}$\n",
        "  $‚àùf_{Y|Œ∏}(y|Œ∏)f_{Œ∏}(Œ∏)$\n",
        "  $‚àù\\e{-\\frac{(y-Œ∏)^2}{2œÉ^2}}\\e{-\\frac{(Œ∏-Œº)^2}{2œÑ^2}}$.\n",
        "  - The analytical solution $(Œ∏|Y=y)‚àº\\Normal(\\frac{y/œÉ^2+Œº/œÑ^2}{1/œÉ^2+1/œÑ^2},\\frac{1}{1/œÉ^2+1/œÑ^2})$ is hard to get for complicated distributions.\n",
        "  - Construct continuous state space Markov chain whose stationary distribution is $f_{Œ∏|Y}(Œ∏|y)$. Let $Œ∏_n=x$, propose $x'=x+œµ_n$ where we generate $œµ_n‚àº\\Normal(0,d^2)$ for some constant $d$. As such $p(x,x')=p(x',x)=f_{œµ_n}(x'-x)$ $=\\frac{1}{\\sqrt{2œÄ}d}e^{-(x'-x)^2/2d^2}$.\n",
        "  - Acceptance $a(x,x')=\\min(\\frac{s(x')p(x',x)}{s(x)p(x,x')},1)$, where $s(x)=f_{Œ∏|Y}(x|y)$ and we don't need to care about the normalizing constants. Therefore $a(x,x')=\\min(\\frac{f_{Œ∏|Y}(x'|y)}{f_{Œ∏|Y}(x|y)},1)$\n",
        "  $=\\min(\\frac{f_{Y|Œ∏}(y|x')f_{Œ∏}(x')}{f_{Y|Œ∏}(y|x)f_{Œ∏}(x)},1)$.\n",
        "  - If $d$ is too large then proposals are often rejected. If $d$ is too small then step sizes are too small and the trace plot looks like Brownian motion."
      ],
      "metadata": {
        "id": "HTw9eSXTuho9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gibbs sampler**: algorithms for joint distribution. Let $X,Y$ be discrete. Construct 2-D Markov chain $(X_0,Y_0),...,(X_n,Y_n)$ whose stationary distribution is $P(X=x,Y=y)$.\n",
        "- **Systematic scan**:  If current state is $(X_n,Y_n)=(x_n,y_n)$, draw $X_{n+1}=x_{n+1}$ from $(X|Y=y_n)$, then draw $Y_{n+1}=y_{n+1}$ from $(Y|X=x_{n+1})$.\n",
        "- **Random scan**: Choose which component to update with uniform probabilities. If X was chosen then draw $X_{n+1}=x_{n+1}$ from $(X|Y=y_n)$ and set $Y_{n+1}=y_n$. Similarly if Y was chosen then draw $Y_{n+1}=y_{n+1}$ from $(Y|X=x_n)$ and set $X_{n+1}=x_n$.\n",
        "  - Random scan Gibbs sampler is Metropolis-Hastings applied to joint distributions, which always accepts proposals because the stationary distribution cancels out the proposal probabilities in the acceptance ratio.\n",
        "  - Proof: For $(x,y)‚Üí(x,y')$, the proposal probabilities are\n",
        "  $p((x,y),(x,y'))=\\frac{1}{2}P(Y=y'|X=x)$ and\n",
        "  $p((x,y'),(x,y))=\\frac{1}{2}P(Y=y|X=x)$.\n",
        "  The acceptance probability is\n",
        "  $\\frac{P(X=x,Y=y')\\frac{1}{2}P(Y=y|X=x)}{P(X=x,Y=y)\\frac{1}{2}P(Y=y'|X=x)}$\n",
        "  $=\\frac{P(Y=y'|X=x)P(X=x)\\frac{1}{2}P(Y=y|X=x)}{P(Y=y|X=x)P(X=x)\\frac{1}{2}P(Y=y'|X=x)}$\n",
        "  $=1$.\n",
        "  For $(x,y)‚Üí(x',y)$, the acceptance probability is also 1."
      ],
      "metadata": {
        "id": "WAnSYxEGE-xC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blitzstein Chapter 12: Markov chain Monte Carlo\n",
        "\n",
        "- e12.1.3 (Zipf): Let $M‚â•2,a>0$, then Zipf PMF $P(X=k)=\\frac{1/k^a}{\\sum_{j=1}^M(1/j^a)}$. Create a Markov chain $X_0,X_1,...$ whose stationary distribution is Zipf distribution and follows constraint $|X_{n+1}-X_n|‚â§1$.\n",
        "  - The proposal chain $P$ is birth-death chain because of the $|X_{n+1}-X_n|‚â§1$ constraint. Construct a symmetric chain so that $p_{ij}=p_{ji}=\\frac{1}{2}$, with the two ends self looping to maintain $\\frac{1}{2}$\n",
        "  - Unnormalized $\\v{s}=\\{1,\\frac{1}{2^a},...,\\frac{1}{M^a}\\}$. The normalizing constants $\\sum_{j=1}^M(1/j^a)$ cancel out in $a_{ij}$ computations.\n",
        "  - Acceptance rate $a_{ij}=\\min(\\frac{s_jp_{ji}}{s_ip_{ij}},1)$\n",
        "  $=\\min(\\frac{i^a}{j^a},1)$\n",
        "\n",
        "- e12.1.4 (**beta independence sampler**): generate $W‚àº\\Beta(a,b)$ using iid $\\Unif(0,1)$ components, then approximate with Markov chain on state space $(0,1)$.\n",
        "  - Applying universality of uniform on beta is hard, but it's easier through $X‚àº\\Gamma(a,1)$ and $Y‚àº\\Gamma(b,1)$ such that $W=\\frac{X}{X+Y}$. Each component $X_i‚àº\\Expo(1)$ has CDF $F_{X_i}(a)=1-e^{-a}$ and $F_{X_i}^{-1}(x)=-\\ln(1-x)$. Therefore $X_i=-\\ln(1-U_i)$.\n",
        "  - Use independence sampler with infinite state space drawn from uniform(0,1). Let $W_0=0.5$. At $W_n=w$, propose a freshly drawn $u‚àº\\Unif(0,1)$. Acceptance probability $a(w,u)=\\min(\\frac{u^{a-1}(1-u)^{b-1}}{w^{a-1}(1-w)^{b-1}},1)$. If accepted $W_{n+1}=u$ otherwise $W_n=w$.\n",
        "\n",
        "- e12.1.6 (substitution cypher): Let $g$ be a substitution cypher that maps normal alphabet sequence a-z to a different permutation.\n",
        "  - Use a Markhov chain where the state space are all possible substitution permutations ($g$ is one such state), and every transition swaps two random positions. As such $p_{gh}=p_{hg}=\\frac{1}{\\binom{26}{2}}$ if states $g$ and $h$ differ only at two positions.\n",
        "  - Let $s(g)$ be a positive score to measure how likely it is to be an exact match with the enciphered text. Then $a_{gh}=\\min(\\frac{s(h)}{s(g)},1)$ and $q_{gh}=p_{gh}a_{gh}$.\n",
        "\n",
        "- e12.1.7 (knapsack): A burglar can maximally carry $w$ weight, and there are $m$ treasures with weights $(w_1,...,w_m)$ and values $(v_1,...,v_m)$. The burglar's inventory is $(x_1,...,x_2)$ where $x_i$ is indicator for whether treasure $i$ was selected.\n",
        "  - The state space $C$ is all inventory combinations that satisfy the weight requirement $\\suml_{i}w_iy_i‚â§w$. Start at $(0,...,0)$. At $X_n=x=(x_1,...,x_m)$, propose $y$ such that a random inventory entry in $x$ is toggled. If $y‚ààC$ then $p_{xy}=p_{yx}=\\frac{1}{m}$ otherwise $p_{xy}=p_{yx}=0$.  \n",
        "  - To optimize for inventory value $V(x)=\\suml_{i}x_iv_i$, create an exponential stationary distribution that emphasizes states with higher values: $s(x)‚àùe^{Œ≤V(x)}$ for some constant $Œ≤$. Therefore $a_{xy}=\\min(e^{Œ≤V(y)-Œ≤V(x)},1)$.\n",
        "  - If $Œ≤$ is high, then convergence is slow and the chain may be stuck at local maxima. If $Œ≤$ is low, then the chain may be much more advanturous but has no incentive to converge to global maxima. **Simulated annealing** (like in stochastic gradient descent) uses small values of $Œ≤$ at first to allow more thorough exploration of the state space, then gradually increasing $Œ≤$ to allow better concentration of the best solutions.\n",
        "\n",
        "- e12.2.6 (chicken-egg binomial-Poisson): a chicken lays $N‚àº\\Pois(Œª)$ eggs, each hatching with unknown probability $p‚àº\\Beta(a,b)$. We only observe $X$ the number of hatchlings. Find $\\E[p|X]$.\n",
        "  - $X|p‚àº\\Pois(pŒª)$. The posterior\n",
        "  $f(p|X=x)=\\frac{P(X=x|p)f_0(p)}{P(X=x)}$\n",
        "  $‚àùe^{-pŒª}(pŒª)^xp^{a-1}q^{b-1}$ is not a known distribution. If $N$ is known then\n",
        "  $(X|p,N=n)‚àº\\Binom(n,p)$ and $(p|X=x,N=n)‚àº\\Beta(a+x,b+n-x)$ by binomial-beta conjugacy and expectation is computed easily.\n",
        "  - To find $N$, we use Gibbs sampling to construct a 2-D chain on $(N,p)$ where we alternately sample $(p|N,X)$ and $(N|p,X)$. At each step $X_j=(n_j,p_j)$, draw $p_{j+1}‚àº\\Beta(a+x,b+n-x)$ and then draw $y‚àº\\Pois((1-p_{j+1})Œª)$ and $n_{j+1}=x+y$. After many cycles we have stationary distributions of both $p$ and $N$."
      ],
      "metadata": {
        "id": "-W0tmhh2n57N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Casella Probability\n"
      ],
      "metadata": {
        "id": "T9ymKH9r-Uc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Differentiating under integral and summation: (Casella 2.4)\n",
        "- **Leibniz rule**: $\\/{d}{dŒ∏}‚à´_{a(Œ∏)}^{b(Œ∏)}f(x,Œ∏)\\ dx=f(b(Œ∏),Œ∏)\\/{d}{dŒ∏}b(Œ∏)-f(a(Œ∏),Œ∏)\\/{d}{dŒ∏}a(Œ∏)+‚à´_{a(Œ∏)}^{b(Œ∏)}\\/{‚àÇ}{‚àÇŒ∏}f(x,Œ∏)\\ dx$\n",
        "  - If $a,b$ are constant then $\\/{d}{dŒ∏}‚à´_a^bf(x,Œ∏)\\ dx=‚à´_a^b\\/{‚àÇ}{‚àÇŒ∏}f(x,Œ∏)\\ dx$\n",
        "  - For $\\/{d}{dŒ∏}‚à´_{-‚àû}^{‚àû}f(x,Œ∏)\\ dx=\\liml_{Œ¥‚Üí0}‚à´_{-‚àû}^{‚àû}\\/{f(x,Œ∏+Œ¥)-f(x,Œ∏)}{Œ¥}\\ dx$, the order of limits and integration will be interchanged, which requires Lebesgue's dominated convergence theorem (Casella 2.4.2 and 2.4.3)\n",
        "- $\\/{d}{dŒ∏}‚à´_{-‚àû}^{‚àû}f(x,Œ∏)\\ dx=‚à´_{-‚àû}^{‚àû}\\/{‚àÇ}{‚àÇŒ∏}f(x,Œ∏)\\ dx$ if $f(x,Œ∏)$ is differentiable wrt $Œ∏$ and there exists $g(x,Œ∏)$ and $Œ¥_0>0$ satisfying\n",
        "  1. $|\\/{‚àÇ}{‚àÇŒ∏}f(x,Œ∏)|_{Œ∏=Œ∏'}‚â§g(x,Œ∏)$ for all $Œ∏'$ such that $|Œ∏'-Œ∏|‚â§Œ¥_0$. The derivatives for all $Œ∏'$ in the neighborhood around $Œ∏$ are dominated and don't blow up $|\\/{‚àÇ}{‚àÇŒ∏}f(x,Œ∏)|‚â§g(x,Œ∏)$.\n",
        "  2. $‚à´_{-‚àû}^{‚àû}g(x,Œ∏)\\ dx\\lt ‚àû$. (Casella 2.4.4)\n",
        "- A derivative can always be taken inside a finite sum due to linearity property.\n",
        "- Let the infinite series $\\sum_{x=0}^‚àûh(Œ∏,x)$ converge (pointwise) for all $Œ∏‚àà(a,b)$ where $a,b‚àà‚Ñù$, then $\\/{d}{dŒ∏}\\sum_{x=0}^‚àûh(Œ∏,x)=\\sum_{x=0}^‚àû\\/{d}{dŒ∏}h(Œ∏,x)$ if\n",
        "  1. $\\/{d}{dŒ∏}h(Œ∏,x)$ is continuous in $Œ∏$ for each $x$,\n",
        "  2. $\\sum_{x=0}^‚àû\\/{d}{dŒ∏}h(Œ∏,x)$ converges uniformly on every closed bounded subinterval of $(a,b)$. Meaning for every $œµ>0$ there exists $N_œµ‚ààùïÄ$ such that $|\\sum_{x=n}^‚àû\\/{d}{dŒ∏}h(Œ∏,x)|\\lt œµ$ for all $n‚â•N$ and all $Œ∏$ simultaneously in the subinterval. (Casella 2.4.8)\n",
        "- Let the infinite series $\\sum_{x=0}^‚àûh(Œ∏,x)$ converge uniformly on $[a,b]$ and that $h(Œ∏,x)$ is continuous in $Œ∏$ for each $x$, then $‚à´_a^b\\sum_{x=0}^‚àûh(Œ∏,x)\\ dŒ∏=\\sum_{x=0}^‚àû‚à´_a^bh(Œ∏,x)\\ dŒ∏$. (Casella 2.4.10)\n"
      ],
      "metadata": {
        "id": "_unIDm6Q_V_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample space** of an experiment is set $S$ of all possible outcomes. (Casella 1.1.1)\n",
        "- **Event** is a collection of possible outcomes and is any subset of $S$. Events are closed under set operations and rules like unions, intersections, complements, communitivity, associativity, distributivity, and De Morgan's.\n",
        "  - $A$ and $B$ are disjoint (mutually exclusive) if $A‚à©B=‚àÖ$. Pairwise disjoint sets can fully partition $S$.\n",
        "- **Sigma algebra** ($\\m{B}$) is a collection of subsets of $S$ and follow 3 properties: (Casella 1.2.1)\n",
        "  1. $‚àÖ‚àà\\m{B}$.\n",
        "  2. If $A‚àà\\m{B}$ then $A^c‚àà\\m{B}$.\n",
        "  3. If $A_1,A_2,...‚àà\\m{B}$ then $\\bigcup_{i=1}^‚àû A_i‚àà\\m{B}$.\n",
        "- **Probability function** $P$ is a function whose domain is the sigma algebra $\\m{B}$ on sample space $S$, and it satisfies 3 Kolmogorov's Axioms of probability:\n",
        "  1. $P(A)‚â•0$ for all $A‚àà\\m{B}$.\n",
        "  2. $P(S)=1$.\n",
        "  3. If $A_1,A_2,...‚àà\\m{B}$ are pairwise disjoint, then $P(\\bigcup_{i=1}^‚àû A_i)=\\sum_{i=1}^‚àû P(A_i)$.\n",
        "  - Let $S=\\{s_1,...,s_n\\}$ be a countable set and $\\m{B}$ be a sigma algebra of $S$. Let $p_1,...,p_n$ be nonnegative numbers that sum to 1. For any $A‚àà\\m{B}$ define $P(A)=\\sum_{i:s_i‚ààA}p_i$, then $P$ is a probability function on $\\m{B}$.\n",
        "- If $P$ is a probability function and $A,B‚àà\\m{B}$ then\n",
        "  1. $P(‚àÖ)=0$.\n",
        "  2. $P(A)‚â§1$.\n",
        "  3. $P(A^c)=1-P(A)$.\n",
        "  4. $P(B)=P(B‚à©A)+P(B‚à©A^c)$.\n",
        "  5. $P(A‚à™B)=P(A)+P(B)-P(A‚à©B)$.\n",
        "  6. If $A‚äÇB$ then $P(A)‚â§P(B)$.\n",
        "  7. $P(A)=\\sum_{i=1}^‚àû P(A‚à©C_i)$ for any partition $C_1,C_2,...$\n",
        "  8. $P(\\bigcup_{i=1}^‚àû A_i)‚â§\\sum_{i=1}^‚àûP(A_i)$ for any sets $A_1,A_2,...$ (**Bonferroni inequality**)\n",
        "  9. $P(\\bigcup_{i=1}^‚àû A_i)=\\sum_{i=1}^nP(A_i)-\\sum_{i\\lt j}^nP(A_i‚à©A_j)+\\sum_{i\\lt j\\lt k}^nP(A_i‚à©A_j‚à©A_k)-...$ for any sets $A_1,A_2,...$ (Inclusion-exclusion Casella 1.8.1)\n",
        "- **Conditional probability**: If $A,B$ are events in $S$ and $P(B)>0$ then $P(A|B)=\\/{P(A‚à©B)}{P(B)}$. (Casella 1.3.2)\n",
        "  - $P(A‚à©B)=P(A|B)P(B)=P(B|A)P(A)$.\n",
        "  - LOTP: $P(B)=\\sum_{j=1}^‚àûP(B|A_j)P(A_j)$\n",
        "  - **Baye's rule**: $P(A_i|B)=\\/{P(B|A_i)P(A_i)}{\\sum_{j=1}^‚àûP(B|A_j)P(A_j)}$\n",
        "  - $A,B$ are **independent** if $P(A‚à©B)=P(A)P(B)$.\n",
        "  - $P(A|B)=P(A)$ and $P(B|A)=P(B)$.\n",
        "  - $(A,B^c)$, $(A^c,B)$, and $(A^c,B^c)$ are each also independent.\n",
        "  \n",
        "- **Notations**: $P_Œ∏(.)$ is parameterized probability, which ultimately produces an expression with $Œ∏$ as a parameter instead of a numeric value. It is sometimes written as $P(.|Œ∏)$ to mean the same.\n",
        "  - Expression $P_Œ∏(\\v{X}‚ààR)$ implies parameter $Œ∏$ is used in the underlying model.\n",
        "  - $P_{Œ∏_0}(.)$ means passing $Œ∏_0$ as the value of the parameter into the underlying model. (Chapter 8)\n",
        "  - $P_{H_0}$ means \"probability under $H_0$\". The parameter values under the assertions of $H_0$ are used in the probability computation. (Chapter 8)\n",
        "  - $\\sup_{Œ∏‚ààŒò_0}P_Œ∏(.)$ means the greatest value of $P_Œ∏(.)$ while limiting values of $Œ∏$ to the set $Œò_0$. (Chapter 8)"
      ],
      "metadata": {
        "id": "eHm3iM-6-Zhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamental theorem of counting**: If a job consists of $k$ separate tasks, the $i$th of which can be done $n_i$ ways, then the job can be done in $n_1√ó...√ón_k$ ways. (Casella 1.2.14)\n",
        "- Ordered, without replacement choosing $k$ times from $n$ options: $\\/{n!}{(n-k)!}$\n",
        "- Ordered, with replacement choosing of $k$ times from $n$ options: $n^k$\n",
        "  - A set of $k$ items made up of $m$ types, where type $i$ appears $k_i$ times and $\\sum_{i=1}^mk_i=k$ has $\\/{k!}{k_1!...k_m!}$ ordered permutations. E.g., MISSISSIPPI has $m=4,k=11$. The total number of ordered permutations is\n",
        "  $\\suml_{k_1+...+k_m=k}\\/{k!}{k_1!...k_m!}=m^k$.\n",
        "- Unordered, without replacement choosing of $k$ times from $n$ options: $\\/{n!}{(n-k)!k!}=\\binom{n}{k}$\n",
        "- Unordered, with replacement choosing of $k$ times from $n$ options: $\\binom{n+k-1}{k}$ (Casella 1.2.19/1.2.20): outcomes are not equally likely. The formula is useful for enumerating the outcomes, but ordered outcomes must be counted to calculate probabilities.\n",
        "  - The number of size $k$ sets drawn from $m$ types with replacement is $\\suml_{k_1+...+k_m=k}1=\\binom{k+m-1}{k}$ where type $i$ appears $k_i$ times. If each type is drawn with uniform probability, then not all sets are equally likely."
      ],
      "metadata": {
        "id": "X-LiMcFTMDpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random variable** is a function from sample space $S$ into real numbers $‚Ñù$.\n",
        "(Casella 1.4.1)\n",
        "- For sample space $S=\\{s_1,...,s_n\\}$ with probability function $P$, we define random variable $X$ with range $\\m{X}=\\{x_1,...,x_m\\}$ and probability function $P_X(X=x_i)$ on $\\m{X}$. We observe $X=x_i$ iff there exists outcome $s_j‚ààS$ such that $X(s_j)=x_i$. Therefore $P_X(X=x_i)=P(s_j‚ààS: X(s_j)=x_i)$ meets the axioms of probability.\n",
        "  - $\\m{X}$ is also referred to as the sample space of $X$: all the values that $X$ can take.\n",
        "  (Casella 2.1)\n",
        "  - $\\m{X}$ is also referred to as the support of $X$: all values of $X$ where PMF/PDF is positive.\n",
        "  (Casella 2.1)\n",
        "- The **cumulative distribution function** or CDF $F_X(x)=P_X(X‚â§x)$ holds 3 properties:\n",
        "(Casella 1.5.1)\n",
        "  1. $\\lim_{x‚Üí-‚àû}F(x)=0$ and $\\lim_{x‚Üí‚àû}F(x)=1$.\n",
        "  2. $F(x)$ is nondecreasing.\n",
        "  3. Right-continuous: $\\lim_{x‚Üìx_0}F(x)=F(x_0)$ due to the $‚â§$ sign. $F$ is continuous at $x_0$ when approached from the right and takes the higher value if there's a jump at $x_0$.\n",
        "- $X$ is **continuous** if $F_X(x)$ is a continuous, and **discrete** if $F_X(x)$ is a step function of $x$.\n",
        "(Casella 1.5.7)\n",
        "- $F_X(x)$ completely determines probability distribution of $X$.\n",
        "  - Let $\\m{B}^1$ be the smallest sigma algebra containing all intervals of the form (a,b), (a,b], [a,b), and [a,b] on $‚Ñù$, then $X$ and $Y$ are **identically distributed** if $P(X‚ààA)=P(Y‚ààA)$ for every $A‚àà\\m{B}^1$.\n",
        "  - $X$ and $Y$ are identically distributed iff $F_X(x)=F_Y(x)$ for every $x$.\n",
        "- The **probability mass function** or PMF for discrete $X$ is nonnegative function $f_X(x)=P(X=x)$.\n",
        "  - $f_X(x)$ is a pmf iff $\\sum_xf_X(x)=1$.\n",
        "- The **probability density function** or PDF for continuous $X$ is nonnegative function $f_X(x)$ that satisfies $F_X(x)=‚à´_{-‚àû}^xf_X(t)\\ dt$.\n",
        "  - $f_X(x)$ is a pdf iff $‚à´_{-‚àû}^‚àûf_X(x)\\ dx=1$.\n",
        "\n",
        "**Probability integral transform (universality of uniform)**: Let $Y=F_X(X)$ where $F_X$ is the CDF of $X$, then $Y‚àº\\Unif(0,1)$\n",
        "(Casella 2.1.10)\n",
        "- If $X$ is discrete and takes value $x_1< ...< x_n$ with CDF $F_X(x_i)=\\sum_{j‚â§i}P(X=x_j)$, define\n",
        "$U=F_X(X)‚àà\\{F_X(x_1), ..., F_X(x_n)\\}$, then\n",
        "the CDF $F_U(x)‚â§x$ looks like a step function that hugs the diagonal from below.\n",
        "\n",
        "**Transformation** $Y=g(X)$ defines a mapping between two sample spaces\n",
        "$g(x):\\m{X}‚Üí\\m{Y}$.\n",
        "PMF and PDF are positive only on these sets and 0 everywhere else, therefore these sets are **support sets**\n",
        "$\\m{X}=\\{x:f_X(x)>0\\}$ and\n",
        "$\\m{Y}=\\{y:y=g(x)\\t{ for some }x‚àà\\m{X}\\}$, and 0 everywhere else.\n",
        "(Casella 2.1)\n",
        "- If $g$ is monotonous, then $\\m{X}‚Üí\\m{Y}$ is one-to-one (each $x$ maps to only one $y$, and each $y$ comes from at most one $x$) and onto (each $y$ has one $x$ such that $y=g(x)$), and $g^{-1}$ is single-valued.\n",
        "  - If $g$ is increasing then\n",
        "  $F_Y(y)=\\{x‚àà\\m{X}:g(x)‚â§y\\}$\n",
        "  $=\\{x‚àà\\m{X}:g^{-1}(g(x))‚â§g^{-1}(y)\\}$\n",
        "  $=\\{x‚àà\\m{X}:x‚â§g^{-1}(y)\\}$. Therefore\n",
        "  $F_Y(y)=F_X(g^{-1}(y)),\\ y‚àà\\m{Y}$ and\n",
        "  $f_Y(y)=f_X(g^{-1}(y))\\/{d}{dy}g^{-1}(y)$\n",
        "  - If $g$ is decreasing then\n",
        "  $F_Y(y)=\\{x‚àà\\m{X}:g(x)‚â§y\\}$\n",
        "  $=\\{x‚àà\\m{X}:g^{-1}(g(x))‚â•g^{-1}(y)\\}$\n",
        "  $=\\{x‚àà\\m{X}:x‚â•g^{-1}(y)\\}$. Therefore\n",
        "  $F_Y(y)=1-F_X(g^{-1}(y)),\\ y‚àà\\m{Y}$ and\n",
        "  $f_Y(y)=-f_X(g^{-1}(y))\\/{d}{dy}g^{-1}(y)$\n",
        "- Let $Y=g(X)$ where $g$ is monotonous, then\n",
        "$f_Y(y)=f_X(g^{-1}(y))|\\/{d}{dy}g^{-1}(y)|,\\ y‚àà\\m{Y}$.\n",
        "(Casella 2.1.5)\n",
        "  - **Summation of preimages**: Let $A_1,...,A_k$ be a partition of $\\m{X}$ such that\n",
        "  $f_X(x)$ is continuous on each $A_i$, $g_1(x),...,g_k(x)$ is monotonous on each $A_i$, $g(x)=g_i(x),\\ x‚ààA_i$, and $\\/{d}{dy}g_i^{-1}(y)$ is continuous on $\\m{Y}$, then\n",
        "  $\\blue{f_Y(y)=\\sum_{i=1}^kf_X(g_i^{-1}(y))|\\/{d}{dy}g_i^{-1}(y)|,\\ y‚àà\\m{Y}}$.\n",
        "  (Casella 2.1.8)\n",
        "  - For $U=|X|$ or $U=X^2$, each has 2 preimages $X=\\red{¬±}U$ and $X=\\red{¬±}\\sqrt{U}$\n",
        "\n",
        "**Expectation**: mean or expected value $\\E g(X)=\\BC\n",
        "‚à´_{x‚àà\\m{X}}g(x)f_X(x)\\ dx \\\\\n",
        "\\sum_{x‚àà\\m{X}}g(x)P(X=x)\n",
        "\\EC$\n",
        "(Casella 2.2)\n",
        "- Linearity\n",
        "$\\E (ag_1(X)+bg_2(Y)+c)=a\\E g_1(X)+b\\E g_2(Y)+c$\n",
        "for any functions $g_1,g_2$ and constants $a,b,c$\n",
        "- $\\min_{b}\\E(X-b)^2=\\E(X-\\E X)^2$\n",
        "  - Proof: $\\E(X-b)^2=\\E (X-\\E X+\\E X-b)^2$\n",
        "  $=\\E(X-\\E X)^2$\n",
        "  $+\\ub{\\E(\\E X-b)^2}{0 \\t{ if }b=\\E X}$\n",
        "  $+\\ub{2\\E((X-\\E X)(\\E X-b))}{0 \\t{ if }b=\\E X}$\n",
        "- $\\E g(X)=\\BC\n",
        "‚à´_{x‚àà\\m{X}}g(x)f_X(x)\\ dx &\\t{integral may be hard} \\\\\n",
        "‚à´_{y‚àà\\m{Y}}yf_Y(y)\\ dy &Y=g(X)\n",
        "\\EC$\n",
        "- **$n$th moment** of $X$ is $Œº_n'=\\E X^n$.\n",
        "(Casella 2.3)\n",
        "- **Variance** is the second central moment $\\E (X-\\E X)^2$.\n",
        "  - $\\Var X=\\E X^2-(\\E X)^2$\n",
        "  - $\\Var(aX+b)=a^2\\Var X$ for constants $a,b$.\n",
        "  \n",
        "- $\\E_Œ∏$ is parameterized expectation, which treats $Œ∏$ as a constant and produces an expression as a function of $Œ∏$. The value of $Œ∏$ is also passed to the parameter in the underlying distribution.\n",
        "\n",
        "- **Moment generating function** of $X$ is $M_X(t)=\\E e^{tX}$ provided the expectation exists and is finite in some open interval around $t=0$. Then $\\E X^n=M_X^{(n)}(0)$. $M_X(t)$ is the Laplace transform of $f_X(x)$.\n",
        "  - MGF $M_X(t)=\\sum_{n=0}^‚àû\\/{t^n}{n!}\\E X^n$ is an analytic function with a radius of convergence in an open interval around $t=0$ (therefore derivatives) and therefore always uniquely determines a distribution. Having infinite set of moments is like MGF existing at $t=0$ but without the open interval, and is too weak to always uniquely characterize a distribution: two distinct random variables may share the same moments.\n",
        "  (Casella 2.3.10)\n",
        "  - $F_X(x)=F_Y(x)$ for all $x$ iff $M_X(t)=M_Y(t)$, just as Laplace transform uniquely determines $f_X(x)$.\n",
        "  - Let $X,Y$ have bounded support, then $F_X(x)=F_Y(x)$ for all $x$ iff $\\E X^r=\\E Y^r$ for all integers $r$.\n",
        "  - If $X_1,X_2,...$ have MGFs $M_{X_i}(t)$ such that $\\liml_{i‚Üí‚àû}M_{X_i}(t)=M_X(t)$ for all $t$ in open interval around $t=0$, then $\\liml_{i‚Üí‚àû}F_{X_i}(t)=F_X(t)$. Convergence of MGFs implies convergence of CDFs.\n",
        "  - $M_{aX+b}(t)=e^{bt}M_X(at)$"
      ],
      "metadata": {
        "id": "fP9iRy8UToAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discrete uniform**: $X‚àº\\DUnif(1,N)$ if $P(X=x|N)=\\/{1}{N},\\ x‚àà[1,N]$ (Casella 3.2)\n",
        "- $\\E X=\\/{1}{N}\\sum_{i=1}^Ni=\\/{1}{N}\\/{N(N+1)}{2}$\n",
        "$=\\/{N+1}{2}$.\n",
        "$\\E X^2=\\/{1}{N}\\sum_{i=1}^Ni^2=\\/{1}{N}\\/{N(N+1)(2N+1)}{6}$\n",
        "$=\\/{(N+1)(2N+1)}{6}$.\n",
        "$\\Var X=\\/{(N+1)(N-1)}{12}$.\n",
        "\n",
        "**Hypergeometric**: $X‚àº\\HGeom(N,M,K)$ if $P(X=x|N,M,K)=\\/{\\binom{M}{x}\\binom{N-M}{K-x}}{\\binom{N}{K}}$.\n",
        "- $\\E X=\\/{KM}{N}$. Proof using $x\\binom{M}{x}=M\\binom{M-1}{x-1}$ and $\\binom{N}{K}=\\/{N}{K}\\binom{N-1}{K-1}$. $\\Var X=\\/{KM}{N}\\/{(N-M)(N-K)}{N(N-1)}$.\n",
        "\n",
        "**Bernoulli**: $X=\\BC 1&\\t{probability }p \\\\0&\\t{probability }1-p\\EC$\n",
        "has PMF $f_X(x)=p^x(1-p)^{1-x}$.\n",
        "- $\\E X=p$. $\\Var X=p(1-p)$. $M_X(t)=pe^t+(1-p)$.\n",
        "\n",
        "**Binomial**: $X‚àº\\Binom(n,p)$ if $X=X_1+...+X_n$ where $X_i‚àº\\t{Bernoulli}(p)$ iid and $P(X=x|n,p)=\\binom{n}{x}p^x(1-p)^{n-x}$.\n",
        "- **Binomial theorem**: $(x+y)^n=\\sum_{i=0}^n\\binom{n}{i}x^iy^{n-i}$.\n",
        "  - If we set $x=p,y=1-p$ we get $1=\\sum_{i=0}^n\\binom{n}{i}p^i(1-p)^{n-i}$.\n",
        "  - If we set $x=1,y=1$ we get $2^n=\\sum_{i=0}^n\\binom{n}{i}$.\n",
        "- $\\E X=np$. $\\Var X=np(1-p)$. $M_X(t)=[pe^t+(1-p)]^n$.\n",
        "- Recursive: $P(X=x)=\\/{n-x+1}{x}\\/{p}{1-p}P(X=x-1)$.\n",
        "- Let $X|N‚àº\\Binom(N,p)$ and $N‚àº\\Pois(Œª)$ then $X‚àº\\Pois(pŒª)$ (Casella 4.4.1)\n",
        "- Let $X|P‚àº\\Binom(n,P)$ and $P‚àº\\Unif(0,1)$ then $X‚àº\\DUnif(1,n+1)$ (Casella p4.31)\n",
        "\n",
        "**Poisson**: $X‚àº\\Pois(Œª)$ if $P(X=x|Œª)=\\/{e^{-Œª}Œª^x}{x!}$.\n",
        "- $\\E X=Œª$. $\\Var X=Œª$. $M_X(t)=e^{Œª(e^t-1)}$. (Proof p3.22)\n",
        "- Recursive: $P(X=x)=\\/{Œª}{x}P(X=x-1)$.\n",
        "- Approximation of binomial: $Œª=np$ for small $p$. (Casella 2.3.13 for MGF proof).\n",
        "- $\\E(Œªg(X))=\\E(Xg(X-1))$ (Casella 3.6.8)\n",
        "- **Poisson-gamma identity**: Let $Y‚àº\\Pois(Œª)$ then $P(Y‚â§k)=P(W>x)$ where $W‚àº\\Gamma(k+1,x/Œª)$\n",
        "- Let $X‚àº\\Pois(Œª)$ and $Y‚àº\\Pois(Œ∏)$ be independent then $X+Y‚àº\\Pois(Œª+Œ∏)$. (Casella 4.3.2, Proof 4.3.1)\n",
        "  - Let $X_1,...,X_n‚àº\\Pois(Œª)$ then $\\sum_iX_i‚àº\\Pois(nŒª)$.\n",
        "- Let $X|Œõ‚àº\\Pois(Œõ)$ and $Œõ‚àº\\Gamma(a,b)$, then $X‚àº\\t{NBinom}(a,\\/{1}{b+1})$ (Casella p4.32) and $Œõ|X‚àº\\Gamma(a+X,\\/{b}{b+1})$ (Casella p7.24).\n",
        "\n",
        "**Geometric**: $X‚àº\\Geom(p)$ if $P(X=x|p)=p(1-p)^{x-1},\\ x‚â•1$. Geometric series $\\sum_{x=1}^‚àûa^{x-1}=\\/{1}{1-a}$.\n",
        "- Let $Y$ be the number of failures before success, then $P(Y=y|p)=p(1-p)^y,\\ y‚â•0$.\n",
        "- $\\E X=\\/{1}{p}$. $\\E Y=\\E X-1=\\/{1-p}{p}$. $\\Var X=\\Var Y=\\/{1-p}{p^2}$. $M_X(t)=\\/{pe^t}{1-(1-p)e^t}$. $M_Y(t)=\\/{p}{1-(1-p)e^t}$\n",
        "- Memoryless: $P(X>s|X>t)=P(X>s-t)$\n",
        "\n",
        "**Negative binomial**: $X‚àº\\t{NBinom}(r,p)$ if $P(X=x|r,p)=\\binom{x-1}{r-1}p^r(1-p)^{x-r},\\ x‚â•r$ (Casella 3.2.9)\n",
        "- Let $Y$ be the number of failures before $r$th success, then $P(Y=y|r,p)=\\binom{r+y-1}{r-1}p^r(1-p)^y,\\ y‚â•0$  (Casella 3.2.10)\n",
        "  - Namesake rewritten $P(Y=y|r,p)=(-1)^y\\binom{-r}{y}p^r(1-p)^y$.\n",
        "- $\\E X=\\/{r}{p}$. $\\E Y=\\E X-r=\\/{r(1-p)}{p}$. $\\Var Y=\\Var X=\\/{r(1-p)}{p^2}=Œº_Y+\\/{1}{r}Œº_Y^2$. (Proof p3.22)\n",
        "- $Y$ converges to $\\Pois(r(1-p))$ as $r‚Üí‚àû$ and $p‚Üí1$. (Proof p3.15)\n",
        "- $\\E(qg(X))=\\E(\\/{X}{X+r-1}g(X-1))$ (Casella 3.6.8)\n",
        "\n",
        "**Uniform**: $X‚àº\\Unif(a,b)$ if $f(x|a,b)=\\/{1}{b-a},\\ x‚àà[a,b]$. (Casella 3.3)\n",
        "- $\\E X=\\/{a+b}{2}$. $\\Var X=\\/{(b-a)^2}{12}$.\n",
        "\n",
        "**Gamma**: $X‚àº\\Gamma(a,b)$ if $f(x|a,b)=\\/{1}{Œì(a)b^a}\\green{x^{a-1}e^{-x/b}},\\ x,a,b>0$. (definition of b is inverse of Blitzstein Œª)\n",
        "- $Œì(a)=‚à´_0^‚àût^{a-1}e^{-t}\\ dt$. $Œì(a+1)=aŒì(a),\\ a>0$. $Œì(n)=(n-1)!$. $Œì(1)=1$. $Œì(\\/{1}{2})=\\sqrt{œÄ}$.\n",
        "- $\\blue{\\E X^n=\\/{b^nŒì(n+a)}{Œì(a)}}$ (Proof p3.17). $\\E X=ab$. $\\Var X=ab^2$. $M_X(t)=(\\/{1}{1-bt})^a,\\ t\\lt \\/{1}{b}$.\n",
        "- $\\green{f(x|a,b)=\\/{e^{-x/b}(x/b)^a}{xŒì(a)}}$ for log-likelihood or\n",
        "$\\green{f(x|a,b)=\\/{e^{-x/b}(x/b)^{a-1}}{bŒì(a)}}$ for scale family and likelihood ratio.\n",
        "- Gamma-Poisson: If $X‚àº\\Gamma(n,b)$ and $Y‚àº\\Pois(\\/{t}{b})$, then $P(X‚â§t)=P(Y‚â•n)$. (Proof p3.19)\n",
        "  - $n$th arrival before time $t$ is equivalent to at least $n$ arrivals at time $t$.\n",
        "- Gamma-Exponential: $\\Gamma(1,b)=\\Expo(b)$.\n",
        "- Let $X_{a,b}‚àº\\Gamma(a,b)$, then $P(u\\lt X_{a,b}\\lt v)=b(f_X(u)-f_X(v))+P(u\\lt X_{a-1,b}\\lt v)$. Proof using integration by parts. (Casella 3.6.3)\n",
        "- Let $X_1‚àº\\Gamma(s,b)$ and $X_2‚àº\\Gamma(t,b)$ be independent, then $U=\\/{X_1}{X_1+X_2}‚àº\\Beta(s,t)$ and $V=X_1+X_2‚àº\\Gamma(s+t,b)$ are independent. (Casella p4.24)\n",
        "- Gamma scaling: if $X‚àº\\Gamma(a,b)$ then $cX‚àº\\Gamma(a,cb)$.\n",
        "\n",
        "**Exponential**: $X‚àº\\Expo(b)$ if $f(x|b)=\\/{1}{b}e^{-x/b},\\ x>0$. (definition of b is inverse of Blitzstein Œª)\n",
        "- $\\E X=b$. $\\Var X=b^2$. $M_X(t)=\\/{1}{1-bt}$.\n",
        "- Memoryless: $P(X>s|X>t)=P(X>s-t)$.\n",
        "- Let $X‚àº\\Unif(0,1)$, then $Y=-\\ln X‚àº\\Expo(1)$.\n",
        "- If $Y=X_{(1)}$ then $\\E[Y]=Œª/n$ (opposite of Blitzstein/Ross definition)\n",
        "\n",
        "**Normal**: $X‚àº\\Normal(Œº,œÉ^2)$ if $f(x|Œº,œÉ^2)=\\/{1}{\\sqrt{2œÄ}œÉ}\\e{-\\/{(x-Œº)^2}{2œÉ^2}},\\ x‚àà‚Ñù$.\n",
        "- $Z=\\/{X-Œº}{œÉ}‚àº\\Normal(0,1)$.\n",
        "- $M_X(t)=\\e{Œºt+\\/{œÉ^2}{2}t^2}$.\n",
        "- Normal-Gamma: Let $w=\\/{z^2}{2}$, then $‚à´_0^‚àûe^{-z^2/2}\\ dz$\n",
        "$=‚à´_0^‚àû\\/{1}{\\sqrt{2}}w^{-1/2}e^{-w}\\ dw$\n",
        "$=\\/{1}{\\sqrt{2}}Œì(\\/{1}{2})$\n",
        "$=\\sqrt{\\/{œÄ}{2}}$.\n",
        "- Chi-square: $Z_1^2+...+Z_p^2‚àºœá_p^2=\\Gamma(\\/{p}{2},2)$\n",
        "  - $\\E g(œá_p^2)=p\\E[\\/{g(œá_{p+2}^2)}{œá_{p+2}^2}]$ (Casella 3.6.7)\n",
        "  - $\\E œá_p^2=p$. $\\Var œá_p^2=2p$.\n",
        "- Stein's lemma: Let $X‚àº\\Normal(Œ∏,œÉ^2)$ and $\\E|g'(X)|\\lt ‚àû$, then $\\E[g(X)(X-Œ∏)]=œÉ^2\\E g'(x)$. (Casella 3.6.5)\n",
        "- Let $X‚àº\\Normal(Œº_X,œÉ_X^2)$ and $Y‚àº\\Normal(Œº_Y,œÉ_Y^2)$ be independent then $X+Y‚àº\\Normal(Œº_X+Œº_Y,œÉ_X^2+œÉ_Y^2)$. (Casella 4.2.14, Proof 4.2.13) Let $X‚àº\\Normal(Œº_X,œÉ_X^2)$ and $Y‚àº\\Normal(Œº_Y,œÉ_Y^2)$, then $aX+bY‚àº\\Normal(aŒº_X+bŒº_Y,a^2œÉ_X^2+b^2œÉ_Y^2+2abœÅœÉ_XœÉ_Y)$. (Casella 4.5.10)\n",
        "- Let $X_1,...,X_n‚àº\\Normal(Œº_i,œÉ_i^2)$. Then\n",
        "$Z=\\suml_{i=1}^n(a_iX_i+b_i)$\n",
        "$‚àº\\Normal\\left(\\suml_{i=1}^n(a_iŒº_i+b_i),\\suml_{i=1}^na_i^2œÉ_i^2\\right)$ (Casella 4.6.10)\n",
        "\n",
        "**Beta**: $X‚àº\\Beta(a,b)$ if $f(x|a,b)=\\/{1}{B(a,b)}x^{a-1}(1-x)^{b-1}, x‚àà(0,1), a,b>0$\n",
        "- $B(a,b)=‚à´_0^1x^{a-1}(1-x)^{b-1}\\ dx=\\/{Œì(a)Œì(b)}{Œì(a+b)}$.\n",
        "- $\\E X^n=\\/{Œì(a+b)Œì(a+n)}{Œì(a+b+n)Œì(a)}$. $\\E X=\\/{a}{a+b}$. $\\Var X=\\/{ab}{(a+b)^2(a+b+1)}$. (Proof p3.22)\n",
        "\n",
        "**Cauchy**: $\\/{Z_1}{Z_2}‚àº\\t{Cauchy}(Œ∏)$. $f(x|Œ∏)=\\/{1}{œÄ}\\/{1}{1+(x-Œ∏)^2}$, where $Œ∏$ is the median. $\\E |X|=‚àû$. No moments. No MGF.\n",
        "\n",
        "**Lognormal**: $\\ln X‚àº\\Normal(Œº,œÉ^2)$ then $f(x|Œº,œÉ^2)=\\/{1}{\\sqrt{2œÄ}œÉx}\\e{-\\/{(\\ln x-Œº)^2}{2œÉ^2}},\\ x‚àà‚Ñù$.\n",
        "\n",
        "**Multinomial**: $(X_1,...,X_n)‚àº\\Mult_n(p_1,...,p_n)$ with $m$ trials has $f(x_1,...,x_n)=\\/{m!}{x_1!...x_n!}p_1^{x_1}...p_n^{x_n}=m!\\prodl_{i=1}^n\\/{p_i^{x_i}}{x_i!}$ as joint PMF where $\\suml_{i=1}^np_i=1,\\ p_i‚àà[0,1]$ and $\\suml_{i=1}^nx_i=m,\\ x_i‚â•0$.\n",
        "- **Multinomial theorem**: Let $\\m{A}=\\left\\{\\v{x}=(x_1,...,x_n): x_i‚â•0, \\suml_{i=1}^nx_i=m\\right\\}$, then $(p_1+...+p_n)^m=\\suml_{\\v{x}‚àà\\m{A}}\\/{m!}{x_1!...x_n!}p_1^{x_1}...p_n^{x_n}$.\n",
        "- **Dirichlet**: generalized beta and multinomial: $f(x,y)=\\/{Œì(a+b+c)}{Œì(a)Œì(b)Œì(c)}x^{a-1}y^{b-1}(1-x-y)^{c-1}$."
      ],
      "metadata": {
        "id": "wN817qmcbAfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exponential family** is conditional PDF/PMF expressed in form $\\red{f(x|\\v{Œ∏})=h(x)c(\\v{Œ∏})\\e{\\suml_{i=1}^kw_i(\\v{Œ∏})t_i(x)}}$ (Casella 3.4)\n",
        "- $h(x)‚â•0$ and $t_i(x)$ are functions of $x$ (no $Œ∏$), and $c(\\v{Œ∏})‚â•0$ and $w_i(\\v{Œ∏})$ are functions of $\\v{Œ∏}$ (no $x$).\n",
        "- If $X$ is a random variable in exponential family form, then (Casella 3.4.2, Proof p3.31)\n",
        "  - $\\blue{\\E[\\suml_{i=1}^k\\/{‚àÇw_i(\\v{Œ∏})}{‚àÇŒ∏_j}t_i(X)]=-\\/{‚àÇ}{‚àÇŒ∏_j}\\ln c(\\v{Œ∏})}$\n",
        "  - $\\blue{\\Var(\\suml_{i=1}^k\\/{‚àÇw_i(\\v{Œ∏})}{‚àÇŒ∏_j}t_i(X))=-\\/{‚àÇ^2}{‚àÇŒ∏_j^2}\\ln c(\\v{Œ∏})-\\E[\\suml_{i=1}^k\\/{‚àÇ^2w_i(\\v{Œ∏})}{‚àÇŒ∏_j^2}t_i(X)]}$\n",
        "- Exponential family form is also written as $\\red{f(x|\\v{Œ∑})=h(x)c^*(\\v{Œ∑})\\e{\\suml_{i=1}^kŒ∑_it_i(x)}}$\n",
        "  - $h(x)$ and $t_i(x)$ are same as before.\n",
        "   **Natural parameters** $\\m{H}=\\left\\{\\v{Œ∑}:‚à´_{-‚àû}^‚àûh(x)\\e{\\suml_{i=1}^kŒ∑_it_i(x)}\\ dx\\lt ‚àû\\right\\}$ where $Œ∑_i=w_i(\\v{Œ∏})$ and\n",
        "  $c^*(\\v{Œ∑})=c(\\v{Œ∏}(\\v{Œ∑}))=\\left[‚à´_{-‚àû}^‚àûh(x)\\e{\\suml_{i=1}^kŒ∑_it_i(x)}\\ dx\\right]^{-1}$ is a normalizing constant that makes the integration converge to 1.\n",
        "  - $\\blue{\\E[t_j(X)]=-\\/{‚àÇ}{‚àÇŒ∑_j}\\ln c^*(\\v{Œ∑})}$ (Proof p3.32)\n",
        "  - $\\blue{\\Var(t_j(X))=-\\/{‚àÇ^2}{‚àÇŒ∑_j^2}\\ln c^*(\\v{Œ∑})}$\n",
        "  - Let $d$ be the dimension of $\\v{Œ∏}$. If $d\\lt k$ then the PDF/PMF is a *curved* exponential family. E.g., normal approximation of binomial is $\\Normal(np,np(1-p))$, which only has $Œ∏_1=p$ as its sole parameter. If $d=k$ then it is a *full* family.\n",
        "\n",
        "**Location and scale families**: Let $f(x)$ be any PDF, and $Œº‚àà‚Ñù,œÉ>0$ be any constants. Then $g(x|Œº,œÉ)=\\/{1}{œÉ}f(\\/{x-Œº}{œÉ})$ is a PDF. Proof by integral variable substitution. (Casella 3.5)\n",
        "- Let $f(x)$ be any PDF. Then $f(x-Œº)$ is the **location family** with standard pdf $f(x)$ and location parameter $Œº‚àà‚Ñù$.\n",
        "- Let $f(x)$ be any PDF. Then $\\/{1}{œÉ}f(\\/{x}{œÉ})$ is the **scale family** with standard pdf $f(x)$ and scale parameter $œÉ>0$.\n",
        "- Let $f_Z(x)$ be any PDF. Then $f_X(x)=\\/{1}{œÉ}f_Z(\\/{x-Œº}{œÉ})$ is the **location-scale family** with $f_Z(x)$, and $X=œÉZ+Œº$ or $Z=\\/{X-Œº}{œÉ}$. Proof by change of variables.\n",
        "  - $\\E X=œÉ\\E Z+Œº$ and $\\Var X=œÉ^2\\Var Z$."
      ],
      "metadata": {
        "id": "whXQeSImlJqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random vector** is a function from a sample space $S$ into $‚Ñù^n$. (Casella 4.1)\n",
        "- Let $(X,Y)$ be discrete bivariate random vector, then $f(x,y)=P(X=x,Y=y)$ from $‚Ñù^2‚Üí‚Ñù$ is the **joint PMF**. $P((X,Y)‚ààA)=\\suml_{(x,y)‚ààA}f(x,y)$ and $\\E g(X,Y)=\\suml_{(x,y)‚àà‚Ñù^2}g(x,y)f(x,y)$. The **marginal PMF** of $X$ is $f_X(x)=P(X=x)=\\suml_{y‚àà‚Ñù}f(x,y)$, and it is same as the univariate PMF of $X$.\n",
        "- For continuous bivariate random vector, $P((X,Y)‚ààA)=\\iint_Af(x,y)\\ dx\\ dy$ and $\\E g(X,Y)=\\iint_{‚Ñù^2}f(x,y)\\ dx\\ dy$. The marginal PMF is $f_X(x)=‚à´_‚Ñùf(x,y)\\ dy$.\n",
        "- **Conditional PMF/PDF** is $f(y|x)=\\/{f(x,y)}{f_X(x)}$ such that $‚à´_‚Ñùf(y|x)\\ dy=1$ and $\\suml_{y‚àà‚Ñù}f(y|x)=1$. Conditional expectation is $\\E[g(Y)|x]=‚à´_{y‚àà‚Ñù}g(y)f(y|x)\\ dy$ and $\\E[g(Y)|x]=\\suml_{y‚àà‚Ñù}g(y)f(y|x)$. (Casella 4.2)\n",
        "  - $\\Var(Y|x)=\\E(Y^2|x)-\\E(Y|x)^2$. (Casella 4.2.4)\n",
        "  - $\\E X=\\E(\\E(X|Y))$ (Casella 4.4.3)\n",
        "  - $\\E XY=\\E[\\E[XY|Y]]=\\E[Y\\E[X|Y]]$\n",
        "  - $\\Var X=\\E(\\Var(X|Y))+\\Var(\\E(X|Y))$ (Casella 4.4.7)\n",
        "  - $X$ has a **mixture distribution** if its distribution depends on a quantity that also has a distribution.\n",
        "- $X,Y$ are **independent** iff $f(x,y)=f_X(x)f_Y(y)$. (Casella 4.2.5) $X,Y$ are independent iff\n",
        "  - there exist $g(x)$ and $h(y)$ such that $f(x,y)=g(x)h(y)$ for all $x,y‚àà‚Ñù$. (Casella 4.2.7)\n",
        "  - for any $A,B‚äÇ‚Ñù$, then $P(X‚ààA,Y‚ààB)=P(X‚ààA)P(Y‚ààB)$. (Casella 4.2.10)\n",
        "  - for any $g(x)$ and $h(y)$, then $\\E(g(X)h(Y))=\\E g(X)\\E h(Y)$.\n",
        "  - for $Z=X+Y$, then $M_Z(t)=M_X(t)M_Y(t)$. (Casella 4.2.12)\n",
        "  - $U=g(X)$ and $V=h(Y)$ are independent (Casella 4.3.5)\n",
        "- Let $(X,Y)$ be continuous random vector on $\\m{A}$ with $f_{X,Y}(x,y)$, then random vector $(U,V)$ on $\\m{B}$ has joint PDF $f_{U,V}(u,v)=f_{X,Y}(x(u,v),y(u,v))|\\/{‚àÇ(x,y)}{‚àÇ(u,v)}|$, where $|\\/{‚àÇ(x,y)}{‚àÇ(u,v)}|=\\BVM\n",
        "\\/{‚àÇx}{‚àÇu} & \\/{‚àÇx}{‚àÇv} \\\\\n",
        "\\/{‚àÇy}{‚àÇu} & \\/{‚àÇy}{‚àÇv}\\EVM\n",
        "=\\/{‚àÇx}{‚àÇu}\\/{‚àÇy}{‚àÇv}-\\/{‚àÇx}{‚àÇv}\\/{‚àÇy}{‚àÇu}$. (Casella 4.3)\n",
        "  - Summation of preimages (2.1.3) also applies. (Casella 4.3.6)\n",
        "- $\\Cov(X,Y)=\\E((X-Œº_X)(Y-Œº_Y))=\\E XY-Œº_XŒº_Y$ and $œÅ_{XY}=\\/{\\Cov(X,Y)}{œÉ_XœÉ_Y}$. (Casella 4.5)\n",
        "  - $X$ and $Y$ are independent then (not iff) $\\Cov(X,Y)=0$ and $œÅ_{XY}=0$. (Casella 4.5.5)\n",
        "  - $\\Var(aX+bY)=a^2\\Var X+b^2\\Var Y+2ab\\Cov(X,Y)$.\n"
      ],
      "metadata": {
        "id": "3hHXuOr-zdIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chebyshev's inequality**: $P(|D|‚â•r)‚â§\\/{\\Var(D)}{r^2}$ and Markov $P(g(X)‚â•r)‚â§\\/{\\E g(X)}{r}$.  (Casella 3.6.1)\n",
        "- Proof: $\\E g(X)=‚à´_{x‚àà\\m{X}}g(x)f(x)\\ dx$\n",
        "$‚â•‚à´_{g(x)‚â•r}g(x)f(x)\\ dx$\n",
        "$‚â•r‚à´_{g(x)‚â•r}f(x)\\ dx$\n",
        "$‚áí\\/{\\E g(X)}{r}‚â•P(g(X)‚â•r)$.\n",
        "- $P(\\/{(X-Œº)^2}{œÉ^2}‚â•t^2)‚â§\\/{1}{t^2}$\n",
        "\n",
        "**H√∂lder's inequality**: Let $p>1,q>1$ such that $\\/{1}{p}+\\/{1}{q}=1$, then $|\\E(XY)|‚â§\\E|XY|‚â§(\\E|X|^p)^{1/p}(\\E|Y|^q)^{1/q}$. (Casella 4.7.2)\n",
        "- Measure theory: $\\norm{XY}_1‚â§\\norm{X}_p\\norm{Y}_q$ (Capinski 5.8)\n",
        "- Proof (part 1): $\\E|XY|‚â•|\\E(XY)|$ by Jensen's inequality for convex function $g(x)=|x|$. Also $|XY|‚â•XY‚â•-|XY|$ therefore $\\E|XY|‚â•|\\E(XY)|$.\n",
        "- **Holder conjugates**: Let $a>0,\\ b>0,\\ p>1,\\ q>1$. If $\\/{1}{p}+\\/{1}{q}=1$ then $\\/{1}{p}a^p+\\/{1}{q}b^q‚â•ab$. Equality iff $a^p=b^q$. (Casella 4.7.1)\n",
        "  - Proof: Let $g(a)=\\/{1}{p}a^p+\\/{1}{q}b^q-ab$.\n",
        "  Find $a_{\\min}$ such that $g(a)‚â•g(a_{\\min})$ for all $a$.\n",
        "  Differentiate $\\/{dg}{da}=a^{p-1}-b=0$\n",
        "  $‚áía_{\\min}=b^{\\/{1}{p-1}}$.\n",
        "  $q=\\/{p}{p-1}$.\n",
        "  $g(a_{\\min})=\\/{1}{p}b^{\\/{p}{p-1}}+\\/{1}{q}b^q-b^{\\/{p}{p-1}}$\n",
        "  $=\\/{1-p}{p}b^{\\/{p}{p-1}}+\\/{1}{q}b^q$\n",
        "  $=0$\n",
        "  $‚áí\\/{1}{p}a^p+\\/{1}{q}b^q-ab‚â•0$.\n",
        "- Proof (part 2):\n",
        "Let $a=\\/{|X|}{(\\E|X|^p)^{1/p}}$\n",
        "$‚áía^p=\\/{|X|^p}{\\E|X|^p}$\n",
        "and $b=\\/{|Y|}{(\\E|Y|^q)^{1/q}}$\n",
        "$‚áía^q=\\/{|Y|^q}{\\E|Y|^q}$,\n",
        "then $\\/{|X|^p}{p\\E|X|^p}+\\/{|Y|^q}{q\\E|Y|^q}‚â•\\/{|XY|}{(\\E|X|^p)^{1/p}(\\E|Y|^q)^{1/q}}$\n",
        "$‚áí\\/{\\E|X|^p}{p\\E|X|^p}+\\/{\\E|Y|^q}{q\\E|Y|^q}‚â•\\/{\\E|XY|}{(\\E|X|^p)^{1/p}(\\E|Y|^q)^{1/q}}$\n",
        "- $\\suml_{i=1}^n|a_ib_i|‚â§\\left(\\suml_{i=1}^na_i^p\\right)^{1/p}\\left(\\suml_{i=1}^nb_i^q\\right)^{1/q}$. Let $p=q=2$ and $b_i=1$ then $\\/{1}{n}\\left(\\suml_{i=1}^n|a_i|\\right)^2‚â§\\suml_{i=1}^na_i^2$. (Casella 4.7.9)\n",
        "\n",
        "**Cauchy-Schwarz inequality**: $|\\E(XY)|‚â§\\E|XY|‚â§\\sqrt{\\E X^2\\E Y^2}$\n",
        "- Measure theory: $\\norm{XY}_1‚â§\\norm{X}_2\\norm{Y}_2$ (Capinski 5.9)\n",
        "- Proof: Holder's inequality with $p=q=2$.\n",
        "- **Covariance inequality**: $\\red{\\Cov(X,Y)^2‚â§\\Var(X)\\Var(Y)}$.\n",
        "  - $\\E[(X-\\E[X])(Y-\\E[Y])]^2‚â§\\E[(X-\\E[X])^2]\\E[(Y-\\E[Y])^2]$\n",
        "  - If $X-\\E[X]=c(Y-\\E[Y])$ then the two sides are equal.\n",
        "- **Vector inequality**: $\\red{(\\v{x}^‚ä§\\v{y})^‚ä§(\\v{x}^‚ä§\\v{y})‚â§(\\v{x}^‚ä§\\v{x})(\\v{y}^‚ä§\\v{y})}$\n",
        "  $\\left(\\suml_{i=1}^nx_iy_i\\right)^2‚â§\\left(\\suml_{i=1}^nx_i^2\\right)\\left(\\suml_{i=1}^ny_i^2\\right)$\n",
        "  - Proof: $\\E[XY]=\\/{1}{n}\\suml_{i=1}^nx_iy_i$.\n",
        "  Then $(\\E[XY])^2‚â§\\E[X^2]\\E[Y^2]$\n",
        "  $‚áí\\left(\\/{1}{n}\\suml_{i=1}^nx_iy_i\\right)^2$\n",
        "  $‚â§\\left(\\/{1}{n}\\suml_{i=1}^nx_i^2\\right)\\left(\\/{1}{n}\\suml_{i=1}^ny_i^2\\right)$\n",
        "  - Let $x_i=1$ and $\\sum_iy_i=1$, then\n",
        "  $\\suml_{i=1}^ny_i^2‚â•\\/{1}{n}$.\n",
        "- $|\\Cov(X,Y)|‚â§œÉ_XœÉ_Y$ and $|œÅ|‚â§1$.\n",
        "\n",
        "**Minkowski's inequality**: Let $p‚â•1$, then $(\\E|X+Y|^p)^{1/p}‚â§(\\E|X|^p)^{1/p}+(\\E|Y|^p)^{1/p}$\n",
        "- Measure theory: triangle inequality in $L^p$ space $\\norm{X+Y}_p‚â§\\norm{X}_p+\\norm{Y}_p$ (Capinski 5.10)\n",
        "- Proof: $\\E|X+Y|^p=\\E(|X+Y||X+Y|^{p-1})$\n",
        "$‚â§\\E((|X|+|Y|)|X+Y|^{p-1})$\n",
        "$=\\E(|X||X+Y|^{p-1})+\\E(|Y||X+Y|^{p-1})$\n",
        "$‚â§(\\E|X|^p)^{1/p}(\\E|X+Y|^{q(p-1)})^{1/q}$\n",
        "$+(\\E|Y|^p)^{1/p}(\\E|X+Y|^{q(p-1)})^{1/q}$\n",
        "$‚áí\\/{\\E|X+Y|^p}{(\\E|X+Y|^{q(p-1)})^{1/q}}$\n",
        "$=(\\E|X+Y|^p)^{1/p}$\n",
        "$‚â§(\\E|X|^p)^{1/p}+(\\E|Y|^p)^{1/p}$.\n",
        "$q(p-1)=p$ and $1-1/q=1/p$.\n",
        "\n",
        "**Jensen's inequality**: If $g(x)$ is convex then $\\E g(X)‚â•g(\\E X)$. Let $x_1,x_2$ be any two points on x-axis. $g(x)$ is convex if $g(Œªx_1+(1-Œª)x_2)‚â§Œªg(x)+(1-Œª)g(x_2)$ for any $Œª‚àà(0,1)$. (Casella 4.7.7)\n",
        "- Proof: Let $a+bx$ be the tangent of $g(x)$ at $x=\\E X$, then $\\E g(X)$ is the mean of $g(x)$ on y-axis for all $x‚àà\\m{X}$, which is greater than point $y=g(\\E X)$. Equality holds iff $P(g(X)=a+bX)=1$, meaning $g(x)$ equals its tangent for all $x‚àà\\m{X}$.\n",
        "- $\\E X^2‚â•(\\E X)^2$\n",
        "- Covariance inequality: $\\E(g(X)h(X))‚â•(\\E g(X))(\\E h(X))$ iff $g(x)$ and $h(x)$ are both nondecreasing or both nonincreasing. Otherwise $\\E(g(X)h(X))‚â§(\\E g(X))(\\E h(X))$."
      ],
      "metadata": {
        "id": "vR5g0nZUcjiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Order statistics** of $X_1,...,X_n$ are the sample values placed in ascending order $X_{(1)}‚â§...‚â§X_{(n)}$. (Casella 5.4.1)\n",
        "- Sample range is $X_{(n)}-X_{(1)}$. Sample median is $X_{((n+1)/2)}$ if $n$ odd, $(X_{(n/2)}+X_{(n/2+1)})/2$ if even.\n",
        "- Let $X_1,...,X_n$ be discrete with PMF $f_X(x_i)=P(X=x_i)$ where $x_i$ is the $i$th value in the support of $X$ sorted in ascending order. Define $P_i=\\suml_{j=1}^if_X(x_i)$, then $P(X_{(j)}‚â§x_i)=\\suml_{k=j}^n\\binom{n}{k}P_i^k(1-P_i)^{n-k}$ is the probability that at least $j$ of $n$ values are less than $x_i$ and $P(X_{(j)}=x_i)=P(X_{(j)}‚â§x_i)-P(X_{(j)}‚â§x_{i-1})$. (Casella 5.4.3)\n",
        "- Let $X_1,...,X_n$ be continuous with CDF $F(x)$ and PDF $f(x)$, then $f_{X_{(j)}}(x)=\\/{n!}{(j-1)!(n-j)!}f(x)[F(x)]^{j-1}[1-F(x)]^{n-j}$.\n",
        "  - if $X_i‚àº\\Unif(0,1)$ then $f_{X_{(j)}}(x)=\\/{Œì(n+1)}{Œì(j)Œì(n-j+1)}x^{j-1}(1-x)^{n-j}$ and $\\blue{X_{(j)}‚àº\\Beta(j,n-j+1)}$.\n",
        "  - $f_{X_{(i)},X_{(j)}}(u,v)=\\/{n!}{(i-1)!(j-i-1)!(n-j)!}f(u)f(v)[F(u)]^{i-1}[F(v)-F(u)]^{j-i-1}[1-F(v)]^{n-j}$ (Casella 5.4.6)\n",
        "- Range: $X_{(n)}-X_{(1)}$: $f_{X_{(1)},X_{(n)}}(u,v)=n(n-1)f(u)f(v)(F(v)-F(u))^{n-2}$.\n",
        "  - Let $U=X_{(1)}$ and\n",
        "  $R=X_{(n)}-X_{(1)}$.\n",
        "  Then $X_{(1)}=U$,\n",
        "  $X_{(n)}=U+R$, and\n",
        "  $\\/{‚àÇ(x_{(1)},x_{(n)})}{‚àÇ(u,r)}=1$.\n",
        "  $f_R(r)=n(n-1)‚à´_{-‚àû}^‚àû[F(u+r)-F(u)]^{n-2}f(u)f(u+r)\\ du$.\n",
        "  - For $X_i‚àº\\Unif(a,b)$: $f_R(r)=n(n-1)r^{n-2}(1-r)$.\n",
        "  $\\blue{R‚àº\\Beta(n-1,2)}$.\n",
        "- If $X_1,...,X_n‚àº\\Unif(0,Œ∏)$ from\n",
        "$f(x_i|Œ∏)=\\/{1}{Œ∏}I_{0< x_i< Œ∏}$, then\n",
        "  - $f_{X_{(n)}}(x)=\\/{n}{Œ∏^n}x^{n-1}$.\n",
        "  $\\E[X_{(n)}]=‚à´_0^Œ∏x\\/{n}{Œ∏^n}x^{n-1}\\ dx$\n",
        "  $=\\/{n}{n+1}Œ∏$.\n",
        "  $\\Var(X_{(1)})=\\Var(X_{(n)})=\\/{n}{(n+2)(n+1)^2}Œ∏^2$.\n",
        "  - $f_{X_{(1)}}(x)=\\/{n}{Œ∏^n}(Œ∏-x)^{n-1}$.\n",
        "  $\\E[X_{(1)}]=‚à´_0^Œ∏\\/{n}{Œ∏^n}(Œ∏-x)^{n-1}x\\ dx$\n",
        "  $=[-\\/{1}{Œ∏^n}x(Œ∏-x)^n]_0^Œ∏-‚à´_0^Œ∏\\/{1}{Œ∏^n}(Œ∏-x)^n\\ dx$\n",
        "  $=\\/{1}{(n+1)Œ∏^n}[-(Œ∏-x)^{n+1}]_0^Œ∏$\n",
        "  $=\\/{1}{n+1}Œ∏$.\n",
        "- If $f(x_i)$ is symmetric about some center $(X_{(n)}-c)‚àº-(X_{(1)}-c)$ then $\\Var(X_{(n)})=\\Var(X_{(1)})$."
      ],
      "metadata": {
        "id": "Wb9ahxpXyz72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating a random variable:\n",
        "- **Uniform**: Starting with $U‚àº\\Unif(0,1)$, then $F_X^{-1}(U)‚àºX$ and $F_X(X)‚àºU$ (Casella 5.6.6)\n",
        "  - $X=-Œª\\ln(1-U)‚àº-Œª\\ln(U)‚àº\\Expo(Œª)$.\n",
        "  - $X=-b\\suml_{j=1}^a\\ln(U_j)‚àº\\Gamma(a,b)$.\n",
        "  - $X=\\/{\\sum_{j=1}^a\\ln(U_j)}{\\sum_{j=1}^{a+b}\\ln(U_j)}‚àº\\Beta(a,b)$.\n",
        "  - $X=-2\\suml_{j=1}^p\\ln(U_j)‚àºœá_{2p}^2$.\n",
        "  - **Discrete generation**: If $Y$ is discrete with support $y_1\\lt...\\lt y_k$, then $P(F_Y(y_i)\\lt U‚â§F_Y(y_{i+1}))=P(Y=y_{i+1})$. To generate $Y‚àºF_Y(y)$, set $Y=y_{i+1}$ if $F_Y(y_i)\\lt U‚â§F_Y(y_{i+1})$.\n",
        "- **Box-Muller**: Let $U_1,U_2‚àº\\Unif(0,1)$ iid, $R=\\sqrt{-2\\ln U_1}$, $Œ∏=2œÄU_2$, $X=R\\cos Œ∏$, and $Y=R\\sin Œ∏$, then $X,Y‚àº\\Normal(0,1)$ iid.\n",
        "- **Accept/Reject**: Let target $Y‚àºf_Y$ and proposal $V‚àºf_V$ have the same support, and calculate $M=\\sup\\limits_{y}\\/{f_Y(y)}{f_V(y)}\\lt‚àû$. At each step generate independent $U‚àº\\Unif(0,1)$ and $V$, and\n",
        "$\\BC\\t{accept }Y=V&\\t{if }U‚â§\\/{1}{M}\\/{f_Y(V)}{f_V(V)} \\\\\\t{reject }V&\\t{otherwise}\\EC$.\n",
        "  - $M=\\sup\\limits_{y}\\/{f_Y(y)}{f_V(y)}$ is a constant representing the largest ratio over support. *Supremum* is *maximum* used for continuous sets. $V$ is chosen such that $f_V$ is easy to sample and $M$ is small (by choosing $V$ with heavier tail than $Y$). $\\Unif(a,b)$ is often used for $Y‚àà[a,b]$, $\\Expo(Œª)$ for Gamma, and $\\t{Laplace}(0,1)$ or $\\t{Cauchy}(0,1)$ for Normal. A misfit $V$ would result in large $M$ and therefore large rejection rate.\n",
        "  $P(\\t{accept})$\n",
        "  $=P(U‚â§\\/{1}{M}\\/{f_Y(V)}{f_V(V)})$\n",
        "  $=‚à´_vP(U‚â§\\/{1}{M}\\/{f_Y(v)}{f_V(v)}|V=v)f_V(v)\\ dv$\n",
        "  $=‚à´_v\\/{1}{M}\\/{f_Y(v)}{f_V(v)}f_V(v)\\ dv$\n",
        "  $=\\/{1}{M}‚à´_vf_Y(v)\\ dv$\n",
        "  $=\\/{1}{M}$.\n",
        "  Then the number of trials needed for acceptance is $\\Geom(\\/{1}{M})$.\n",
        "  - (Casella 5.6.7, Blitzstein 10.2.5) Let $Y‚àº\\Beta(a,b)$, $U‚àº\\Unif(0,1)$, $V‚àº\\Unif(0,1)$ and $c‚â•\\max_y f_Y(y)$. Create a box $V‚àà[0,1]$ and $cU‚àà[0,c]$ where $(V,cU)$ is a point uniformly distributed in the box. Then\n",
        "  $P(Y‚â§y)=cP(V‚â§y,cU‚â§f_Y(V))$.\n",
        "  For $y=1$,\n",
        "  $c=\\/{1}{P(cU‚â§f_Y(V))}$. Therefore\n",
        "  $P(Y‚â§y)=\\/{P(V‚â§y,cU‚â§f_Y(V))}{P(cU‚â§f_Y(V))}$\n",
        "  $=P(V‚â§y|U‚â§f_Y(V))$.\n",
        "  For each $(U,V)$ pair, $\\BC \\t{accept }Y=V & \\t{if }U‚â§\\/{1}{c}f_Y(V) \\\\ \\t{reject }V & \\t{otherwise} \\EC$.\n",
        "- **Metropolis MCMC**: Let target $Y‚àºf_Y$ and proposal $V‚àºf_V$ have the same support. Generate $V$ and let $Z_0=V$. Then for $i=1,2,...$ generate $U_i‚àº\\Unif(0,1)$ and $V_i$. Calculate $œÅ_i=\\min(\\/{f_Y(V_i)}{f_V(V_i)}\\/{f_V(Z_{i-1})}{f_Y(Z_{i-1})}, 1)$. Then set $Z_i=\\BC V_i&U_i‚â§œÅ_i\\\\Z_{i-1}&\\t{otherwise}\\EC$."
      ],
      "metadata": {
        "id": "r6q9X6fqzBAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casella Chapter 1: Probability Theory\n",
        "\n",
        "- 1.27a: Verify $\\sum_{k=0}^n(-1)^k\\binom{n}{k}=0,\\ n‚â•2$. Entries are symmetrical across a row of Pascal's triangle. If $n$ odd the row has even number of elements and alternating -1 will render the sum 0. If $n$ even\n",
        "$\\sum_{k=0}^n(-1)^k\\binom{n}{k}$\n",
        "$=\\binom{n}{0}+\\binom{n}{n}+\\sum_{k=1}^{n-1}(-1)^k\\binom{n}{k}$\n",
        "$=\\binom{n}{0}+\\binom{n}{n}+\\sum_{k=1}^{n-1}(-1)^k[\\binom{n-1}{k-1}+\\binom{n-1}{k}]$\n",
        "$=\\binom{n}{0}+\\binom{n}{n}-\\binom{n-1}{0}-\\binom{n-1}{n-1}$\n",
        "$=0$ the sum goes up one level in the triangle.\n",
        "\n",
        "- 1.27b: Verify $\\sum_{k=1}^nk\\binom{n}{k}=n2^{n-1},\\ n‚â•2$. Use $k\\binom{n}{k}=n\\binom{n-1}{k-1}$. Then\n",
        "$\\sum_{k=1}^nk\\binom{n}{k}$\n",
        "$=n\\sum_{k=0}^{n-1}\\binom{n-1}{k}$\n",
        "$=n2^{n-1}$. The sum across row $n$ on the triangle is $2^n$.\n",
        "\n",
        "- 1.42d: Verify $k-\\binom{k}{2}+\\binom{k}{3}-...¬±\\binom{k}{k}=1$. Rewrite\n",
        "$k-\\binom{k}{2}+\\binom{k}{3}-...¬±\\binom{k}{k}$\n",
        "$=\\sum_{i=0}^k(-1)^{i+1}\\binom{k}{i}+1=1$.\n",
        "\n",
        "Casella Chapter 2: Transformations and expectations\n",
        "\n",
        "- 2.3.13 (Poisson approximation): Let $X‚àº\\Binom(n,p)$ and $Y‚àº\\Pois(Œª)$, then $M_X(t)=[pe^t+(1-p)]^n$ and $M_Y(t)=e^{Œª(e^t-1)}$. If we set $p=\\/{Œª}{n}$ then\n",
        "$M_X(t)=[1+\\/{Œª}{n}(e^t-1)]^n$\n",
        "$‚âà[e^{\\/{Œª}{n}(e^t-1)}]^n$\n",
        "$=M_Y(t)$.\n",
        "\n",
        "- 2.4.5: $f_X(x)=\\/{1}{Œª}e^{-x/Œª},\\ 0\\lt x\\lt ‚àû$.\n",
        "$\\/{d}{dŒª}\\E X^n=\\/{d}{dŒª}‚à´_0^‚àûx^n\\/{1}{Œª}e^{-x/Œª}\\ dx$\n",
        "$=‚à´_0^‚àûx^n\\/{d}{dŒª}\\/{1}{Œª}e^{-x/Œª}\\ dx$\n",
        "$=‚à´_0^‚àûx^n[\\/{-1}{Œª^2}e^{-x/Œª}+\\/{1}{Œª}e^{-x/Œª}\\/{x}{Œª^2}]\\ dx$\n",
        "$=‚à´_0^‚àû\\/{x^n}{Œª^2}e^{-x/Œª}[\\/{x}{Œª}-1]\\ dx$\n",
        "$=\\/{1}{Œª^2}\\E X^{n+1}-\\/{1}{Œª}\\E X^n$.\n",
        "  - $|\\/{‚àÇ}{‚àÇŒª}\\/{x^ne^{-x/Œª}}{Œª}|$\n",
        "  $=\\/{x^ne^{-x/Œª}}{Œª^2}|\\/{x}{Œª}-1|$\n",
        "  $‚â§\\/{x^ne^{-x/Œª}}{Œª^2}|\\/{x}{Œª}+1|=g(x,Œª)$\n",
        "\n",
        "- 2.4.6: Let $X‚àº\\Normal(Œº,1)$, then $M_X(t)=\\/{1}{\\sqrt{2œÄ}}‚à´_{-‚àû}^‚àûe^{tx}e^{-(x-Œº)^2/2}\\ dx$. Find $M_X'(0)$.\n",
        "$\\/{d}{dt}M_X(t)$\n",
        "$=\\/{d}{dt}\\E e^{tX}$\n",
        "$=\\E \\/{‚àÇ}{‚àÇt}e^{tX}$\n",
        "$=\\E(Xe^{tX})$.\n",
        "  - $|\\/{‚àÇ}{‚àÇt}e^{tX}|=|xe^{tx}e^{-(x-Œº)^2/2}|$\n",
        "  $‚â§|x|e^{tx}e^{-(x-Œº)^2/2}|=g(x,t)$\n",
        "\n",
        "- 2.11: Let $Z‚àº\\Normal(0,1)$.\n",
        "  - Find $\\E Z^2$ directly.\n",
        "  $\\E Z^2=‚à´_{-‚àû}^‚àûx^2\\/{1}{\\sqrt{2œÄ}}e^{-x^2/2}\\ dx$\n",
        "  $=\\/{1}{\\sqrt{2œÄ}}[-x^2e^{-x^2/2}]_{-‚àû}^‚àû$\n",
        "  $+‚à´_{-‚àû}^‚àû\\/{1}{\\sqrt{2œÄ}}e^{-x^2/2}\\ dx$\n",
        "  $=1$.\n",
        "  - Find $\\E Z^2$ using transformation.\n",
        "  Let $Y=Z^2$. Then $Z=¬±\\sqrt{Y}$ and $|\\/{dz}{dy}|=\\/{1}{2\\sqrt{y}}$.\n",
        "  $f_Y(y)=\\/{\\red{2}}{2\\sqrt{y}}\\/{1}{\\sqrt{2œÄ}}e^{-y/2}$\n",
        "  $=\\/{1}{\\sqrt{2œÄy}}e^{-y/2},\\ y‚â•0$.\n",
        "  $\\E Y=\\/{1}{\\sqrt{2œÄ}}‚à´_0^‚àû\\sqrt{y}e^{-y/2}\\ dy$\n",
        "  $=\\/{1}{\\sqrt{2œÄ}}[-2\\sqrt{y}e^{-y/2}]_0^‚àû$\n",
        "  $+‚à´_0^‚àû\\/{1}{\\sqrt{2œÄy}}e^{-y/2}\\ dy$\n",
        "  $=1$.\n",
        "  - Find PDF, mean, and variance of $Y=|Z|$. Then\n",
        "  $Y=¬±Y$. $|\\/{dz}{dy}|=1$.\n",
        "  $f_Y(y)=\\/{\\red{2}}{\\sqrt{2œÄ}}e^{-y^2/2}$\n",
        "  $=\\sqrt{\\/{2}{œÄ}}e^{-y^2/2},\\ y>0$.\n",
        "  $\\E Y=‚à´_0^‚àûy\\sqrt{\\/{2}{œÄ}}e^{-y^2/2}\\ dy$\n",
        "  $=[-\\sqrt{\\/{2}{œÄ}}e^{-y^2/2}]_0^‚àû$\n",
        "  $=\\sqrt{\\/{2}{œÄ}}$.\n",
        "  $\\E Y^2=‚à´_0^‚àûy^2\\sqrt{\\/{2}{œÄ}}e^{-y^2/2}\\ dy$\n",
        "  $=[-\\sqrt{\\/{2}{œÄ}}ye^{-y^2/2}]_0^‚àû$\n",
        "  $+\\sqrt{\\/{2}{œÄ}}‚à´_0^‚àûe^{-y^2/2}\\ dy$\n",
        "  $=\\sqrt{\\/{2}{œÄ}}\\sqrt{2œÄ}\\/{1}{2}$\n",
        "  $=1$.\n",
        "  $\\Var(Y)=1-\\/{2}{œÄ}$.\n",
        "\n",
        "- 2.18: If $X$ is continuous with median $m$, show $\\min_a \\E |X-a|=\\E |X-m|$.\n",
        "$\\E |X-a|=‚à´_{-‚àû}^{‚àû} |x-a|f(x)\\ dx$\n",
        "$=‚à´_a^‚àû(x-a)f(x)\\ dx+‚à´_{-‚àû}^a(a-x)f(x)\\ dx$.\n",
        "$\\/{d}{da}\\E |X-a|$\n",
        "$=\\/{d}{da}‚à´_a^‚àû(x-a)f(x)\\ dx+\\/{d}{da}‚à´_{-‚àû}^a(a-x)f(x)\\ dx$\n",
        "$=‚à´_a^‚àû\\/{‚àÇ}{‚àÇa}(x-a)f(x)\\ dx+‚à´_{-‚àû}^a\\/{‚àÇ}{‚àÇa}(a-x)f(x)\\ dx$\n",
        "$=‚à´_a^‚àû-af(x)\\ dx+‚à´_{-‚àû}^aaf(x)\\ dx$\n",
        "$=a[-1+F(a)+F(a)]=0$\n",
        "$‚áíF(a)=\\/{1}{2}$."
      ],
      "metadata": {
        "id": "C-lJTPwG251k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casella Chapter 3: Common Families of Distributions\n",
        "\n",
        "- 3.4.1 (binomial): PMF $f(x|p)=\\binom{n}{x}p^x(1-p)^{n-x}$\n",
        "$=\\binom{n}{x}(1-p)^n(\\/{p}{1-p})^x$\n",
        "$=\\binom{n}{x}(1-p)^n\\e{\\ln(\\/{p}{1-p})x}$\n",
        "$=h(x)c(p)\\e{w_1(p)t_1(x)}$ where $h(x)=0$ unless $x‚àà[0,n]$ and $c(p)=0$ unless $p‚àà(0,1)$.\n",
        "\n",
        "- 3.4.3 (binomial):\n",
        "  - $\\/{‚àÇw_1(p)}{‚àÇp}=\\/{‚àÇ}{‚àÇp}\\ln(\\/{p}{1-p})$\n",
        "  $=\\/{1-p}{p}(\\/{1}{1-p}+\\/{p}{(1-p)^2})$\n",
        "  $=\\/{1-p}{p}\\/{1}{(1-p)^2}$\n",
        "  $=\\/{1}{p(1-p)}$.\n",
        "  - $-\\/{‚àÇ}{‚àÇp}\\ln c(p)=-\\/{‚àÇ}{‚àÇp}\\ln (1-p)^n$\n",
        "  $=\\/{n(1-p)^{n-1}}{(1-p)^n}$\n",
        "  $=\\/{n}{1-p}$.\n",
        "  - $\\E[\\/{‚àÇ}{‚àÇp}w_1(p)t_1(X)]=-\\/{‚àÇ}{‚àÇp}\\ln c(p)$\n",
        "  $‚áí\\E[\\/{1}{p(1-p)}X]=\\/{n}{1-p}$\n",
        "  $‚áí\\E X=np$.\n",
        "  - $-\\/{‚àÇ^2}{‚àÇp^2}\\ln c(p)=\\/{n}{(1-p)^2}$\n",
        "  - $\\/{‚àÇ^2w_1(p)}{‚àÇp^2}=\\/{2p-1}{p^2(1-p)^2}$\n",
        "  - $\\Var(\\/{1}{p(1-p)}X)$\n",
        "  $=\\/{n}{(1-p)^2}-\\/{2p-1}{p^2(1-p)^2}\\E X$\n",
        "  $=\\/{np^2-(2p+1)np}{p^2(1-p)^2}$\n",
        "  $=\\/{np-np^2}{p^2(1-p)^2}$\n",
        "  $‚áí\\Var(X)=np(1-p)$.\n",
        "\n",
        "- 3.4.4 (normal): $f(x|Œº,œÉ^2)=\\/{1}{\\sqrt{2œÄœÉ}}\\e{-\\/{(x-Œº)^2}{2œÉ^2}}$\n",
        "$=\\/{1}{\\sqrt{2œÄ}œÉ}\\e{-\\/{Œº^2}{2œÉ^2}-\\/{x^2}{2œÉ^2}+\\/{xŒº}{œÉ^2}}$\n",
        "$=\\/{1}{\\sqrt{2œÄ}œÉ}\\e{-\\/{Œº^2}{2œÉ^2}}\\e{-\\/{x^2}{2œÉ^2}+\\/{xŒº}{œÉ^2}}$.\n",
        "  - $h(x)=1$, $c(Œº,œÉ)=\\/{1}{\\sqrt{2œÄ}œÉ}\\e{-\\/{Œº^2}{2œÉ^2}}$, $w_1(Œº,œÉ)=\\/{1}{œÉ^2}$, $w_2(Œº,œÉ)=\\/{Œº}{œÉ^2}$, $t_1(x)=\\/{-x^2}{2}$, $t_2(x)=x$\n",
        "  - By convention $w_1$ is positive and simpler while $t_1$ holds more algebraic complexity.\n",
        "\n",
        "- 3.4.6 (normal): $f(x|Œº,œÉ^2)=\\/{1}{\\sqrt{2œÄ}œÉ}\\e{-\\/{Œº^2}{2œÉ^2}}\\e{-\\/{x^2}{2œÉ^2}+\\/{xŒº}{œÉ^2}}$.\n",
        "Let $Œ∑_1=\\/{1}{œÉ^2}$, $Œ∑_2=\\/{Œº}{œÉ^2}$.\n",
        "$f(x|Œ∑_1,Œ∑_2)=\\/{\\sqrt{Œ∑_1}}{\\sqrt{2œÄ}}\\e{-\\/{Œ∑_2}{2Œ∑_1}}\\e{-Œ∑_1\\/{x^2}{2}+Œ∑_2x}$.\n",
        "$\\m{H}=\\{(Œ∑_1,Œ∑_2):Œ∑_1>0, Œ∑_2‚àà‚Ñù\\}$.\n",
        "\n",
        "- 3.15: Let $X‚àº\\t{nbinom}(r,p)$ and $P(X=x)=\\binom{x+r-1}{r-1}p^r(1-p)^x$. Show convergence to Poisson with $p‚Üí1,r‚Üí‚àû$.\n",
        "  - Use $q=1-p$ instead: $P(X=x)=\\binom{x+r-1}{r-1}(1-q)^rq^x$. Stirling's formula: $n!‚âà\\sqrt{2œÄn}(\\/{n}{e})^n$. Then $\\liml_{r‚Üí‚àû}\\binom{x+r-1}{r-1}$\n",
        "  $‚âà\\/{\\sqrt{x+r-1}}{\\sqrt{r-1}}\\/{(x+r-1)^{x+r-1}}{(r-1)^{r-1}x!}$\n",
        "  $‚âà\\/{r^x}{x!}$.\n",
        "  $\\liml_{p‚Üí1,r‚Üí‚àû} P(X=x)‚âà\\/{r^x}{x!}e^{-qr}q^x$\n",
        "  $=\\/{e^{-qr}(qr)^x}{x!}$.\n",
        "  - Let $Y‚àº\\QGeom(p)$, then $M_Y(t)=\\suml_{i=0}^‚àûe^{ti}(1-p)^ip$\n",
        "  $=p\\suml_{i=0}^‚àû(e^t(1-p))^i$\n",
        "  $=\\/{p}{1-e^t(1-p)}$. Then\n",
        "  $M_X(t)=\\left(\\/{p}{1-e^t(1-p)}\\right)^r$.\n",
        "  $\\liml_{p‚Üí1,r‚Üí‚àû}M_X(t)=(\\/{1-q}{1-qe^t})^r$\n",
        "  $‚âàe^{-qr}e^{e^tqr}$\n",
        "  $=e^{qr(e^t-1)}$.\n",
        "\n",
        "- 3.17: Let $X‚àº\\Gamma(a,b)$, find $\\E X^n$.\n",
        "$\\E X^n=‚à´_0^‚àûx^n\\/{e^{-x/b}(x/b)^a}{xŒì(a)} dx$\n",
        "$=\\/{Œì(a+n)}{Œì(a)(1/b)^n}‚à´_0^‚àû\\/{e^{-x/b}(x/b)^{a+n}}{xŒì(a+n)}\\ dx$\n",
        "$=\\/{Œì(a+n)}{Œì(a)(1/b)^n}$.\n",
        "\n",
        "- 3.18: Let $Y‚àº\\t{nbinom}(r,p)$. Show MGF of $pY$ converges to gamma(r,1) as $p‚Üí0$.\n",
        "$M_Y(t)=(\\/{p}{1-e^t(1-p)})^r$.\n",
        "$M_{pY}(t)=(\\/{p}{1-e^{pt}(1-p)})^r$\n",
        "$‚âà(\\/{p}{1-(1+pt)(1-p)})^r$\n",
        "$=(\\/{p}{1-(1+pt-p-p^2t)})^r$\n",
        "$=(\\/{1}{pt+1-t})^r$.\n",
        "$\\liml_{p‚Üí0}M_{pY}(t)=(\\/{1}{1-t})^r$\n",
        "\n",
        "- 3.19: Show $‚à´_x^‚àû\\/{z^{a-1}e^{-z}}{Œì(a)}\\ dz=\\sum_{y=0}^{a-1}\\/{x^ye^{-x}}{y!}$. LHS is Gamma(a,1), and RHS is Pois(x).\n",
        "$‚à´_x^‚àû\\/{z^{a-1}e^{-z}}{Œì(a)}\\ dz$\n",
        "$=[\\/{-z^{a-1}e^{-z}}{Œì(a)}]_x^{‚àû}$\n",
        "$+(a-1)‚à´_x^‚àû\\/{z^{a-2}e^{-z}}{Œì(a)}\\ dz$\n",
        "$=\\/{x^{a-1}e^{-x}}{(a-1)!}$\n",
        "$+\\/{x^{a-2}e^{-x}}{(a-2)!}$\n",
        "$+...+‚à´_x^‚àûe^{-z}\\ dz$\n",
        "$=\\sum_{y=0}^{a-1}\\/{x^ye^{-x}}{y!}$.\n",
        "Or $P(X_{a,1}‚â•x)=P(Y_{x}‚â§a-1)$.\n",
        "\n",
        "- 3.22: Prove $\\E X$ and $\\Var X$.\n",
        "  - Let $X‚àº\\Pois(Œª)$. Find $\\Var X$.\n",
        "  $\\E X(X-1)=\\sum_{x=1}^‚àûx(x-1)\\/{e^{-Œª}Œª^x}{x!}$\n",
        "  $=Œª^2\\sum_{x=2}^‚àû\\/{e^{-Œª}Œª^{x-2}}{(x-2)!}=Œª^2$.\n",
        "  $\\E X^2=Œª^2+Œª$.\n",
        "  $\\Var X=\\E X^2-(\\E X)^2=Œª$.\n",
        "  - Let $X‚àº\\t{nbinom}(r,p)$. Find $\\Var X$.\n",
        "  $P(X=x)=\\binom{r+x-1}{r-1}p^rq^x$.\n",
        "  $\\E X(X-1)=\\sum_{x=1}^‚àûx(x-1)\\/{(r+x-1)!}{(r-1)!x!}p^rq^x$\n",
        "  $=\\sum_{x=2}^‚àû(r+1)r\\/{q^2}{p^2}\\/{(r+x-1)!}{(r+1)!(x-2)!}p^{r+2}q^{x-2}$\n",
        "  $=(r+1)r\\/{q^2}{p^2}\\sum_{x=2}^‚àû\\binom{r+x-1}{r+1}p^{r+2}q^{x-2}$\n",
        "  $=(r+1)r\\/{q^2}{p^2}$.\n",
        "  $\\E X=\\sum_{x=1}^‚àûx\\/{(r+x-1)!}{(r-1)!x!}p^rq^x$\n",
        "  $=\\/{rq}{p}\\sum_{x=1}^‚àû\\/{(r+x-1)!}{r!(x-1)!}p^{r+1}q^{x-1}$\n",
        "  $=\\/{rq}{p}\\sum_{x=1}^‚àû\\binom{r+x-1}{r}p^{r+1}q^{x-1}$\n",
        "  $=\\/{rq}{p}$.\n",
        "  $\\E X^2=(r+1)r\\/{q^2}{p^2}+\\/{rq}{p}$.\n",
        "  $\\Var X=\\E X^2-(\\E X)^2$\n",
        "  $=(r+1)r\\/{q^2}{p^2}+\\/{rq}{p}-\\/{(rq)^2}{p^2}$\n",
        "  $=r\\/{q^2}{p^2}+\\/{rq}{p}$\n",
        "  $=\\/{rq}{p^2}$.\n",
        "  - Let $X‚àº\\Beta(a,b)$. Find $\\E X^n$.\n",
        "  $\\E X^n=‚à´_0^1\\/{Œì(a+b)}{Œì(a)Œì(b)}x^{a+n-1}(1-x)^{b-1}\\ dx$\n",
        "  $=\\/{Œì(a+b)Œì(a+n)}{Œì(a+n+b)Œì(a)}$.\n",
        "\n",
        "- 3.28/3.29: exponential families\n",
        "  - Normal: $f(x)=\\/{1}{\\sqrt{2œÄ}œÉ}\\e{-\\/{(x-Œº)^2}{2œÉ^2}}$\n",
        "  $=\\/{1}{\\sqrt{2œÄ}œÉ}\\e{-\\/{x^2}{2œÉ^2}+\\/{Œºx}{œÉ^2}-\\/{Œº^2}{2œÉ^2}}$.\n",
        "  $f(x|Œº)=\\ob{\\/{1}{\\sqrt{2œÄ}œÉ}\\e{-\\/{x^2}{2œÉ^2}}}{h(x)}\\ob{\\e{-\\/{Œº^2}{2œÉ^2}}}{c(Œº)}\\e{\\/{Œºx}{œÉ^2}}$.\n",
        "  $f(x|œÉ)=\\ob{\\/{1}{\\sqrt{2œÄ}œÉ}}{c(œÉ)}\\e{-\\/{(x-Œº)^2}{2œÉ^2}}$.\n",
        "  $f(x|Œº,œÉ)=\\ob{\\/{1}{\\sqrt{2œÄ}}}{h(x)}\\ob{\\/{1}{œÉ}\\e{-\\/{Œº^2}{2œÉ^2}}}{c^*(\\v{Œ∑})}\\e{-\\/{x^2}{2œÉ^2}+\\/{Œºx}{œÉ^2}}$.\n",
        "  $\\m{H}=\\{(Œ∑_1=\\/{-1}{2œÉ^2},Œ∑_2=\\/{Œº}{œÉ^2}):Œ∑_1\\lt0,Œ∑_2‚àà‚Ñù\\}$.\n",
        "  - Gamma: $f(x)=\\/{e^{-x/b}(x/b)^a}{xŒì(a)}$.\n",
        "  $f(x|a)=\\ob{e^{-x/b}}{h(x)}\\ob{\\/{1}{b^aŒì(a)}}{c(a)}\\e{(a-1)\\ln x}$.\n",
        "  $f(x|b)=\\ob{\\/{x^{a-1}}{Œì(a)}}{h(x)}\\ob{\\/{1}{b^a}}{c(b)}\\e{-\\/{x}{b}}$.\n",
        "  $f(x|a,b)=\\ob{\\/{1}{b^aŒì(a)}}{c(a,b)}\\e{-\\/{x}{b}}\\e{(a-1)\\ln x}$.\n",
        "  $\\m{H}=\\{(Œ∑_1=-\\/{1}{b},Œ∑_2=a-1):Œ∑_1\\lt0,Œ∑_2>-1\\}$.\n",
        "  - Beta: $f(x)=\\/{Œì(a+b)}{Œì(a)Œì(b)}x^{a-1}(1-x)^{b-1}$.\n",
        "  $f(x|a)=\\ob{\\/{Œì(a+b)}{Œì(a)Œì(b)}}{c(a)}\\ob{(1-x)^{b-1}}{h(x)}\\e{(a-1)\\ln x}$.\n",
        "  - Poisson: $f(x)=\\/{e^{-Œª}Œª^x}{x!}$.\n",
        "  $f(x|Œª)=\\ob{e^{-Œª}}{c(Œª)}\\ob{\\/{1}{x!}}{h(x)}\\e{x\\ln Œª}$.\n",
        "  $\\m{H}=\\{(Œ∑_1=\\ln Œª):Œ∑_1‚àà‚Ñù\\}$.\n",
        "  - NBinom: $f(x)=\\binom{r+x-1}{r-1}p^r(1-p)^x$.\n",
        "  $f(x|p)=\\ob{\\binom{r+x-1}{r-1}}{h(x)}\\ob{p^r}{c(p)}\\e{x\\ln(1-p)}$.\n",
        "\n",
        "- 3.31: $1=‚à´h(x)c(\\v{Œ∏})\\e{\\suml_{i=1}^kw_i(\\v{Œ∏})t_i(x)}\\ dx$. Differentiate both sides.\n",
        "$0=\\/{‚àÇ}{‚àÇŒ∏_j}‚à´h(x)c(\\v{Œ∏})\\e{\\suml_{i=1}^kw_i(\\v{Œ∏})t_i(x)}\\ dx$\n",
        "$=‚à´h(x)\\/{‚àÇc(\\v{Œ∏})}{‚àÇŒ∏_j}\\e{\\suml_{i=1}^kw_i(\\v{Œ∏})t_i(x)}\\ dx$\n",
        "$+‚à´h(x)c(\\v{Œ∏})\\e{\\suml_{i=1}^kw_i(\\v{Œ∏})t_i(x)}\\left[\\suml_{i=1}^k\\/{‚àÇw_i(\\v{Œ∏})}{‚àÇŒ∏_j} t_i(x)\\right]\\ dx$\n",
        "$=‚à´h(x)c(\\v{Œ∏})\\/{‚àÇ\\ln c(\\v{Œ∏})}{‚àÇŒ∏_j}\\e{\\suml_{i=1}^kw_i(\\v{Œ∏})t_i(x)}\\ dx$\n",
        "$+\\E\\left[\\suml_{i=1}^k\\/{‚àÇw_i(\\v{Œ∏})}{‚àÇŒ∏_j} t_i(x)\\right]\\ dx$\n",
        "$=\\/{‚àÇ}{‚àÇŒ∏_j}\\ln c(\\v{Œ∏})+\\E\\left[\\suml_{i=1}^k\\/{‚àÇw_i(\\v{Œ∏})}{‚àÇŒ∏_j} t_i(x)\\right]\\ dx$\n",
        "\n",
        "- 3.32: In $f(x|\\v{Œ∑})=h(x)c^*(\\v{Œ∑})\\e{\\suml_{i=1}^kŒ∑_it_i(x)}$. Original parameters were $\\v{Œ∏}=(Œ∏_1,...,Œ∏_d)$, and the natural parameters are $\\v{Œ∑}=(Œ∑_1,...,Œ∑_k)$.\n",
        "  - Multivariate chain rule: Let $z=f(x,y)$, $x=g(t)$, and $y=h(t)$, then\n",
        "  $\\/{dz}{dt}=\\/{dz}{dx}\\/{dx}{dt}+\\/{dz}{dy}\\/{dy}{dt}$. Therefore\n",
        "  $\\/{‚àÇ}{‚àÇŒ∏_j}=\\/{‚àÇ}{‚àÇŒ∑_1}\\/{‚àÇŒ∑_1}{‚àÇŒ∏_j}+...+\\/{‚àÇ}{‚àÇŒ∑_k}\\/{‚àÇŒ∑_k}{‚àÇŒ∏_j}$\n",
        "  $=\\suml_{i=1}^k\\/{dŒ∑_i}{dŒ∏_j}\\/{d}{dŒ∑_i}$.\n",
        "  - $Œ∑_j=w_j(\\v{Œ∏})$ and $c^*(\\v{Œ∑})=c(\\v{Œ∏}(\\v{Œ∑}))$.\n",
        "  LHS $\\E[\\suml_{i=1}^k\\/{‚àÇw_i(\\v{Œ∏})}{‚àÇŒ∏_j}t_i(X)]$\n",
        "  $=\\suml_{i=1}^k\\/{‚àÇŒ∑_i}{‚àÇŒ∏_j}\\E[t_i(X)]$.\n",
        "  RHS $-\\/{‚àÇ}{‚àÇŒ∏_j}\\ln c(\\v{Œ∏})$\n",
        "  $=-\\suml_{i=1}^k\\/{‚àÇŒ∑_i}{‚àÇŒ∏_j}\\/{‚àÇ}{‚àÇŒ∑_i}\\ln c^*(\\v{Œ∑})$. Therefore $\\E[t_i(X)]=-\\/{‚àÇ}{‚àÇŒ∑_i}\\ln c^*(\\v{Œ∑})$."
      ],
      "metadata": {
        "id": "7-C_wHmytFVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casella Chapter 4: Multiple random variables\n",
        "\n",
        "- 4.2.13 (sum of independent normals): Let $X‚àº\\Normal(Œº,œÉ^2)$ and $Y‚àº\\Normal(Œ≥,œÑ^2)$ be independent.\n",
        "$M_X(t)=\\e{Œºt+\\/{œÉ^2t^2}{2}}$ and\n",
        "$M_Y(t)=\\e{Œ≥t+\\/{œÑ^2t^2}{2}}$. Then\n",
        "$M_{X+Y}(t)=\\e{(Œº+Œ≥)t+\\/{(œÉ^2+œÑ^2)t^2}{2}}$.\n",
        "Therefore $X+Y‚àº\\Normal(Œº+Œ≥,œÉ^2+œÑ^2)$.\n",
        "\n",
        "- 4.3.1 (sum of independent Poissons): Let $X‚àº\\Pois(Œª)$ and $Y‚àº\\Pois(Œ∏)$ be independent. Let $U=X+Y$ and $V=Y$.\n",
        "$f_{X,Y}(x,y)=\\/{e^{-Œ∏}Œ∏^y}{y!}\\/{e^{-Œª}Œª^x}{x!}$.\n",
        "$f_{U,V}(u,v)=\\/{e^{-Œ∏}Œ∏^v}{v!}\\/{e^{-Œª}Œª^{u-v}}{(u-v)!},\\ u‚â•v$.\n",
        "$f_U(u)=\\suml_{v=0}^u\\/{e^{-Œ∏}Œ∏^v}{v!}\\/{e^{-Œª}Œª^{u-v}}{(u-v)!}$\n",
        "$=\\/{e^{-(Œ∏+Œª)}}{u!}\\suml_{v=0}^u\\binom{u}{v}Œ∏^vŒª^{u-v}$\n",
        "$=\\/{e^{-(Œ∏+Œª)}(Œ∏+Œª)^u}{u!}$.\n",
        "Therefore $X+Y‚àº\\Pois(Œ∏+Œª)$.\n",
        "\n",
        "- 4.4.1 (Binomial-Poisson hierarchy): $Y‚àº\\Pois(Œª)$ eggs laid, $X|Y‚àº\\Binom(Y,p)$ eggs survive and hatch.\n",
        "$P(X=x)=\\suml_{y=x}^‚àûP(X=x,Y=y)$\n",
        "$=\\suml_{y=x}^‚àûP(X=x|y)P(Y=y)$\n",
        "$=\\suml_{y=x}^‚àû\\binom{y}{x}p^xq^{y-x}\\/{e^{-Œª}Œª^y}{y!}$\n",
        "$=\\suml_{y=x}^‚àû\\/{y!e^{-pŒª}(pŒª)^xe^{-qŒª}(qŒª)^{y-x}}{x!(y-x)!y!}$\n",
        "$=\\/{e^{-pŒª}(pŒª)^x}{x!}\\suml_{y=x}^‚àû\\/{e^{-qŒª}(qŒª)^{y-x}}{(y-x)!}$\n",
        "$=\\/{e^{-pŒª}(pŒª)^x}{x!}$.\n",
        "Therefore $X‚àº\\Pois(pŒª)$.\n",
        "\n",
        "- 4.4.5 (3-stage hierarchy): $Y|Œõ‚àº\\Pois(Œõ)$ eggs laid, $X|Y‚àº\\Binom(Y,p)$ eggs survive, $Œõ‚àº\\Expo(b)$. Find $\\E X$.\n",
        "$\\E X=\\E(\\E(X|Y))$\n",
        "$=\\E(pY)$\n",
        "$=p\\E(\\E(Y|Œõ))$\n",
        "$=p\\E(Œõ)$\n",
        "$=pb$.\n",
        "  - (Poisson-Gamma): $P(Y=y)=‚à´_ŒªP(Y=y|Œª)f(Œª)\\ dŒª$\n",
        "  $=‚à´_0^‚àû\\/{e^{-Œª}Œª^y}{y!}\\/{1}{b}e^{-Œª/b}\\ dŒª$\n",
        "  $=\\/{1}{b}‚à´_0^‚àû\\/{e^{-Œª(1+\\/{1}{b})}Œª^{y+1}}{ŒªŒì(y+1)}\\ dŒª$\n",
        "  $=\\/{1}{b(1+\\/{1}{b})^{y+1}}‚à´_0^‚àû\\/{e^{-Œª(1+\\/{1}{b})}(Œª(1+\\/{1}{b}))^{y+1}}{ŒªŒì(y+1)}\\ dŒª$\n",
        "  $=\\/{1}{b(1+\\/{1}{b})^{y+1}}$\n",
        "  $=\\/{b^y}{(b+1)^{y+1}}$\n",
        "  $=\\/{1}{b+1}(\\/{b}{b+1})^y$.\n",
        "  Therefore $Y‚àº\\t{NBinom}(1,\\/{1}{b+1})$.\n",
        "\n",
        "- 4.4.6/4.4.8 (Beta-binomial): Let $X|P‚àº\\Binom(n,P)$ and $P‚àº\\Beta(a,b)$. Find $\\E X$ and $\\Var X$.\n",
        "$\\E X=\\E(\\E(X|P))$\n",
        "$=\\E(nP)$\n",
        "$=\\/{na}{a+b}$.\n",
        "$\\Var(\\E(X|P))=\\Var(nP)=\\/{n^2ab}{(a+b)^2(a+b+1)}$.\n",
        "$\\E(\\Var(X|P))=\\E(nP(1-P))$\n",
        "$=\\/{nŒì(a+b)}{Œì(a)Œì(b)}‚à´_0^1p^a(1-p)^b\\ dp$\n",
        "$=\\/{nab}{(a+b)(a+b+1)}$.\n",
        "$\\Var(X)=\\/{nab(n+a+b)}{(a+b)^2(a+b+1)}$.\n",
        "\n",
        "- 4.7.8: Let arithmetic $a_A=\\/{1}{n}\\suml_{i=1}^na_i$,\n",
        "geometric $a_G=(\\prodl_{i=1}^na_i)^{1/n}$,\n",
        "harmonic $a_H=(\\/{1}{n}\\suml_{i=1}^n\\/{1}{a_i})^{-1}$. $a_A‚â•a_G‚â•a_H$.\n",
        "$a_A=\\E X$.\n",
        "$\\ln a_G=\\/{1}{n}\\suml_{i=1}^n\\ln a_i=\\E(\\ln X)‚áí\\ln a_A‚â•\\ln a_G$.\n",
        "$\\/{1}{a_H}=\\/{1}{n}\\suml_{i=1}^n\\/{1}{a_i}$\n",
        "$=\\E \\/{1}{X}‚áí\\/{1}{a_H}‚â•\\/{1}{a_A}‚áía_A‚â•a_H$.\n",
        "$\\ln \\/{1}{a_H}=\\ln \\E \\/{1}{X}$.\n",
        "$\\ln \\/{1}{a_G}=\\E \\ln \\/{1}{X}$\n",
        "$‚áí\\ln \\/{1}{a_H}‚â•\\ln \\/{1}{a_G}$\n",
        "$‚áía_G‚â•a_H$.\n",
        "\n",
        "- 4.17 (exponential-geometric): let $X‚àº\\Expo(1)$, and $Y$ be the integer part of $X+1$ so that $Y=i+1$ iff $i‚â§X\\lt i+1$. Find distribution of $Y$.\n",
        "$P(Y=i+1)=‚à´_i^{i+1}e^{-x}\\ dx$\n",
        "$=e^{-i}-e^{-(i+1)}$\n",
        "$=e^{-i}(1-e^{-1})$\n",
        "$=(\\/{1}{e})^i(1-\\/{1}{e})$.\n",
        "Therefore $Y‚àº\\Geom(1-\\/{1}{e})$.\n",
        "\n",
        "- 4.19a: Let $X_1,X_2‚àº\\Normal(0,1)$ iid. Find PDF of $U=\\/{(X_1-X_2)^2}{2}$.\n",
        "  - $X_1-X_2‚àº\\Normal(0,2)$ and by scale\n",
        "  $\\/{X_1-X_2}{\\sqrt{2}}‚àº\\Normal(0,1)$.\n",
        "  Therefore $U‚àºœá_1^2‚àº\\Gamma(\\/{1}{2},2)$.\n",
        "  $f_U(x)=\\/{e^{-x/2}(x/2)^{1/2}}{x\\sqrt{œÄ}}$\n",
        "  $=\\/{1}{\\sqrt{2œÄx}}e^{-x/2}$\n",
        "  - Let $V=\\/{X_1+X_2}{2}$\n",
        "  $‚áíX_2=2V-X_1$.\n",
        "  $X_1=¬±\\sqrt{2U}+2V-X1$\n",
        "  $‚áíX_1=V¬±\\sqrt{\\/{U}{2}}$ and\n",
        "  $X_2=V‚àì\\sqrt{\\/{U}{2}}$.\n",
        "  $|\\/{‚àÇ(x_1,x_2)}{‚àÇ(u,v)}|=\\/{1}{2\\sqrt{U/2}}$.\n",
        "  $f_{U,V}=\\/{\\red{2}}{2œÄ\\sqrt{2u}}e^{-(v+\\sqrt{u/2})^2/2}e^{-(v-\\sqrt{u/2})^2/2}$\n",
        "  $=\\/{1}{œÄ\\sqrt{2u}}e^{-(v^2+2v\\sqrt{u/2}+u/2+v^2-2v\\sqrt{u/2}+u/2)/2}$\n",
        "  $=\\/{1}{œÄ\\sqrt{2u}}e^{-(2v^2+u)/2}$.\n",
        "  $f_U(u)=\\/{1}{2œÄ\\sqrt{2u}}e^{-u/2}‚à´_{-‚àû}^‚àûe^{-v^2}\\ dv$.\n",
        "  Let $v^2=x^2/2$ then $dv=\\/{1}{\\sqrt{2}}\\ dx$.\n",
        "  $f_U(u)=\\/{1}{2œÄ\\sqrt{2u}}e^{-u/2}\\/{1}{\\sqrt{2}}‚à´_{-‚àû}^‚àûe^{-x^2/2}\\ dx$\n",
        "  $=\\/{1}{\\sqrt{2œÄu}}e^{-u/2}$\n",
        "\n",
        "- 4.19b (Gamma-Beta): let $X_1‚àº\\Gamma(a_1,1)$ and $X_2‚àº\\Gamma(a_2,1)$ be independent. Find marginal distribution of $U=\\/{X_1}{X_1+X_2}$.\n",
        "Let $V=X_1+X_2$.\n",
        "$X_2=V-X_1$,\n",
        "$X_1=\\/{UX_2}{1-U}$\n",
        "$=\\/{U(V-X_1)}{1-U}$.\n",
        "$X_1=UV$.\n",
        "$X_2=(1-U)V$.\n",
        "$|\\/{‚àÇ(x_1,x_2)}{‚àÇ(u,v)}|=v(1-u)+uv=v$.\n",
        "$f_{U,V}(u,v)=v\\/{e^{-uv}(uv)^{a_1}}{uvŒì(a_1)}\\/{e^{-(1-u)v}((1-u)v)^{a_2}}{(1-u)vŒì(a_2)}$\n",
        "$=v\\/{e^{-v}(uv)^{a_1-1}((1-u)v)^{a_2-1}}{Œì(a_1)Œì(a_2)}$.\n",
        "$f_U(u)=\\/{u^{a_1-1}(1-u)^{a_2-1}}{Œì(a_1)Œì(a_2)}‚à´_0^‚àûv^{a_1+a_2-1}e^{-v}\\ dv$\n",
        "$=\\/{Œì(a_1+a_2)}{Œì(a_1)Œì(a_2)}u^{a_1-1}(1-u)^{a_2-1}$.\n",
        "Therefore $U‚àº\\Beta(a_1,a_2)$\n",
        "\n",
        "- 4.20: $X_1,X_2‚àº\\Normal(0,œÉ^2)$ iid. Find distribution of $Y_1=X_1^2+X_2^2$ and $Y_2=\\/{X_1}{\\sqrt{Y_1}}$.\n",
        "$X_1=\\sqrt{Y_1}Y_2$ and $X_2=¬±\\sqrt{Y_1(1-Y_2^2)}$.\n",
        "$|\\/{‚àÇ(x_1,x_2)}{‚àÇ(y_1,y_2)}|=|\\/{y_2}{2\\sqrt{y_1}}\\/{‚àìy_1y_2}{\\sqrt{y_1(1-y_2^2)}}-\\sqrt{y_1}\\/{¬±(1-y_2^2)}{2\\sqrt{y_1(1-y_2^2)}}|$\n",
        "$=|\\/{‚àìy_2^2‚àì(1-y_2^2)}{2\\sqrt{1-y_2^2}}|$\n",
        "$=\\/{1}{2\\sqrt{1-y_2^2}}$.\n",
        "$f_{U,V}(u,v)=\\/{\\red{2}}{4œÄœÉ^2}\\/{1}{\\sqrt{1-y_2^2}}e^{-\\/{y_1}{2œÉ^2}}$\n",
        "\n",
        "- 4.26 (first principles of joint, marginal, independence): $X‚àº\\Expo(Œª)$ and $Y‚àº\\Expo(Œº)$. Let $Z=\\min(X,Y)$ and $W=I_{Z=X}$. Find joint distribution of $Z$ and $W$ and show they are independent.\n",
        "  - $P(Z‚â§a,W=0)=P(Y‚â§a,Y‚â§X)$\n",
        "  $=\\iint_{y‚â§a,x‚â•y}ŒªŒºe^{-Œªx}e^{-Œºy}\\ dx\\ dy$\n",
        "  $=‚à´_{y‚â§a}Œºe^{-Œºy}\\ dy‚à´_{x‚â•y}Œªe^{-Œªx}\\ dx$\n",
        "  $=‚à´_{y‚â§a}Œºe^{-(Œº+Œª)y}\\ dy$\n",
        "  $=\\/{Œº}{Œº+Œª}(1-e^{-(Œº+Œª)a})$.\n",
        "  Similarly $P(Z‚â§a,W=1)=\\/{Œª}{Œº+Œª}(1-e^{-(Œº+Œª)a})$.\n",
        "  - $P(W=0)=P(Z‚â§‚àû,W=0)=\\/{Œº}{Œº+Œª}$, and\n",
        "  $P(W=1)=\\/{Œª}{Œº+Œª}$.\n",
        "  $P(Z‚â§a)=P(Z‚â§a,W=0)+P(Z‚â§a,W=1)$\n",
        "  $=1-e^{-(Œº+Œª)a}$.\n",
        "  Therefore $P(Z‚â§a,W=w)=P(Z‚â§a)P(W=w)$.\n",
        "\n",
        "- 4.30: $Y|X=x‚àº\\Normal(x,x^2)$, $X‚àº\\Unif(0,1)$. Find $\\E Y$, $\\Var Y$, and $\\Cov(X,Y)$.\n",
        "$\\E Y=\\E[\\E(Y|X)]$\n",
        "$=\\E X$\n",
        "$=\\/{1}{2}$.\n",
        "$\\Var(\\E(Y|X))=\\Var(X)$\n",
        "$=\\/{1}{12}$.\n",
        "$\\E(\\Var(Y|X))=\\E X^2$\n",
        "$=\\/{1}{12}+(\\/{1}{2})^2=\\/{1}{3}$.\n",
        "$\\Var(Y)=\\/{5}{12}$.\n",
        "$\\Cov(X,Y)=\\E XY-\\E X\\E Y$\n",
        "$=\\E[X\\E(Y|X)]-\\E X\\E Y$\n",
        "$=\\E X^2-\\/{1}{2}\\/{1}{2}=\\/{1}{12}$.\n",
        "\n",
        "- 4.31 (binomial-uniform): $Y|X‚àº\\Binom(n,X)$ and $X‚àº\\Unif(0,1)$. Find $\\E Y$, $\\Var Y$, joint PDF, and marginal PDF.\n",
        "  - $\\E Y=\\E[\\E(Y|X)]$\n",
        "  $=\\E[nX]$\n",
        "  $=\\/{n}{2}$.\n",
        "  $\\E[\\Var(Y|X)]=\\E[nX(1-X)]$\n",
        "  $=\\/{n}{2}-\\/{n}{3}=\\/{n}{12}$\n",
        "  $\\Var(\\E(Y|X))=\\Var(nX)=\\/{n^2}{12}$.\n",
        "  $\\Var(Y)=\\/{n(n+1)}{12}$.\n",
        "  - $f_{X,Y}(x,y)=f_{Y|X}(y|x)f_X(x)$\n",
        "  $=\\binom{n}{y}x^y(1-x)^{n-y}$.\n",
        "  $f_Y(y)=\\binom{n}{y}‚à´_0^1x^y(1-x)^{n-y}\\ dx$\n",
        "  $=\\/{Œì(n+1)}{Œì(y+1)Œì(n-y+1)}‚à´_0^1x^y(1-x)^{n-y}\\ dx$\n",
        "  $=\\/{Œì(n+1)}{Œì(n+2)}‚à´_0^1\\/{Œì(n+2)}{Œì(y+1)Œì(n-y+1)}x^y(1-x)^{n-y}\\ dx$\n",
        "  $=\\/{Œì(n+1)}{Œì(n+2)}$\n",
        "  $=\\/{1}{n+1}$.\n",
        "\n",
        "- 4.32 (poisson-gamma): $Y|Œõ‚àº\\Pois(Œõ)$ and $Œõ‚àº\\Gamma(a,b)$. Find marginal PDF.\n",
        "$f(y,Œª)=\\/{e^{-Œª}Œª^y}{y!}\\/{e^{-Œª/b}(Œª/b)^a}{ŒªŒì(a)}$\n",
        "$=\\/{1}{y!Œì(a)b^a}\\/{e^{-Œª(1+1/b)}Œª^{y+a}}{Œª}$\n",
        "$=\\/{Œì(y+a)}{y!Œì(a)b^a(1+1/b)^{y+a}}\\/{e^{-Œª(1+1/b)}(Œª(1+1/b))^{y+a}}{ŒªŒì(y+a)}$.\n",
        "$f(y)=\\/{Œì(y+a)}{y!Œì(a)b^a(1+1/b)^{y+a}}$\n",
        "$=\\binom{y+a-1}{a-1}(\\/{1}{b})^a(\\/{1}{1+1/b})^{y+a}$\n",
        "$=\\binom{y+a-1}{a-1}(\\/{1}{b}\\/{b}{b+1})^a(\\/{b}{b+1})^y$\n",
        "$=\\binom{y+a-1}{a-1}(\\/{1}{b+1})^a(\\/{b}{b+1})^y$.\n",
        "Therefore $Y‚àº\\t{NBinom}(a,\\/{1}{b+1})$.\n",
        "\n",
        "- 4.34 (beta-binomial): $X|P‚àº\\Binom(n,P)$ and $P‚àº\\Beta(a,b)$. Find marginal $X$.\n",
        "$f(x,p)=\\binom{n}{x}p^x(1-p)^{n-x}\\/{Œì(a+b)}{Œì(a)Œì(b)}p^{a-1}(1-p)^{b-1}$\n",
        "$=\\binom{n}{x}\\/{Œì(a+b)}{Œì(a)Œì(b)}p^{x+a-1}(1-p)^{n-x+b-1}$.\n",
        "$f(x)=\\binom{n}{x}\\/{Œì(a+b)}{Œì(a)Œì(b)}\\/{Œì(x+a)Œì(n-x+b)}{Œì(n+a+b)}$"
      ],
      "metadata": {
        "id": "YeG4Q2LpdXt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample, Normal, Convergence\n",
        "- $\\suml_{i=1}^n(x_i-\\bar{x})^2=\\suml_{i=1}^nx_i^2-n\\bar{x}^2$ and $\\suml_{i=1}^n(x_i-a)^2=\\suml_{i=1}^n(x_i-\\bar{x})^2+n(\\bar{x}-a)^2$\n",
        "- $\\E[g(T_n)]‚Üíg(\\E[T_n])$ and $\\Var(g(T_n))‚Üí[g'(E[T_n])]^2\\Var(T_n)$\n",
        "- $\\/{(n-1)S^2}{œÉ^2}‚àºœá_{n-1}^2$ and $F_{n_X-1,n_Y-1}=\\/{S_X^2/S_Y^2}{œÉ_X^2/œÉ_Y^2}=\\/{S_X^2/œÉ_X^2}{S_Y^2/œÉ_Y^2}$"
      ],
      "metadata": {
        "id": "uVbeIY9o3e-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random sample** of size $n$ from infinite population are iid random variables $X_1,...,X_n$ where each $X_i$ has marginal PDF/PMF $f(x)$, joint PDF/PMF $f(x_1,...,x_n)=\\prodl_{i=1}^nf(x_i)$, and parametrically $f(x_1,...,x_n|Œ∏)=\\prodl_{i=1}^nf(x_i|Œ∏)$.\n",
        "\n",
        "- Sampling finite population $N$ without replacement is not iid: $P(X_2=x)=\\suml_{i=1}^NP(X_2=x|X_1=x_i)P(X_1=x_i)$\n",
        "\n",
        "- $Y=T(X_1,...,X_n)$ is a **statistic** for random sample of size $n$ where $T(x_1,...,x_n)$ is a $\\m{X}^n‚Üí‚Ñù$ function. It is also called an estimator because it is intended to approximate a population parameter.\n",
        "\n",
        "  - $\\bar{X}=\\/{1}{n}\\suml_{i=1}^nX_i$ (Casella 5.2.2) and\n",
        "  - $S^2=\\/{1}{n-1}\\suml_{i=1}^n(X_i-\\bar{X})^2$  (Casella 5.2.3)\n",
        "\n",
        "- For a numbers $x_1,...,x_n$ and $\\bar{x}=\\/{1}{n}\\sum_{i=1}^nx_i$, then\n",
        "(Casella 5.2.4)\n",
        "\n",
        "  1. $\\min\\limits_a\\suml_{i=1}^n(x_i-a)^2=\\suml_{i=1}^n(x_i-\\bar{x})^2$. Or $\\blue{\\suml_{i=1}^n(x_i-a)^2‚â•\\suml_{i=1}^n(x_i-\\bar{x})^2}$.\n",
        "\n",
        "  2. $\\blue{\\suml_{i=1}^n(x_i-\\bar{x})^2=\\suml_{i=1}^nx_i^2-n\\bar{x}^2}$.\n",
        "  - Proof:\n",
        "  $\\sum_{i=1}^n(x_i-a)^2$\n",
        "  $=\\sum_{i=1}^n(x_i-\\bar{x}+\\bar{x}-a)^2$\n",
        "  $=\\sum_{i=1}^n(x_i-\\bar{x})^2$\n",
        "  $\\ub{+2(\\bar{x}-a)\\sum_{i=1}^n(x_i-\\bar{x})}{0}$\n",
        "  $+\\sum_{i=1}^n(\\bar{x}-a)^2$.\n",
        "  If $a=\\bar{x}$ then $\\sum_{i=1}^n(x_i-a)^2=\\sum_{i=1}^n(x_i-\\bar{x})^2$. If $a=0$ then $\\sum_{i=1}^n(x_i-\\bar{x})^2=\\sum_{i=1}^nx_i^2-n\\bar{x}^2$.\n",
        "\n",
        "  3. $\\blue{\\suml_{i=1}^n(x_i-Œº)^2=\\suml_{i=1}^n(x_i-\\bar{x})^2+n(\\bar{x}-Œº)^2}$.\n",
        "  - proof: $\\suml_{i=1}^n(x_i-Œº)^2$\n",
        "  $=\\sum_{i=1}^n(x_i-\\bar{x}+\\bar{x}-Œº)^2$\n",
        "  $=\\sum_{i=1}^n(x_i-\\bar{x})^2+2\\ub{\\sum_{i=1}^n(x_i-\\bar{x})}{0}(\\bar{x}-Œº)+n(\\bar{x}-Œº)^2$\n",
        "  $=\\sum_{i=1}^n(x_i-\\bar{x})^2+n(\\bar{x}-Œº)^2$.\n",
        "\n",
        "- For a random sample, $\\E\\left[\\suml_{i=1}^ng(X_i)\\right]=n\\E[g(X_1)]$ and $\\Var\\left(\\suml_{i=1}^ng(X_i)\\right)=n\\Var(g(X_1))$\n",
        "(Casella 5.2.5)\n",
        "\n",
        "- For a random sample with mean $Œº$ and variance $œÉ^2$, then $\\E \\bar{X}=Œº$, $\\Var \\bar{X}=\\/{œÉ^2}{n}$, and $\\E S^2=œÉ^2$\n",
        "(Casella 5.2.6)\n",
        "\n",
        "- For a random sample, $M_{X_1+...+X_n}(t)=(M_{X_i}(t))^n$ and $M_{\\bar{X}}(t)=(M_{X_i}(t/n))^n$.\n",
        "(Casella 5.2.7)\n",
        "\n",
        "- For a random sample $Z_1,...,Z_n$, if $X_i=œÉZ_i+Œº$ then $\\bar{X}=œÉ\\bar{Z}+Œº$.\n",
        "\n",
        "- Let $X_1,...,X_n$ iid have PDF/PMF\n",
        "$f_{X_j}(x_j|\\v{Œ∏})=h(x_j)c(\\v{Œ∏})\\e{\\suml_{i=1}^kw_i(\\v{Œ∏})t_i(x_j)}$,\n",
        "where $\\v{Œ∏}=(Œ∏_1,...,Œ∏_d),\\ d=k$. Then $T(\\v{X})=\\left(\\suml_{j=1}^nt_1(X_j),...,\\suml_{j=1}^nt_k(X_j)\\right)$ have joint distribution $f_T(\\v{s}|\\v{Œ∏})=H(\\v{s})[c(\\v{Œ∏})]^n\\e{\\suml_{i=1}^kw_i(\\v{Œ∏})s_i}$ if $\\{(w_1(\\v{Œ∏}),...,w_k(\\v{Œ∏})), \\v{Œ∏}‚ààŒò\\}$ contains an open subset of $‚Ñù^k$.\n",
        "(Casella 5.2.11)\n",
        "\n",
        "  - $d=k$ is required for the parameter set to contain an open subset of $‚Ñù^k$. This theorem says the sample distributions of estimators on full exponential family populations are themselves exponential families."
      ],
      "metadata": {
        "id": "dMstIZFH3i8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chi-squared distribution**: Let $Z‚àº\\Normal(0,1)$, then $Z^2‚àºœá_1^2$. If $X_1‚àºœá_p^2$ and $X_2‚àºœá_q^2$ are independent, then $X_1+X_2‚àºœá_{p+q}^2$. (Casella 5.3.2)\n",
        "- $X_1-X_2‚àºÃ∏ œá_{p-q}^2$. Chi-squared is strictly positive but $X_1-X_2$ could be negative.\n",
        "- $\\blue{\\E œá_p^2=p}$, $\\blue{\\Var(œá_p^2)=2p}$, and $œá_p^2‚àº\\Gamma(\\/{p}{2},2)‚áí\\blue{\\E (œá_p^2)^n=\\/{2^nŒì(\\/{p}{2}+n)}{Œì(\\/{p}{2})}}$\n",
        "\n",
        "**Cochran's Theorem**: Let $\\v{Z}‚àº\\Normal(0,\\v{I}_n)$ be $n√ó1$ iid random vector. Then\n",
        "$Q=\\v{Z}^‚ä§\\v{Z}=\\suml_{i=1}^nZ_i^2$\n",
        "can be partitioned into $k$ quadratic forms:\n",
        "$Q=\\v{Z}^‚ä§\\v{A}_1\\v{Z}+...+\\v{Z}^‚ä§\\v{A}_k\\v{Z}$.\n",
        "This is equivalent to the identity\n",
        "$\\v{I}=\\v{A}_1+...+\\v{A}_k$\n",
        "where $\\v{A}_i$ is a symmetric idempotent $n√ón$ matrix with rank $r_i$. If one of the following equivalent conditions are met then all 3 are met:\n",
        "\n",
        "1. (rank condition): $r_1+...+r_k=n$.\n",
        "\n",
        "2. (idempotent condition): $\\v{A}_i^2=\\v{A}_i$.\n",
        "\n",
        "3. (orthogonality condition): $\\v{A}_i\\v{A}_j=\\v{0}$ for all $i\\neq j$.\n",
        "\n",
        "  Then the following statements are true:\n",
        "\n",
        "1. (distribution): $Q_i=\\v{Z}^‚ä§\\v{A}_i\\v{Z}‚àºœá_{r_i}^2$.\n",
        "\n",
        "2. (independence): $Q_1,...,Q_k$ are mutually independent.\n",
        "\n",
        "- Corollary: If $\\v{A}$ is a symmetric idempotent matrix with rank $r$, then $\\v{Z}^‚ä§\\v{A}\\v{Z}‚àºœá_r^2$.\n",
        "  \n",
        "**Cochran's Theorem**: Let $Z_1,...,Z_n‚àº\\Normal(0,1)$ iid, and $Q=\\suml_{i=1}^nZ_i^2‚àºœá_n^2$. Then $Q=Q_1+...+Q_k$ where each $Q_i$ is a quadratic form with rank $r_i$ degrees of freedom. If $\\suml_{i=1}^kr_i=n$, then $Q_1,...,Q_k$ are independent and $Q_i‚àºœá_{r_i}^2$.\n",
        "\n",
        "- **Quadratic form** is a polynomial where every term is $cZ_i^2$ or $cZ_iZ_j$ with no linear terms $cZ_i$ or constants $c$. A quadratic form takes the notation $Q_i=\\v{Z}^‚ä§\\v{A}_i\\v{Z}$ where $\\v{Z}=[Z_1,...,Z_n]$ and $\\v{A}_i$ is an idempotent (meaning $\\v{A}_i^2=\\v{A}_i$) symmetric matrix. $\\v{A}_i$ has only '1' and '0' eigenvalues iff it is idempotent.\n",
        "\n",
        "- **Degrees of freedom** (rank) $r_i$ is the number of '1' eigenvalues that $\\v{A}_i$ has. Rank $r_i$ is the number of iid $Z^2$-s that $Q_i$ is algebraically equivalent to. E.g., $Q_1=Z_1^2+Z_2^2$ has $r_1=2$, and $Q_2=\\/{(Z_1+Z_2)^2}{2}=(\\/{Z_1+Z_2}{\\sqrt{2}})^2$ has $r_2=1$.\n",
        "\n",
        "**Normal zero covariance ‚áî independence**: Let $X_1,...,X_n‚àº\\Normal(Œº_j,œÉ_j^2)$ be independent. Let $U_{i=1..k}=\\suml_{j=1}^na_{ij}X_j$ and $V_{r=1..m}=\\suml_{j=1}^nb_{rj}X_j$ where $k+m‚â§n$. Then\n",
        "\n",
        "1. (**pairwise independent**) $U_i$ and $V_r$ are independent iff $\\Cov(U_i,V_r)=\\suml_{j=1}^na_{ij}b_{rj}œÉ_j^2=0$.\n",
        "\n",
        "2. (**vector independent**) $\\v{U}$ and $\\v{V}$ are independent iff all pairs $U_i$ and $V_r$ are independent.\n",
        "(Casella 5.3.3)\n",
        "\n",
        "- Proof (zero covariance ‚áí independent): Without loss of generality, we only need to prove bivariate standard normal case, as all other cases can be built from its linear combinations. Let $Z_1,Z_2‚àº\\Normal(0,1)$, $U=a_1X_i+a_2Z_2$ and $V=b_1Z_1+b_2Z_2$. Start with joint PDF $f_{Z_1,Z_2}(z_1,z_2)$, apply change of variables to find PDF $f_{U,V}(u,v)$. Impose $\\Cov(U,V)=0$ to allow factoring of the joint PDF $f_{U,V}(u,v)=f_U(u)f_V(v)$.\n",
        "\n",
        "**Normal samples**: Let $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$ iid then (Casella 5.3.1)\n",
        "1. $\\bar{X}$ and $S^2$ are independent\n",
        "2. $\\bar{X}‚àº\\Normal(Œº,œÉ^2/n)$ and $\\blue{\\/{\\bar{X}-Œº}{œÉ/\\sqrt{n}}‚àº\\Normal(0,1)}$\n",
        "3. $\\/{n-1}{œÉ^2}S^2‚àºœá_{n-1}^2$. Therefore $S^2‚àº(œÉ^2,\\/{2œÉ^4}{n-1})$\n",
        "\n",
        "- Part 1 proof (Cochran's): Partition\n",
        "$Q=\\suml_{i=1}^n(\\/{X_i-Œº}{œÉ})^2$\n",
        "$=\\/{\\sum_{i=1}^n(X_i-Œº)^2}{œÉ}$\n",
        "$=\\/{\\sum_{i=1}^n(X_i-\\bar{X})^2}{œÉ^2}+\\/{n(\\bar{X}-Œº)^2}{œÉ^2}$\n",
        "$=\\/{(n-1)S^2}{œÉ^2}+(\\/{\\sqrt{n}(\\bar{X}-Œº)}{œÉ})^2$.\n",
        "It is well known\n",
        "$\\/{(n-1)S^2}{œÉ^2}‚àºœá_{n-1}^2$ and $(\\/{\\sqrt{n}(\\bar{X}-Œº)}{œÉ})^2‚àºœá_1^2$.\n",
        "Therefore $\\bar{X}$ and $S^2$ are independent.\n",
        "\n",
        "- Part 1 proof: $\\bar{X}$ and $S^2$ are independent.\n",
        "Let $\\v{1}=\\BM1\\\\\\vdots\\\\1\\EM$,\n",
        "$\\v{X}=\\BM X_1\\\\\\vdots\\\\X_n\\EM$, and\n",
        "$\\v{P}=\\/{\\v{1}\\v{1}^‚ä§}{\\v{1}^‚ä§\\v{1}}$\n",
        "$=\\/{1}{n}\\v{1}\\v{1}^‚ä§‚àà‚Ñù^{n√ón}$\n",
        "be projection matrix such that\n",
        "$\\v{PX}=\\BM\\bar{X}\\\\\\vdots\\\\\\bar{X}\\EM$\n",
        "and\n",
        "$(\\v{I}-\\v{P})\\v{X}=\\BM X_1-\\bar{X}\\\\\\vdots\\\\X_n-\\bar{X}\\EM$ are orthogonal complements.\n",
        "\n",
        "- Part 3 proof: $\\/{n-1}{œÉ^2}S^2‚àºœá_{n-1}^2$.\n",
        "$\\/{n-1}{œÉ^2}S^2=\\/{1}{œÉ^2}\\suml_{i=1}^n(X_i-\\bar{X})^2$\n",
        "$=\\/{1}{œÉ^2}\\suml_{i=1}^n(Œº+œÉZ_i-Œº-œÉ\\bar{Z})^2$\n",
        "$=\\suml_{i=1}^n(Z_i-\\bar{Z})^2$\n",
        "$=\\suml_{i=1}^nZ_i^2-n\\bar{Z}^2$.\n",
        "Let $\\v{Z}=(Z_1,...,Z_n)$,\n",
        "$\\v{P}=\\/{1}{n}\\v{1}\\v{1}^‚ä§$, then\n",
        "$\\v{PZ}=(\\bar{Z},...,\\bar{Z})$ and\n",
        "$(\\v{I}-\\v{P})\\v{Z}=(Z_1-\\bar{Z},...,Z_n-\\bar{Z})$ are independent.\n",
        "$\\norm{\\v{Z}}^2=\\suml_{i=1}^nZ_i^2‚àºœá_n^2$.\n",
        "$\\norm{\\v{PZ}}^2=n\\bar{Z}^2‚àºœá_1^2$ because\n",
        "$\\bar{Z}‚àº\\Normal(0,\\/{1}{n})$\n",
        "$‚áí\\sqrt{n}\\bar{Z}‚àº\\Normal(0,1)$.\n",
        "$\\norm{(\\v{I}-\\v{P})\\v{Z}}^2=\\suml_{i=1}^nZ_i^2-n\\bar{Z}^2$.\n",
        "By triangle hypotenuse,\n",
        "$\\norm{\\v{Z}}^2=\\norm{\\v{PZ}}^2+\\norm{(\\v{I}-\\v{P})\\v{Z}}^2$. By 5.3.2 addition of independent Chi-squared, $\\norm{(\\v{I}-\\v{P})\\v{Z}}^2‚àºœá_{n-1}^2$."
      ],
      "metadata": {
        "id": "ydQ5Iy5THQmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**t distribution**: Let $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$ be a random sample. We want to find the distribution of $Œº$ from $\\/{\\bar{X}-Œº}{œÉ/\\sqrt{n}}‚àº\\Normal(0,1)$ but $œÉ$ is unknown.\n",
        "$\\red{t_{n-1}=\\/{\\bar{X}-Œº}{S/\\sqrt{n}}=\\/{(\\bar{X}-Œº)}{œÉ/\\sqrt{n}}\\/{1}{\\sqrt{S^2/œÉ^2}}‚àº\\/{Z}{\\sqrt{œá_{n-1}^2/(n-1)}}}$ where **numerator and denominator are independent**.\n",
        "(Casella 5.3.4)\n",
        "\n",
        "- PDF $f_{T_p}(t)=\\/{Œì(\\/{p+1}{2})}{Œì(\\/{p}{2})}\\/{1}{(pœÄ)^{1/2}}\\/{1}{(1+t^2/p)^{(p+1)/2}},\\ t‚àà‚Ñù$\n",
        "- $t_p$ has $p-1$ moments, and no MGF. $t_1$ (n=2) is Cauchy and has no mean, $t_2$ has no variance. $\\blue{\\E t_p=0,\\ p>1}$ and $\\blue{\\Var(t_p)=\\/{p}{p-2},\\ p>2}$ (Proof: Casella 5.18)\n",
        "\n",
        "**F distribution**: Let $X_1,...,X_n‚àº\\Normal(Œº_X,œÉ_X^2)$ and $Y_1,...,Y_m‚àº\\Normal(Œº_Y,œÉ_Y^2)$ be independent random samples. We want the distribution of $œÉ_X^2/œÉ_Y^2$.\n",
        "$\\red{F_{n-1,m-1}=\\/{S_X^2/S_Y^2}{œÉ_X^2/œÉ_Y^2}=\\/{S_X^2/œÉ_X^2}{S_Y^2/œÉ_Y^2}}$ where $\\/{(n-1)S_X^2}{œÉ_X^2}‚àºœá_{n-1}^2$ and $\\/{(m-1)S_Y^2}{œÉ_Y^2}‚àºœá_{m-1}^2$ are independent.\n",
        "- PDF $f_{F_{p,q}}(x)=\\/{Œì(\\/{p+q}{2})}{Œì(\\/{p}{2})Œì(\\/{q}{2})}\\left(\\/{p}{q}\\right)^{p/2}\\/{x^{(p/2)-1}}{[1+(p/q)x]^{(p+q)/2}},\\ x>0$ (Casella 5.3.6)\n",
        "- $\\red{F_{p,q}‚àº\\/{œá_p^2/p}{œá_q^2/q}}$, $\\blue{\\E F_{p,q}=\\/{q}{q-2},\\ q>2}$ and $\\blue{\\Var(F_{p,q})=2(\\/{q}{q-2})^2(\\/{p+q-2}{p(q-4)}),\\ q>4}$ (Proof: Casella 5.17)\n",
        "1. $1/F_{p,q}‚àºF_{q,p}$.\n",
        "2. $t_q^2‚àºF_{1,q}$\n",
        "3. $\\/{(p/q)F_{p,q}}{1+(p/q)F_{p,q}}‚àº\\Beta(\\/{p}{2},\\/{q}{2})$\n",
        "- $\\E F_{n-1,m-1}=\\/{m-1}{m-3}$. Proof:\n",
        "$\\E \\/{1}{œá_p^2}$\n",
        "$=\\/{Œì(\\/{p}{2}-1)}{2Œì(\\/{p}{2})}$\n",
        "$=\\/{1}{2(\\/{p}{2}-1)}$\n",
        "$=\\/{1}{p-2}$.\n",
        "$\\E \\/{œá_{n-1}^2/(n-1)}{œá_{m-1}^2/(m-1)}$\n",
        "$=\\E\\/{œá_{n-1}^2}{n-1}\\E\\/{m-1}{œá_{m-1}^2}$\n",
        "$=\\/{n-1}{n-1}\\/{m-1}{m-3}$"
      ],
      "metadata": {
        "id": "cfPHwhdTsLFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convergence**:\n",
        "- Sequence $X_1,X_2,...$ **converges in probability** to $X$ if for every $œµ>0$, $\\liml_{n‚Üí‚àû}P(|X_n-X|\\lt œµ)=1$. (Casella 5.5.1)\n",
        "\n",
        "  - Each $|X_n-X|$ is likely within $œµ$ as $n‚Üí‚àû$.\n",
        "\n",
        "  - **Weak law of large numbers**: Let $X_1,X_2,...$ be iid sequence with $X_i‚àº(Œº,œÉ^2)$, and $\\bar{X}_n=\\/{1}{n}\\sum_{i=1}^nX_i$. Then for every $œµ>0$, $\\liml_{n‚Üí‚àû}P(|\\bar{X}_n-Œº|\\ltœµ)=1$. That is, $\\bar{X}_n\\arr{p}Œº$. (Casella 5.5.2)\n",
        "\n",
        "  - Proof: Chebyshev: $P(|\\bar{X}_n-Œº|‚â•œµ)$\n",
        "  $‚â§\\/{\\Var(\\bar{X}_n)}{œµ^2}$\n",
        "  $=\\/{œÉ^2}{\\red{n}œµ^2}‚Üí0$ as $n‚Üí‚àû$.\n",
        "\n",
        "- Sequence $X_1,X_2,...$ **converges almost surely** to $X$ if for every $œµ>0$, $P(\\liml_{n‚Üí‚àû}|X_n-X|\\lt œµ)=1$. (Casella 5.5.6)\n",
        "\n",
        "  - All $|X_n-X|$ are likely within $œµ$ as $n‚Üí‚àû$.\n",
        "\n",
        "  - **Strong law of large numbers**: Let $X_1,X_2,...$ be iid sequence with $X_i‚àº(Œº,œÉ^2)$, and $\\bar{X}_n=\\/{1}{n}\\sum_{i=1}^nX_i$. Then for every $œµ>0$, $P(\\liml_{n‚Üí‚àû}|\\bar{X}_n-Œº|\\ltœµ)=1$. That is, $\\bar{X}_n\\larr{a.s.}Œº$. (Casella 5.5.9)\n",
        "\n",
        "  - Both WLLN and SLLN hold even if $œÉ^2$ is not finite.\n",
        "\n",
        "- Sequence $X_1,X_2,...$ **converges in distribution** to $X$ if $\\liml_{n‚Üí‚àû}F_{X_n}(x)=F_X(x)$. (Casella 5.5.10)\n",
        "\n",
        "  - $\\green{X_n\\arr{p}X‚áíX_n\\arr{d}X}$. (Casella 5.5.12)\n",
        "\n",
        "  - $\\green{X_n\\arr{p}Œº‚áîX_n\\arr{d}Œº}$. (Casella 5.5.13)\n",
        "\n",
        "- Let $h(x)$ be a continuous function and $X_n‚ÜíX$, then $h(X_n)‚Üíh(X)$. (Casella 5.5.4)\n",
        "\n",
        "- **Slutsky's theorem**: If $X_n\\arr{d}X$ and $Y_n\\arr{p}a$ then $Y_nX_n\\arr{d}aX$ and $X_n+Y_n\\arr{d}X+a$. (Casella 5.5.17)\n",
        "\n",
        "  - If $\\/{\\bar{X}_n-Œº}{œÉ/\\sqrt{n}}\\arr{d}\\Normal(0,1)$ and $\\/{œÉ}{S_n}\\arr{p}1$ then $\\/{\\bar{X}_n-Œº}{S_n/\\sqrt{n}}=\\/{œÉ}{S_n}\\/{\\bar{X}_n-Œº}{œÉ/\\sqrt{n}}\\arr{d}\\Normal(0,1)$\n",
        "\n",
        "**Central limit theorem**: Let $X_1,X_2,...$ be iid sequence with $X_i‚àº(Œº,œÉ^2\\lt‚àû)$, and $\\bar{X}_n=\\/{1}{n}\\sum_{i=1}^nX_i$. Then $\\blue{\\sqrt{n}(\\bar{X}_n-Œº)/œÉ\\arr{d}\\Normal(0,1)}$. (Casella 5.5.15)\n",
        "\n",
        "- Proof: Let $Y_i=\\/{X_i-Œº}{œÉ}‚àº(0,1)$ then\n",
        "$\\/{\\bar{X}_n-Œº}{œÉ/\\sqrt{n}}=\\/{1}{\\sqrt{n}}\\suml_{i=1}^nY_i$.\n",
        "MGF $M_{(\\bar{X}_n-Œº)/(œÉ/\\sqrt{n})}(t)=M_{\\sum_{i=1}^nY_i/\\sqrt{n}}(t)$\n",
        "$=M_{\\sum_{i=1}^nY_i}(\\/{t}{\\sqrt{n}})$\n",
        "$=\\left[M_{Y_i}(\\/{t}{\\sqrt{n}})\\right]^n$\n",
        "$=\\left[\\suml_{k=0}^‚àûM_{Y_i}^{(k)}(0)\\/{(t/\\sqrt{n})^k}{k!}\\right]^n$\n",
        "$=\\left[1+0+\\/{(t/\\sqrt{n})^2}{2}+R_{Y_i}(\\/{t}{\\sqrt{n}})\\right]^n$\n",
        "$=\\left[1+\\/{1}{n}\\left(\\/{t^2}{2}+nR_{Y_i}(\\/{t}{\\sqrt{n}})\\right)\\right]^n$.\n",
        "By Taylor 5.5.21,\n",
        "$\\liml_{n‚Üí‚àû}\\/{R_{Y_i}(\\/{t}{\\sqrt{n}})}{(t/\\sqrt{n})^2}$\n",
        "$=0$\n",
        "$=\\liml_{n‚Üí‚àû}nR_{Y_i}(\\/{t}{\\sqrt{n}})$.\n",
        "Therefore\n",
        "$\\liml_{n‚Üí‚àû}M_{(\\bar{X}_n-Œº)/(œÉ/\\sqrt{n})}(t)$\n",
        "$=\\liml_{n‚Üí‚àû}\\left[1+\\/{t^2}{2n}\\right]^n$\n",
        "$=e^{t^2/2}$.\n",
        "\n",
        "**Delta Method**: Let $\\v{T}$ be consistent sequence ofestimators of $\\v{Œ∏}$, meaning $\\v{T}$ and $\\v{Œ∏}$ converge as $n‚Üí‚àû$. This convergence means $\\v{T}$ and $\\v{Œ∏}$ are in the neighboorhood of each other, which allows first-order Taylor expansion to linearly approximate $g(\\v{T})$ in the neighborhood of $\\v{Œ∏}$ and\n",
        "$\\red{g(\\v{T})‚âàg(\\v{Œ∏})+Dg(\\v{Œ∏})(\\v{T}-\\v{Œ∏})=g(\\v{Œ∏})+\\suml_{i=1}^kg_i'(\\v{Œ∏})(t_i-Œ∏_i)}$. Asymptotically we have\n",
        "$\\red{\\E_Œ∏[g(\\v{T})]‚âàg(\\v{Œ∏})}$ and $\\red{\\Var_Œ∏(g(\\v{T}))‚âà\\suml_{i=1}^k[g_i'(\\v{Œ∏})]^2\\Var_Œ∏(T_i)}$.\n",
        "\n",
        "- Proof: Because $\\v{T}‚Üí\\v{Œ∏}$ as $n‚Üí‚àû$, Lagrange's remainder $R_1=\\/{g''(\\v{z})}{2}(\\v{T}-\\v{Œ∏})^2‚Üí0$ for some $\\v{z}$ between $\\v{T}$ and $\\v{Œ∏}$.\n",
        "$\\E_Œ∏[g(\\v{T})]$\n",
        "$‚âà\\E_Œ∏[g(\\v{Œ∏})+\\suml_{i=1}^kg_i'(\\v{Œ∏})(t_i-Œ∏_i)]$\n",
        "$=g(\\v{Œ∏})$\n",
        "$+\\suml_{i=1}^kg_i'(\\v{Œ∏})\\ob{\\E_Œ∏(T_i-Œ∏_i)}{=0\\t{ if }\\E_Œ∏[T_i]=Œ∏_i}$\n",
        "$=g(\\v{Œ∏})$.\n",
        "$\\Var_Œ∏(g(\\v{T}))$\n",
        "$=\\E_Œ∏[(g(\\v{T})-g(\\v{Œ∏}))^2]$\n",
        "$‚âà\\E_Œ∏[\\left(\\suml_{i=1}^kg_i'(\\v{Œ∏})(T_i-Œ∏_i)\\right)^2]$\n",
        "$=\\suml_{i=1}^k[g_i'(\\v{Œ∏})]^2\\Var_Œ∏ T_i+2\\suml_{i\\lt j}g_i'(\\v{Œ∏})g_j'(\\v{Œ∏})\\ob{\\Cov_Œ∏(T_i,T_j)}{=0}$.\n",
        "\n",
        "- Let $T_n$ be a sequence of estimators, then $\\blue{\\E[g(T_n)]‚Üíg(\\E[T_n])}$ and $\\blue{\\Var(g(T_n))‚Üí[g'(E[T_n])]^2\\Var(T_n)}$\n",
        "\n",
        "  - $\\E_Œ∏[g(\\v{T})]‚Üíg(\\E_Œ∏[\\v{T}])$ is linear approximation of Jensen's inequality. $\\Var(X)=\\E[X^2]-\\E[X]^2$\n",
        "  $‚áí\\E[X^2]\\neq\\E[X]^2$, but\n",
        "  $\\Var(\\bar{X}_n)=\\/{\\Var(X_i)}{n}‚Üí0$\n",
        "  $‚áí\\E[\\bar{X}_n^2]‚Üí\\E[\\bar{X}_n]^2$.\n",
        "\n",
        "- **First-order Delta Method**: Let $Y_n$ be a sequence of estimators for $Œ∏$ that satisfies $\\sqrt{n}(Y_n-Œ∏)\\arr{d}\\Normal(0,œÉ^2)$. If $g'(Œ∏)\\neq0$, then $\\blue{\\sqrt{n}[g(Y_n)-g(Œ∏)]\\arr{d}\\Normal(0,œÉ^2[g'(Œ∏)]^2)}$.\n",
        "(Casella 5.5.24)\n",
        "\n",
        "  - $\\sqrt{n}(Y_n-Œ∏)\\arr{d}\\Normal(0,œÉ^2)$ implies $\\liml_{n‚Üí‚àû}\\E[Y_n]=Œ∏$ and $\\green{œÉ^2‚â°\\Var(\\sqrt{n}Y_n)}$ because\n",
        "  $\\Var(\\sqrt{n}(Y_n-Œ∏))=\\Var(\\sqrt{n}Y_n)$\n",
        "\n",
        "  - Proof: Because $Y_n$ and $Œ∏$ converge, We use Taylor expansion for $g(Y_n)$ in the neighborhood of $Œ∏$.\n",
        "  $g(Y_n)=g(Œ∏)+g'(Œ∏)(Y_n-Œ∏)$\n",
        "  $‚áí\\sqrt{n}(g(Y_n)-g(Œ∏))=g'(Œ∏)\\sqrt{n}(Y_n-Œ∏)$\n",
        "  $\\arr{d}\\Normal(0,œÉ^2[g'(Œ∏)]^2)$\n",
        "\n",
        "- **Second-order Delta Method**: If $\\sqrt{n}(Y_n-Œ∏)\\arr{d}\\Normal(0,œÉ^2)$, but $g'(Œ∏)=0$ and $g''(Œ∏)\\neq0$, we use $g(Y_n)=g(Œ∏)+\\/{g''(Œ∏)}{2}(Y_n-Œ∏)^2$, and $\\blue{n[g(Y_n)-g(Œ∏)]\\arr{d}œÉ^2\\/{g''(Œ∏)}{2}œá_1^2}$ (Casella 5.5.26)\n",
        "\n",
        "  - Proof: $g(Y_n)=g(Œ∏)+\\/{g''(Œ∏)}{2}(Y_n-Œ∏)^2$\n",
        "  $‚áín[g(Y_n)-g(Œ∏)]=\\/{g''(Œ∏)}{2}[\\sqrt{n}(Y_n-Œ∏)]^2$\n",
        "  $\\arr{d}\\/{g''(Œ∏)}{2}[\\Normal(0,œÉ^2)]^2$\n",
        "  $=\\/{g''(Œ∏)}{2}[œÉ\\Normal(0,1)]^2$\n",
        "  $=\\/{g''(Œ∏)}{2}œÉ^2œá_1^2$."
      ],
      "metadata": {
        "id": "IdXDVWpSvIm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casella Chapter 5: Properties of a random sample\n",
        "\n",
        "- 5.2.12: $X_i‚àº\\t{Bernoulli}(p)$ has PMF\n",
        "$f_{X_i}(x|p)=p^x(1-p)^{1-x}$\n",
        "$=(1-p)(\\/{p}{1-p})^x$\n",
        "$=(1-p)\\e{x\\ln(\\/{p}{1-p})}$.\n",
        "$T_1=\\suml_{j=1}^nX_j$ has PMF\n",
        "$f_T(u|p)=\\binom{n}{u}p^u(1-p)^{n-u}$\n",
        "$=\\binom{n}{u}(1-p)^n\\e{u\\ln(\\/{p}{1-p})}$\n",
        "\n",
        "- 5.5.3/5.5.4 (convergence of $S_n^2$ and $S_n$): $P(|S_n^2-œÉ^2|\\lt œµ)$\n",
        "  $=P((S_n^2-œÉ^2)^2\\lt œµ^2)$\n",
        "  $‚â§\\/{\\E(S_n^2-œÉ^2)^2}{œµ^2}$\n",
        "  $=\\/{\\Var(S_n^2)}{œµ^2}$. $S_n^2$ will converge to $œÉ^2$ if $\\Var(S_n^2)‚Üí0$ as $n‚Üí‚àû$.\n",
        "  Let $h(X)=\\sqrt{X}$, then under these conditions $S_n$ will converge to $œÉ$ as well.\n",
        "\n",
        "- 5.5.11 (maximum of uniforms): Let $X_1,X_2,...‚àº\\Unif(0,1)$ and $X_{(n)}=\\max_{1‚â§i‚â§n}X_i$, where does $X_{(n)}$ converge in distribution?\n",
        "$F_{X_{(n)}}(x)=x^n$.\n",
        "$P(|X_{(n)}-1|‚â•œµ)=\\ob{P(X_{(n)}‚â•1+œµ)}{0}$\n",
        "$+P(X_{(n)}‚â§1-œµ)$\n",
        "$=F_{X_{(n)}}(1-œµ)$\n",
        "$=(1-œµ)^n‚Üí0$.\n",
        "$X_{(n)}$ converges in probability to $1$. Let $œµ=\\/{t}{n}$ then $P(X_{(n)}‚â§1-\\/{t}{n})=(1-\\/{t}{n})^n$\n",
        "$=e^{-t}$\n",
        "\n",
        "- 5.5.19/22: We observe $X_1,...,X_n‚àº\\t{Bernoulli}(p)$, where the odds of success is $\\/{p}{1-p}$. We observe $\\hat{p}=\\suml_i\\/{X_i}{n}$ and use that to calculate the odds:\n",
        "$g(p)=\\/{p}{1-p}$, and\n",
        "$g'(p)=\\/{1}{(1-p)^2}$. Then\n",
        "$\\Var(\\/{\\hat{p}}{1-\\hat{p}})‚âàg'(p)^2\\Var(\\hat{p})$\n",
        "$=\\/{p(1-p)}{n(1-p)^4}=\\/{p}{n(1-p)^3}$.\n",
        "\n",
        "- 5.5.23: Let $\\E_ŒºX=Œº$. We want to use first order approximation on function $g(Œº)$. Then\n",
        "$g(X)=g(Œº)+g'(Œº)(X-Œº)$.\n",
        "$\\E_Œºg(X)‚âàg(Œº)$ and\n",
        "$\\Var_Œº(g(X))‚âà[g'(Œº)]^2\\Var_Œº(X)$.\n",
        "\n",
        "- 5.5.25: Let $g(X)=\\/{1}{X}$. $\\E_Œº \\/{1}{X}‚âà\\/{1}{Œº}$ and $\\Var_Œº(\\/{1}{X})‚âà\\/{\\Var_Œº(X)}{Œº^4}$.\n",
        "$\\Var_Œº(\\sqrt{n}(X-Œº))=n\\Var_Œº(X)$ and\n",
        "$\\Var_Œº(\\sqrt{n}(\\bar{X}-Œº))=\\Var_Œº(X_1)$.\n",
        "Then\n",
        "$\\sqrt{n}(\\/{1}{\\bar{X}}-\\/{1}{Œº})\\arr{d}\\Normal(0,\\/{\\Var_Œº(X_1)}{Œº^4})$ and\n",
        "$\\Var_Œº(\\/{1}{\\bar{X}})‚âà\\/{1}{n}\\/{\\Var_ŒºX_1}{Œº^4}$. Because we don't know $œÉ^2$ we approximate $Œº‚Üí\\bar{X}$ and $\\Var_Œº(X_1)‚ÜíS^2$ into the right side of delta method and we get\n",
        "$\\sqrt{n}(\\/{1}{\\bar{X}}-\\/{1}{Œº})\\arr{d}\\Normal(0,\\/{S^2}{\\bar{X}^4})$ and\n",
        "$\\Var(\\sqrt{n}\\/{1}{\\bar{X}})=\\/{S^2}{\\bar{X}^4}$.\n",
        "\n",
        "- 5.5.27: Suppose $g(Œº_X,Œº_Y)=\\/{Œº_X}{Œº_Y}$ where $Œº_X$ and $Œº_Y$ are non-zero.\n",
        "Then $\\/{‚àÇg}{‚àÇŒº_X}=\\/{1}{Œº_Y}$ and\n",
        "$\\/{‚àÇg}{‚àÇŒº_Y}=\\/{-Œº_X}{Œº_Y^2}$.\n",
        "Then $\\Var(\\/{X}{Y})=\\/{\\Var X}{Œº_Y^2}+\\/{Œº_X^2\\Var Y}{Œº_Y^4}-2\\/{1}{Œº_Y}\\/{Œº_X}{Œº_Y^2}\\Cov(X,Y)$\n",
        "$=\\/{Œº_X^2}{Œº_Y^2}(\\/{\\Var X}{Œº_X^2}+\\/{\\Var Y}{Œº_Y^2}-2\\/{\\Cov(X,Y)}{Œº_XŒº_Y})$\n",
        "\n",
        "- 5.2: Let $X_1,X_2,...$ be continuous iid with PDF $f(x)$ representing annual rainfall. Find the distribution on number of years $N$ such that $X_N>X_1$.\n",
        "$P(N=k|X_1=x)=F(x)^{k-1}(1-F(x))$, and\n",
        "$P(N=k)=‚à´_{-‚àû}^‚àûF(x)^{k-1}(1-F(x))f(x)\\ dx$.\n",
        "\n",
        "- 5.3: Let $X_1,...,X_n$ be iid with $F_X$ where $\\E X_i=Œº$. Let $Y_1,...,Y_n$ be indicator for $X_i>Œº$. Find distribution of $Y=\\sum_{i=1}^nY_i$.\n",
        "$P(Y=k)=\\binom{n}{k}F_X(Œº)^{n-k}(1-F_X(Œº))^k$.\n",
        "Therefore $Y‚àº\\Binom(n,1-F_X(Œº))$.\n",
        "\n",
        "- 5.8: For random sample $X_1,...,X_n$ where $\\bar{X}$ and $S^2$ are as previously defined,\n",
        "$S^2=\\/{1}{2n(n-1)}\\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)^2$\n",
        "$=\\/{1}{2n(n-1)}[2\\sum_{i=1}^n\\sum_{j=1}^n(X_i-\\bar{X})^2-2\\sum_{i=1}^n(X_i-\\bar{X})\\sum_{j=1}^n(X_j-\\bar{X})]$\n",
        "$=\\/{1}{2n(n-1)}2n\\sum_{i=1}^n(X_i-\\bar{X})^2$\n",
        "$=\\/{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2$.\n",
        "\n",
        "- 5.11 (sample stdev bias): $S^2=\\/{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2$ and $\\E S^2=œÉ^2$. What is $\\E S$?\n",
        "According to Jensen's inequality,\n",
        "$\\E \\sqrt{S^2}‚â§\\sqrt{\\E S^2}$\n",
        "$‚áí\\E S‚â§œÉ$.\n",
        "\n",
        "- 5.13 (S bias): Find $g(x)$ such that $\\E g(S^2)=œÉ$.\n",
        "$\\E \\sqrt{S^2}$\n",
        "$=\\sqrt{\\/{œÉ^2}{n-1}}\\E \\sqrt{\\/{n-1}{œÉ^2}S^2}$\n",
        "$=\\sqrt{\\/{œÉ^2}{n-1}}‚à´_0^‚àû\\sqrt{x}f_{œá_{n-1}^2}(x)\\ dx$\n",
        "$=\\sqrt{\\/{œÉ^2}{n-1}}‚à´_0^‚àûx^{1/2}\\/{e^{-x/2}(x/2)^{(n-1)/2}}{xŒì(\\/{n-1}{2})}\\ dx$\n",
        "$=\\sqrt{\\/{œÉ^2}{n-1}}\\/{2^{1/2}Œì(\\/{n}{2})}{Œì(\\/{n-1}{2})}‚à´_0^‚àû\\/{e^{-x/2}(x/2)^{n/2}}{xŒì(\\/{n}{2})}\\ dx$\n",
        "$=\\/{\\sqrt{2}Œì(\\/{n}{2})}{\\sqrt{n-1}Œì(\\/{n-1}{2})}œÉ$.\n",
        "Therefore $\\E \\/{\\sqrt{n-1}Œì(\\/{n-1}{2})}{\\sqrt{2}Œì(\\/{n}{2})}\\sqrt{S^2}=œÉ$.\n",
        "\n",
        "- 5.15 (mean and variance recursive update): $\\blue{\\bar{X}_{n+1}=\\/{X_{n+1}+n\\bar{X}_n}{n+1}}$.\n",
        "Then $nS_{n+1}^2=\\suml_{i=1}^{n+1}(X_i-\\bar{X}_{n+1})^2$\n",
        "$=\\suml_{i=1}^{n+1}(X_i-\\/{X_{n+1}+n\\bar{X}_n}{n+1})^2$\n",
        "$=\\suml_{i=1}^{n+1}(X_i-\\bar{X}_n-\\/{X_{n+1}-\\bar{X}_n}{n+1})^2$\n",
        "$=\\suml_{i=1}^{n}(X_i-\\bar{X}_n)^2+(X_{n+1}-\\bar{X}_n)^2$\n",
        "$-\\/{2(X_{n+1}-\\bar{X}_n)^2}{n+1}+\\/{(n+1)(X_{n+1}-\\bar{X}_n)^2}{(n+1)^2}$\n",
        "$=(n-1)S_n^2+\\/{n}{n+1}(X_{n+1}-\\bar{X}_n)^2$.\n",
        "Therefore\n",
        "$\\blue{S_{n+1}^2=\\/{n-1}{n}S_n^2+\\/{1}{n+1}(X_{n+1}-\\bar{X}_n)^2}$.\n",
        "\n",
        "- 5.17 (F-distribution): Let $X‚àºF_{p,q}$, then $X=\\/{S_X^2/œÉ_X^2}{S_Y^2/œÉ_Y^2}$ where $\\/{pS_X^2}{œÉ_X^2}‚àºœá_p^2$ and $\\/{qS_Y^2}{œÉ_Y^2}‚àºœá_q^2$.\n",
        "  - Derive $f_X(x)$. Let $U‚àºœá_p^2$ and $V‚àºœá_q^2$ be independent. Let $X=\\/{U/p}{V/q}$ and $Y=U+V$.\n",
        "  Then $U=\\/{\\/{p}{q}XY}{1+\\/{p}{q}X}$ and $V=\\/{Y}{1+\\/{p}{q}X}$.\n",
        "  $|\\/{‚àÇ(u,v)}{‚àÇ(x,y)}|=\\/{\\/{p}{q}y}{(1+\\/{p}{q}x)^2}$.\n",
        "  $f_{U,V}(u,v)=\\/{e^{-u/2}(u/2)^{p/2}}{uŒì(p/2)}\\/{e^{-v/2}(v/2)^{q/2}}{vŒì(q/2)}$.\n",
        "  $f_{X,Y}(x,y)=\\/{\\/{p}{q}y}{(1+\\/{p}{q}x)^2}\\/{e^{-y/2}}{Œì(p/2)Œì(q/2)2^{(p+q)/2}}(\\/{\\/{p}{q}xy}{1+\\/{p}{q}x})^{p/2-1}(\\/{y}{1+\\/{p}{q}x})^{q/2-1}$.\n",
        "  $f_X(x)=\\/{Œì(\\/{p+q}{2})}{Œì(\\/{p}{2})Œì(\\/{q}{2})}(\\/{p}{q})^{p/2}\\/{x^{p/2-1}}{(1+\\/{p}{q}x)^{(p+q)/2}}‚à´_0^‚àû\\/{e^{-y/2}(y/2)^{(p+q)/2}}{yŒì(\\/{p+q}{2})}\\ dy$\n",
        "  - Derive $\\E X$ and $\\Var X$. Note $\\E (œá_p^2)^n=\\/{2^nŒì(\\/{p}{2}+n)}{Œì(\\/{p}{2})}$.\n",
        "  $\\E X=\\E[\\/{œá_p^2}{p}]\\E[\\/{q}{œá_q^2}]$\n",
        "  $=\\/{p}{p}\\/{q}{q-2}=\\/{q}{q-2}$.\n",
        "  $\\E X^2=\\E[\\/{(œá_p^2)^2}{p^2}]\\E[\\/{q^2}{(œá_q^2)^2}]$\n",
        "  $=\\/{4Œì(\\/{p}{2}+2)}{p^2Œì(\\/{p}{2})}\\/{q^2Œì(\\/{q}{2}-2)}{4Œì(\\/{q}{2})}$\n",
        "  $=\\/{4(\\/{p}{2}+1)(\\/{p}{2})}{p^2}\\/{q^2}{4(\\/{q}{2}-1)(\\/{q}{2}-2)}$\n",
        "  $=\\/{(p+2)q^2}{p(q-2)(q-4)}$.\n",
        "  $\\Var(X)=\\/{(p+2)q^2}{p(q-2)(q-4)}-\\/{q^2}{(q-2)^2}$\n",
        "  $=2(\\/{q}{q-2})^2(\\/{p+q-2}{p(q-4)})$.\n",
        "  - Show that $Y=\\/{pX}{q+pX}$ has beta distribution.\n",
        "  $X=\\/{qY}{p(1-Y)}$ and\n",
        "  $|\\/{‚àÇx}{‚àÇy}|=\\/{q}{p(1-y)}+\\/{qy}{p^2(1-y)^2}$\n",
        "  $=\\/{q}{p(1-y)^2}$.\n",
        "  $f_X(x)=\\/{Œì(\\/{p+q}{2})}{Œì(\\/{p}{2})Œì(\\/{q}{2})}(\\/{p}{q})^{p/2}\\/{x^{p/2-1}}{(1+\\/{p}{q}x)^{(p+q)/2}}$.\n",
        "  $f_Y(y)=\\/{Œì(\\/{p+q}{2})}{Œì(\\/{p}{2})Œì(\\/{q}{2})}y^{\\/{p}{2}-1}(1-y)^{\\/{q}{2}-1}$.\n",
        "  Therefore $Y‚àº\\Beta(\\/{p}{2},\\/{q}{2})$.\n",
        "\n",
        "- 5.18 (t-distribution): Let $X‚àºt_p$. Then $X=\\/{Z}{\\sqrt{œá_p^2/p}}$.\n",
        "  - Find $\\E X$ and $\\Var X$.\n",
        "  $\\E X=0,\\ p>1$\n",
        "  $\\E X^2=\\E œá_1^2 \\E \\/{p}{œá_p^2}$\n",
        "  $=\\/{p}{p-2}$.\n",
        "  $\\Var(X)=\\/{p}{p-2}$ and $X^2‚àºF_{1,p}$\n",
        "  - Find distributional limit of $X$ as $p‚Üí‚àû$.\n",
        "  By SLLN,\n",
        "  $œá_p^2/p=\\/{1}{p}\\suml_{i=1}^pZ_i^2$\n",
        "  $=\\bar{Z^2}_p$\n",
        "  $\\larr{a.s.}\\E Z^2=1$.\n",
        "  Therefore by Slutsky's theorem, $\\/{Z}{\\sqrt{œá_p^2/p}}\\arr{d}\\Normal(0,1)$.\n",
        "  - Find distributional limit of $qF_{q,p}$ as $p‚Üí‚àû$.\n",
        "  By Slutsky's  theorem,\n",
        "  $qF_{q,p}=\\/{œá_q^2}{œá_p^2/p}\\arr{d}œá_q^2$.\n",
        "\n",
        "- 5.21: What is the probability that larger of two continuous iid random variables will exceed the population median?\n",
        "Let $X_1,...,X_n$ be iid. Then\n",
        "$P(\\max(X_1,...,X_n)>m)$\n",
        "$=1-P(\\max(X_1,...,X_n)‚â§m)$\n",
        "$=1-F(m)^n$\n",
        "$=1-(\\/{1}{2})^n$.\n",
        "\n",
        "- 5.22: Let $X_1,X_2‚àº\\Normal(0,1)$, and $Y=\\min(X_1,X_2)$. What is the distribution of $Y^2$?\n",
        "$P(Y‚â§y)=1-P(Y>y)$\n",
        "$=1-P(X_1>y)^2$\n",
        "$=1-(1-Œ¶(y))^2$\n",
        "$=2Œ¶(y)-Œ¶(y)^2$.\n",
        "$P(Y^2‚â§y)=P(-\\sqrt{y}‚â§Y‚â§\\sqrt{y})$\n",
        "$=P(Y‚â§\\sqrt{y})-P(Y‚â§-\\sqrt{y})$\n",
        "$=2Œ¶(\\sqrt{y})-Œ¶(\\sqrt{y})^2$\n",
        "$-2Œ¶(-\\sqrt{y})+Œ¶(-\\sqrt{y})^2$\n",
        "$=2Œ¶(\\sqrt{y})-Œ¶(\\sqrt{y})^2$\n",
        "$-2(1-Œ¶(\\sqrt{y}))+(1-Œ¶(\\sqrt{y}))^2$\n",
        "$=2Œ¶(\\sqrt{y})-1$.\n",
        "$f_{Y^2}(y)=\\/{1}{\\sqrt{y}}œï(\\sqrt{y})$.\n",
        "$Y^2‚àºœá_1^2$ (Casella p2.11)\n",
        "\n",
        "- 5.23: Let $U_1,...,U_X‚àº\\Unif(0,1)$ iid. Let $P(X=x)=\\/{c}{x!},\\ c=\\/{1}{e-1}$. Find the distribution of $Z=\\min(U_1,...,U_X)$.\n",
        "$f_{Z|X}(z|x)=\\/{x!}{(x-1)!}(1-z)^{x-1}$\n",
        "$=x(1-z)^{x-1}$.\n",
        "$f_Z(z)=\\suml_{x=1}^‚àû\\/{c}{x!}x(1-z)^{x-1}$\n",
        "$=c\\suml_{y=0}^‚àû\\/{(1-z)^y}{y!}$\n",
        "$=\\/{e^{1-z}}{e-1}$.\n",
        "\n",
        "- 5.24: Let $X_1,...,X_n$ have PDF $f(x)=\\BC 1/Œ∏ &0\\lt x\\ltŒ∏ \\\\ 0 &\\t{otherwise}\\EC$. Let $U=X_{(1)}$ and $V=X_{(n)}$. Show $\\/{U}{V}$ and $V$ are independent.\n",
        "$F(x)=\\/{x}{Œ∏}$.\n",
        "$f_{U,V}(u,v)=nf(u)(F(v)-F(u))^{n-2}(n-1)f(v)$\n",
        "$=\\/{n(n-1)}{Œ∏^n}(v-u)^{n-2}$.\n",
        "Let $W=\\/{U}{V}$.\n",
        "Then $|\\/{‚àÇ(u,v)}{‚àÇ(w,v)}|=v$.\n",
        "$f_{W,V}(w,v)=\\/{n(n-1)}{Œ∏^n}v(v-vw)^{n-2}$\n",
        "$=\\/{n(n-1)}{Œ∏^n}v^{n-1}(1-w)^{n-2}$.\n",
        "Therefore independent.\n",
        "\n",
        "- 5.30: $\\bar{X}_1$ and $\\bar{X}_2$ are means of two independent samples of size $n$ with variance $œÉ^2$. Find $n$ so that $P(|\\bar{X}_1-\\bar{X}_2|\\lt \\/{œÉ}{5})‚âà0.99$.\n",
        "By Chebyshev,\n",
        "$P(|\\bar{X}_1-\\bar{X}_2|>\\/{œÉ}{5})$\n",
        "$‚â§\\/{\\Var|\\bar{X}_1-\\bar{X}_2|}{œÉ^2/5^2}$\n",
        "$=\\/{2œÉ^2/n}{œÉ^2/5^2}$\n",
        "$‚â§0.01$\n",
        "$‚áín‚â•5000$.\n",
        "\n",
        "- 5.31: $\\bar{X}_n$ where $n=100$ and $X_i‚àº(Œº,œÉ^2=9)$. Find limits where $\\bar{X}_n-Œº$ lies in with probability at least 0.90.\n",
        "By Chebyshev, $P(|\\bar{X}_n-Œº|‚â•r)$\n",
        "$‚â§\\/{\\Var(\\bar{X}_n)}{r^2}$\n",
        "$=\\/{œÉ^2}{nr^2}$\n",
        "$‚â§0.1$\n",
        "$‚áír^2‚â•0.9$\n",
        "$‚áíP(-0.94\\lt \\bar{X}-Œº\\lt 0.94)‚â•0.9$.\n",
        "\n",
        "- 5.32: Let $X_1,X_2,...\\arr{p}a$ where $P(X_i>0)=1$.\n",
        "Let $Y_i=\\sqrt{X_i}$ and $Y_i'=\\/{a}{X_i}$.\n",
        "If $X_i\\arr{p}X$ then $g(X_i)\\arr{p}g(X)$.\n",
        "Therefore $Y_i\\arr{p}\\sqrt{a}$.\n",
        "By Slutsky's theorem, $Y_i'\\arr{p}\\/{a}{a}=1$.\n",
        "\n",
        "- 5.36: $(Y|N=n)‚àºœá_{2n}^2$ and $N‚àº\\Pois(Œ∏)$.\n",
        "  - Find $\\E Y$ and $\\Var(Y)$.\n",
        "  $\\E Y=\\E[\\E[Y|N]]$\n",
        "  $=\\E[2N]$\n",
        "  $=2Œ∏$.\n",
        "  $\\E[\\Var(Y|N)]=\\E[4N]$\n",
        "  $=4Œ∏$.\n",
        "  $\\Var(\\E[Y|N])=\\Var(2N)$\n",
        "  $=4Œ∏$.\n",
        "  $\\Var(Y)=8Œ∏$.\n",
        "  - Find the convergence of $\\/{Y-\\E Y}{\\sqrt{\\Var(Y)}}$.\n",
        "  $(Y|N=n)‚àºœá_{2n}^2‚àºY_1+...+Y_{n}$ where $Y_i‚àº\\Expo(2)$ iid.\n",
        "  $\\/{Y-\\E Y}{\\sqrt{\\Var(Y)}}$\n",
        "  $=\\/{n\\bar{Y}-nŒº_{Y_1}}{\\sqrt{\\Var(n\\bar{Y})}}$\n",
        "  $=\\/{\\bar{Y}-Œº_{Y_1}}{œÉ_{Y_1}/\\sqrt{n}}$\n",
        "  $\\arr{d}\\Normal(0,1)$ as $n‚Üí‚àû$.\n",
        "\n",
        "- 5.42: Let $X_1,X_2,...$ iid, and $X_{(n)}=\\max_{1‚â§i‚â§n}X_i$. Then $F_{X_{(n)}}(x)=F_{X_i}(x)^n$.\n",
        "  - If $X_i‚àº\\Beta(1,b)$, find $v$ so that $n^v(1-X_{(n)})$ converges in distribution.\n",
        "  $f_{X_i}(x)=b(1-x)^{b-1}$.\n",
        "  $F_{X_i}(x)=‚à´_0^tb(1-t)^{b-1}\\ dt$\n",
        "  $=[-(1-t)^b]_0^x$\n",
        "  $=1-(1-x)^b$.\n",
        "  $F_{X_{(n)}}(x)=(1-(1-x)^b)^n$.\n",
        "  $P(n^v(1-X_{(n)})‚â§t)=1-P(X_{(n)}‚â§1-\\/{t}{n^v})$\n",
        "  $=1-(1-(\\/{t}{n^v})^b)^n$\n",
        "  $‚Üí1-(e^{-t^b/n^{vb}})^n$\n",
        "  $=1-e^{-t^bn^{1-vb}}$.\n",
        "  For convergence we want $n$ to disappear in the exponent. Let $v=1/b$. Then\n",
        "  $P(n^{1/b}(1-X_{(n)})‚â§t)‚âà1-e^{-t^b}$ Weibull.\n",
        "  - If $X_i‚àº\\Expo(1)$, find $a_n$ so that $X_{(n)}-a_n$ converges in distribution.\n",
        "  $P(X_{(n)}-a_n‚â§t)=P(X_{(n)}‚â§t+a_n)$\n",
        "  $=(1-e^{-t-a_n})^n$.\n",
        "  For convergence, let $a_n=\\ln n$, then\n",
        "  $P(X_{(n)}-\\ln n‚â§t)$\n",
        "  $=(1-\\/{1}{n}e^{-t})^n$\n",
        "  $‚Üíe^{-e^{-t}}$ Gumbel.\n",
        "  "
      ],
      "metadata": {
        "id": "-CutxC69_L4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sufficiency and Likelihood"
      ],
      "metadata": {
        "id": "oz31vvbrttOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistic** $T(\\v{X}):\\m{X}‚Ü¶\\m{T}$ where $\\m{T}=\\{t:t=T(\\v{x})\\t{ for some }\\v{x}‚àà\\m{X}\\}$ defines a partition of $\\m{X}$ into sets $A_t=\\{\\v{x}:T(\\v{x})=t\\}$. If $T(\\v{x})=T(\\v{y})$ then both $\\v{x}‚ààA_t$ and $\\v{y}‚ààA_t$.\n",
        "\n",
        "- **Sufficiency principle**: Let $\\v{X}=X_1,...,X_n$ be a random sample. If $T(\\v{X})$ is a sufficient statistic for $Œ∏$, then any inference about $Œ∏$ should depend on $\\v{X}$ only through the value $T(\\v{X})$.\n",
        "If $\\v{x}$ and $\\v{y}$ are two sample points such that $T(\\v{x})=T(\\v{y})$ then the inference about $Œ∏$ should be the same whether $\\v{X}=\\v{x}$ or $\\v{X}=\\v{y}$ is observed.\n",
        "\n",
        "- Notation: $T$ is a shorthand for $T(\\v{X})$, as it is a random variable and also a function of a random sample.\n",
        "\n",
        "$T(\\v{X})$ is a **sufficient statistic for $Œ∏$** if $P_Œ∏(\\v{X}=\\v{x}|T=T(\\v{x}))=P(\\v{X}=\\v{x}|T=T(\\v{x}))$ the conditional distribution of $\\v{X}$ given $T$ does not depend on $Œ∏$\n",
        "(Casella 6.2.1)\n",
        "\n",
        "- $T$ is sufficient if $f(\\v{X}|T,Œ∏)=f(\\v{X}|T)$ or $f(Œ∏|T,\\v{X})=f(Œ∏|T)$ (Cruikshank)\n",
        "\n",
        "- If $p(\\v{x}|Œ∏)$ is the joint PDF/PMF of $\\v{X}$ and $q(t|Œ∏)$ is the PDF/PMF of $T$, then $T$ is a sufficient statistic for $Œ∏$ if $\\/{p(\\v{x}|Œ∏)}{q(T(\\v{x})|Œ∏)}$ is constant as a function of $Œ∏$ for all $\\v{x}‚àà\\m{X}$.\n",
        "(Casella 6.2.2)\n",
        "\n",
        "  - Proof: Let $T(\\v{x})=t$. Then $P_Œ∏(\\v{X}=\\v{x}|T\\neq t)=0$.\n",
        "  $P_Œ∏(\\v{X}=\\v{x}|T=t)$\n",
        "  $=\\/{P_Œ∏(\\v{X}=\\v{x},T=t)}{P_Œ∏(T=t)}$\n",
        "  $=\\/{P_Œ∏(\\v{X}=\\v{x})}{P_Œ∏(T=t)}$\n",
        "  $=\\/{p(\\v{x}|Œ∏)}{q(t|Œ∏)}$.\n",
        "\n",
        "  - Theorem comes directly from definition of sufficiency, but finding PMF/PDF of $T$ is hard.\n",
        "\n",
        "- **Fisher-Neymann factorization**: Let $f(\\v{x}|Œ∏)$ be the joint PDF/PMF of sample $\\v{X}$.\n",
        "Statistic $T(\\v{X})$ is sufficient for $Œ∏$ iff there exists nonnegative functions $g(t|Œ∏)$ and $h(\\v{x})$ such that for all $\\v{x}$ and all parameter values $Œ∏$, $\\red{f(\\v{x}|Œ∏)=g(T(\\v{x})|Œ∏)h(\\v{x})}$.\n",
        "(Casella 6.2.6)\n",
        "\n",
        "  - Proof (discrete): Let $T$ be sufficient for $Œ∏$. Then\n",
        "  $f(\\v{x}|Œ∏)=P_Œ∏(\\v{X}=\\v{x})$\n",
        "  $=P_Œ∏(\\v{X}=\\v{x},T=T(\\v{x}))$\n",
        "  $=\\blue{P_Œ∏(T=T(\\v{x}))}\\green{P(\\v{X}=\\v{x}|T=T(\\v{x}))}$\n",
        "  $=\\blue{g(T|Œ∏)}\\green{h(\\v{x})}$.\n",
        "  Let $A_t=\\{\\v{x}:T(\\v{x})=t\\}$ be the subset of all $\\v{x}$ sharing the same $T(\\v{x})$ then\n",
        "  $\\/{f(\\v{x}|Œ∏)}{q(T(\\v{x})|Œ∏)}$\n",
        "  $=\\/{g(T(\\v{x})|Œ∏)h(\\v{x})}{q(T(\\v{x})|Œ∏)}$\n",
        "  $=\\/{g(T(\\v{x})|Œ∏)h(\\v{x})}{\\sum_{\\v{y}‚ààA_{T(\\v{x})}}g(T(\\v{y})|Œ∏)h(\\v{y})}$\n",
        "  $=\\/{h(\\v{x})}{\\sum_{\\v{y}‚ààA_{T(\\v{x})}}h(\\v{y})}$\n",
        "  which does not depend on $Œ∏$.\n",
        "\n",
        "  - $\\blue{g(t|Œ∏)}$ has no $\\v{x}$ appearing in it, and is often the PDF/PMF of $T$, and $\\green{h(\\v{x})}$ has no $T$ or $Œ∏$ appearing in it.\n",
        "\n",
        "  - $\\blue{g(t|Œ∏)}$ is the $Œ∏$-dependent **kernel** of\n",
        "  $f_T(t|Œ∏)=\\sum_{\\v{x}:T(\\v{x})=t}f(\\v{x}|Œ∏)$\n",
        "  $=\\sum_{\\v{y}‚ààA_{T(\\v{x})}}f(\\v{x}|Œ∏)$. The PDF/PMF is\n",
        "  $f_T(t|Œ∏)=\\blue{g(t|Œ∏)}\\suml_{\\v{x}:T(\\v{x})=t}\\green{h(\\v{x})}$\n",
        "  or\n",
        "  $f_T(t|Œ∏)=\\blue{g(t|Œ∏)}‚à´_{\\v{x}:T(\\v{x})=t}\\green{h(\\v{x})}\\ d\\v{x}$.\n",
        "  (Gemini/ChatGPT)\n",
        "\n",
        "- Let $X_1,...,X_n$ iid have PDF/PMF\n",
        "$f_{X_j}(x_j|\\v{Œ∏})=h(x_j)c(\\v{Œ∏})\\e{\\suml_{i=1}^kw_i(\\v{Œ∏})t_i(x_j)}$\n",
        "where $\\v{Œ∏}=(Œ∏_1,...,Œ∏_d),\\ d‚â§k$ (curved or full). Then\n",
        "$T(\\v{X})=\\left(\\suml_{j=1}^nt_1(X_j),\\ ...,\\ \\suml_{j=1}^nt_k(X_j)\\right)$\n",
        "is a sufficient statistic for $\\v{Œ∏}$.\n",
        "(Casella 6.2.10)\n",
        "\n",
        "$T(\\v{X})$ is **minimal sufficient statistic** if for any other sufficient statistic $T^*$, $T$ is a function of $T^*$.\n",
        "(Casella 6.2.11)\n",
        "\n",
        "- If $T(\\v{x})$ is sufficient statistic and $r$ is one-to-one (with inverse), then $T^*(\\v{x})=r(T(\\v{x}))$ is also sufficient statistic.\n",
        "\n",
        "  - Proof: $f(\\v{x}|Œ∏)=g(T(\\v{x})|Œ∏)h(\\v{x})$\n",
        "  $=g(r^{-1}(T^*(\\v{x}))|Œ∏)h(\\v{x})$\n",
        "  $=g^*(T^*(\\v{x})|Œ∏)h(\\v{x})$.\n",
        "\n",
        "  - If $\\suml_{i=1}^nX_i$ is a sufficient statistic then $\\bar{X}$ is also sufficient. Similarly if $T$ is minimal sufficient then $T^*=r(T)$ is also minimal sufficient. Minimal sufficient statistic is not unique.\n",
        "\n",
        "- $T(\\v{X})$ is the most reduced form of data with maximal reduction while still retaining all information about parameter $Œ∏$. All extraneous information in the sample had been eliminated.\n",
        "\n",
        "  - Proof: $T$ partitions $\\m{X}$ into $\\{A_{t}\\}$ while $T^*$ partitions $\\m{X}$ into $\\{B_{t^*}\\}$.\n",
        "  With $\\v{x}$ and $\\v{y}$ data points, $T$ being a function of all other $T^*$s means\n",
        "  $T^*(\\v{x})=T^*(\\v{y})$\n",
        "  $‚áír(T(\\v{x}))=r(T(\\v{y}))$\n",
        "  $‚áíT(\\v{x})=T(\\v{y})$.\n",
        "  I.e., for every $B_{t^*}$ there is some $A_{t}$ such that $B_{t^*}‚äÜA_{t}$.\n",
        "  Therefore, $\\{A_{t}\\}$ is coarser (fewer and larger groups) than any other partition $\\{B_{t^*}\\}$.\n",
        "\n",
        "- **Lehmann-Scheffe Criterion**: Let $f(\\v{x}|Œ∏)$ be the joint PMF/PDF of sample $\\v{X}$. Suppose there exists $T$ such that $\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}$ is constant as a function of $Œ∏$ iff $T(\\v{x})=T(\\v{y})$, then $T$ is a minimal sufficient statistic for $Œ∏$.\n",
        "(Casella 6.2.13)\n",
        "\n",
        "  - If $x$ and $y$ yield a likelihood ratio $\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}$ that is independent of $Œ∏$ when $T(\\v{x})=T(\\v{y})$ then $T$ captures exactly the difference in data that is explained by $Œ∏$.\n",
        "\n",
        "  - Proof (sufficiency): Let $\\m{T}=\\{t:t=T(\\v{x})\\t{ for }\\v{x}‚àà\\m{X}\\}$ such that $T:\\m{X}‚Üí\\m{T}$, and let $\\{A_t\\}$ be the partition of $\\m{X}$ by $T$. For each $A_t$ choose $\\v{x}_{T(\\v{x})}‚ààA_t$. Then the theorem says $\\/{f(\\v{x}|Œ∏)}{f(\\v{x}_{T(\\v{x})}|Œ∏)}$ is constant in $Œ∏$ for all $\\v{x}‚ààA_t$. Let $h(\\v{x})=\\/{f(\\v{x}|Œ∏)}{f(\\v{x}_{T(\\v{x})}|Œ∏)}$ and $g(t|Œ∏)=f(\\v{x}_t|Œ∏)$ then $f(\\v{x}|Œ∏)=g(T(\\v{x})|Œ∏)h(\\v{x})=f(\\v{x}_{T(\\v{x})}|Œ∏)\\/{f(\\v{x}|Œ∏)}{f(\\v{x}_{T(\\v{x})}|Œ∏)}$.\n",
        "\n",
        "  - Proof (minimality): Let $T^*$ be any other sufficient statistic such that $f(\\v{x}|Œ∏)=g^*(T^*(\\v{x})|Œ∏)h^*(\\v{x})$. With $T^*(\\v{x})=T^*(\\v{y})$ the theorem says $\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}=\\/{g^*(T^*(\\v{x})|Œ∏)h^*(\\v{x})}{g^*(T^*(\\v{y})|Œ∏)h^*(\\v{y})}$ is constant in $Œ∏$ implies $T(\\v{x})=T(\\v{y})$. Therefore $T^*(\\v{x})=T^*(\\v{y})‚áíT(\\v{x})=T(\\v{y})$ and $T$ is a function of $T'$.\n",
        "\n",
        "  - How to use: first simplify $\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}$ into an expression containing $Œ∏$, $T(\\v{x})$ and $T(\\v{y})$. Then set $T(\\v{x})=T(\\v{y})$ and check to see $Œ∏$ no longer appearing in the expression.\n",
        "\n"
      ],
      "metadata": {
        "id": "08k-LUKTHrkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$T(\\v{X})$ is **complete statistic** and its PDF/PMF family $f_T(t|Œ∏)$ is **complete** if $\\E_Œ∏[g(T)]=0$ for all $Œ∏$ implies $P_Œ∏(g(T)=0)=1$ for all $Œ∏$.\n",
        "(Casella 6.2.21)\n",
        "\n",
        "- Completeness condition says if $E_Œ∏[g(T)]=0$ for all $Œ∏$ then $g(T)=0$ almost surely.\n",
        "\n",
        "- Completeness of $T$ is a feature of the distribution family of $T$, e.g., $\\{\\Normal(Œº,œÉ^2):-‚àû\\ltŒº\\lt‚àû,œÉ^2>0\\}$, not the distribution of the underlying $X_i$.\n",
        "\n",
        "- **Ancillary statistic** $S(\\v{X})$ is a statistic whose distribution does not depend on $Œ∏$.\n",
        "(Casella 6.2.16)\n",
        "\n",
        "  - Ancillary and minimal sufficient statistics are not independent, as demonstrated in example 6.2.15 and 6.2.20. In most situations they are independent.\n",
        "  \n",
        "  - If $S(\\v{X})$ is ancillary, then $\\E_Œ∏[S(\\v{X})]$ is constant as a function of $Œ∏$.\n",
        "\n",
        "- Let $T$ be sufficient and $h$ be a function. If $h(T)$ is an ancillary statistic, then $T$ is not complete. See Casella e6.10.\n",
        "\n",
        "  - Proof: Let $h(T)$ be ancillary. Then $\\E_Œ∏[h(T)]=c$ for all $Œ∏$. Then $\\E_Œ∏[h(T)-c]=0$. Then $g(T)=h(T)-c$ is non-trivial therefore $T$ is not complete.\n",
        "\n",
        "  - For location family, range $X_{(n)}-X_{(1)}$ is ancillary. See Casella e6.2.18. For example $\\Unif(Œ∏,Œ∏+1)$ has ancillary $X_{(n)}-X_{(1)}$ because the range cancels out $Œ∏$ and $\\E[X_{(n)}-X_{(1)}]=c$ for all $Œ∏$, and therefore $T=(X_{(1)},X_{(n)})$ is sufficient but not complete.\n",
        "\n",
        "  - For scale family, any $\\/{X_i}{X_j}$, or $\\/{X_{(i)}}{X_{(j)}}$ is ancillary, because the ratio cancels out $Œ∏$ and $\\E[\\/{X_i}{X_j}]=c$ for all $Œ∏$. See Casella e6.2.19. For example, $\\Unif(Œ∏,2Œ∏)$ has ancillary $\\/{X_{(1)}}{X_{(n)}}$, and therefore $T=(X_{(1)},X_{(n)})$ is sufficient but not complete.\n",
        "\n",
        "- **Basu theorem**: If $T(\\v{X})$ is complete and sufficient, then $T(\\v{X})$ is independent of every ancillary statistic.\n",
        "(Casella 6.2.24)\n",
        "\n",
        "  - Proof (discrete): Let $S$ be ancillary. Then\n",
        "  (1)\n",
        "  $P(S=s)$\n",
        "  $=\\suml_{t‚àà\\m{T}}P(S=s|T=t)P_Œ∏(T=t)$\n",
        "  by LOTP.\n",
        "  Separately,\n",
        "  (2)\n",
        "  $P(S=s)$\n",
        "  $=P(S=s)\\suml_{t‚àà\\m{T}}P_Œ∏(T=t)$\n",
        "  $=\\suml_{t‚àà\\m{T}}P(S=s)P_Œ∏(T=t)$.\n",
        "  $g(t)=P(S=s|T=t)-P(S=s)$ is independent of $Œ∏$.\n",
        "  Then subtract (1)-(2):\n",
        "  $0=\\suml_{t‚àà\\m{T}}g(t)P_Œ∏(T=t)$\n",
        "  $=\\E_Œ∏[g(T)]$.\n",
        "  Because $T$ is complete, $\\E_Œ∏[g(T)]=0‚áíg(T)=0$ and\n",
        "  $P(S=s|T=t)=P(S=s)$ for all $t$.\n",
        "  I.e., $S$ and $T$ are independent.\n",
        "\n",
        "- Let $X_1,...,X_n$ iid have PDF/PMF\n",
        "$f_{X_j}(x_j|\\v{Œ∏})=h(x_j)c(\\v{Œ∏})\\e{\\suml_{i=1}^kw_i(\\v{Œ∏})t_i(x_j)}$\n",
        "where $\\v{Œ∏}=(Œ∏_1,...,Œ∏_d),\\ d=k$.\n",
        "Then\n",
        "$T(\\v{X})=\\left(\\suml_{j=1}^nt_1(X_j),\\ ...,\\ \\suml_{j=1}^nt_k(X_j)\\right)$\n",
        "is complete if $\\{(w_1(\\v{Œ∏}),...,w_k(\\v{Œ∏})), \\v{Œ∏}‚ààŒò\\}$ contains an open subset of $‚Ñù^k$. (Casella 6.2.25)\n",
        "\n",
        "  - $d=k$ is required for the parameter set to contain an open subset of $‚Ñù^k$, just like the joint PDF of $T(\\v{X})$ in Casella 5.2.11. If $X_i$ is in a full exponential family, $T(\\v{X})$ also belong to an exponential family.\n",
        "\n",
        "- If $T(\\v{X})$ is complete and sufficient, then it is also minimal sufficient.\n",
        "(Casella 6.2.28)\n",
        "\n",
        "  - Proof:\n",
        "  Let $T$ be complete and sufficient for $Œ∏$ such that $E_Œ∏[T]=œÑ(Œ∏)$, then Lehmann-Scheffe theorem (7.3.23) says $T$ is the unique UMVUE of $œÑ(Œ∏)$.\n",
        "  Let $T'$ be any other sufficient statistic for $Œ∏$, then\n",
        "  $h(T')=E_Œ∏[T|T']$\n",
        "  is independent of $Œ∏$ and is a valid unbiased estimator of $œÑ(Œ∏)$\n",
        "  that Rao-Blackwell theorem (7.3.17) says is uniformly better than $T$.\n",
        "  Because UMVUE is unique, together these theorems imply\n",
        "  $T=h(T')$ for all $Œ∏$.\n",
        "  Therefore $T$ is minimal sufficient.\n"
      ],
      "metadata": {
        "id": "v3F-2xI3SRCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Likelihood function** $L(Œ∏|\\v{x})=f(\\v{x}|Œ∏)$ is function of $Œ∏$ given that $\\v{X}=\\v{x}$ is observed. (Casella 6.3.1)\n",
        "\n",
        "- We run identical trials repeatedly. Each trial results in a different set of sample data $\\v{x}$, and from that we use to construct a slightly different likelihood function $L(Œ∏|\\v{x})$ to represent the sample mountain.\n",
        "\n",
        "  - $f(\\v{x}|Œ∏)$ answers the question \"Given the true parameter $Œ∏$, what is the probability of seeing the data sample $\\v{x}$\"? It is a probability distribution over all possible $\\v{x}$.\n",
        "\n",
        "  - $L(Œ∏|\\v{x})$ answers the question \"Given the observation of $\\v{x}$, what is the relative plausibility of various parameters $Œ∏$\"? It is not a probability distribution of all possible $Œ∏$ - we don't care about its values - we only care about the shape in the form of the ratio of heights at any two points on the function.\n",
        "\n",
        "- **Likelihood principle**: If $\\v{x}$ and $\\v{y}$ are such that $L(Œ∏|\\v{x})=C(\\v{x},\\v{y})L(Œ∏|\\v{y})$ for all $Œ∏$, then inferences about $Œ∏$ drawn from $\\v{x}$ and $\\v{y}$ should be identical.\n",
        "\n",
        "  - All the evidence about $Œ∏$ from an experiment is contained in the shape of the likelihood function $L(Œ∏|\\v{x})$.\n",
        "\n",
        "- **Formal sufficiency principle**: Consider an experiment $E=(\\v{X},Œ∏,\\{f(\\v{x}|Œ∏)\\})$ and suppose $T(\\v{X})$ is sufficient for $Œ∏$. If $T(\\v{x})=T(\\v{y})$ then $\\t{Ev}(E,\\v{x})=\\t{Ev}(E,\\v{y})$.\n",
        "\n",
        "  - $\\t{Ev}(E,\\v{x})$ is evidence about $Œ∏$ arising from experiment $E$ and observed sample $\\v{x}$.\n",
        "  \n",
        "  - $\\t{Ev}(E,\\v{x})$ depends on $E$ and $\\v{x}$ only through $T(\\v{X})$.\n",
        "\n",
        "- **Conditionality principle**: $E_1=(\\v{X}_1,Œ∏,\\{f_1(\\v{x}_1|Œ∏)\\})$ and $E_2=(\\v{X}_2,Œ∏,\\{f_2(\\v{x}_2|Œ∏)\\})$ are two experiments where $Œ∏$ is same in both. Let $P(J=1)=P(J=2)=\\/{1}{2}$ be a coin flip and then $E_J$ is performed: $E^*=((j,\\v{X}_j),Œ∏,\\{f^*((j,\\v{x}_j)|Œ∏)\\})$ where $f^*((j,\\v{x}_j)|Œ∏)=\\/{1}{2}f_j(\\v{x}_j|Œ∏)$. Then $\\t{Ev}(E^*,(j,\\v{x}_j))=\\t{Ev}(E_j,\\v{x}_j)$.\n",
        "\n",
        "  - Observed sample is a tuple $(j,\\v{x}_j)$. FSP now works with $T(j,\\v{x}_j)$ and $\\t{Ev}(E^*,(j,\\v{x}_j))$.\n",
        "\n",
        "  - If one of two experiments is randomly chosen and performed, then $\\t{Ev}$ only depends on the experiment performed as if it were nonrandomly decided.\n",
        "  $L^*(Œ∏)‚àùf^*((j,\\v{x}_j)|Œ∏)=\\/{1}{2}f_j(\\v{x}_j|Œ∏)‚àùf_j(\\v{x}_j|Œ∏)$.\n",
        "\n",
        "- **Formal likelihood principle**: $E_1=(\\v{X}_1,Œ∏,\\{f_1(\\v{x}_1|Œ∏)\\})$ and $E_2=(\\v{X}_2,Œ∏,\\{f_2(\\v{x}_2|Œ∏)\\})$ have common $Œ∏$. Suppose $\\v{x}_1^*$ and $\\v{x}_2^*$ are sample points from $E_1$ and $E_2$ such that $L(Œ∏|\\v{x}_2^*)=CL(Œ∏|\\v{x}_1^*)$ for all $Œ∏$. Then $\\t{Ev}(E_1,\\v{x}_1^*)=\\t{Ev}(E_2,\\v{x}_2^*)$.\n",
        "\n",
        "  - **Likelihood principle corollary**: If $E=(\\v{X},Œ∏,\\{f(\\v{x}|Œ∏)\\})$ is an experiment, then $\\t{Ev}(E,\\v{x})$ should depend on $E$ and $\\v{x}$ only through $L(Œ∏|\\v{x})$.\n",
        "\n",
        "**Birnbaum's theorem**: FSP + CP ‚áî FLP. (Casella 6.3.6)\n",
        "\n",
        "- Proof (FSP+CP‚áíFLP): Setup $E_1=(\\v{X}_1,Œ∏,\\{f_1(\\v{x}_1|Œ∏)\\})$, $E_2=(\\v{X}_2,Œ∏,\\{f_2(\\v{x}_2|Œ∏)\\})$, and $E^*=((j,\\v{X}_j),Œ∏,\\{f^*((j,\\v{x}_j)|Œ∏)\\})$ where $f^*((j,\\v{x}_j)|Œ∏)=\\/{1}{2}f_j(\\v{x}_j|Œ∏)$. Suppose $\\v{x}_1^*‚àà\\v{X}_1$, $\\v{x}_2^*‚àà\\v{X}_2$, and $f_2(\\v{x}_2^*|Œ∏)=c^{-1}f_1(\\v{x}_1^*|Œ∏)$ for all $Œ∏$.\n",
        "  - Define statistic\n",
        "  $T(j,\\v{x}_j)=\\BC\n",
        "  (1,\\v{x}_1^*)&(j,\\v{x}_j)‚àà\\{(1,\\v{x}_1^*),(2,\\v{x}_2^*)\\}\\\\\n",
        "  (j,\\v{x}_j)&\\t{otherwise: to span entire $E^*$}\n",
        "  \\EC$ and prove sufficiency for $E^*$.\n",
        "  - (main cases) If $(j,\\v{x}_j)‚àà\\{(1,\\v{x}_1^*),(2,\\v{x}_2^*)\\}$, then\n",
        "  $T=(1,\\v{x}_1^*)$,\n",
        "  $f^*((1,\\v{x}_1^*)|Œ∏)=f^*(T|Œ∏)$, and\n",
        "  $f^*((2,\\v{x}_2^*)|Œ∏)=\\/{c^{-1}}{2}f_1(\\v{x}_1^*|Œ∏)$\n",
        "  $=c^{-1}f^*((1,\\v{x}_1^*)|Œ∏)$\n",
        "  $=c^{-1}f^*(T|Œ∏)$.\n",
        "  - (otherwise) If $(j,\\v{x}_j)‚àâ\\{(1,\\v{x}_1^*),(2,\\v{x}_2^*)\\}$, then\n",
        "  $T=(j,\\v{x}_j)$, and\n",
        "  $f^*((j,\\v{x}_j)|Œ∏)=f^*(T|Œ∏)$.\n",
        "  - Factorization succeeds\n",
        "  $f^*((j,\\v{x}_j)|Œ∏)=g(T(j,\\v{x}_j)|Œ∏)h(\\v{x})$ and $T$ is sufficient for $E^*$.\n",
        "  - FSP says If $T(1,\\v{x}_1^*)=T(2,\\v{x}_2^*)$, then $\\t{Ev}(E^*,(1,\\v{x}_1^*))=\\t{Ev}(E^*,(2,\\v{x}_2^*))$.\n",
        "  - CP says $\\t{Ev}(E^*,(1,\\v{x}_1^*))=\\t{Ev}(E_1,\\v{x}_1^*)$ and\n",
        "  $\\t{Ev}(E^*,(2,\\v{x}_2^*))=\\t{Ev}(E_2,\\v{x}_2^*)$.\n",
        "  Therefore $\\t{Ev}(E_1,\\v{x}_1^*)=\\t{Ev}(E_2,\\v{x}_2^*)$\n",
        "\n",
        "- Proof (FLP‚áíCP): $f^*((j,\\v{x}_j)|Œ∏)=\\/{1}{2}f_j(\\v{x}_j|Œ∏)$ therefore $L^*(Œ∏|(j,\\v{x}_j))=CL_j(Œ∏|\\v{x}_j)$ satisfying the premise of CP and FLP.\n",
        "  - FLP says then $\\t{Ev}(E^*,(j,\\v{x}_j))=\\t{Ev}(E_j,\\v{x}_j)$, which is CP.\n",
        "\n",
        "- Proof (FLP‚áíFSP): Let $E=(\\v{X},Œ∏,\\{f(\\v{x}|Œ∏)\\})$ and suppose $T$ is sufficient, satisfying premise of FSP.\n",
        "If $T(\\v{x})=T(\\v{y})=t$, then by factorization\n",
        "$\\/{f(\\v{x}|Œ∏)}{h(\\v{x})}=\\/{f(\\v{y}|Œ∏)}{h(\\v{y})}$\n",
        "$‚áíf(\\v{x}|Œ∏)=\\/{h(\\v{x})}{h(\\v{y})}f(\\v{y}|Œ∏)$\n",
        "$‚áíL(Œ∏|\\v{x})=CL(Œ∏|\\v{y})$, satisfying premise of FLP.\n",
        "  - FLP says then $\\t{Ev}(E,\\v{x})=\\t{Ev}(E,\\v{y})$, which is FSP."
      ],
      "metadata": {
        "id": "DCcmBRxz1yOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casella Chapter 6: Principles of Data Reduction\n",
        "\n",
        "- 6.2.3 (binomial sufficient statistic):\n",
        "Let $X_1,...,X_n‚àº\\t{Bernoulli}(Œ∏)$ iid where $0\\ltŒ∏\\lt1$. Let $T(\\v{X})=X_1+...+X_n$. Then\n",
        "$\\/{p(\\v{x}|Œ∏)}{q(T(\\v{x})=t|Œ∏)}$\n",
        "$=\\/{\\prod_{i=1}^nŒ∏^{x_i}(1-Œ∏)^{1-x_i}}{\\binom{n}{t}Œ∏^t(1-Œ∏)^{n-t}}$\n",
        "$=\\binom{n}{t}^{-1}Œ∏^{\\sum x_i-t}(1-Œ∏)^{t-\\sum x_i}$\n",
        "$=\\binom{n}{\\sum x_i}^{-1}$ which does not depend on $Œ∏$ therefore $T(\\v{X})=X_1+...+X_n$ is a sufficient statistic for $Œ∏$.\n",
        "\n",
        "- 6.2.4 (normal sufficient statistic):\n",
        "Let $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$ iid where $œÉ^2$ is known. Let $T(\\v{X})=\\bar{X}$.\n",
        "Then $f(\\v{x}|Œº)=\\prodl_{i=1}^n\\/{1}{\\sqrt{2œÄ}œÉ}\\e{-\\/{(x_i-Œº)^2}{2œÉ^2}}$\n",
        "$=(2œÄœÉ^2)^{-n/2}\\e{\\/{-1}{2œÉ^2}\\sum_{i=1}^n(x_i-Œº)^2}$\n",
        "$=(2œÄœÉ^2)^{-n/2}\\e{-\\sum_{i=1}^n\\/{(x_i-\\bar{x})^2}{2œÉ^2}-\\/{n(\\bar{x}-Œº)^2}{2œÉ^2}}$.\n",
        "And $q(T(\\v{x})|Œº)=(2œÄœÉ^2/n)^{-1/2}\\e{-\\/{(\\bar{x}-Œº)^2}{2œÉ^2/n}}$.\n",
        "Then $\\/{f(\\v{x}|Œº)}{q(T(\\v{x})|Œº)}$\n",
        "$=n^{1/2}(2œÄœÉ^2)^{-(n-1)/2}\\e{-\\/{\\sum_{i=1}^n(x_i-\\bar{x})^2}{2œÉ^2}}$.\n",
        "  - 6.2.7: $f(\\v{x}|Œº)=(2œÄœÉ^2)^{-n/2}\\e{-\\sum_{i=1}^n\\/{(x_i-\\bar{x})^2}{2œÉ^2}-\\/{n(\\bar{x}-Œº)^2}{2œÉ^2}}$.\n",
        "  Here\n",
        "  $g(\\bar{x}|Œº)=\\e{-\\/{n(\\bar{x}-Œº)^2}{2œÉ^2}}$\n",
        "  while\n",
        "  $h(\\v{x})=(2œÄœÉ^2)^{-n/2}\\e{-\\sum_{i=1}^n\\/{(x_i-\\bar{x})^2}{2œÉ^2}}$.\n",
        "\n",
        "- 6.2.8 (Uniform):\n",
        "Let $X_1,...,X_n‚àº\\DUnif(1,Œ∏)$ iid. Then\n",
        "$f(\\v{x}|Œ∏)=(\\/{1}{Œ∏})^nI_{\\max_ix_i‚â§Œ∏}$.\n",
        "If we define $T(\\v{x})=\\max_ix_i$, then\n",
        "$g(t|Œ∏)=(\\/{1}{Œ∏})^nI_{t‚â§Œ∏}$ and\n",
        "$h(\\v{x})=1$.\n",
        "The dependence on $T(\\v{x})$ can be explicitly shown in\n",
        "$g(t|Œ∏)=\\prodl_{i=1}^nŒ∏^{-1}I_{[1,Œ∏]}(x_i)$\n",
        "$=Œ∏^{-n}I_{[1,Œ∏]}(T(\\v{x}))$.\n",
        "\n",
        "- 6.2.9 (2 unknowns):\n",
        "Let $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$ iid with unknown $\\v{Œ∏}=(Œº,œÉ^2)$ where $T_1(\\v{X})=\\bar{x}$ and $T_2(\\v{X})=s^2$.\n",
        "$f(\\v{x}|Œº,œÉ^2)$\n",
        "$=(2œÄœÉ^2)^{-n/2}\\e{-\\sum_{i=1}^n\\/{(x_i-\\bar{x})^2}{2œÉ^2}-\\/{n(\\bar{x}-Œº)^2}{2œÉ^2}}$\n",
        "$=(2œÄœÉ^2)^{-n/2}\\e{-\\/{(n-1)s^2}{2œÉ^2}-\\/{n(\\bar{x}-Œº)^2}{2œÉ^2}}$\n",
        "$=(2œÄœÉ^2)^{-n/2}\\e{-\\/{n(\\bar{x}-Œº)^2+(n-1)s^2}{2œÉ^2}}$.\n",
        "  - 6.2.14: Then\n",
        "  $\\/{f(\\v{x}|Œº,œÉ^2)}{f(\\v{y}|Œº,œÉ^2)}$\n",
        "  $=\\/{(2œÄœÉ^2)^{-n/2}\\e{-[n(\\bar{x}-Œº)^2+(n-1)s_X^2]/2œÉ^2}}{(2œÄœÉ^2)^{-n/2}\\e{-[n(\\bar{y}-Œº)^2+(n-1)s_Y^2]/2œÉ^2}}$\n",
        "  $=\\e{-\\/{n(\\bar{x}^2-\\bar{y}^2)-2nŒº(\\bar{x}-\\bar{y})+(n-1)(s_X^2-s_Y^2)}{2œÉ^2}}$,\n",
        "  which is 1 if $\\bar{x}=\\bar{y}$ and $s_X^2=s_Y^2$. Therefore $T=(\\bar{X},S^2)$ is minimal sufficient for $(Œº,œÉ^2)$ if $\\v{x}$ and $\\v{y}$ yield the same $T$.\n",
        "\n",
        "- 6.2.15 (Uniform minimal sufficient):\n",
        "Let $X_1,...,X_n‚àº\\Unif(Œ∏,Œ∏+1),\\ -‚àû\\ltŒ∏\\lt‚àû$. Then the joint PDF is\n",
        "$f(\\v{x}|Œ∏)$\n",
        "$=I_{Œ∏< x_i< Œ∏+1}$\n",
        "$=I_{Œ∏< \\min_ix_i, \\max_ix_i-1< Œ∏}$\n",
        "$=I_{X_{(n)}-1\\ltŒ∏\\lt X_{(1)}}$.\n",
        "Statistics\n",
        "$T=(X_{(1)},X_{(n)})$ is minimal sufficient when\n",
        "$\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}=1$ and independent of $Œ∏$, which is satisfied if $\\v{x}$ and $\\v{y}$ yield the same $T$ and therefore same boundary for $Œ∏$.\n",
        "  - 6.2.17 (Range is ancillary):\n",
        "  $f(x_{(1)},x_{(n)}|Œ∏)=n(n-1)(x_{(n)}-x_{(1)})I_{Œ∏\\lt x_{(1)}\\lt x_{(n)}\\ltŒ∏+1}$.\n",
        "  Let $r=x_{(n)}-x_{(1)}$ and\n",
        "  $m=(x_{(1)}+x_{(n)})/2$, then\n",
        "  $x_{(1)}=\\/{2m-r}{2}$,\n",
        "  $x_{(n)}=\\/{2m+r}{2}$,\n",
        "  and $|\\/{‚àÇ(x_{(1)},x_{(n)})}{‚àÇ(m,r)}|=1$.\n",
        "  Then\n",
        "  $f(r,m|Œ∏)=n(n-1)r^{n-2}I_{0\\lt r\\lt1, Œ∏+r/2\\lt m\\lt Œ∏+1-r/2}$.\n",
        "  $f(r|Œ∏)=‚à´_{Œ∏+r/2}^{Œ∏+1-r/2}n(n-1)r^{n-2}\\ dm$\n",
        "  $=n(n-1)r^{n-2}(1-r)I_{0\\lt r\\lt1}$.\n",
        "  - Statistic\n",
        "  $T'(\\v{X})=(X_{(1)}, X_{(n)}-X_{(1)})$\n",
        "  is also minimal sufficient, but contains ancillary range.\n",
        "  Therefore ancillary and minimal sufficient are not independent.\n",
        "  \n",
        "- 6.2.18 (location family ancillary):\n",
        "Let $X_1,...,X_n$ be iid from a location family with CDF\n",
        "$F(x-Œ∏), -‚àû\\ltŒ∏\\lt‚àû$.\n",
        "Then range $R$ is ancillary.\n",
        "Let $X_i=Z_i+Œ∏$, then\n",
        "$F_R(r|Œ∏)$\n",
        "$=P_Œ∏(R‚â§r)$\n",
        "$=P_Œ∏(X_{(n)}-X_{(1)}‚â§r)$\n",
        "$=P_Œ∏((Z_{(n)}+Œ∏)-(Z_{(1)}+Œ∏‚â§r)$\n",
        "$=P_Œ∏(Z_{(n)}-Z_{(1)}‚â§r)$.\n",
        "\n",
        "- 6.2.19 (scale family ancillary):\n",
        "Let $X_1,...,X_n$ be iid from a scale family with CDF\n",
        "$F(x/œÉ), œÉ>0$.\n",
        "Then any statistic on\n",
        "$\\/{X_1}{X_n},...,\\/{X_{n-1}}{X_n}$ is ancillary.\n",
        "Let $X_i=œÉZ_i$, then\n",
        "$F(y_1,...,y_{n-1}|œÉ)$\n",
        "$=P_œÉ(\\/{X_1}{X_n}‚â§y_1,...,\\/{X_{n-1}}{X_n}‚â§y_{n-1})$\n",
        "$=P_œÉ(\\/{Z_1}{Z_n}‚â§y_1,...,\\/{Z_{n-1}}{Z_n}‚â§y_{n-1})$.\n",
        "\n",
        "- 6.2.20:\n",
        "Let $X_1,X_2$ be iid observations from\n",
        "$P_Œ∏(X=Œ∏)=\\/{1}{3}$,\n",
        "$P_Œ∏(X=Œ∏+1)=\\/{1}{3}$,\n",
        "$P_Œ∏(X=Œ∏+2)=\\/{1}{3}$.\n",
        "Let $R=X_{(2)}-X_{(1)}$ and\n",
        "$M=(X_{(1)}+X_{(2)})/2$,\n",
        "then $(M,R)$ is minimal sufficient even though $R$ is ancillary.\n",
        "\n",
        "- 6.2.22 (binomial complete sufficient):\n",
        "Let $T‚àº\\Binom(n,p),\\ 0\\lt p\\lt1$.\n",
        "$\\E_pg(T)$\n",
        "$=\\sum_{t=0}^ng(t)\\binom{n}{t}p^t(1-p)^{n-t}$\n",
        "$=(1-p)^n\\sum_{t=0}^ng(t)\\binom{n}{t}(\\/{p}{1-p})^t$.\n",
        "Then $\\E_pg(T)=0$\n",
        "$=\\sum_{t=0}^ng(t)\\binom{n}{t}(\\/{p}{1-p})^t$\n",
        "can only be true if $g(t)=0$ for all $p$.\n",
        "\n",
        "- 6.2.23 (Uniform complete sufficient):\n",
        "Let $X_i‚àº\\Unif(0,Œ∏),\\ Œ∏>0$ iid, and $T=X_{(n)}$.\n",
        "Then $F_X(x|Œ∏)=\\/{x}{Œ∏}$, and\n",
        "$f_T(t|Œ∏)=\\/{n}{Œ∏^n}t^{n-1}$.\n",
        "$T$ is complete if\n",
        "$\\E_Œ∏[g(T)]=‚à´_0^Œ∏g(t)nt^{n-1}Œ∏^{-n}\\ dt$\n",
        "$=0$ implies $g(T)=0$ as.\n",
        "Clean up the constants and let $h(Œ∏)=‚à´_0^Œ∏g(t)t^{n-1}\\ dt$.\n",
        "If $\\E[g(T)]=0$ then so are\n",
        "$h(Œ∏)=0$ and $\\/{d}{dŒ∏}h(Œ∏)=0$.\n",
        "We have\n",
        "$\\/{d}{dŒ∏}‚à´_0^Œ∏g(t)t^{n-1}\\ dt$\n",
        "$=\\ob{g(Œ∏)Œ∏^{n-1}}{\\t{Leibniz rule}}+‚à´_0^Œ∏\\/{‚àÇ}{‚àÇŒ∏}\\ob{[g(t)t^{n-1}]}{\\t{constant in $Œ∏$}}\\ dt$\n",
        "$=g(Œ∏)Œ∏^{n-1}=0$.\n",
        "Therefore $g(Œ∏)=0$ for all $Œ∏>0$ and $g(T)=0$ for all $0< T< Œ∏$. Therefore $T$ is complete.\n",
        "\n",
        "- 6.2.26: Let $X_1,...,X_n‚àº\\Expo(Œ∏)$ iid. $f_{X_i}(x|Œ∏)=Œ∏e^{-x/Œ∏}$ forms a scale family that has ancillary statistic $g(\\v{X})=\\/{X_n}{X_1+...+X_n}$. It is also an exponential family with $t_1(x)=x$ and $w_1(Œ∏)=\\/{1}{Œ∏}$. There $T(\\v{X})=\\sum_{i=1}^nX_i$ is complete and sufficient. Therefore $g$ and $T$ are independent.\n",
        "  - Prove minimal with Lehmann-Scheffe:\n",
        "  $f(\\v{x}|Œ∏)=Œ∏^n\\e{-\\/{1}{Œ∏}\\sum_{i=1}^nx_i}I_{x_i>0}$.\n",
        "  Then\n",
        "  $\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}$\n",
        "  $=\\e{-\\/{1}{Œ∏}\\sum_{i=1}^n(x_i-y_i)}I_{x_i>0}$\n",
        "  $=\\e{-\\/{1}{Œ∏}(T(\\v{X})-T(\\v{Y}))}$.\n",
        "  When $T(\\v{X})=T(\\v{Y})$, a constant remains.\n",
        "  - Find $\\E_Œ∏g(\\v{X})$.\n",
        "  By exponential distribution,\n",
        "  $Œ∏=\\E_Œ∏[X_n]$\n",
        "  $=\\E_Œ∏[g(\\v{X})T(\\v{X})]$\n",
        "  $=\\E_Œ∏[g(\\v{X})]\\E_Œ∏[T(\\v{X})]$\n",
        "  $‚áí\\E_Œ∏[g(\\v{X})]$\n",
        "  $=\\/{Œ∏}{\\E_Œ∏[T(\\v{X})]}$\n",
        "  $=\\/{1}{n}$.\n",
        "\n",
        "- 6.3.2 (nbinom): Let $X‚àº\\t{NBinom}(3,p)$. If $x=2$ is observed, then\n",
        "$L(p|x=2)=\\binom{4}{2}p^3(1-p)^2$\n",
        "\n",
        "- 6.3.5/6.3.7 (binomial/nbinom): Let $E_1$ be tossing a coin 20 times and recording heads, then $\\{f_1(x_1|p)\\}‚àº\\Binom(20,p)$. Let $E_2$ be tossing coin until the 7th head occurs and recording tails, then $\\{f_2(x_2|p)\\}‚àº\\t{NBinom}(7,p)$. Suppose $x_1=7$ heads occurred in 20 tosses, and $x_2=13$ tails occurred before 7 heads. Then $L(p|x_1=7)=\\binom{20}{7}p^7(1-p)^{13}$ and $L(p|x_2=13)=\\binom{19}{6}p^7(1-p)^{13}$. FLP states same conclusion regarding $p$ should be made in both cases.\n",
        "\n",
        "- 6.1: Let $X$ be one observation from $\\Normal(0,œÉ^2)$. Is $T=|X|$ sufficient?\n",
        "$f_X(x|œÉ^2)=\\/{1}{\\sqrt{2œÄ}œÉ}e^{-\\/{x^2}{2œÉ^2}}$\n",
        "$=\\/{1}{\\sqrt{2œÄ}œÉ}e^{-\\/{T^2}{2œÉ^2}}$\n",
        "$=g(T|œÉ^2)h(x)$.\n",
        "\n",
        "- 6.2: Let $X_1,...,X_n$ iid with $f_{X_i}(x|Œ∏)=e^{iŒ∏-x}I_{x‚â•iŒ∏}$. Prove $T=\\min_i(X_i/i)$ is sufficient.\n",
        "$f(\\v{x}|Œ∏)=\\prodl_{i=1}^n\\e{iŒ∏-x_i}I_{x_i/i‚â•Œ∏}$\n",
        "$=\\e{\\/{n(n-1)Œ∏}{2}}I_{T‚â•Œ∏}\\e{-\\sum_{i=1}^nx_i}$\n",
        "$=g(T|Œ∏)h(\\v{x})$.\n",
        "\n",
        "- 6.3: Let $X_1,...,X_n$ iid with\n",
        "$f_{X_i}(x|Œº,œÉ)=\\/{1}{œÉ}e^{-(x-Œº)/œÉ}$\n",
        "$I_{Œº< x< ‚àû,0< œÉ< ‚àû}$. Find a sufficient statistic.\n",
        "$f(\\v{x}|Œº,œÉ)$\n",
        "$=\\/{1}{œÉ^n}\\e{\\sum_{i=1}^n\\/{-(x_i-Œº)}{œÉ}}I_{\\min_ix_i>Œº}$\n",
        "$=\\/{1}{œÉ^n}\\e{\\/{-\\sum_{i=1}^nx_i}{œÉ}-\\/{nŒº}{œÉ}}I_{\\min_ix_i>Œº}$\n",
        "$=\\/{1}{œÉ^n}\\e{\\/{-T_1}{œÉ}-\\/{nŒº}{œÉ}}I_{T_2>Œº}$.\n",
        "$T=(\\sum_{i=1}^nx_i, X_{(1)})$.\n",
        "\n",
        "- 6.5: Let $X_1,...,X_n$ iid with $f(x_i|Œ∏)=\\/{1}{2iŒ∏}I_{-i(Œ∏-1)< x_i< i(Œ∏+1)}$. Find sufficient statistic.\n",
        "Boundary\n",
        "$-i(Œ∏-1)< x_i< i(Œ∏+1)$\n",
        "$‚áíŒ∏-1< \\/{x_i}{i}< Œ∏+1$\n",
        "$‚áíŒ∏< \\min_i\\/{x_i}{i}+1, \\max_i\\/{x_i}{i}-1< Œ∏$\n",
        "$‚áí\\max_i\\/{x_i}{i}-1< Œ∏< \\min_i\\/{x_i}{i}+1$.\n",
        "Joint PDF\n",
        "$f(\\v{x}|Œ∏)$\n",
        "$=\\/{1}{(2Œ∏)^n\\prod_ii}I_{T_1-1< Œ∏< T_2+1}$,\n",
        "$T=(\\max_i\\/{x_i}{i}, \\min_i\\/{x_i}{i})$.\n",
        "\n",
        "- 6.6: Let $X_1,...,X_n‚àº\\Gamma(a,b)$ iid. Find sufficient statistic.\n",
        "$f(\\v{x}|a,b)$\n",
        "$=\\prodl_{i=1}^n\\/{e^{-x_i/b}(x_i/b)^a}{x_iŒì(a)}$\n",
        "$=\\/{e^{-\\sum_ix_i/b}\\prod_ix_i^{a-1}}{(b^aŒì(a))^n}$\n",
        "$=\\/{e^{-T_1/b}T_2^{a-1}}{(b^aŒì(a))^n}$.\n",
        "$T=(\\sum_ix_i,\\prod_ix_i)$.\n",
        "\n",
        "- 6.8: Let $X_1,...,X_n$ iid with location family $f(x_i-Œ∏)$. Show $T=(X_{(1)},...,X_{(n)})$ are sufficient and cannot be reduced further.\n",
        "$f(\\v{x}|Œ∏)=\\prod_{i=1}^nf(x_i-Œ∏)$\n",
        "$=\\/{1}{n!}f(x_{(1)},...,x_{(n)}|Œ∏)$\n",
        "$=g(T|Œ∏)h(\\v{x})$.\n",
        "$\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}$\n",
        "$=\\/{f(x_{(1)},...,x_{(n)}|Œ∏)}{f(y_{(1)},...,y_{(n)}|Œ∏)}$\n",
        "$=1$\n",
        "when $T(\\v{x})=T(\\v{y})$.\n",
        "\n",
        "- 6.9:  Let $X_1,...,X_n$ iid. Find a minimal sufficient.\n",
        "  - (normal) $f(x_i|Œ∏)=\\/{1}{\\sqrt{2œÄ}}e^{-(x_i-Œ∏)^2/2}$.\n",
        "  $f(\\v{x}|Œ∏)=(2œÄ)^{n/2}e^{-\\sum_i(x_i-Œ∏)^2/2}$\n",
        "  $=(2œÄ)^{n/2}\\e{-\\/{\\sum_ix_i^2}{2}+\\/{2Œ∏n\\bar{x}}{2}-\\/{nŒ∏^2}{2}}$.\n",
        "  $\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}$\n",
        "  $=\\e{-\\/{\\sum_ix_i^2-y_i^2}{2}+Œ∏n(\\bar{x}-\\bar{y})}$,\n",
        "  which is constant in $Œ∏$ when\n",
        "  $T=\\bar{X}$ and $T(\\v{x})=T(\\v{y})$.\n",
        "  - (location exponential) $f(x_i|Œ∏)=e^{-(x_i-Œ∏)}I_{x_i>Œ∏}$.\n",
        "  $f(\\v{x}|Œ∏)=\\e{-\\sum_ix_i}\\e{nŒ∏}I_{X_{(1)}>Œ∏}$.\n",
        "  $\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}$\n",
        "  $=\\e{-\\sum_i(x_i-y_i)}\\/{I_{X_{(1)}>Œ∏}}{I_{Y_{(1)}>Œ∏}}$, which is constant in $Œ∏$ when $T=X_{(1)}$ and $T(\\v{x})=T(\\v{y})$.\n",
        "  - (logistic) $f(x_i|Œ∏)=\\/{e^{-(x_i-Œ∏)}}{(1+e^{-(x_i-Œ∏)})^2}$.\n",
        "  $f(\\v{x}|Œ∏)$\n",
        "  $=\\/{e^{-\\sum_i(x_i-Œ∏)}}{\\prod_i(1+e^{-(x_i-Œ∏)})^2}$\n",
        "  $=\\/{n!e^{-\\sum_i(x_i-Œ∏)}}{\\prod_i(1+e^{-(x_{(i)}-Œ∏)})^2}$.\n",
        "  $\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}$\n",
        "  $=e^{-\\sum_i(x_i-y_i)}(\\prodl_{i=1}^n\\/{1+e^{-(y_{(i)}-Œ∏)}}{1+e^{-(x_{(i)}-Œ∏)}})^2$,\n",
        "  which is constant in $Œ∏$ when $T=(X_{(1)},...,X_{(n)})$ and $T(\\v{X})=T(\\v{Y})$.\n",
        "  - (Cauchy) $f(x_i|Œ∏)=\\/{1}{œÄ[1+(x_i-Œ∏)^2]}$.\n",
        "  $f(\\v{x}|Œ∏)=\\/{n!}{œÄ^n\\prod_i(1+(x_{(i)}-Œ∏)^2)}$.\n",
        "  $\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}$\n",
        "  $=\\prodl_{i=1}^n\\/{1+(y_{(i)}-Œ∏)^2}{1+(x_{(i)}-Œ∏)^2}$,\n",
        "  which is constant in $Œ∏$ when $T=(X_{(1)},...,X_{(n)})$ and $T(\\v{X})=T(\\v{Y})$.\n",
        "  \n",
        "  - 6.11: Let $Y_i=X_{(n)}-X_{(i)}$. Determine whether $Y_1,...,Y_{n-1}$ is ancillary and independent of minimal sufficient.\n",
        "  All were location families. Let $Z_i=X_i-Œ∏$.\n",
        "  Then $Y_i=(Z_{(n)}+Œ∏)-(Z_{(i)}+Œ∏)$ is independent of $Œ∏$.\n",
        "  Normal and exponential are exponential families and complete. Logistic and Cauchy have minimal sufficient order statistics, which $Y_i$ depend on.\n",
        "\n",
        "- 6.10: Let $X_1,...,X_n‚àº\\Unif(Œ∏,Œ∏+1)$. Then $T=(X_{(1)},X_{(n)})$ is minimal sufficient. Show it is not complete.\n",
        "$\\Unif(Œ∏,Œ∏+1)$ is $\\Unif(0,1)$ shifted by $Œ∏$ and is therefore a location family.\n",
        "$R=X_{(n)}-X_{(1)}‚àº\\Beta(n-1,2)$.\n",
        "$\\E[R]=\\/{n-1}{(n-1)+2}$.\n",
        "Then $0=\\E_Œ∏[g(T)]$\n",
        "$=\\E_Œ∏[X_{(n)}-X_{(1)}-\\/{n-1}{n+1}]$.\n",
        "Therefore $g(T)\\neq0$ and $T$ is not complete.\n",
        "\n",
        "- 6.12: Let $N$ have support $(1,2,...)$ with probabilities $(p_1,p_2,...)$ where $‚àë_ip_i=1$. Let $X|N‚àº\\Binom(N,Œ∏)$.\n",
        "  - Prove $(X,N)$ is minimal sufficient.\n",
        "  Let $X_1,...,X_N$ be trials.\n",
        "  $f(x_i|Œ∏)=Œ∏^{x_i}(1-Œ∏)^{1-x_i}$.\n",
        "  $f(\\v{x},n|Œ∏)=Œ∏^{\\sum_ix_i}(1-Œ∏)^{n-\\sum_ix_i}$\n",
        "  $=Œ∏^{X}(1-Œ∏)^{n-X}$.\n",
        "  $\\/{f(\\v{x}|Œ∏)}{f(\\v{y}|Œ∏)}$\n",
        "  $=Œ∏^{X-Y}(1-Œ∏)^{Y-X}$\n",
        "  $=1$\n",
        "  if $T(\\v{x})=T(\\v{y})$.\n",
        "  - Prove $X/N$ is unbiased for $Œ∏$ and find variance.\n",
        "  $\\E[\\/{X}{N}]=\\E[\\E[\\/{X}{N}|N]]$\n",
        "  $=\\E[\\/{1}{N}\\E[X|N]]$\n",
        "  $=\\E[\\/{1}{N}NŒ∏]$\n",
        "  $=Œ∏$.\n",
        "  $\\Var(\\E[\\/{X}{N}|N])$\n",
        "  $=\\Var(\\/{1}{N}\\E[X|N])$\n",
        "  $=\\Var(Œ∏(1-Œ∏))$\n",
        "  $=0$.\n",
        "  $\\E[\\Var(\\/{X}{N}|N)]$\n",
        "  $=\\E[\\/{1}{N^2}NŒ∏(1-Œ∏)]$\n",
        "  $=\\E[\\/{1}{N}]Œ∏(1-Œ∏)$.\n",
        "\n",
        "- 6.13: Let $X_1,X_2$ iid from $f(x_i|a)=ax_i^{a-1}e^{-x_i^a}I_{x_i>0,a>0}$. Show $\\/{\\ln X_1}{\\ln X_2}$ is ancillary.\n",
        "Let $Y_i=\\ln X_i$.\n",
        "Then $|\\/{‚àÇx_i}{‚àÇy_i}|=e^{y_i}$.\n",
        "$f(y_i|a)=ae^{y_i(a-1)}e^{-e^{ay_i}}e^{y_i}$\n",
        "$=a\\e{ay_i-e^{ay_i}}$\n",
        "is a scale family.\n",
        "Let $Z_i=aY_i$, then $|\\/{‚àÇy_i}{‚àÇz_i}|=\\/{1}{a}$\n",
        "then $f(z_i|a)=\\e{z_i-e^{z_i}}$.\n",
        "$\\/{\\ln X_1}{\\ln X_2}$\n",
        "$=\\/{Y_1}{Y_2}$\n",
        "$=\\/{Z_1}{Z_2}$\n",
        "$=\\e{z_1-z_2+e^{z_2}-e^{z_1}}$,\n",
        "independent of $a$.\n",
        "\n",
        "- 6.14: Let $X_1,...,X_n$ iid from location family. Show $M-\\bar{X}$ is ancillary where $M$ is median.\n",
        "$P(M-\\bar{X}‚â§a)$\n",
        "$=P(X_{(n/2)}-\\/{\\sum_iX_i}{n}‚â§a)$\n",
        "$=P(Z_{(n/2)}+Œ∏-\\/{\\sum_iZ_i}{n}-Œ∏‚â§a)$\n",
        "$=P(Z_{(n/2)}-\\/{\\sum_iZ_i}{n}‚â§a)$\n",
        "is independent of $Œ∏$.\n",
        "\n",
        "- 6.15 (Curved exponential, test of completeness): Let $X_1,...,X_n‚àº\\Normal(Œ∏,aŒ∏^2)$ iid where $a$ is known and $Œ∏>0$.\n",
        "  - Show the parameter space dose not contain a 2-D open set.\n",
        "  $f(x_i|Œ∏)=\\/{1}{\\sqrt{2œÄa}Œ∏}\\e{-\\/{(x_i-Œ∏)^2}{2aŒ∏^2}}$\n",
        "  $=\\/{1}{\\sqrt{2œÄa}Œ∏}\\e{-\\/{x_i^2}{2aŒ∏^2}+\\/{x_i}{aŒ∏}-\\/{1}{2}}$.\n",
        "  Parameters $(w_1(Œ∏),w_2(Œ∏))=\\{(\\/{-1}{2aŒ∏^2}, \\/{1}{aŒ∏}):Œ∏‚àà‚Ñù\\}$\n",
        "  describes a parabolic curve instead of a $‚Ñù^2$ open set.\n",
        "  - Show $T=(\\bar{X},S^2)$ is sufficient but not complete.\n",
        "  $T=(\\sum_it_1(x_i),\\sum_it_2(x_i))$\n",
        "  $=(\\sum_ix_i^2,\\sum_ix_i)$ is sufficient.\n",
        "  $\\E[\\bar{X}^2]=\\Var(\\bar{X})+\\E[\\bar{X}]^2$\n",
        "  $=\\/{aŒ∏^2}{n}+Œ∏^2$\n",
        "  $=\\/{(a+n)Œ∏^2}{n}$.\n",
        "  $\\E[S^2]=aŒ∏^2$.\n",
        "  Let $g(\\bar{X},S^2)=\\/{n}{a+n}\\bar{X}^2-\\/{1}{a}S^2$ non-trivial.\n",
        "  $\\E[g(T)]=0$. Therefore $T$ is not complete.\n",
        "\n",
        "- 6.17 (Geometric): Let $X_1,...,X_n‚àº\\Geom(Œ∏)$ iid. Show $‚àëX_i$ is sufficient and complete.\n",
        "$f(x_i|Œ∏)=Œ∏(1-Œ∏)^{x_i-1}$\n",
        "$=\\/{Œ∏}{1-Œ∏}\\e{x_i\\ln(1-Œ∏)}$,\n",
        "full exponential $d=k$.\n",
        "$T=\\sum_iX_i$ is sufficient and complete.\n",
        "\n",
        "- 6.18 (Poisson): Let $X_1,...,X_n‚àº\\Pois(Œª)$ iid. Show $\\sum X_i$ is complete.\n",
        "$f(x_i|Œª)=\\/{e^{-Œª}Œª^{x_i}}{x_i!}$\n",
        "$=\\/{e^{-Œª}}{x_i!}\\e{x_i\\ln(Œª)}$,\n",
        "full exponential $d=k$.\n",
        "$T=\\sum_iX_i$ is sufficient and complete.\n",
        "\n",
        "- 6.20: Let $X_i,...,X_n$ iid. Find complete sufficient.\n",
        "  - $f(x_i|Œ∏)=\\/{2x_i}{Œ∏^2}I_{0< x_i< Œ∏}$.\n",
        "  $f(\\v{x}|Œ∏)=(\\/{2}{Œ∏^2})^n\\prod_ix_iI_{X_{(1)}>0,X_{(n)}< Œ∏}$\n",
        "  $=(\\/{2}{Œ∏^2})^nI_{X_{(n)}< Œ∏}\\prod_ix_iI_{X_{(1)}>0}$.\n",
        "  $T=X_{(n)}$ is sufficient.\n",
        "  CDF $F(x)=\\/{x^2}{Œ∏^2}$.\n",
        "  $F_T(t|Œ∏)=F(t)^n$.\n",
        "  $f_T(t|Œ∏)=nF(t)^{n-1}f(t)$\n",
        "  $=n\\/{t^{2(n-1)}}{Œ∏^{2(n-1)}}\\/{2t}{Œ∏^2}I_{0< t< Œ∏}$\n",
        "  $=\\/{2n}{Œ∏^{2n}}t^{2n-1}I_{0< t< Œ∏}$.\n",
        "  $\\E[g(t)]=‚à´_0^Œ∏g(t)\\/{2n}{Œ∏^{2n}}t^{2n-1}\\ dt=0$\n",
        "  $\\imply{d/dt}g(Œ∏)\\/{2n}{Œ∏^{2n}}Œ∏^{2n-1}$\n",
        "  $=g(Œ∏)\\/{2n}{Œ∏}$ for all $Œ∏$.\n",
        "  Therefore $g(t)=0$ and $T$ is complete.\n",
        "  - $f(x_i|Œ∏)=\\/{Œ∏}{(1+x_i)^{1+Œ∏}}$.\n",
        "  Then $f(x_i|Œ∏)=Œ∏(1+x_i)^{-(1+Œ∏)}$\n",
        "  $=Œ∏\\e{-(1+Œ∏)\\ln(1+x_i)}$ is full exponential.\n",
        "  $T=\\sum_i\\ln(1+X_i)$ is sufficient and complete.\n",
        "  - $f(x_i|Œ∏)=\\/{(\\ln Œ∏)Œ∏^{x_i}}{Œ∏-1}I_{0< x_i< 1}$.\n",
        "  Then $f(x_i|Œ∏)=\\/{\\ln Œ∏}{Œ∏-1}\\e{x_i\\ln Œ∏}$ is full exponential.\n",
        "  $T=\\sum_iX_i$ is sufficient and complete.\n",
        "  - $f(x_i|Œ∏)=\\binom{2}{x_i}Œ∏^{x_i}(1-Œ∏)^{2-x_i}$.\n",
        "  Then $f(x_i|Œ∏)=\\binom{2}{x_i}(1-Œ∏)^2(\\/{Œ∏}{1-Œ∏})^{x_i}$\n",
        "  $=\\binom{2}{x_i}(1-Œ∏)^2\\e{x_i\\ln(\\/{Œ∏}{1-Œ∏})}$ is full exponential.\n",
        "  $T=\\sum_iX_i$ is sufficient and complete.\n",
        "\n",
        "- 6.21: Let $X$ be one observation from $f(x|Œ∏)=(\\/{Œ∏}{2})^{|x|}(1-Œ∏)^{1-|x|},\\ x=-1,0,1$. Is $X$ sufficient and complete? Is $|X|$ sufficient and complete?\n",
        "$f(x|Œ∏)=(1-Œ∏)(\\/{Œ∏}{2(1-Œ∏)})^{|x|}$\n",
        "$=(1-Œ∏)\\e{|x|\\ln \\/{Œ∏}{2(1-Œ∏)}}$.\n",
        "$X$ is always sufficient, but $E[X]=0$ for all $Œ∏$ because $P(X=1)=P(X=-1)$. Therefore $T=X$ is not complete.\n",
        "$|X|‚àº\\t{Bernoulli}(Œ∏)$ is exponential family and therefore sufficient and complete.\n",
        "\n",
        "- 6.22: Let $X_1,...,X_n$ iid from $f(x_i|Œ∏)=Œ∏x_i^{Œ∏-1}$. Find complete sufficient.\n",
        "$f(x_i|Œ∏)=Œ∏\\e{(Œ∏-1)\\ln x_i}$.\n",
        "$T=\\sum_i\\ln X_i$ is complete and sufficient.\n",
        "\n",
        "- 6.23: Let $X_1,...,X_n‚àº\\Unif(Œ∏,2Œ∏)$ iid. Find minimal sufficient.\n",
        "$f(x_i|Œ∏)=\\/{1}{Œ∏}I_{Œ∏< x_i< 2Œ∏}$.\n",
        "$f(\\v{x}|Œ∏)=\\/{1}{Œ∏^n}I_{X_{(1)}>Œ∏, X_{(n)}< 2Œ∏}$\n",
        "$=\\/{1}{Œ∏^n}I_{X_{(n)}/2< Œ∏< X_{(1)}}$.\n",
        "$T=(X_{(1)},X_{(n)})$ is sufficient, and minimal when min/max are maintained across samples.\n",
        "Range $h(T)=X_{(n)}-X_{(1)}$ is a function of $T$ and is ancillary, therefore $T$ is not complete."
      ],
      "metadata": {
        "id": "GA5zAKFKMU1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Point Estimation: Bias and Variance"
      ],
      "metadata": {
        "id": "hQzNpIpI5qOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Point estimator** is any function $W(X_1,...,X_n)$ of a sample. Any statistic is a point estimator.\n",
        "(Casella 7.1.1)\n",
        "\n",
        "**Method of moments estimator**: Let $X_1,...,X_n$ be sample from PDF/PMF $f(x|Œ∏_1,...,Œ∏_k)$, then $m_j=\\/{1}{n}\\suml_{i=1}^nX_i^j$ are estimators for population moments $Œº_j'(Œ∏_1,...,Œ∏_k)=\\E[X^j]$. Obtain the first $k$ sample moments, set $m_j=Œº_j'(\\tilde{Œ∏}_1,...,\\tilde{Œ∏}_k)$ and we can solve for $(\\tilde{Œ∏}_1,...,\\tilde{Œ∏}_k)$ as estimator for $(Œ∏_1,...,Œ∏_k)$.\n",
        "(Casella 7.2.1)\n",
        "\n",
        "**Maximum likelihood estimator**: Let $\\hat{Œ∏}(\\v{x})$ be parameter value that maximizes $l(Œ∏|\\v{x})=\\ln L(Œ∏|\\v{x})$ as a function of $Œ∏$ with $\\v{x}$ fixed. Then $\\hat{Œ∏}(\\v{X})$ is the **MLE** of $Œ∏$.\n",
        "(Casella 7.2.4)\n",
        "\n",
        "- From sample data $\\v{x}$ we construct $l(Œ∏|\\v{x})=\\suml_{i=1}^nl(Œ∏|x_i)$. The MLE $\\hat{Œ∏}=\\arg\\sup_Œ∏ l(Œ∏|\\v{x})$ is at the peak of this sum. We use **score function** $l'(Œ∏|\\v{x})=0$ to get $\\hat{Œ∏}$, then ensure $l''(Œ∏|\\v{x})|_{Œ∏=\\hat{Œ∏}}< 0$.\n",
        "\n",
        "- **MLE Invariance**: If $\\hat{Œ∏}$ is the MLE of $Œ∏$, then for any function $œÑ$ the MLE of $œÑ(Œ∏)$ is $œÑ(\\hat{Œ∏})$. (Casella 7.2.10)\n",
        "\n",
        "  - For re-parametrized $Œ∑=œÑ(Œ∏)$, **induced likelihood function**\n",
        "  $l^*(Œ∑|\\v{x})=l(œÑ^{-1}(Œ∑)|\\v{x})=l(Œ∏|\\v{x})$\n",
        "  when $œÑ(Œ∏)$ is one-to-one:\n",
        "  $\\sup_Œ∑l^*(Œ∑|\\v{x})=\\sup_Œ∏l(Œ∏|\\v{x})$ and\n",
        "  $\\hat{Œ∑}=œÑ(\\hat{Œ∏})$.\n",
        "  In practice when $œÑ(Œ∏)$ is many-to-one, we define\n",
        "  $l^*(Œ∑|\\v{x})=\\sup\\limits_{Œ∏:œÑ(Œ∏)=Œ∑}l(Œ∏|\\v{x})$\n",
        "  to ensure $\\hat{Œ∑}=œÑ(\\hat{Œ∏})$.\n",
        "\n",
        "  - Proof: Let $l^*(\\hat{Œ∑}|\\v{x})=\\sup\\limits_Œ∑l^*(Œ∑|\\v{x})$ and\n",
        "  $l^*(Œ∑|\\v{x})=\\sup\\limits_{Œ∏:œÑ(Œ∏)=Œ∑}l(Œ∏|\\v{x})$.\n",
        "  Then\n",
        "  $l^*(\\hat{Œ∑}|\\v{x})=\\sup\\limits_Œ∑\\sup\\limits_{Œ∏:œÑ(Œ∏)=Œ∑}l(Œ∏|\\v{x})$\n",
        "  $=\\sup\\limits_Œ∏l(Œ∏|\\v{x})$\n",
        "  $=l(\\hat{Œ∏}|\\v{x})$.\n",
        "  Furthermore,\n",
        "  $l^*(\\hat{Œ∑}|\\v{x})=\\sup\\limits_{Œ∏:œÑ(Œ∏)=œÑ(\\hat{Œ∏})}l(Œ∏|\\v{x})$\n",
        "  $=l^*(œÑ(\\hat{Œ∏})|\\v{x})$.\n",
        "\n",
        "**Bayes Estimators**: where classical approach treats parameter $Œ∏$ as an unknown fixed quantity, Bayesian approach treats $Œ∏$ as a random variable with a prior distribution $œÄ(Œ∏)$, which is updated with sample into posterior distribution $œÄ(Œ∏|\\v{x})$.\n",
        "\n",
        "- $œÄ(Œ∏|\\v{x})=\\/{f(\\v{x}|Œ∏)œÄ(Œ∏)}{m(\\v{x})}$ where $m(\\v{x})=‚à´_Œ∏ f(\\v{x}|Œ∏)œÄ(Œ∏)\\ dŒ∏$ is the marginal of $\\v{X}$.\n",
        "Then the Bayes estimator $\\hat{Œ∏}=\\E[Œ∏|\\v{x}]$.\n",
        "\n",
        "- Let $\\m{F}$ and $\\m{P}$ be two families of PDF/PMFs. $\\m{P}$ is **conjugate family** for $\\m{F}$ if for all $f(\\v{x}|Œ∏)‚àà\\m{F}$ and all prior $œÄ(Œ∏)$ in $\\m{P}$, posterior $œÄ(Œ∏|\\v{x})$ is also in $\\m{P}$ for all $\\v{x}‚àà\\m{X}$. (Casella 7.2.15)\n",
        "\n",
        "**Expectation-maximization** algorithm: let $(Y_1,...,Y_n)$ be incomplete data with PDF $g(\\v{y}|Œ∏)$, which needs augmented data $X_1,...,X_n$ with PDF $k(\\v{x}|Œ∏,\\v{y})$ to become complete $(\\v{Y},\\v{X})$ with PDF $f(\\v{y},\\v{x}|Œ∏)$. EM maximizes $L(Œ∏|\\v{y})$ by iteratively working with $L(Œ∏|\\v{y},\\v{x})$ and $k(\\v{x}|Œ∏,\\v{y})$.\n",
        "\n",
        "- **E-Step**:\n",
        "$k(\\v{x}|Œ∏,\\v{y})=\\/{f(\\v{y},\\v{x}|Œ∏)}{g(\\v{y}|Œ∏)}$.\n",
        "Then\n",
        "$\\ln L(Œ∏|\\v{y})=\\ln L(Œ∏|\\v{y},\\v{x})-\\ln k(\\v{x}|Œ∏,\\v{y})$.\n",
        "Iterate $Œ∏^{(r)}=(b^{(r)},t^{(r)})$. Let $Œ∏'$ be the current guess, and take $\\E[.|Œ∏=Œ∏']$:\n",
        "$\\E_{Œ∏'}[\\ln L(Œ∏|\\v{y})|\\v{y}]=\\E_{Œ∏'}[\\ln L(Œ∏|\\v{y},\\v{X})|\\v{y}]-\\E_{Œ∏'}[\\ln k(\\v{X}|Œ∏,\\v{y})|\\v{y}]$\n",
        "$‚áí\\ln L(Œ∏|\\v{y})=\\E_{Œ∏'}[\\ln L(Œ∏|\\v{y},\\v{X})|\\v{y}]-\\E_{Œ∏'}[\\ln k(\\v{X}|Œ∏,\\v{y})|\\v{y}]$\n",
        "\n",
        "- **M-step**: Find $Œ∏^{(r+1)}=\\arg\\max_Œ∏\\E[\\ln L(Œ∏|\\v{y},\\v{X})|Œ∏^{(r)},\\v{y}]$\n",
        "\n",
        "- **Monotonic EM sequence**: The sequence $\\{\\hat{Œ∏}_{(r)}\\}$ defined in M-step satisfies $L(\\hat{Œ∏}^{(r+1)}|\\v{y})‚â•L(\\hat{Œ∏}^{(r)}|\\v{y})$ with equality iff successive iterations yield the same value of the maximized expected complete-data log likelihood $\\E[\\ln L(\\hat{Œ∏}^{(r+1)}|\\v{y},\\v{X})|\\hat{Œ∏}^{(r)},\\v{y}]$\n",
        "$=\\E[\\ln L(\\hat{Œ∏}^{(r)}|\\v{y},\\v{X})|\\hat{Œ∏}^{(r)},\\v{y}]$\n",
        "(Casella 7.2.20)"
      ],
      "metadata": {
        "id": "-Ailavb456Ui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean square error** of estimator $W$ of parameter $Œ∏$ is $\\red{\\t{MSE}_Œ∏(W)=\\E_Œ∏(W-Œ∏)^2}$.\n",
        "(Casella 7.3.1)\n",
        "\n",
        "- $\\Bias_Œ∏ W=\\E_Œ∏W-Œ∏$. An **unbiased** estimator satisfies $\\E_Œ∏W=Œ∏$ for all $Œ∏$.\n",
        "(Casella 7.3.2)\n",
        "\n",
        "- $\\red{\\t{MSE}_Œ∏(W)=\\Var_Œ∏(W)+(\\Bias_Œ∏ W)^2}$.\n",
        "Proof: $\\E_Œ∏(W-Œ∏)^2=\\E_Œ∏(W-\\E_Œ∏W+\\E_Œ∏W-Œ∏)^2$\n",
        "$=\\E_Œ∏(W-\\E_Œ∏W)^2+\\E_Œ∏\\ob{(\\E_Œ∏W-Œ∏)^2}{\\t{constant}}+2\\E_Œ∏[\\ob{(W-\\E_Œ∏W)}{\\E[‚ãÖ]=0}\\ob{(\\E_Œ∏W-Œ∏)}{\\t{constant}}]$\n",
        "$=\\Var_Œ∏W+(\\ob{\\E_Œ∏W-Œ∏}{\\Bias_Œ∏ W})^2$.\n",
        "\n",
        "- As e7.3.4 shows, $\\hat{œÉ}_{MLE}^2$ has lower MSE than $S^2$, but it biases downward and understimates. $œÉ^2$ is a scale parameter, which has parameter space $[0,‚àû)$. If an estimator biases downward and underestimates the parameter there's a limit to how much its MSE property is affected. Estimator errors above or below location parameters like $Œº$ affect the MSE equally.\n",
        "\n",
        "**M-estimator**: Sample mean $\\bar{X}_n$ minimizes $\\sum_i(x_i-a)^2$, sample medium $M_n$ minimizes $\\sum_i|x_i-a|$. M-estimator minimizes $\\suml_{i=1}^nœÅ(x_i-a)$ where $œÅ(x_i-a)=\\BC\n",
        "\\/{1}{2}(x_i-a)^2 &\\t{if normal }|x_i-a|‚â§k\\\\\n",
        "k|x_i-a|-\\/{1}{2}(x_i-a)^2 &\\t{if outlier }|x_i-a|‚â•k\n",
        "\\EC$\n",
        "penalizes data points either using mean-like or medium-like estimator. M-estimator is $\\hat{Œ∏}_M=\\arg\\inf_Œ∏\\sum_iœÅ(x_i-Œ∏)$.\n",
        "- Let $œà=\\/{‚àÇœÅ}{‚àÇŒ∏}$, then $\\hat{Œ∏}_M$ is consistent estimator to the true $Œ∏$ that minimizes $\\sum_iœÅ(x_i-Œ∏)$. By Taylor expansion\n",
        "$\\suml_{i=1}^nœà(x_i-\\hat{Œ∏}_M)=0$\n",
        "$‚âà\\suml_{i=1}^nœà(x_i-Œ∏)+(\\hat{Œ∏}_M-Œ∏)\\suml_{i=1}^nœà'(x_i-Œ∏)$\n",
        "$‚áí\\sqrt{n}(\\hat{Œ∏}_M-Œ∏)=\\/{-\\/{1}{\\sqrt{n}}\\sum_iœà(x_i-Œ∏)}{\\/{1}{n}\\sum_iœà'(x_i-Œ∏)}$.\n",
        "Numerator assumes $\\E_{Œ∏}[œà]=0$ and uses CLT\n",
        "$-\\/{1}{\\sqrt{n}}\\sum_iœà(x_i-Œ∏)\\arr{d}\\Normal(0,\\E_{Œ∏}[œà(x_i-Œ∏)^2])$.\n",
        "Denominator uses LLN\n",
        "$\\/{1}{n}\\sum_iœà'(x_i-Œ∏)\\arr{p}\\E_{Œ∏}[œà'(x_i-Œ∏)]$.\n",
        "Then asymptotic\n",
        "$\\Var(\\sqrt{n}(\\hat{Œ∏}_M-Œ∏))=\\/{\\E_{Œ∏}[œà(x_i-Œ∏)^2]}{\\E_{Œ∏}[œà'(x_i-Œ∏)]^2}$.\n",
        "\n",
        "**Decision theory**: After observing $\\v{X}=\\v{x}$, an estimation action $Œ¥‚àà\\m{A}$ regarding $Œ∏‚ààŒò$ is made. Squared error loss has **loss function** $L(Œ∏,Œ¥)=(Œ¥-Œ∏)^2$. The quality of estimator $Œ¥(\\v{x})$ of $Œ∏$ is quantified via the **risk function** $R(Œ∏,Œ¥)=\\E_Œ∏[L(Œ∏,Œ¥)]$, which measures the average loss incurred while using $Œ¥$. MSE is the risk function for squared error loss.\n",
        "\n",
        "**Bayes risk** $\\red{‚à´_ŒòR(Œ∏,Œ¥)œÄ(Œ∏)\\ dŒ∏}$ computes the risk function averaged under the prior. **Bayes rule with respect to $œÄ$** is the estimator $Œ¥^œÄ=\\arg\\min_Œ¥‚à´_ŒòR(Œ∏,Œ¥)œÄ(Œ∏)\\ dŒ∏$.\n",
        "\n",
        "- Bayes risk $‚à´_ŒòR(Œ∏,Œ¥)œÄ(Œ∏)\\ dŒ∏$\n",
        "$=\\ob{‚à´_Œò[‚à´_\\m{X}L(Œ∏,Œ¥(\\v{x}))f(\\v{x}|Œ∏)\\ d\\v{x}]œÄ(Œ∏)\\ dŒ∏}{\\E_{œÄ(Œ∏)}\\E_{f(\\v{x}|Œ∏)}[L(Œ∏,Œ¥)]}$\n",
        "$=\\ob{‚à´_\\m{X}[\\red{‚à´_ŒòL(Œ∏,Œ¥(\\v{x}))œÄ(Œ∏|\\v{x})\\ dŒ∏}]m(\\v{x})\\ d\\v{x}}{\\E_{m(\\v{x})}\\E_{œÄ(Œ∏|\\v{x})}[L(Œ∏,Œ¥)]}$\n",
        "because $f(\\v{x}|Œ∏)œÄ(Œ∏)=œÄ(Œ∏|\\v{x})m(\\v{x})$.\n",
        "What's the bracket is the **posterior expected loss**, which integrates away all dependency on $Œ∏$ and is a function of $\\v{x}$ that we can use to compute Bayes rule.\n",
        "\n",
        "- We want to solve for Baye's rule $Œ¥^œÄ$.\n",
        "When we try to solve it from the definition\n",
        "$Œ¥^œÄ=\\arg\\min_Œ¥\\E_{œÄ(Œ∏)}\\E_{f(\\v{x}|Œ∏)}[L(Œ∏,Œ¥)]$\n",
        "we get an integral that we cannot compute from.\n",
        "When we solve it from the posterior expected loss\n",
        "$Œ¥^œÄ=\\arg\\min_Œ¥\\E_{œÄ(Œ∏|\\v{x})}[L(Œ∏,Œ¥)]$, we get an explicit analytical function of $\\v{x}$.\n",
        "Therefore\n",
        "$\\red{Œ¥^œÄ=\\arg\\min_Œ¥‚à´_ŒòL(Œ∏,Œ¥(\\v{x}))œÄ(Œ∏|\\v{x})\\ dŒ∏}$.\n",
        "\n",
        "- For $L(Œ∏,Œ¥)=(Œ¥-Œ∏)^2$ squared error loss,\n",
        "$Œ¥^œÄ=\\arg\\min_Œ¥‚à´_Œò(Œ∏-Œ¥)^2œÄ(Œ∏|\\v{x})\\ dŒ∏$\n",
        "$=\\arg\\min_Œ¥\\E[(Œ∏-Œ¥)^2|\\v{x}]$,\n",
        "then $Œ¥^œÄ(\\v{x})=\\E[Œ∏|\\v{x}]$.\n",
        "\n",
        "- For $L(Œ∏,Œ¥)=|Œ¥-Œ∏|$ absolute error loss,\n",
        "$Œ¥^œÄ=\\arg\\min_Œ¥\\E[|Œ∏-Œ¥||\\v{x}]$,\n",
        "then $Œ¥^œÄ(\\v{x})=\\t{median of }œÄ(Œ∏|\\v{x})$.\n",
        "\n"
      ],
      "metadata": {
        "id": "BHPP204C1SI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cram√©r-Rao**: Let $X_1,...,X_n$ be sample from $f(\\v{x}|Œ∏)$ and let estimator $W(\\v{X})$ satisfy\n",
        "$\\/{d}{dŒ∏}\\E_Œ∏W(\\v{X})=‚à´_\\m{X}\\/{‚àÇ}{‚àÇŒ∏}[W(\\v{x})f(\\v{x}|Œ∏)]\\ d\\v{x}$\n",
        "and $\\Var_Œ∏W(\\v{X})< ‚àû$, then Cram√©r-Rao lower bound is\n",
        "$\\red{\\Var_Œ∏(W(\\v{X}))‚â•\\/{(\\/{d}{dŒ∏}\\E_Œ∏W(\\v{X}))^2}{\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))^2]}}$\n",
        "(Casella 7.3.9)\n",
        "\n",
        "- **Regularity condition**: $\\/{d}{dŒ∏}\\E_Œ∏W=‚à´_\\m{X}\\/{‚àÇ}{‚àÇŒ∏}[W(\\v{x})f(\\v{x}|Œ∏)]\\ d\\v{x}$\n",
        "says $W(\\v{x})f(\\v{x}|Œ∏)$ is differentiable with respect to $Œ∏$ and is dominated to move the derivative inside the integral, and support of $f(\\v{x}|Œ∏)$ does not depend on $Œ∏$ as per Leibniz rule in Casella 2.4.4:\n",
        "$\\/{d}{dŒ∏}‚à´_{a(Œ∏)}^{b(Œ∏)}f(x,Œ∏)\\ dx=\\ob{f(b(Œ∏),Œ∏)\\/{d}{dŒ∏}b(Œ∏)-f(a(Œ∏),Œ∏)\\/{d}{dŒ∏}a(Œ∏)}{\\t{these terms appear if $Œ∏$ is in the limits of integration}}$\n",
        "$+‚à´_{a(Œ∏)}^{b(Œ∏)}\\/{‚àÇ}{‚àÇŒ∏}f(x,Œ∏)\\ dx$.\n",
        "Condition fails if the support of $\\v{X}$ is a function of $Œ∏$. E.g., $X_i‚àº\\Unif(0,Œ∏)$ in e7.3.13.\n",
        "\n",
        "- Numerator $(\\/{d}{dŒ∏}\\E_Œ∏W)^2=(\\/{d}{dŒ∏}œÑ(Œ∏))^2$. If $W$ is an unbiased estimator then $(\\/{d}{dŒ∏}\\E_Œ∏W)^2=1$.\n",
        "\n",
        "- Denominator $\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))^2]=\\Var_Œ∏(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))$ is the **Fisher information** or information number of the sample. As we have more information about $Œ∏$, the bound on the variance of $W$ becomes smaller. Fisher information is also the variance of the **score function** $\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)$ used in MLE determination.\n",
        "\n",
        "  - $\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)$ measures how sharply the PDF changes in the neighborhood of $Œ∏$, or the peak curvature. The peak curvature tells us how well the observed $W$ characterizes $Œ∏$. If slope is flat then observed $W$ does not provide much information about $Œ∏$, Fisher information is low, and variance is high. If pdf is 1 at a single point then $W=Œ∏$, Fisher information is infinite and variance is zero.\n",
        "\n",
        "  - For normal sample (e7.2.5), the score is\n",
        "  $\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)=\\/{n}{œÉ^2}(\\bar{x}-Œ∏)$.\n",
        "  Its variance is a measure of the spread of $\\bar{x}$ around $Œ∏$, indicating how much information about $Œ∏$ does $\\bar{x}$ really have.\n",
        "\n",
        "- Proof: by Cauchy-Schwarz $[\\Cov(W,\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))]^2‚â§\\Var(W)\\Var(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))$.\n",
        "From the regularity requirement\n",
        "$\\/{d}{dŒ∏}\\E_Œ∏W=\\/{d}{dŒ∏}‚à´_\\m{X}W(\\v{x})f(\\v{x}|Œ∏)\\ d\\v{x}$\n",
        "$=‚à´_\\m{X}W(\\v{x})\\/{\\/{‚àÇ}{‚àÇŒ∏}f(\\v{x}|Œ∏)}{f(\\v{x}|Œ∏)}f(\\v{x}|Œ∏)\\ d\\v{x}$\n",
        "$=\\E_Œ∏[W\\blue{\\/{\\/{‚àÇ}{‚àÇŒ∏}f(\\v{x}|Œ∏)}{f(\\v{x}|Œ∏)}}]$\n",
        "$=\\E_Œ∏[W\\blue{\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)}]$.\n",
        "\n",
        "  - Let $W=1$ then we get\n",
        "  $\\/{d}{dŒ∏}\\E_Œ∏W=\\E_Œ∏[W\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)]$\n",
        "  $‚áí\\blue{0=\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)]}$.\n",
        "\n",
        "  - Numerator:\n",
        "  $\\Cov(W,\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))$\n",
        "  $=\\E_Œ∏[W\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)]-\\E_Œ∏[W]\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)]$\n",
        "  $=\\/{d}{dŒ∏}\\E_Œ∏W$.\n",
        "\n",
        "  - Denominator:\n",
        "  $\\Var(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))$\n",
        "  $=\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))^2]$\n",
        "  $-(\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)])^2$\n",
        "  $=\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))^2]$.\n",
        "\n",
        "- **CRLB, iid**: If $X_1,...,X_n$ are iid from $f(x_i|Œ∏)$, then\n",
        "$\\red{\\Var_Œ∏(W(\\v{X}))‚â•\\/{(\\/{d}{dŒ∏}\\E_Œ∏W(\\v{X}))^2}{\\blue{n}\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\blue{x_i}|Œ∏))^2]}}$\n",
        "(Casella 7.3.10)\n",
        "\n",
        "  - Proof:\n",
        "  $\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))^2]$\n",
        "  $=\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln \\prod_{i=1}^n f(x_i|Œ∏))^2]$\n",
        "  $=\\E_Œ∏[(\\sum_{i=1}^n\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏))^2]$\n",
        "  $=\\sum_{i=1}^n\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏))^2]$\n",
        "  $+\\sum_{i\\neq j}\\ob{\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏)\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_j|Œ∏)]}{=\\E[‚ãÖ]\\E[‚ãÖ]\\t{ by independence, }\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏)]=0}$\n",
        "  $=n\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏))^2]$.\n",
        "\n",
        "  - Or:\n",
        "  $\\E_Œ∏[(l'(Œ∏|\\v{x}))^2]$\n",
        "  $=\\E_Œ∏[(\\sum_il'(Œ∏|x_i))^2]$\n",
        "  $=\\sum_i\\E_Œ∏[(l'(Œ∏|x_i))^2]$\n",
        "  $+\\sum_{i\\neq j}\\E_Œ∏[l'(Œ∏|x_i)]\\E_Œ∏[l'(Œ∏|x_j)]$\n",
        "  $=n\\E_Œ∏[(l'(Œ∏|x_i))^2]$\n",
        "\n",
        "- **Second derivative Fisher**: If $f(x_i|Œ∏)$ satisfies\n",
        "$\\/{d}{dŒ∏}\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏)]=‚à´\\/{‚àÇ}{‚àÇŒ∏}[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x|Œ∏)) f(x|Œ∏)]\\ dx$ condition, then\n",
        "$\\red{\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x|Œ∏))^2]=-\\E_Œ∏[\\/{‚àÇ^2}{‚àÇŒ∏^2}\\ln f(x|Œ∏)]}$.\n",
        "(Casella 7.3.11)\n",
        "\n",
        "  - The second derivative represents the peak curvature,  indicating how well an observed $\\bar{x}$ characterizes $Œ∏$.\n",
        "\n",
        "  - Similar regularity condition as in Cram√©r-Rao:\n",
        "  $\\/{d}{dŒ∏}\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏)]$\n",
        "  $=\\/{d}{dŒ∏}‚à´[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏)]f(x_i|Œ∏)\\ dx$\n",
        "  $=‚à´\\/{‚àÇ}{‚àÇŒ∏}[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x|Œ∏)) f(x|Œ∏)]\\ dx$\n",
        "\n",
        "  - Proof:\n",
        "  $\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏)]=0$\n",
        "  $‚áí\\/{d}{dŒ∏}\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏)]=0$.\n",
        "  Use product rule:\n",
        "  $\\/{d}{dŒ∏}\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏)]$\n",
        "  $=‚à´\\/{‚àÇ}{‚àÇŒ∏}[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x|Œ∏)) f(x|Œ∏)]\\ dx$\n",
        "  $=‚à´\\/{‚àÇ^2}{‚àÇŒ∏^2}\\ln f(x|Œ∏) f(x|Œ∏)\\ dx$\n",
        "  $+‚à´(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x|Œ∏))(\\/{‚àÇ}{‚àÇŒ∏}f(x|Œ∏))\\ dx$\n",
        "  $=\\E_Œ∏[\\/{‚àÇ^2}{‚àÇŒ∏^2}\\ln f(x_i|Œ∏)]$\n",
        "  $+‚à´(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x|Œ∏))\\/{\\/{‚àÇ}{‚àÇŒ∏}f(x|Œ∏)}{f(x|Œ∏)} f(x|Œ∏)\\ dx$\n",
        "  $=\\E_Œ∏[\\/{‚àÇ^2}{‚àÇŒ∏^2}\\ln f(x_i|Œ∏)]$\n",
        "  $+\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏))^2]=0$\n",
        "\n",
        "- **CRLB attainment**: Let $X_1,...,X_n$ be iid from $f(x_i|Œ∏)$ satisfying Cram√©r-Rao conditions. If $W(\\v{X})$ is an unbiased estimator of $œÑ(Œ∏)$ then $W$ attains CRLB iff $a(Œ∏)[W-œÑ(Œ∏)]=\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)$ for function $a(Œ∏)$.\n",
        "(Casella 7.3.15)\n",
        "\n",
        "  - Expressing $\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏)$ in the form $a(Œ∏)[w(x_i)-t(Œ∏)]$ will identify $W(\\v{x})=\\sum_{i=1}^nw(x_i)$ and $œÑ(Œ∏)=nt(Œ∏)$ where $W$ attains CRLB as unbiased estimator of $œÑ(Œ∏)$. (see e7.38)\n",
        "\n",
        "  - Proof: by Cram√©r-Rao\n",
        "  $\\Cov_Œ∏(W,\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))^2‚â§\\Var_Œ∏(W)\\Var_Œ∏(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))$.\n",
        "  By Cauchy-Schwarz covariance inequality,\n",
        "  $\\Cov(A,B)^2=\\Var(A)\\Var(B)$ iff $(A-\\E[A])=c(B-\\E[B])$.\n",
        "  Similarly Cram√©r-Rao equality is attained iff\n",
        "  $c(Œ∏)(W-\\ob{\\E_Œ∏[W]}{=œÑ(Œ∏)})=(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)-\\ob{\\E_Œ∏[\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏)]}{=0})$"
      ],
      "metadata": {
        "id": "xe7imoNcSxsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UMVUE**: Estimator $W$ is a best unbiased estimator or uniform minimum variance unbiased estimator (UMVUE) of $œÑ(Œ∏)$ if $\\E_Œ∏[W]=œÑ(Œ∏)$ and any other $W'$ with $\\E_Œ∏[W']=œÑ(Œ∏)$ satisfies $\\Var_Œ∏(W)‚â§\\Var_Œ∏(W')$ for all $Œ∏$.\n",
        "(Casella 7.3.7)\n",
        "\n",
        "- Unlike MLE invariance, UMVUE is not invariant. I.e., $\\E_Œ∏[W]=Œ∏$ does NOT imply $E_Œ∏[œÑ(W)]=œÑ(Œ∏)$ unless $œÑ$ is linear.\n",
        "\n",
        "  - Proof: Jensen's inequality: $\\E[g(X)]‚â•g(\\E[X])$ if $g$ is convex.\n",
        "\n",
        "- **Rao-Blackwell**: Let $W$ be unbiased estimator of $œÑ(Œ∏)$ and $T$ be sufficient statistic for $Œ∏$. Define $\\red{œï(T)=\\E[W|T]}$. Then $\\E_Œ∏[œï(T)]=œÑ(Œ∏)$ and $\\Var_Œ∏(œï(T))‚â§\\Var_Œ∏(W)$ for all $Œ∏$. That is $œï(T)$ is a uniformly better unbiased estimator of $œÑ(Œ∏)$.\n",
        "(Casella 7.3.17)\n",
        "\n",
        "  - Conditioning any unbiased estimator on a sufficient statistic will result in uniform improvement.\n",
        "\n",
        "  - Some distributions have a tuple sufficient statistic. E.g., $X_i‚àº\\Unif(Œ∏,2Œ∏)$ sufficient statistic is $T=(X_{(1)},X_{(n)})$, then either $X_{(1)}$ or $X_{(n)}$ by itself is not sufficient. UMVUE is $\\E[W|X_{(1)},X_{(n)}]$. However if $W$ is a function of either $X_{(1)}$ or $X_{(n)}$ then $W=\\E[W|X_{(1)},X_{(n)}]$ and no improvement can be gained via Rao-Blackwell. See e7.46.\n",
        "\n",
        "  - Proof: By Adam and Eve,\n",
        "  $\\E_Œ∏[\\E[W|T]]=\\E_Œ∏[W]=œÑ(Œ∏)$,\n",
        "  and\n",
        "  $\\Var(W)=\\Var(\\E[W|T])+\\E[\\Var(W|T)]$\n",
        "  $=\\Var_Œ∏(œï(T))+\\E[\\Var(W|T)]$.\n",
        "  Because $T$ is sufficient, $œï(T)=\\E[W|T]$ is a function of $\\v{X}$ with no dependency on $Œ∏$ and is a valid estimator.\n",
        "\n",
        "- If $W$ is UMVUE of $œÑ(Œ∏)$ then $W$ is unique.\n",
        "(Casella 7.3.19)\n",
        "\n",
        "  - Proof: Suppose $W'$ is a competing UMVUE, then\n",
        "  $\\Var(W')=\\Var(W)$ and $\\E_Œ∏[W]=\\E_Œ∏[W']=œÑ(Œ∏)$.\n",
        "  Let $W^*=\\/{1}{2}(W+W')$.\n",
        "  We show $W=W'$ by first noting\n",
        "  $\\Var_Œ∏(W^*)=\\Var_Œ∏(\\/{1}{2}W+\\/{1}{2}W')$\n",
        "  $=\\/{1}{4}\\Var_Œ∏(W)+\\/{1}{4}\\Var_Œ∏(W')$\n",
        "  $+\\/{1}{2}\\Cov_Œ∏(W,W')$\n",
        "  $‚â§\\/{1}{4}\\Var_Œ∏(W)+\\/{1}{4}\\Var_Œ∏(W')$\n",
        "  $+\\/{1}{2}\\sqrt{\\Var_Œ∏(W)\\Var_Œ∏(W')}$\n",
        "  $=\\Var(W)$.\n",
        "  The Cauchy-Schwarz inequality must be $=$ for all $Œ∏$ because $W$ has minimum variance, which proves $\\Cov(W,W')=\\Var(W)$. Cauchy-Schwarz equality occurs iff\n",
        "  $a(Œ∏)(W-œÑ(Œ∏))=(W'-œÑ(Œ∏))$\n",
        "  $‚áíW'=a(Œ∏)W+œÑ(Œ∏)(a(Œ∏)-1)$.\n",
        "  Then we go back to the covariance:\n",
        "  $\\Cov_Œ∏(W,W')$\n",
        "  $=\\Cov_Œ∏(W,a(Œ∏)W+b(Œ∏))$\n",
        "  $=a(Œ∏)\\Cov_Œ∏(W,W)$\n",
        "  $=a(Œ∏)\\Var_Œ∏(W)$.\n",
        "  Therefore $a(Œ∏)=1$ and $W=W'$.\n",
        "\n",
        "- If $E_Œ∏[W]=œÑ(Œ∏)$, then $W$ is UMVUE of $œÑ(Œ∏)$ iff $W$ is uncorrelated with all unbiased estimators of 0.\n",
        "(Casella 7.3.20)\n",
        "\n",
        "  - Proof (UMVUE‚áíuncorrelated): Let $U$ be an estimator such that $\\E_Œ∏[U]=0$, and let $W'=W+aU$.\n",
        "  Then\n",
        "  $\\Var_Œ∏(W')=\\Var_Œ∏(W)+a^2\\Var_Œ∏(U)+2a\\Cov_Œ∏(W,U)$\n",
        "  is minimized by\n",
        "  $\\/{‚àÇ\\Var_Œ∏(W')}{‚àÇa}=0$\n",
        "  $‚áía=-\\/{\\Cov_Œ∏(W,U)}{\\Var_Œ∏(U)}$.\n",
        "  Furthermore,\n",
        "  $a=0‚áí\\Cov_Œ∏(W,U)=0$\n",
        "  because $\\Var_Œ∏(W')=\\Var_Œ∏(W)$ is the lowest variance it can have.\n",
        "  \n",
        "    - If $\\Cov_{Œ∏_0}(W,U)\\neq0$ for some $Œ∏_0$ then we can achieve\n",
        "    $a^2\\Var_Œ∏(U)+2a\\Cov_Œ∏(W,U)< 0$\n",
        "    and\n",
        "    $\\Var_{Œ∏_0}(W')< \\Var_{Œ∏_0}(W)$.\n",
        "\n",
        "  - Proof (uncorrelated‚áíUMVUE): Let unbiased estimator $W$ be uncorrelated with unbiased estimators of 0. Let $W'$ be any other estimator satisfying $\\E_Œ∏[W]=\\E_Œ∏[W']=œÑ(Œ∏)$, then\n",
        "  $\\E_Œ∏[W'-W]=0$ is unbiased estimator of 0.\n",
        "  $\\Var_Œ∏(W')=\\Var_Œ∏(W)+\\Var_Œ∏(W'-W)+\\ob{2\\Cov_Œ∏(W,W'-W)}{=0\\t{ due to $W$ being uncorrelated}}$\n",
        "  $=\\Var_Œ∏(W)+\\ob{\\Var_Œ∏(W'-W)}{‚â•0}$.\n",
        "\n",
        "- **Lehmann-Scheff√© Theorem**: Let $T$ be a complete and sufficient statistic for $Œ∏$ and let $œï(T)$ be any unbiased estimator as a function of $T$ such that $\\E[œï(T)]=œÑ(Œ∏)$. Then $œï(T)$ is the unique UMVUE of $œÑ(Œ∏)$.\n",
        "(Casella 7.3.23)\n",
        "\n",
        "  - Proof: Let $U$ be any unbiased estimator of 0, $T$ be a sufficient statistic of $Œ∏$, and $W=œï(T)$ be an unbiased estimator of $Œ∏$. Then we use Rao-Blackwell to build another estimator of 0: $U'=\\E_Œ∏[U|T]$ so that checking correlation against $U$ or against $U'$ are equivalent:\n",
        "  $\\Cov_Œ∏(W,U)=\\E_Œ∏[WU]$\n",
        "  $=\\E_Œ∏[\\E_Œ∏[WU|T]]$\n",
        "  $=\\E_Œ∏[\\ob{W\\E_Œ∏[U|T]}{\\E_Œ∏[œï(T)|T]=œï(T)}]$\n",
        "  $=\\Cov_Œ∏(W,U')$.\n",
        "  But if $T$ is complete, then $\\E_Œ∏[g(T)]=0$ implies $g(T)=0$ as, and $\\E_Œ∏[U']=0$ implies $U'=0$ as.\n",
        "  Therefore, checking estimator correlation against every possible unbiased estimator of 0 is the same as checking correlation against 0! $\\Cov_Œ∏(W,0)=0$ and $W=œï(T)$ is UMVUE.\n",
        "\n",
        "- Let $T$ be complete and sufficient for $Œ∏$. Let $T'$ be any other statistic such that $\\E[T]=\\E[T']=œÑ(Œ∏)$. Then $\\E[T'|T]=T$ and $\\Var(T')‚â•\\Var(T)$. (Casella e7.52 solution manual)\n",
        "\n",
        "  - See e7.49 and e7.52.\n",
        "\n",
        "  - Proof: Because $T$ is complete and sufficient, then $T$ is UMVUE for $œÑ(Œ∏)$ by Lehmann-Scheff√©. Rao-Blackwell estimator $\\E[T'|T]$ is a function of $T$ and is therefore also UMVUE for $œÑ(Œ∏)$ by Lehmann-Scheff√©. $\\E[T'|T]=T$ because UMVUE is unique. Rao-Blackwell says $\\E[T'|T]=T$ is uniformly better estimator than $T'$. Therefore $\\Var(T')‚â•\\Var(T)$."
      ],
      "metadata": {
        "id": "4o-mwT60E3_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- 7.2.1 (normal method of moments): Let $X_1,...,X_n‚àº\\Normal(Œ∏,œÉ^2)$ iid.\n",
        "Parameters $Œ∏_1=Œ∏$ and $Œ∏_2=œÉ^2$.\n",
        "Moments $Œº_1'=Œ∏_1$ and $Œº_2'=Œ∏_2+Œ∏_1^2$.\n",
        "Set $m_1=\\tilde{Œ∏}_1$ and $m_2=\\tilde{Œ∏}_2+\\tilde{Œ∏}_1^2$.\n",
        "Estimators $\\tilde{Œ∏}_1=m_1$ and $\\tilde{Œ∏}_2=m_2-\\tilde{Œ∏}_1^2$.\n",
        "\n",
        "- 7.2.2 (binomial method of moments): Let $X_1,...,X_n‚àº\\Binom(k,p)$ iid.\n",
        "Parameters $Œ∏_1=k$ and $Œ∏_2=p$.\n",
        "Moments $Œº_1'=Œ∏_1Œ∏_2$ and $Œº_2'=Œ∏_1Œ∏_2(1-Œ∏_2)+Œ∏_1^2Œ∏_2^2$.\n",
        "Set $m_1=\\tilde{Œ∏}_1\\tilde{Œ∏}_2$ and $m_2=\\tilde{Œ∏}_1\\tilde{Œ∏}_2(1-\\tilde{Œ∏}_2)+\\tilde{Œ∏}_1^2\\tilde{Œ∏}_2^2$.\n",
        "Estimators $\\tilde{Œ∏}_1=\\/{m_1^2}{m_1^2+m_1-m_2}$ and\n",
        "$\\tilde{Œ∏}_2=\\/{m_1}{\\tilde{Œ∏}_1}$.\n",
        "\n",
        "- 7.2.5 (normal MLE by differentiation): Let $X_1,...,X_n‚àº\\Normal(Œ∏,1)$ iid.\n",
        "$L(Œ∏|\\v{x})=(2œÄ)^{-n/2}e^{-\\sum_i(x_i-Œ∏)^2/2}$.\n",
        "Differentiation gives\n",
        "$\\sum_i(x_i-Œ∏)=0$\n",
        "$‚áín\\bar{x}-nŒ∏=0$\n",
        "$‚áí\\hat{Œ∏}=\\bar{x}$.\n",
        "Second derivative gives\n",
        "$\\/{d^2}{dŒ∏^2}L(Œ∏|\\v{x})< 0$.\n",
        "Therefore $\\hat{Œ∏}=\\bar{x}$ is a maximum.\n",
        "\n",
        "- 7.2.6 (7.2.5 direct maximization): Because\n",
        "$\\sum_{i=1}^n(x_i-a)^2‚â•\\sum_{i=1}^n(x_i-\\bar{x})^2$, then\n",
        "$e^{-\\sum_{i=1}^n(x_i-Œ∏)^2/2}‚â§e^{-\\sum_{i=1}^n(x_i-\\bar{x})^2/2}$. Hence $\\hat{Œ∏}=\\bar{X}$.\n",
        "\n",
        "- 7.2.7 (Bernoulli MLE): Let $X_1,...,X_n‚àº\\t{Bernoulli}(p)$.\n",
        "$L(p|\\v{x})=p^{\\sum_ix_i}(1-p)^{n-\\sum_ix_i}$\n",
        "$‚áí\\ln L(p|\\v{x})=\\sum_ix_i\\ln p+(n-\\sum_ix_i)\\ln (1-p)$.\n",
        "Differentiate\n",
        "$0=\\/{\\sum_ix_i}{p}-\\/{n-\\sum_ix_i}{1-p}$\n",
        "$‚áí\\/{1-p}{p}=\\/{n-\\sum_ix_i}{\\sum_ix_i}$\n",
        "$‚áí\\hat{p}=\\/{\\sum_ix_i}{n},\\ 0< p< 1$.\n",
        "\n",
        "- 7.2.8 (normal with nonnegative mean): Let $X_1,...,X_n‚àº\\Normal(Œ∏,1),\\ Œ∏‚â•0$ iid.\n",
        "When $\\bar{X}$ is negative and outside of parameter range, $L(Œ∏|\\v{x})$ is decreasing for $Œ∏‚â•0$.\n",
        "Therefore\n",
        "$\\hat{Œ∏}=\\BC \\bar{X}&\\bar{X}‚â•0\\\\\n",
        "0&\\t{otherwise}\\EC$.\n",
        "\n",
        "- 7.2.9 (binomial MLE computation): Let $X_1,...,X_n‚àº\\Binom(k,p)$ where $k$ is unknown.\n",
        "$L(k|\\v{x},p)=\\prodl_{i=1}^n\\binom{k}{x_i}p^{x_i}(1-p)^{k-x_i}$.\n",
        "The MLE is $\\hat{k}‚â•\\max_ix_i$ such that\n",
        "$\\/{L(\\hat{k}|\\v{x},p)}{L(\\hat{k}-1|\\v{x},p)}‚â•1$ and\n",
        "$\\/{L(\\hat{k}+1|\\v{x},p)}{L(\\hat{k}|\\v{x},p)}< 1$.\n",
        "  - To prove uniqueness of $k$,\n",
        "  $\\/{L(k|\\v{x},p)}{L(k-1|\\v{x},p)}$\n",
        "  $=\\prodl_{i=1}^n\\/{\\binom{k}{x_i}(1-p)^{k-x_i}}{\\binom{k-1}{x_i}(1-p)^{k-1-x_i}}$\n",
        "  $=\\prodl_{i=1}^n\\/{k!x_i!(k-1-x_i)!(1-p)}{(k-1)!x_i!(k-x_i)!}$\n",
        "  $=\\/{(k(1-p))^n}{\\prod_{i=1}^nk-x_i}‚â•1$\n",
        "  $‚áí(k(1-p))^n‚â•\\prod_{i=1}^n(k-x_i)$\n",
        "  $‚áí(1-p)^n‚â•\\prod_{i=1}^n(1-\\/{x_i}{k})$.\n",
        "  Solve for $\\hat{k}$ in\n",
        "  $n\\ln(1-p)-\\sum_{i=1}^n\\ln(1-\\/{x_i}{\\hat{k}})‚â•0$\n",
        "  by numerical method such as Newton-Raphson.\n",
        "\n",
        "- 7.2.11 (normal with 2 unknowns): Let $X_1,...,X_n‚àº\\Normal(Œ∏,œÉ^2)$. Then\n",
        "$L(Œ∏,œÉ^2|\\v{x})=(2œÄœÉ^2)^{-n/2}e^{-\\sum_i(x_i-Œ∏)^2/2œÉ^2}$\n",
        "and\n",
        "$\\ln L(Œ∏,œÉ^2|\\v{x})=-\\/{n}{2}\\ln(2œÄœÉ^2)-\\/{1}{2œÉ^2}\\sum_i(x_i-Œ∏)^2$.\n",
        "Differentiate\n",
        "$\\/{‚àÇ}{‚àÇŒ∏}\\ln L(Œ∏,œÉ^2|\\v{x})=\\/{1}{œÉ^2}\\sum_i(x_i-Œ∏)$\n",
        "$‚áí\\hat{Œ∏}=\\bar{x}$.\n",
        "$\\/{‚àÇ}{‚àÇœÉ^2}\\ln L(Œ∏,œÉ^2|\\v{x})=-\\/{n}{2œÉ^2}+\\/{1}{2œÉ^4}\\sum_i(x_i-Œ∏)^2$\n",
        "$‚áí\\hat{œÉ}^2=\\/{1}{n}\\sum_i(x_i-\\bar{x})^2$.\n",
        "$S^2=\\/{n}{n-1}\\hat{œÉ}^2_{MLE}$.\n",
        "\n",
        "- 7.2.14 (binomial Bayes): Let $X_1,...,X_n‚àº\\t{Bernoulli}(p)$ where $p‚àº\\Beta(a,b)$ prior then $Y‚àº\\Binom(n,p)$.\n",
        "$f(y|p)œÄ(p)=\\binom{n}{p}p^y(1-p)^{n-y}\\/{Œì(a+b)}{Œì(a)Œì(b)}p^{a-1}(1-p)^{b-1}$\n",
        "$=\\binom{n}{p}\\/{Œì(a+b)}{Œì(a)Œì(b)}p^{y+a-1}(1-p)^{n-y+b-1}$.\n",
        "$f(y)=‚à´_0^1f(y|p)œÄ(p)\\ dp$\n",
        "$=\\binom{n}{p}\\/{Œì(a+b)Œì(y+a)Œì(n-y+b)}{Œì(a)Œì(b)Œì(a+b+n)}$.\n",
        "$f(p|y)=\\/{f(y|p)œÄ(p)}{f(y)}$\n",
        "$=\\/{Œì(n+a+b)}{Œì(y+a)Œì(n-y+b)}p^{y+a-1}(1-p)^{n-y+b-1}$.\n",
        "Then $p|y‚àº\\Beta(y+a,n-y+b)$.\n",
        "Then the Bayes estimator\n",
        "$\\hat{p}=\\E[p|y]=\\/{y+a}{a+b+n}$.\n",
        "\n",
        "- 7.2.17 (Multiple Poisson MLE): We observe independent $X_1,...,X_n‚àº\\Pois(t_i)$ and $Y_1,...,Y_n‚àº\\Pois(bt_i)$.\n",
        "$f((x_1,y_1),...,(x_n,y_n)|b,t_1,...,t_n)$\n",
        "$=\\prodl_{i=1}^n\\/{e^{-t_i}t_i^{x_i}}{x_i!}\\/{e^{-bt_i}(bt_i)^{y_i}}{y_i!}$.\n",
        "Create $\\ln(f)$ and strip constants:\n",
        "$l(b,t)=\\suml_{i=1}^n-t_i+x_i\\ln(t_i)-bt_i+y_i\\ln(bt_i)$\n",
        "$=\\suml_{i=1}^ny_i\\ln(b)+(x_i+y_i)\\ln(t_i)-(1+b)t_i$.\n",
        "$\\/{‚àÇl}{‚àÇb}=\\/{\\sum_i y_i}{b}-\\sum_it_i$\n",
        "$‚áí\\red{\\hat{b}}=\\/{\\sum_iy_i}{\\sum_it_i}$.\n",
        "$\\/{‚àÇl}{‚àÇt_i}=\\/{x_i+y_i}{t_i}-(1+b)$\n",
        "$‚áí\\red{\\hat{t}_i}=\\/{x_i+y_i}{1+b}$.\n",
        "$b=\\/{(1+b)\\sum_iy_i}{\\sum_i(x_i+y_i)}$\n",
        "$‚áíb\\sum_i(x_i+y_i)=(b+1)\\sum_iy_i$\n",
        "$‚áíb\\sum_ix_i=\\sum_iy_i$\n",
        "$‚áí\\red{\\hat{b}}=\\/{\\sum_iy_i}{\\sum_ix_i}$.\n",
        "\n",
        "  - $((x_1,y_1),...,(x_n,y_n))$ is complete data and\n",
        "  $f((x_1,y_1),...,(x_n,y_n)|b,t_1,...,t_n)$ is complete-data likelihood.\n",
        "  If $x_1$ was missing,\n",
        "  $\\suml_{x_1=0}^‚àû f((x_1,y_1),...,(x_n,y_n)|b,t_1,...,t_n)$ is incomplete-data likelihood that need maximize for MLE.\n",
        "  \n",
        "  - 7.2.18 (missing data): If $x_1$ is missing, then\n",
        "  $L(b,t_1,...,t_n|y_1,(x_2,y_2),...,(x_n,y_n))$\n",
        "  $=[\\prodl_{i=2}^n\\/{e^{-t_i}t_i^{x_i}}{x_i!}][\\prodl_{i=1}^n\\/{e^{-bt_i}(bt_i)^{y_i}}{y_i!}]$ is the incomplete data likelihood.\n",
        "  $\\red{\\hat{b}}=\\/{\\sum_{i=1}^ny_i}{\\sum_{i=1}^nt_i}$,\n",
        "  $\\red{\\hat{t}_i}=\\/{x_i+y_i}{1+b},\\ i=2..n$,\n",
        "  and\n",
        "  $\\red{\\hat{t}_1}=\\/{y_1}{\\hat{b}}$.\n",
        "  Now $\\hat{b}$ and $\\hat{t}$ reference each other.\n",
        "\n",
        "  - 7.2.19 (EM): Let $(\\v{x},\\v{y})=((x_1,y_1),...,(x_n,y_n))$ and $(\\v{x}_{(-1)},\\v{y})=(y_1,(x_2,y_2),...,(x_n,y_n))$ denote incomplete data.\n",
        "  $\\E[\\ln L(b,t_1,...,t_n|(\\v{x},\\v{y}))|t^{(r)},(\\v{x}_{(-1)},\\v{y})]$\n",
        "  $=\\E\\left[\\ln\\prodl_{i=1}^n\\/{e^{-t_i}t_i^{x_i}}{x_i!}\\/{e^{-bt_i}(bt_i)^{y_i}}{y_i!}|t^{(r)},(\\v{x}_{(-1)},\\v{y})\\right]$\n",
        "  $=\\suml_{x_1=0}^‚àû\\ln\\left(\\prodl_{i=1}^n\\/{e^{-t_i}t_i^{x_i}}{x_i!}\\/{e^{-bt_i}(bt_i)^{y_i}}{y_i!}\\right)\\/{e^{-t_1^{(r)}}(t_1^{(r)})^{x_1}}{x_1!}$\n",
        "  $=\\suml_{x_1=0}^‚àû\\ln\\left(\\prodl_{i=2}^n\\/{e^{-t_i}t_i^{x_i}}{x_i!}\\prodl_{i=1}^n\\/{e^{-bt_i}(bt_i)^{y_i}}{y_i!}\\/{e^{-t_1}t_1^{x_1}}{x_1!}\\right)\\/{e^{-t_1^{(r)}}(t_1^{(r)})^{x_1}}{x_1!}$\n",
        "  $=\\ln\\left(\\prodl_{i=2}^n\\/{e^{-t_i}t_i^{x_i}}{x_i!}\\prodl_{i=1}^n\\/{e^{-bt_i}(bt_i)^{y_i}}{y_i!}\\right)$\n",
        "  $+\\suml_{x_1=0}^‚àû\\ln(\\/{e^{-t_1}t_1^{x_1}}{x_1!})\\/{e^{-t_1^{(r)}}(t_1^{(r)})^{x_1}}{x_1!}$.\n",
        "  Finally,\n",
        "  $\\hat{b}^{(r+1)}=\\/{\\sum_{i=1}^ny_i}{t_1^{(r)}+\\sum_{i=2}^nx_i}$,\n",
        "  $\\hat{t}_1^{(r+1)}=\\/{\\hat{t}_1^{(r)}+y_1}{\\hat{b}^{r+1}+1}$,\n",
        "  and\n",
        "  $\\hat{t}_j^{(r+1)}=\\/{x_j+y+j}{\\hat{b}^{(r+1)}+1},\\ j=2..n$\n",
        "\n",
        "- 7.3.3 ($\\bar{X}$ and $S^2$ MSE): $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$ iid. Find the MSE of $\\bar{X}$ and $S^2$.\n",
        "$\\E(\\bar{X}-Œº)^2=\\Var(\\bar{X})+\\Bias(\\bar{X})^2$\n",
        "$=\\/{œÉ^2}{n}$.\n",
        "$\\E(S^2-œÉ^2)^2=\\Var(S^2)+\\Bias(S^2)^2$.\n",
        "$\\Var\\/{(n-1)S^2}{œÉ^2}=2(n-1)$\n",
        "$‚áí\\Var(S^2)=\\/{2(n-1)œÉ^4}{(n-1)^2}$\n",
        "$=\\/{2œÉ^4}{n-1}$.\n",
        "\n",
        "- 7.3.4 ($\\hat{œÉ}_{MLE}^2$ MSE): $\\hat{œÉ}_{MLE}^2=\\/{n-1}{n}S^2$. Find MSE.\n",
        "$\\E(\\hat{œÉ}^2-œÉ^2)^2=\\Var(\\hat{œÉ}^2)+\\Bias(\\hat{œÉ}^2)^2$.\n",
        "$\\Var(\\hat{œÉ}^2)=\\Var\\/{(n-1)S^2}{n}$\n",
        "$=\\/{2(n-1)œÉ^4}{n^2}$.\n",
        "$\\Bias(\\hat{œÉ}^2)^2=(\\E[\\hat{œÉ}^2]-œÉ^2)^2$\n",
        "$=(\\/{n-1}{n}œÉ^2-œÉ^2)^2$\n",
        "$=\\/{œÉ^4}{n^2}$.\n",
        "$\\E(\\hat{œÉ}^2-œÉ^2)^2=\\/{(2n-1)œÉ^4}{n^2}$.\n",
        "Therefore $\\hat{œÉ}_{MLE}^2$ ($\\/{(2n-1)œÉ^4}{n^2}$) has smaller MSE than $S^2$ ($\\/{2œÉ^4}{n-1}$) by trading away unbiased for smaller variance.\n",
        "\n",
        "- 7.3.5 (MSE of Binomial estimators): $X_1,...,X_n‚àº\\t{Bernoulli}(p)$ iid. Compare MSE of $\\hat{p}_{MLE}$ and Bayes estimator.\n",
        "  - MLE (7.2.7)\n",
        "  $\\hat{p}_{MLE}=\\bar{X}$ where $\\E[\\hat{p}]=p$.\n",
        "  $\\E_p(\\hat{p}-p)^2=\\Var_p(\\bar{X})$\n",
        "  $=\\/{p(1-p)}{n}$.\n",
        "  - Bayes (7.2.14) $Y|p‚àº\\Binom(n,p)$ then\n",
        "  $\\hat{p}_{B}=\\/{Y+a}{a+b+n}$.\n",
        "  $\\E_p[\\hat{p}]=\\/{np+a}{a+b+n}$.\n",
        "  $\\E_p(\\hat{p}-p)^2=\\Var_p(\\/{Y+a}{a+b+n})+(\\E_p[\\/{Y+a}{a+b+n}]-p)^2$\n",
        "  $=\\/{\\Var(Y)}{(a+b+n)^2}+(\\/{np+a}{a+b+n}-p)^2$.\n",
        "  Let $a=b=\\sqrt{n/4}$ then\n",
        "  $\\E_p(\\hat{p}-p)^2=\\/{n}{4(n+\\sqrt{n})^2}$.\n",
        "\n",
        "- 7.3.8/7.3.12 (Poisson unbiased estimation): $X_1,...,X_n‚àº\\Pois(Œª)$ iid. Both $\\E_Œª\\bar{X}=Œª$ and $\\E_ŒªS^2=Œª$ are unbiased estimators. Compare.\n",
        "  - First we find the CRLB:\n",
        "  $f(x_i|Œª)=\\/{e^{-Œª}Œª^{x_i}}{x_i!}$.\n",
        "  $‚àÇ_Œª^2\\ln f(\\v{x}|Œª)$\n",
        "  $=‚àÇ_Œª^2[-nŒª+\\sum_ix_i\\ln Œª]$\n",
        "  $=‚àÇ_Œª[\\/{n\\bar{x}}{Œª}]$\n",
        "  $=-\\/{n\\bar{x}}{Œª^2}$.\n",
        "  $\\Var(W)‚â•\\/{1}{-\\E_Œª[‚àÇ_Œª^2\\ln f(\\v{x}|Œª)]}$\n",
        "  $=\\/{Œª^2}{n\\E_Œª[\\bar{x}]}$\n",
        "  $=\\/{Œª}{n}$.\n",
        "  - $\\Var(\\bar{X})=\\/{Œª}{n}$ which meets CRLB.\n",
        "  $\\Var(S^2)=\\/{Œª}{n}+\\/{2Œª^2}{n-1}>\\Var(\\bar{X})$.\n",
        "\n",
        "- 7.3.13 (Uniform - CR fail): $X_1,...,X_n‚àº\\Unif(0,Œ∏)$ iid with pdf $f(x_i|Œ∏)=\\/{1}{Œ∏},\\ 0< x_i< Œ∏$.\n",
        "  - Find CRLB.\n",
        "  $\\E_Œ∏[(‚àÇ_Œ∏\\ln f(x_i|Œ∏))^2]$\n",
        "  $=\\E_Œ∏[(-‚àÇ_Œ∏ \\ln Œ∏)^2]$\n",
        "  $=\\/{1}{Œ∏^2}$.\n",
        "  Therefore $\\Var(W)‚â•\\/{Œ∏^2}{n}$.\n",
        "  - Find unbiased estimator. Try $X_{(n)}$.\n",
        "  $F(a)=‚à´_0^a\\/{1}{Œ∏}\\ dx=\\/{a}{Œ∏}$.\n",
        "  $F_{X_{(n)}}(x)=(\\/{x}{Œ∏})^n$.\n",
        "  $\\E[X_{(n)}]=‚à´_0^Œ∏x\\/{n}{Œ∏^n}x^{n-1}\\ dx$\n",
        "  $=\\/{n}{n+1}Œ∏$.\n",
        "  Therefore $\\/{n+1}{n}X_{(n)}$ is an unbiased estimator for $Œ∏$.\n",
        "  - Find its variance.\n",
        "  $\\E[X_{(n)}^2]=‚à´_0^Œ∏x^2\\/{n}{Œ∏^n}x^{n-1}\\ dx$\n",
        "  $=\\/{n}{n+2}Œ∏^2$.\n",
        "  $\\Var(X_{(n)})=\\/{n}{n+2}Œ∏^2-\\/{n^2}{(n+1)^2}Œ∏^2$\n",
        "  $=\\/{n(n^2+2n+1)-n^3-2n^2}{(n+2)(n+1)^2}Œ∏^2$\n",
        "  $=\\/{n}{(n+2)(n+1)^2}Œ∏^2$.\n",
        "  $\\Var(\\/{n+1}{n}X_{(n)})=\\/{1}{n(n+2)}Œ∏^2$.\n",
        "  $X_{(n)}$ has lower variance than CRLB.\n",
        "  - How did CR fail? Leibniz's rule\n",
        "  $\\/{d}{dŒ∏}\\E[W(x)]$\n",
        "  $=\\/{d}{dŒ∏}‚à´_0^Œ∏W(x)f(x|Œ∏)\\ dx$\n",
        "  $=\\/{d}{dŒ∏}‚à´_0^Œ∏W(x)\\/{1}{Œ∏}\\ dx$\n",
        "  $=\\/{W(Œ∏)}{Œ∏}+‚à´_0^Œ∏\\/{‚àÇ}{‚àÇŒ∏}[W(x)\\/{1}{Œ∏}]\\ dx$\n",
        "  $\\neq‚à´_0^Œ∏\\/{‚àÇ}{‚àÇŒ∏}[W(x)\\/{1}{Œ∏}]\\ dx$\n",
        "  as required by Cramer-Rao.\n",
        "  The $Œ∏$ in the limits of integration created an additional linear term under Leibniz's rule.\n",
        "  Therefore we would still have no idea if $\\/{n+1}{n}X_{(n)}$ was UMVUE for $Œ∏$.\n",
        "  - 7.3.22 (6.2.23 completeness):\n",
        "  $f_{X_{(n)}}(x)=\\/{n}{Œ∏^n}x^{n-1}$.\n",
        "  Let $Y=X_{(n)}$ and $W=\\/{n+1}{n}Y$.\n",
        "  $Y$ is complete if $\\E[g(Y)]=‚à´_0^Œ∏\\/{n}{Œ∏^n}g(y)y^{n-1}\\ dy=0$ implies $g(Y)=0$ as.\n",
        "  Clean up the constants and let $h(Œ∏)=‚à´_0^Œ∏g(y)y^{n-1}\\ dy$.\n",
        "  If $\\E[g(Y)]=0$ then so are\n",
        "  $h(Œ∏)=0$ and $\\/{d}{dŒ∏}h(Œ∏)=0$.\n",
        "  We have\n",
        "  $\\/{d}{dŒ∏}‚à´_0^Œ∏g(y)y^{n-1}\\ dy$\n",
        "  $=\\ob{g(Œ∏)Œ∏^{n-1}}{\\t{Leibniz rule}}+‚à´_0^Œ∏\\/{‚àÇ}{‚àÇŒ∏}\\ob{[g(y)y^{n-1}]}{\\t{constant in $Œ∏$}}\\ dy$\n",
        "  $=g(Œ∏)Œ∏^{n-1}=0$.\n",
        "  Therefore $g(Œ∏)=0$ for all $Œ∏>0$ and $g(Y)=0$ for all $0< Y< Œ∏$. Therefore $Y$ is complete.\n",
        "  $W$ is a function of complete and sufficient $Y$, therefore $W$ is unique UMVUE for $Œ∏$.\n",
        "\n",
        "- 7.3.14 (Normal $S^2$): $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$ iid. Find estimator for $œÉ^2$.\n",
        "$-\\E[‚àÇ_{œÉ^2}^2\\ln f(x_i|Œ∏)]$\n",
        "$=-\\E[‚àÇ_{œÉ^2}^2(-\\ln œÉ-\\/{(x_i-Œº)^2}{2œÉ^2})]$\n",
        "$=\\E[‚àÇ_v^2(\\ln \\sqrt{v}+\\/{(x_i-Œº)^2}{2v})]$\n",
        "$=\\E[‚àÇ_v(\\/{1}{2v}-\\/{(x_i-Œº)^2}{2v^2})]$\n",
        "$=\\E[-\\/{1}{2v^2}+\\/{(x_i-Œº)^2}{v^3}]$\n",
        "$=\\E_{Œº,œÉ^2}[-\\/{1}{2œÉ^4}+\\/{(x_i-Œº)^2}{œÉ^6}]$\n",
        "$=-\\/{1}{2œÉ^4}+\\/{\\E_{Œº,œÉ^2}[(x_i-Œº)^2]}{œÉ^6}$\n",
        "$=\\/{1}{2œÉ^4}$.\n",
        "CR $\\Var_{Œº,œÉ^2}(W)‚â•\\/{2œÉ^4}{n}$.\n",
        "However, $\\Var_{Œº,œÉ^2}(S^2)=\\/{2œÉ^4}{n-1}$ in 7.3.3, which failed to attain CRLB. We still have no idea if $S^2$ was indeed the UMVUE for $œÉ^2$.\n",
        "  - 7.3.16: Why did $S^2$ fail attainment?\n",
        "  Let $W$ be unbiased estimator for $œÉ^2$ that achieves attainment, then\n",
        "  $c(œÉ^2)(W-œÉ^2)$\n",
        "  $=‚àÇ_{œÉ^2}\\ln f(\\v{x}|Œ∏)$\n",
        "  $=‚àÇ_{œÉ^2}(-n\\ln œÉ-\\/{\\sum_i(x_i-Œº)^2}{2œÉ^2})$\n",
        "  $=‚àÇ_v(-\\/{n}{2}\\ln v-\\/{\\sum_i(x_i-Œº)^2}{2v})$\n",
        "  $=-\\/{n}{2œÉ^2}+\\/{\\sum_i(x_i-Œº)^2}{2œÉ^4}$\n",
        "  $=\\/{n}{2œÉ^4}(\\/{\\sum_i(x_i-Œº)^2}{n}-œÉ^2)$.\n",
        "  Therefore $W=\\/{\\sum_i(x_i-Œº)^2}{n}$, which requires $Œº$ and is not a valid estimator.\n",
        "\n",
        "- 7.3.18 (insufficient statistic): Let $X_1,X_2‚àº\\Normal(Œ∏,1)$ iid. Statistic $\\bar{X}=\\/{1}{2}(X_1+X_2)$ has $E_Œ∏\\bar{X}=Œ∏$ and $\\Var_Œ∏\\bar{X}=\\/{1}{2}$. Condition on $X_1$.\n",
        "$œï(X_1)=\\E[\\bar{X}|X_1]$\n",
        "$=\\/{1}{2}(X_1+\\E[X_2])$\n",
        "$=\\/{1}{2}(X_1+Œ∏)$.\n",
        "Dependence on $Œ∏$, therefore $œï(X_1)$ is not an estimator.\n",
        "\n",
        "- 7.3.21 (unbiased estimator of 0): Let $X‚àº\\Unif(Œ∏,Œ∏+1)$ be one observation. Then $\\E_Œ∏[X]=Œ∏+\\/{1}{2}$ and $\\Var_Œ∏(X)=\\/{1}{12}$.\n",
        "$X-\\/{1}{2}$ is an unbiased estimator of $Œ∏$. Let $h(X)=\\sin(2œÄX)$ then $\\E_Œ∏[h(X)]=‚à´_Œ∏^{Œ∏+1}\\sin(2œÄx)\\ dx=0$ and $h(X)$ is an unbiased estimator of $0$.\n",
        "Then\n",
        "$\\Cov_Œ∏(X-\\/{1}{2},\\sin(2œÄX))$\n",
        "$=\\Cov_Œ∏(X,\\sin(2œÄX))$\n",
        "$=\\E_Œ∏[X\\sin(2œÄX)]-\\E_Œ∏[X]\\E_Œ∏[\\sin(2œÄx)]$\n",
        "$=‚à´_Œ∏^{Œ∏+1}x\\sin(2œÄx)\\ dx$\n",
        "$=[-\\/{x\\cos(2œÄx)}{2œÄ}]_Œ∏^{Œ∏+1}$\n",
        "$+‚à´_Œ∏^{Œ∏+1}\\/{\\cos(2œÄx)}{2œÄ}\\ dx$\n",
        "$=-\\/{(Œ∏+1)\\cos(2œÄŒ∏+2œÄ)}{2œÄ}$\n",
        "$+\\/{Œ∏\\cos(2œÄŒ∏)}{2œÄ}$\n",
        "$+0$\n",
        "$=\\/{\\cos(2œÄŒ∏)}{2œÄ}$.\n",
        "Correlation $\\neq$ 0 therefore not UMVUE.\n",
        "\n",
        "- 7.3.24: Let $X_1,...,X_n‚àº\\Binom(k,Œ∏)$ iid. Find UMVUE for $œÑ(Œ∏)=kŒ∏(1-Œ∏)^{k-1}$.\n",
        "$f(x_i|Œ∏)=\\binom{k}{x_i}Œ∏^{x_i}(1-Œ∏)^{k-x_i}$\n",
        "$=\\binom{k}{x_i}(1-Œ∏)^k(\\/{Œ∏}{1-Œ∏})^{x_i}$\n",
        "$=\\binom{k}{x_i}(1-Œ∏)^k\\e{x_i\\ln\\/{Œ∏}{1-Œ∏}}$.\n",
        "Then\n",
        "$S=\\sum_iX_i$ is a sufficient and complete statistic for $Œ∏$.\n",
        "Let $h(X_i)=\\BC\n",
        "1 &X_i=1\\\\\n",
        "0 &\\t{otherwise}\n",
        "\\EC$, then\n",
        "$\\E_Œ∏[h(X_i)]=\\sum_{x_i=1}^kh(x_i)f(x_i|Œ∏)$\n",
        "$=f(1)=kŒ∏(1-Œ∏)^{k-1}$.\n",
        "Therefore UMVUE for $œÑ(Œ∏)$ is\n",
        "$œï(S)=\\E[h(X_1)|S]$\n",
        "$=P(X_1=1|\\sum_{i=1}^nX_i=S)$\n",
        "$=\\/{P_Œ∏(X_1=1,\\sum_{i=1}^nX_i=S)}{P_Œ∏(\\sum_{i=1}^nX_i=S)}$\n",
        "$=\\/{P_Œ∏(X_1=1)P_Œ∏(\\sum_{i=2}^nX_i=S-1)}{P_Œ∏(\\sum_{i=1}^nX_i=S)}$\n",
        "$=\\/{kŒ∏(1-Œ∏)^{k-1}\\binom{k(n-1)}{S-1}Œ∏^{S-1}(1-Œ∏)^{k(n-1)-(S-1)}}{\\binom{nk}{S}Œ∏^S(1-Œ∏)^{nk-S}}$\n",
        "$=\\/{k\\binom{k(n-1)}{S-1}}{\\binom{nk}{S}}$,\n",
        "which is independent of $Œ∏$ and is a valid estimator.\n",
        "\n",
        "- 7.3.26 (risk function of $Œ¥_b=bS^2$): $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$ iid. Consider estimators $Œ¥_b=bS^2,\\ b>0$. Recall $\\Var(S^2)=\\/{2œÉ^4}{n-1}$.\n",
        "Risk function for $Œ¥_b$ is\n",
        "$R((Œº,œÉ^2),Œ¥_b)$\n",
        "$=\\Var(bS^2)+(\\E[bS^2]-œÉ^2)^2$\n",
        "$=\\/{2b^2œÉ^4}{n-1}+(b-1)^2œÉ^4$\n",
        "$=(\\/{2b^2}{n-1}+(b-1)^2)œÉ^4$,\n",
        "which has minimum value with $b=\\/{n-1}{n+1}$\n",
        "and $\\tilde{S}^2=\\/{n-1}{n+1}S^2$.\n",
        "\n",
        "- 7.2: Let $X_1,...,X_n‚àº\\Gamma(a,b)$. Find $\\hat{b}_{MLE}$ assuming $a$ is known.\n",
        "$f(x_i|b)=\\/{e^{-x_i/b}(x_i/b)^a}{x_iŒì(a)}$.\n",
        "$L(b|\\v{x})=\\/{e^{-\\sum_ix_i/b}\\prod_ix_i^a/b^{na}}{\\prod_ix_iŒì(a)^n}$.\n",
        "$l(b|\\v{x})=-\\/{1}{b}\\sum_ix_i+a\\sum_i\\ln(x_i)-na\\ln(b)$.\n",
        "$\\/{‚àÇl}{‚àÇb}=\\/{\\sum_ix_i}{b^2}-\\/{na}{b}$\n",
        "$‚áí\\hat{b}=\\/{\\sum_ix_i}{na}$.\n",
        "$\\/{‚àÇ^2l}{‚àÇb^2}|_{b=\\hat{b}}=[\\/{na}{b^2}-\\/{2\\sum_ix_i}{b^3}]_{b=\\hat{b}}< 0$.\n",
        "\n",
        "- 7.6: Let $X_1,...,X_n$ iid from $f(x_i|Œ∏)=Œ∏x_i^{-2},\\ 0< Œ∏‚â§x_i$.\n",
        "  - Find sufficient statistic.\n",
        "  $f(\\v{x}|Œ∏)=Œ∏^n\\prod_ix_i^{-2}I_{X_{(1)}‚â•Œ∏}$.\n",
        "  Sufficient statistic $T=X_{(1)}$.\n",
        "  - Find the MLE of $Œ∏$.\n",
        "  $l(Œ∏|\\v{x})=n\\lnŒ∏$ is strictly increasing with $Œ∏$.\n",
        "  Therefore $\\hat{Œ∏}=X_{(1)}$.\n",
        "  - Find method of moments estimator for $Œ∏$.\n",
        "  $\\E[X]=Œ∏‚à´_Œ∏^‚àûx^{-1}\\ dx$\n",
        "  $=Œ∏[\\ln x]_Œ∏^‚àû$\n",
        "  $=‚àû$.\n",
        "  Therefore method of moments estimator does not exist.\n",
        "\n",
        "- 7.7: $X_1,...,X_n$ iid from $f(x_i|Œ∏)=\\BC\n",
        "I_{0< x_i< 1}&Œ∏=0 \\\\\n",
        "\\/{1}{2\\sqrt{x_i}}I_{0< x_i< 1}&Œ∏=1\\EC$.\n",
        "Find MLE of $Œ∏$.\n",
        "$L(Œ∏|\\v{x})=\\BC\n",
        "I_{0< x_{(1)}, x_{(n)}< 1}&Œ∏=0\\\\\n",
        "2^{-n}\\prod_ix_i^{-1/2}I_{0< x_{(1)}, x_{(n)}< 1}&Œ∏=1\n",
        "\\EC$.\n",
        "Then $\\hat{Œ∏}=\\BC\n",
        "0&1>2^{-n}\\prod_ix_i^{-1/2} \\\\\n",
        "1&1‚â§2^{-n}\\prod_ix_i^{-1/2}\n",
        "\\EC$.\n",
        "\n",
        "- 7.8: One observation $X$ is taken from $\\Normal(0,œÉ^2)$.\n",
        "  - Find an unbiased estimator for $œÉ^2$.\n",
        "  $œÉ^2=\\E[X^2]-\\E[X]‚áí\\E[X^2]=œÉ^2$.\n",
        "  - Find MLE of $œÉ$.\n",
        "  $L(œÉ|x)=\\/{1}{\\sqrt{2œÄ}œÉ}e^{-x^2/2œÉ^2}$.\n",
        "  $l(œÉ)=-\\ln(œÉ)-\\/{x^2}{2œÉ^2}$.\n",
        "  $\\/{‚àÇl}{‚àÇœÉ}=\\/{-1}{œÉ}+\\/{x^2}{œÉ^3}$.\n",
        "  $\\hat{œÉ}=|x|$.\n",
        "  $\\/{‚àÇ^2l}{‚àÇœÉ^2}|_{œÉ=\\hat{œÉ}}=[\\/{1}{œÉ^2}-\\/{3x^2}{œÉ^4}]_{œÉ=\\hat{œÉ}}< 0$.\n",
        "\n",
        "- 7.9: Let $X_1,...,X_n$ iid from $f(x_i|Œ∏)=\\/{1}{Œ∏}I_{0‚â§x_i‚â§Œ∏}$.\n",
        "  - Method of moments estimator:\n",
        "  $\\E[X_i]=\\/{1}{Œ∏}‚à´_0^Œ∏x\\ dx=\\/{Œ∏}{2}$\n",
        "  $‚áí\\tilde{Œ∏}=2\\bar{X}$.\n",
        "  $\\E[\\tilde{Œ∏}]=2\\E[X_i]=Œ∏$ and\n",
        "  $\\Var(\\tilde{Œ∏})=\\/{4\\Var(X_i)}{n}=\\/{Œ∏^2}{3n}$.\n",
        "  - MLE:\n",
        "  $L(Œ∏|\\v{x})=Œ∏^{-n}I_{X_{(n)}‚â§Œ∏}$ is strictly decreasing with $Œ∏$. Therefore $\\hat{Œ∏}=X_{(n)}$.\n",
        "  $f_{X_{(n)}}(x)=nx^{n-1}\\/{1}{Œ∏^n}I_{0‚â§x‚â§Œ∏}$.\n",
        "  $\\E[X_{(n)}]=\\/{n}{Œ∏^n}‚à´_0^Œ∏x^n\\ dx$\n",
        "  $=\\/{n}{n+1}Œ∏$.\n",
        "  $\\E[X_{(n)}^2]=\\/{n}{Œ∏^n}‚à´_0^Œ∏x^{n+1}\\ dx$\n",
        "  $=\\/{n}{n+2}Œ∏^2$.\n",
        "  $\\Var(X_{(n)})=Œ∏^2(\\/{n}{n+2}-\\/{n^2}{(n+1)^2})$\n",
        "  $=\\/{nŒ∏^2}{(n+2)(n+1)^2}$.\n",
        "  MLE estimator for $Œ∏$ is biased, but has much small variance.\n",
        "\n",
        "- 7.11: $X_1,...,X_n$ iid from $f(x_i|Œ∏)=Œ∏x_i^{Œ∏-1}$.\n",
        "  - Find MLE of $Œ∏$. $L(Œ∏|\\v{x})=Œ∏^n\\e{(Œ∏-1)\\sum_i\\ln(x_i)}$.\n",
        "  $l(Œ∏)=n\\ln(Œ∏)+(Œ∏-1)\\sum_i\\ln(x_i)$.\n",
        "  $\\/{‚àÇl}{‚àÇŒ∏}=\\/{n}{Œ∏}+\\sum_i\\ln(x_i)$\n",
        "  $‚áí\\hat{Œ∏}=\\/{-n}{\\sum_i\\ln(x_i)}$.\n",
        "  $\\/{‚àÇ^2l}{‚àÇŒ∏^2}=\\/{-n}{Œ∏^2}< 0$.\n",
        "  - Show $\\Var(\\hat{Œ∏})‚Üí0$ as $n‚Üí‚àû$.\n",
        "  $f(x_i|Œ∏)=Œ∏e^{(Œ∏-1)\\ln x_i}$.\n",
        "  Let $Y_i=-\\ln X_i$ then\n",
        "  $|\\/{‚àÇx_i}{‚àÇy_i}|=e^{-y_i}$ and\n",
        "  $f(y_i|Œ∏)=Œ∏e^{-y_i}e^{-(Œ∏-1)y_i}$\n",
        "  $=Œ∏e^{-Œ∏y_i}$.\n",
        "  $\\sum_iY_i‚àº\\Gamma(n,1/Œ∏)$\n",
        "  $‚áí\\hat{Œ∏}=\\/{n}{\\sum_iy_i}$.\n",
        "  $\\E(\\hat{Œ∏}^2)=\\/{n^2(1/Œ∏)^{-2}Œì(n-2)}{Œì(n)}$\n",
        "  $=\\/{n^2Œ∏^2}{(n-1)(n-2)}$.\n",
        "  $\\E(\\hat{Œ∏})=\\/{n(1/Œ∏)^{-1}Œì(n-1)}{Œì(n)}$\n",
        "  $=\\/{nŒ∏}{n-1}$.\n",
        "  $\\Var(\\hat{Œ∏})=\\/{n^2Œ∏^2[(n-1)-(n-2)]}{(n-1)^2(n-2)}$\n",
        "  $=\\/{n^2Œ∏^2}{(n-1)^2(n-2)}$\n",
        "  $‚Üí0$ as $n‚Üí‚àû$.\n",
        "  - Find the method of moments estimator for $Œ∏$.\n",
        "  $\\E[X]=Œ∏‚à´_0^1x^Œ∏\\ dx$\n",
        "  $=\\/{Œ∏}{Œ∏+1}$\n",
        "  $=\\bar{X}$\n",
        "  $‚áí\\tilde{Œ∏}=\\/{\\bar{X}}{1-\\bar{X}}$.\n",
        "\n",
        "- 7.12: $X_1,...,X_n$ iid from $P_Œ∏(X=x)=Œ∏^x(1-Œ∏)^{1-x}$ where $0‚â§Œ∏‚â§\\/{1}{2}$.\n",
        "$\\E[X]=\\tilde{Œ∏}=\\bar{X}$.\n",
        "$L(Œ∏|\\v{x})=Œ∏^{\\sum_ix_i}(1-Œ∏)^{n-\\sum_ix_i}$.\n",
        "$l(Œ∏)=\\sum_ix_i\\ln(Œ∏)+(n-\\sum_ix_i)\\ln(1-Œ∏)$.\n",
        "$\\/{‚àÇl}{‚àÇŒ∏}=\\/{\\sum_ix_i}{Œ∏}-\\/{n-\\sum_ix_i}{1-Œ∏}$.\n",
        "$\\/{‚àÇ^2l}{‚àÇŒ∏^2}=-\\/{\\sum_ix_i}{Œ∏^2}-\\/{n-\\sum_ix_i}{(1-Œ∏)^2}< 0$.\n",
        "$\\sum_ix_i-\\sum_ix_iŒ∏=nŒ∏-\\sum_ix_iŒ∏$\n",
        "$‚áí\\hat{Œ∏}=\\bar{X}$.\n",
        "Because of $Œ∏$ restrictions,\n",
        "$\\hat{Œ∏}=\\min(\\bar{X},\\/{1}{2})$.\n",
        "\n",
        "- 7.14 (4.26): $f(x|Œª)=\\/{1}{Œª}e^{-x/Œª}$ and $f(y|Œº)=\\/{1}{Œº}e^{-y/Œº}$ are independent. We observe $Z=\\min(X,Y)$ and $W=I_{Z=X}$.\n",
        "  - Find joint distribution of $Z$ and $W$.\n",
        "  $P(Z‚â§a,W=0)=‚à´_{y=0}^af(y)‚à´_{x=y}^‚àûf(x)\\ dx\\ dy$\n",
        "  $=‚à´_{y=0}^a\\/{1}{Œº}e^{-y/Œº}e^{-y/Œª}\\ dy$\n",
        "  $=\\/{1/Œº}{1/Œº+1/Œª}(1-e^{-a(1/Œº+1/Œª)})$.\n",
        "  $f(z_i,0)=\\/{1}{Œº}e^{-z_i(1/Œº+1/Œª)}$.\n",
        "  $f(z_i,1)=\\/{1}{Œª}e^{-z_i(1/Œº+1/Œª)}$.\n",
        "  $f(z_i,w_i)=Œº^{-(1-w_i)}Œª^{-w_i}e^{-z_i(1/Œº+1/Œª)}$.\n",
        "  - Let $(Z_i,W_i),\\ i=1..n$ iid. Find MLE of $Œª$ and $Œº$.\n",
        "  $L(Œª,Œº|\\v{z},\\v{w})=Œº^{-n+\\sum_iw_i}Œª^{-\\sum_iw_i}e^{-(1/Œº+1/Œª)\\sum_iz_i}$.\n",
        "  $l(Œª,Œº)=(-n+\\sum_iw_i)\\ln(Œº)-\\sum_iw_i\\ln(Œª)-(1/Œº+1/Œª)\\sum_iz_i$.\n",
        "  $\\/{‚àÇl}{‚àÇŒª}=-\\/{\\sum_iw_i}{Œª}+\\/{\\sum_iz_i}{Œª^2}$\n",
        "  $‚áí\\hat{Œª}=\\/{\\sum_iz_i}{\\sum_iw_i}$.\n",
        "  $\\/{‚àÇl}{‚àÇŒº}=-\\/{n-\\sum_iw_i}{Œº}+\\/{\\sum_iz_i}{Œª^2}$\n",
        "  $‚áí\\hat{Œº}=\\/{\\sum_iz_i}{n-\\sum_iw_i}$.\n",
        "\n",
        "- 7.19: Suppose $Y_1,...,Y_n$ satisfy $Y_i=Œ≤x_i+œµ_i$, $x_i$ fixed constants, $œµ_i‚àº\\Normal(0,œÉ^2)$, and $Œ≤$ and $œÉ$ unknown.\n",
        "  - Find sufficient statistic for $(Œ≤,œÉ^2)$.\n",
        "  $f(y_i|Œ≤,œÉ^2)=\\/{1}{\\sqrt{2œÄ}œÉ}e^{-(y_i-Œ≤x_i)^2/2œÉ^2}$\n",
        "  $=\\/{1}{\\sqrt{2œÄ}œÉ}\\e{-\\/{y_i^2}{2œÉ^2}+\\/{2y_iŒ≤x_i}{2œÉ^2}-\\/{Œ≤^2x_i^2}{2œÉ^2}}$.\n",
        "  Therefore $T=(\\sum_iY_i^2,\\sum_ix_iY_i)$ are sufficient.\n",
        "  - Find MLE of $Œ≤$.\n",
        "  $l(Œ≤)=\\/{\\sum_ix_iy_i}{œÉ^2}Œ≤-\\/{\\sum_ix_i^2}{2œÉ^2}Œ≤^2$.\n",
        "  $\\/{‚àÇl}{‚àÇŒ≤}=\\/{\\sum_ix_iy_i}{œÉ^2}-\\/{\\sum_ix_i^2}{œÉ^2}Œ≤$.\n",
        "  $\\/{‚àÇ^2l}{‚àÇŒ≤^2}=-\\/{\\sum_ix_i^2}{œÉ^2}< 0$.\n",
        "  $‚áí\\hat{Œ≤}=\\/{\\sum_ix_iy_i}{\\sum_ix_i^2}$.\n",
        "  $\\E[\\hat{Œ≤}]=\\/{\\sum_ix_i\\E[y_i]}{\\sum_ix_i^2}$\n",
        "  $=\\/{\\sum_ix_i^2Œ≤}{\\sum_ix_i^2}$\n",
        "  $=Œ≤$.\n",
        "  - Find distribution of $\\hat{Œ≤}$.\n",
        "  $\\hat{Œ≤}=\\/{\\sum_ix_i(Œ≤x_i+œµ_i)}{\\sum_ix_i^2}$\n",
        "  $=Œ≤+\\/{\\sum_ix_iœµ_i}{\\sum_ix_i^2}$.\n",
        "  $x_iœµ_i‚àº\\Normal(0,x_i^2œÉ^2)$.\n",
        "  $\\sum_ix_iœµ_i‚àº\\Normal(0,\\sum_ix_i^2œÉ^2)$.\n",
        "  $\\/{\\sum_ix_iœµ_i}{\\sum_ix_i^2}‚àº\\Normal(0,œÉ^2\\/{\\sum_ix_i^2}{(\\sum_ix_i^2)^2})$.\n",
        "  $\\hat{Œ≤}‚àº\\Normal(Œ≤,\\/{œÉ^2}{\\sum_ix_i^2})$.\n",
        "\n",
        "- 7.20: Same $Y_1,...,Y_n$ as 7.19.\n",
        "  - Show $\\sum_iY_i/\\sum_iX_i$ is unbiased estimator of $Œ≤$.\n",
        "  $\\E[\\/{\\sum_iy_i}{\\sum_ix_i}]$\n",
        "  $=\\/{\\sum_i\\E[y_i]}{\\sum_ix_i}$\n",
        "  $=\\/{\\sum_ix_iŒ≤}{\\sum_ix_i}$\n",
        "  $=Œ≤$.\n",
        "  - Calculate the variance of $\\sum_iY_i/\\sum_iX_i$.\n",
        "  $\\Var(\\/{\\sum_iy_i}{\\sum_ix_i})$\n",
        "  $=\\/{\\sum_i\\Var(y_i)}{(\\sum_ix_i)^2}$\n",
        "  $=\\/{nœÉ^2}{n^2\\bar{x}^2}$\n",
        "  $=\\/{œÉ^2}{n\\bar{x}^2}$.\n",
        "\n",
        "- 7.22 (Normal-normal conjugacy): Let $X_1,...,X_n‚àº\\Normal(Œ∏,œÉ^2)$ iid. $Œ∏‚àº\\Normal(Œº,œÑ^2)$, while $œÉ^2$, $Œº$, $œÑ^2$ are all known.\n",
        "  - Find joint PDF of $\\bar{X}$ and $Œ∏$.\n",
        "  $\\bar{X}‚àº\\Normal(Œ∏,\\/{œÉ^2}{n})$.\n",
        "  $f(\\bar{x}|Œ∏)=\\/{1}{\\sqrt{2œÄœÉ^2/n}}\\e{-\\/{(\\bar{x}-Œ∏)^2}{2œÉ^2/n}}$.\n",
        "  $f(\\bar{x},Œ∏)=\\/{1}{2œÄœÉœÑ/\\sqrt{n}}\\e{-\\/{(\\bar{x}-Œ∏)^2}{2œÉ^2/n}-\\/{(Œ∏-Œº)^2}{2œÑ^2}}$.\n",
        "  - Find marginal distribution $m(\\bar{X})$.\n",
        "  $\\bar{X}‚àº\\Normal(Œº,œÑ^2)+\\Normal(0,\\/{œÉ^2}{n})$\n",
        "  where the 2 normals are independent.\n",
        "  Therefore $\\bar{X}‚àº\\Normal(Œº,\\/{œÉ^2}{n}+œÑ^2)$.\n",
        "  - Find posterior distribution $œÄ(Œ∏|\\bar{x})$.\n",
        "  $œÄ(Œ∏|\\bar{x})=\\/{f(\\bar{x},Œ∏)}{m(\\bar{x})}$\n",
        "  $‚àùf(\\bar{x},Œ∏)$\n",
        "  is normally distributed by conjugacy.\n",
        "  We strip away constants to find $(Œ∏-m)^2/2s^2$.\n",
        "  The denominator $m(\\bar{x})$ is treated as constant.\n",
        "  Let $a=\\/{n}{œÉ^2}$ and $b=\\/{1}{œÑ^2}$,\n",
        "  then the exponent\n",
        "  $-\\/{(\\bar{x}-Œ∏)^2}{2œÉ^2/n}-\\/{(Œ∏-Œº)^2}{2œÑ^2}$\n",
        "  $=-\\/{1}{2}(a(Œ∏-\\bar{x})^2+b(Œ∏-Œº)^2)$.\n",
        "  Strip and combine into $(Œ∏-m)^2$:\n",
        "  $a(Œ∏-\\bar{x})^2+b(Œ∏-Œº)^2$\n",
        "  $=aŒ∏^2-2aŒ∏\\bar{x}+a\\bar{x}^2+bŒ∏^2-2bŒ∏Œº+bŒº^2$\n",
        "  $=(a+b)Œ∏^2-2(a\\bar{x}+bŒº)Œ∏+a\\bar{x}^2+bŒº^2$\n",
        "  $=(a+b)[Œ∏^2-2\\/{a\\bar{x}+bŒº}{a+b}Œ∏+(\\/{a\\bar{x}+bŒº}{a+b})^2]-\\/{(a\\bar{x}+bŒº)^2}{a+b}+a\\bar{x}^2+bŒº^2$.\n",
        "  Clean away constants to get the exponent:\n",
        "  $\\e{-\\/{a+b}{2}(Œ∏-\\/{a\\bar{x}+bŒº}{a+b})^2}$.\n",
        "  Therefore $Œ∏|\\bar{x}‚àº\\Normal(\\/{a\\bar{x}+bŒº}{a+b}, \\/{1}{a+b})$.\n",
        "\n",
        "- 7.24 (4.32 Poisson-Gamma conjugacy): $X_1,...,X_n‚àº\\Pois(Œª)$ iid where $Œª‚àº\\Gamma(a,b)$.\n",
        "  - Find posterior distribution of $Œª$.\n",
        "  $f(\\v{x}|Œª)œÄ(Œª)=(\\prod_i\\/{e^{-Œª}Œª^{x_i}}{x_i!})\\/{e^{-Œª/b}(Œª/b)^a}{ŒªŒì(a)}$\n",
        "  $=\\/{e^{-nŒª}Œª^{\\sum_ix_i}}{\\prod_ix_i!}\\/{e^{-Œª/b}(Œª/b)^a}{ŒªŒì(a)}$\n",
        "  $‚áíœÄ(Œª|\\v{x})‚àù\\/{e^{Œª(n+1/b)}Œª^{a+\\sum_ix_i}}{Œª}$.\n",
        "  Therefore\n",
        "  $Œª|\\v{x}‚àº\\Gamma(a+n\\bar{x}, \\/{b}{nb+1})$.\n",
        "  - Find posterior mean and variance.\n",
        "  $\\E[Œª|\\v{x}]=\\/{(a+n\\bar{x})b}{nb+1}$ and\n",
        "  $\\Var(Œª|\\v{x})=\\/{(a+n\\bar{x})b^2}{(nb+1)^2}$.\n",
        "\n",
        "- 7.37: $X_1,...,X_n$ from $f(x_i|Œ∏)=\\/{1}{2Œ∏},\\ -Œ∏< x_i< Œ∏$. Find UMVUE.\n",
        "$f(\\v{x}|Œ∏)=\\/{1}{(2Œ∏)^n}I_{Œ∏>|X|_{(n)}}$.\n",
        "Let $Y_i=|X_i|$, then\n",
        "$f(y_i|Œ∏)=\\/{1}{Œ∏},\\ 0‚â§ y_i< Œ∏$\n",
        "and\n",
        "$f_{Y_{(n)}}(y)=\\/{n}{Œ∏^n}y^{n-1}$, which is sufficient and complete by 7.3.22.\n",
        "$\\E_Œ∏[Y_{(n)}]=\\/{n}{n+1}Œ∏$.\n",
        "Therefore $\\/{n+1}{n}|X|_{(n)}$ is UMVUE.\n",
        "\n",
        "- 7.38: Find unbiased estimator for $g(Œ∏)$ that achieves CRLB.\n",
        "  - $f(x_i|Œ∏)=Œ∏x_i^{Œ∏-1},\\ 0< x_i< 1$.\n",
        "  Fisher information $‚àÇ_Œ∏^2\\ln f(x_i|Œ∏)$\n",
        "  $=‚àÇ_Œ∏^2[\\lnŒ∏+(Œ∏-1)\\ln x_i]=‚àÇ_Œ∏[\\/{1}{Œ∏}+\\ln x_i]$\n",
        "  $=\\/{-1}{Œ∏^2}$.\n",
        "  CRLB $\\Var(W)‚â•\\/{(g'(Œ∏))^2}{-n\\E[‚àÇ_Œ∏^2\\ln f(x_i|Œ∏)]}$\n",
        "  $=\\/{Œ∏^2(g'(Œ∏))^2}{n}$\n",
        "  Attainment is possible if\n",
        "  $\\blue{a[W-g(Œ∏)]}=‚àÇ_Œ∏\\ln f(\\v{x}|Œ∏)$\n",
        "  $=‚àÇ_Œ∏[n\\lnŒ∏+(Œ∏-1)\\sum_i\\ln x_i]$\n",
        "  $=\\/{n}{Œ∏}+\\sum_i\\ln x_i$\n",
        "  $=\\blue{n(\\overline{\\ln X}-\\/{-1}{Œ∏})}$.\n",
        "  Confirm $W=\\overline{\\ln X}$ as unbiased for $g(Œ∏)=-\\/{1}{Œ∏}$, where CRLB $\\Var(W)‚â•\\/{Œ∏^2(1/Œ∏^4)}{n}$\n",
        "  $=\\/{1}{nŒ∏^2}$.\n",
        "  Let $Y_i=-\\ln X_i$, then\n",
        "  $f(y_i|Œ∏)=e^{-y_i}Œ∏e^{-(Œ∏-1)y_i}$\n",
        "  $=Œ∏e^{-Œ∏y_i}$\n",
        "  $‚àº\\Expo(\\/{1}{Œ∏})$.\n",
        "  $\\E[W]=\\E[-Y_1]$\n",
        "  $=-\\/{1}{Œ∏}$.\n",
        "  $\\Var(W)=\\/{\\Var(-Y_1)}{n}=\\/{1}{nŒ∏^2}$.\n",
        "  - $f(x_i|Œ∏)=\\/{\\ln Œ∏}{Œ∏-1}Œ∏^{x_i},\\ 0< x< 1$.\n",
        "  Score $‚àÇ_Œ∏\\ln f(x_i|Œ∏)$\n",
        "  $=‚àÇ_Œ∏[\\ln(\\lnŒ∏)-\\ln(Œ∏-1)+x_i\\lnŒ∏]$\n",
        "  $=\\/{1/Œ∏}{\\lnŒ∏}-\\/{1}{Œ∏-1}+\\/{x_i}{Œ∏}$\n",
        "  $=\\/{1}{Œ∏}[\\/{1}{\\lnŒ∏}-\\/{Œ∏}{Œ∏-1}+x_i]$.\n",
        "  Therefore $W=n\\bar{X}$ attains CRLB and is unbiased for $n[\\/{Œ∏}{Œ∏-1}-\\/{1}{\\lnŒ∏}]$.\n",
        "  $\\E[n\\bar{X}]=n\\E[X_1]$\n",
        "  $=\\/{n\\lnŒ∏}{Œ∏-1}‚à´_0^1xŒ∏^x\\ dx$\n",
        "  $=\\/{n\\lnŒ∏}{Œ∏-1}[\\/{Œ∏}{\\lnŒ∏}-\\/{Œ∏-1}{(\\ln Œ∏)^2}]$\n",
        "  $=\\/{nŒ∏}{Œ∏-1}-\\/{n}{\\lnŒ∏}$.\n",
        "\n",
        "- 7.40: $X_1,...,X_n‚àº\\t{Bernoulli}(p)$. Show $\\bar{X}$ attains CRLB.\n",
        "$f(x_i|p)=p^{x_i}(1-p)^{1-x_i}$.\n",
        "Score $‚àÇ_p[x_i\\ln p+(1-x_i)\\ln(1-p)]$\n",
        "$=\\/{x_i}{p}-\\/{1-x_i}{1-p}$\n",
        "$=\\/{1}{p(1-p)}[(1-p)x_i-p+px_i]$\n",
        "$=\\/{1}{p(1-p)}[x_i-p]=a(p)[W-œÑ(p)]$.\n",
        "Therefore $\\bar{X}$ attains CRLB and is unbiased for $p$.\n",
        "\n",
        "- 7.41: $X_1,...,X_n‚àº(Œº,œÉ^2)$ iid sample.\n",
        "  - Show that $\\sum_ia_iX_i$ is unbiased estimator for $Œº$ if $\\sum_ia_i=1$.\n",
        "  $\\E[\\sum_{i=1}^na_iX_i]$\n",
        "  $=\\sum_{i=1}^na_i\\E[X_i]$\n",
        "  $=\\sum_{i=1}^na_iŒº=Œº$.\n",
        "  - Find the one with minimum variance.\n",
        "  $\\Var(\\sum_{i=1}^na_iX_i)$\n",
        "  $=\\sum_{i=1}^na_i^2\\Var(X_i)$\n",
        "  $=\\sum_{i=1}^na_i^2œÉ^2$.\n",
        "  By Cauchy-Schwarz, $\\sum_{i=1}^na_i^2‚â•\\/{1}{n}$. At minimum $\\Var(\\sum_{i=1}^na_iX_i)‚â•\\/{œÉ^2}{n}$,\n",
        "  which is when $\\sum_{i=1}^na_iX_i=\\bar{X}$.\n",
        "\n",
        "- 7.46 (sufficient but incomplete, Rao-Blackwell fail): $X_1,...,X_n‚àº\\Unif(Œ∏,2Œ∏)$ iid.\n",
        "  - Find method of moments estimator of $Œ∏$.\n",
        "  $\\E[X_1]=\\/{Œ∏+2Œ∏}{2}=\\/{3}{2}Œ∏$.\n",
        "  Therefore $\\tilde{Œ∏}=\\/{2}{3}\\bar{X}$.\n",
        "  - Find MLE $\\hat{Œ∏}$ and constant $k$ such that $\\E_Œ∏[k\\hat{Œ∏}]=Œ∏$.\n",
        "  $f(\\v{x}|Œ∏)=\\/{1}{Œ∏^n}I_{x_{(n)}/2< Œ∏< x_{(1)}}$.\n",
        "  Score $l'(Œ∏)=-\\/{n}{Œ∏}< 0$ for all $Œ∏>0$.\n",
        "  Therefore $\\hat{Œ∏}=\\/{X_{(n)}}{2}$.\n",
        "  $\\E[\\/{X_{(n)}}{2}]=\\/{1}{2}(1+\\/{n}{n+1})Œ∏$. Therefore $k=\\/{n+1}{2n+1}$ and\n",
        "  $\\E[\\/{n+1}{2n+1}X_{(n)}]=Œ∏$.\n",
        "  - Find sufficient statistics. Is it complete?\n",
        "  $T=(X_{(1)},X_{(n)})$ is sufficient but not complete for $Œ∏$ because $\\/{X_{(1)}}{X_{(n)}}$ is ancillary for scale family $\\Unif(1,2)$ scaled by $Œ∏$.\n",
        "  $h(T)=\\/{X_{(1)}}{X_{(n)}}$ and $\\E[h(T)]=c$. Then $g(T)=h(T)-c\\neq0$ but $\\E[g(T)]=0$ therefore $T$ is not complete.\n",
        "  - Which one can be improved by using sufficiency?\n",
        "  CRLB cannot be used due to parametrized support.\n",
        "  For method of moments,\n",
        "  $(X_i|X_{(1)},X_{(n)})‚àº\\Unif(X_{(1)},X_{(n)})$ therefore UMVUE for $Œ∏$ is\n",
        "  $\\E[\\tilde{Œ∏}|X_{(1)},X_{(n)}]$\n",
        "  $=\\/{2}{3}\\E[\\bar{X}|X_{(1)},X_{(n)}]$\n",
        "  $=\\/{2}{3}\\/{X_{(1)}+X_{(n)}}{2}$\n",
        "  $=\\/{X_{(1)}+X_{(n)}}{3}$.\n",
        "  For MLE,\n",
        "  $\\E[\\/{n+1}{2n+1}X_{(n)}|X_{(1)},X_{(n)}]$\n",
        "  $=\\/{n+1}{2n+1}\\E[X_{(n)}|X_{(1)},X_{(n)}]$\n",
        "  $=\\/{n+1}{2n+1}X_{(n)}$.\n",
        "  \n",
        "- 7.47: $X_1,...,X_n‚àº\\Normal(Œ∏,œÉ^2)$ iid are measurements on radius of a circle.\n",
        "$\\E[\\bar{X}^2]=\\Var(\\bar{X})+\\E[\\bar{X}]^2$\n",
        "$=\\/{œÉ^2}{n}+Œ∏^2$.\n",
        "Then $W=\\bar{X}^2-\\/{S^2}{n}$ is an unbiased estimator for $Œ∏^2$.\n",
        "\n",
        "- 7.48: $X_1,...,X_n‚àº\\t{Bernoulli}(p)$ iid. Show MLE of $p$ attains CRLB.\n",
        "$f(\\v{x}|p)=p^{\\sum_ix_i}(1-p)^{n-\\sum_ix_i}$.\n",
        "Score $l'(p)=\\/{n\\bar{x}}{p}-\\/{n-n\\bar{x}}{1-p}$\n",
        "$=\\/{n}{p(1-p)}[\\bar{x}-p]$.\n",
        "$l'(p)=0‚áí\\hat{p}=\\bar{X}$.\n",
        "$l''(p)=-\\/{\\bar{x}}{p^2}-\\/{1-\\bar{x}}{(1-p)^2}< 0$.\n",
        "\n",
        "- 7.49 (Rao-Blackwell, Lehmann-Scheffe, unique UMVUE): $X_1,...,X_n‚àº\\Expo(Œª)$.\n",
        "  - Find unbiased estimator of $Œª$ based on $Y=X_{(1)}$.\n",
        "  $\\E[Y]=Œª/n$. Then $W=nY$ is an unbiased estimator for $Œª$.\n",
        "  - Find a better estimator than $W$.\n",
        "  $f(x_i|Œª)=\\/{1}{Œª}\\e{-\\/{x_i}{Œª}}$ is full exponential family, therefore $\\sum_iX_i=n\\bar{X}$ is sufficient and complete. No function of $\\bar{X}$ is ancillary to scale family.\n",
        "  Therefore Rao-Blackwell\n",
        "  $W'=\\E[W|\\bar{X}]$\n",
        "  $=\\E[nX_{(1)}|\\bar{X}]$ is UMVUE by Lehmann-Scheffe.\n",
        "  $\\bar{X}$ is also UMVUE because $\\E[\\bar{X}]=Œª$.\n",
        "  UMVUE is unique therefore $W'=\\E[nX_{(1)}|\\bar{X}]=\\bar{X}$.\n",
        "\n",
        "- 7.52 (7.3.8 Rao-Blackwell, Lehman-Scheffe, unique UMVUE): $X_1,...,X_n‚àº\\Pois(Œª)$.\n",
        "  - Prove $\\bar{X}$ is UMVUE for $Œª$ with CRLB.\n",
        "  $f(x_i|Œª)=\\/{e^{-Œª}Œª^{x_i}}{x_i!}$.\n",
        "  Score $l'(Œª)=‚àÇ_Œª[-Œª+x_i\\ln Œª]$\n",
        "  $=-1+\\/{x_i}{Œª}$\n",
        "  $=\\/{1}{Œª}[x_i-Œª]$. Therefore $\\bar{X}$ attains CRLB and is umbiased for $Œª$.\n",
        "  - Prove $\\bar{X}$ is UMVUE for $Œª$.\n",
        "  $f(x_i|Œª)=\\/{e^{-Œª}}{x_i!}\\e{-x_i\\ln Œª}$ is full exponential family. $\\bar{X}$ is sufficient and complete. By Lehmann-Scheffe it is UMVUE.\n",
        "  - Prove $\\E[S^2|\\bar{X}]=\\bar{X}$.\n",
        "  $\\E[S^2]=Œª$.\n",
        "  By Rao-Blackwell $\\E[S^2|\\bar{X}]$ is better than $S^2$. By Lehman-Scheffe $\\E[S^2|\\bar{X}]$ is a function of sufficient and complete therefore it is unique UMVUE.\n",
        "  Lehmann-Scheffe says $\\bar{X}$ is also unique UMVUE.\n",
        "  Because unique, $\\E[S^2|\\bar{X}]=\\bar{X}$.\n",
        "\n",
        "- 7.60: $X_1,...,X_n‚àº\\Gamma(a,b)$ iid with $a$ known. Find UMVUE of $1/b$.\n",
        "$f(x_i|b)=\\/{e^{-x_i/b}(x_i/b)^a}{x_iŒì(a)}$\n",
        "$=\\/{1}{x_iŒì(a)}\\e{-\\/{x_i}{b}+a\\ln x_i-a\\ln b}$.\n",
        "$\\sum_iX_i$ is sufficient and complete.\n",
        "Score $l'(b)=‚àÇ_b[-\\/{x_i}{b}-a\\ln b]$\n",
        "$=\\/{x_i}{b^2}-\\/{a}{b}$\n",
        "$=\\/{1}{b^2}[x_i-ab]$. Therefore $\\E[\\bar{X}/a]=b$.\n",
        "Let $S=n\\bar{X}‚àº\\Gamma(na,b)$.\n",
        "$\\E[1/S]=\\/{b^{-1}Œì(na-1)}{Œì(na)}$\n",
        "$=\\/{1}{(na-1)b}$.\n",
        "Therefore $\\E[\\/{na-1}{S}]=\\/{1}{b}$\n",
        "and UMVUE is\n",
        "$\\E[\\/{na-1}{S}|S]=\\/{na-1}{\\sum_iX_i}$."
      ],
      "metadata": {
        "id": "jEbk2UmQwIZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis Testing"
      ],
      "metadata": {
        "id": "Qeo4CDIGAeja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis** is an inference statement about $Œ∏$.\n",
        "(Casella 8.1.1)\n",
        "A hypothesis test has a **null hypothesis** $H_0:Œ∏‚ààŒò_0$ and **alternative hypothesis** $H_1:Œ∏‚ààŒò_1=Œò_0^c$.\n",
        "(Casella 8.1.2)\n",
        "**Hypothesis testing** specifies $Œò_0$ the sample values or **acceptance region** that $H_0$ is decided as true, and $Œò_1$ or **rejection/critical region** that $H_1$ is decided is true.\n",
        "(Casella 8.1.3)\n",
        "\n",
        "**Likelihood ratio test (LRT)** statistic for testing $H_0:Œ∏‚ààŒò_0$ vs $H_1:Œ∏‚ààŒò_0^c$ is statistic $Œõ(\\v{x})=\\/{\\sup_{Œ∏‚ààŒò_0}L(Œ∏|\\v{x})}{\\sup_{Œ∏‚ààŒò_0‚à™Œò_1}L(Œ∏|\\v{x})}=\\/{L(\\hat{Œ∏}_0|\\v{x})}{L(\\hat{Œ∏}|\\v{x})}$. LRT is any test with rejection region $R=\\{\\v{x}:Œõ(\\v{x})‚â§c\\},\\ 0‚â§c‚â§1$\n",
        "(Casella 8.2.1)\n",
        "\n",
        "- **Critical value** $c$ is the cut-off threshold for $H_0$ rejection. **Under $H_0$** means \"if $H_0$ is true\", or \"assuming the null hypothesis $H_0$ is correct\" or \"if $Œ∏‚ààŒò_0$\". LRT is max likelihood under $H_0$ vs overall max. If $Œò_0^c$ explains observed $\\v{x}$ significantly better than $Œò_0$ then $H_0$ should be rejected.\n",
        "\n",
        "- If $T$ is a sufficient statistic for $Œ∏$ and $Œõ^*(t)$ and $Œõ(\\v{x})$ are LRT statistics based on $T$ and $\\v{X}$, then $Œõ^*(T(\\v{x}))=Œõ(\\v{x})$ for every $\\v{x}‚àà\\m{X}$.\n",
        "(Casella 8.2.4)\n",
        "\n",
        "  - Proof: $Œõ(\\v{x})=\\/{\\sup_{Œò_0}L(Œ∏|\\v{x})}{\\sup_ŒòL(Œ∏|\\v{x})}$\n",
        "  $=\\/{\\sup_{Œò_0}g(T|Œ∏)h(\\v{x})}{\\sup_Œòg(T|Œ∏)h(\\v{x})}$\n",
        "  $=\\/{\\sup_{Œò_0}L^*(Œ∏|T)}{\\sup_ŒòL^*(Œ∏|T)}$\n",
        "  $=Œõ^*(T)$\n",
        "\n",
        "**Power function** of a hypothesis test with rejection region $R‚äÜ\\m{X}$ is $Œ≤(Œ∏)=P_Œ∏(\\v{X}‚ààR)$\n",
        "(Casella 8.3.1)\n",
        "\n",
        "- **A hypothesis test is a rejection region** $œï(\\v{x})=I_{\\v{x}‚ààR}$ such that $\\E_Œ∏[œï(\\v{x})]=Œ≤(Œ∏)=P_Œ∏(\\v{X}‚ààR)$. The power function is the probability of a rejection region rejecting $H_0$ given the true value of $Œ∏$.\n",
        "\n",
        "  - $\\E_Œ∏[œï(\\v{x})]=‚à´_\\m{X}œï(\\v{x})f(\\v{x}|Œ∏)\\ d\\v{x}$\n",
        "  $=‚à´_{\\v{x}‚ààR}f(\\v{x}|Œ∏)\\ d\\v{x}$\n",
        "  $=P_Œ∏(\\v{x}‚ààR)$\n",
        "  $=Œ≤(Œ∏)$\n",
        "\n",
        "- Ideally $Œ≤(Œ∏)=0$ for all $Œ∏‚ààŒò_0$ (size 0) and $Œ≤(Œ∏)=1$ for all $Œ∏‚ààŒò_1$ (power 1). Under $H_0$, the test incorrectly rejects $H_0$ to make **Type 1 error** (false rejection) with probability $P_{Œ∏‚ààH_0}(\\v{X}‚ààR)=Œ≤(Œ∏)$. Under $H_1$, the test incorrectly accepts $H_0$ to make **Type 2 error** with probability $P_{Œ∏‚ààH_1}(\\v{X}‚ààR^c)=1-Œ≤(Œ∏)$.\n",
        "\n",
        "  - **Goal**: In search for a good test, we limit false rejection $\\sup_{Œ∏‚ààH_0}P_Œ∏(\\v{X}‚ààR)=Œ±$, then maximize $P_{Œ∏‚ààH_1}(\\v{X}‚ààR)$ to reduce false acceptance.\n",
        "\n",
        "- A test with power function $Œ≤(Œ∏)$ is a **size-$Œ±$** if $\\sup_{Œ∏‚ààŒò_0}Œ≤(Œ∏)=Œ±$ and **level-$Œ±$** if $\\sup_{Œ∏‚ààŒò_0}Œ≤(Œ∏)‚â§Œ±$.\n",
        "(Casella 8.3.5/6)\n",
        "\n",
        "  - **What does all this mean?**: A size-$Œ±$ test is a rejection region that, if $H_0$ is true, rejects $H_0$ with probability $Œ±$.\n",
        "  LRT is a ratio of likelihoods of sample data $\\v{X}$. To test $H_0:Œª=Œª_0$ where $Œª=œÉ_X^2/œÉ_Y^2$, the LRT is still a ratio of joint PDFs $Œõ(\\v{x},\\v{y})=\\/{\\sup_{Œª=Œª_0}L(œÉ_X^2,œÉ_Y^2|\\v{x},\\v{y})}{\\sup_ŒªL(œÉ_X^2,œÉ_Y^2|\\v{x},\\v{y})}$.\n",
        "  Now we solve for **critical value** $c_Œ±$ such that $\\sup_{Œ∏‚ààŒò_0}P_Œ∏(Œõ‚â§c_Œ±)=Œ±$, then $œï=I(Œõ‚â§c_Œ±)$ is the size-$Œ±$ test that we want.\n",
        "  However to solve for $c_Œ±$ we need to the probability distribution of $Œõ$, which is why in practice Wilks theorem ($c_Œ±=e^{-\\/{1}{2}œá_{1,Œ±}^2}$) is used.\n",
        "\n",
        "- A test is **unbiased** if $Œ≤(Œ∏_1)‚â•Œ≤(Œ∏_0)$ for every $Œ∏_0‚ààŒò_0$ and $Œ∏_1‚ààŒò_0^c$.\n",
        "(Casella 8.3.9)\n",
        "\n",
        "**Decision theory**: After observing $\\v{X}=\\v{x}$, a hypothesis action $Œ¥‚àà\\m{A}$ regarding $Œ∏‚ààŒò$ is made: $Œ¥(\\v{x})=\\BC\n",
        "a_0 &\\t{accept }H_0:Œ∏‚ààŒò_0\\\\\n",
        "a_1 &\\t{reject }H_0:Œ∏‚ààŒò_0\n",
        "\\EC$ and the rejection region is $R=\\{\\v{x}:Œ¥(\\v{x})=a_1\\}$.\n",
        "\n",
        "- The loss function quantifies cost of wrong decision $L(Œ∏,a_0)=\\BC\n",
        "0 &Œ∏‚ààŒò_0\\\\\n",
        "c_2 &Œ∏‚ààŒò_1\n",
        "\\EC$ and $L(Œ∏,a_1)=\\BC\n",
        "c_1 &Œ∏‚ààŒò_0\\\\\n",
        "0 &Œ∏‚ààŒò_1\n",
        "\\EC$ where $c_1$ and $c_2$ are cost of making a type 1 or type 2 error, which can be set to $c(Œ∏-Œ∏_0)^2$ for example.\n",
        "\n",
        "- The risk function $R(Œ∏,Œ¥)=\\E_Œ∏[L(Œ∏,Œ¥(\\v{X}))]$\n",
        "$=c_1P_Œ∏(Œ¥(\\v{X})=a_1)+c_2P_Œ∏(Œ¥(\\v{X})=a_0)$\n",
        "$=c_1Œ≤(Œ∏)+c_2(1-Œ≤(Œ∏))$"
      ],
      "metadata": {
        "id": "7xG3iLzbAnrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uniformly most powerful**: Let $\\m{C}$ be a class of tests for $H_0:Œ∏‚ààŒò_0$ vs $H_1:Œ∏‚ààŒò_1$. A test $Œ≤(Œ∏)‚àà\\m{C}$ is **UMP** if $Œ≤(Œ∏)‚â•Œ≤^*(Œ∏)$ for all $Œ∏‚ààŒò_1$ and all $Œ≤^*(Œ∏)‚àà\\m{C}$.\n",
        "(Casella 8.3.11)\n",
        "\n",
        "- Amongst all tests that satisfy $P_{Œ∏‚ààH_0}(\\v{X}‚ààR)=Œ±$, the UMP test has the highest $P_{Œ∏‚ààH_1}(\\v{X}‚ààR)$ for all $Œ∏‚ààH_1$.\n",
        "\n",
        "- **Neyman-Pearson Lemma**: Testing $H_0:Œ∏=Œ∏_0$ vs $H_1:Œ∏=Œ∏_1$ on $f(\\v{x}|Œ∏)$ using rejection region $R$ satisfying NP conditions $\\BC\n",
        "\\v{x}‚ààR&\\t{if }f(\\v{x}|Œ∏_1)>kf(\\v{x}|Œ∏_0)\\\\\n",
        "\\v{x}‚ààR^c&\\t{if }f(\\v{x}|Œ∏_1)< kf(\\v{x}|Œ∏_0)\n",
        "\\EC$ for some $k‚â•0$ and $Œ±=P_{Œ∏_0}(\\v{X}‚ààR)$ then\n",
        "(Casella 8.3.12)\n",
        "  1. (sufficiency) any test satisfying NP conditions is a UMP level-$Œ±$ test.\n",
        "  2. (necessity) if a test exists satisfying NP conditions, then every UMP level-$Œ±$ test is a size-$Œ±$ test and satisfies NP conditions.\n",
        "\n",
        "  - Proof (sufficiency): let $œï=I_{\\v{x}‚ààR}$\n",
        "  be a level-$Œ±$ test function satisfying NP conditions, and let $œï'$ represent any other level-$Œ±$ test. Then\n",
        "  $0‚â§‚à´\\ob{\\green{[œï-œï']}\\blue{[f(\\v{x}|Œ∏_1)-kf(\\v{x}|Œ∏_0)]}}{\\t{either $\\green{[+]}\\blue{[+]}$ or $\\green{[-]}\\blue{[-]}$}}\\ d\\v{x}$\n",
        "  $=‚à´[œï-œï']f(\\v{x}|Œ∏_1)\\ d\\v{x}$\n",
        "  $-k‚à´[œï-œï']f(\\v{x}|Œ∏_0)\\ d\\v{x}$\n",
        "  $=Œ≤(Œ∏_1)-Œ≤'(Œ∏_1)-k[Œ≤(Œ∏_0)-Œ≤'(Œ∏_0)]$\n",
        "  $‚áíŒ≤(Œ∏_1)‚â•Œ≤'(Œ∏_1)$ because\n",
        "  $Œ≤(Œ∏_0)=Œ≤'(Œ∏_0)=Œ±$.\n",
        "  Therefore $œï$ is the UMP level-$Œ±$ test.\n",
        "\n",
        "  - Proof (necessity): $Œ≤'(Œ∏)$ attaining UMP status  requires $Œ≤(Œ∏_0)=Œ≤'(Œ∏_0)=Œ±$ as well as $Œ≤(Œ∏_1)=Œ≤'(Œ∏_1)$. The $\\green{[+]}\\blue{[+]}$ or $\\green{[-]}\\blue{[-]}$ integral inequality above can only be satisfied if $œï'=œï$.\n",
        "\n",
        "  - NP lemma is the foundation of the LRT test. $\\/{f(\\v{x}|Œ∏_1)}{f(\\v{x}|Œ∏_0)}‚â•k$ LRT is the most powerful level-$Œ±$ test.\n",
        "\n",
        "- **NP Corollary**: Suppose $T(\\v{X})$ is sufficient for $Œ∏$ with PDF/PMF $f_T(t|Œ∏)$. Then rejection region $S‚àà\\m{T}$ is a UMP level-$Œ±$ test based on $T$ if it satisfies $\\BC\n",
        "t‚ààS &\\t{if }f_T(t|Œ∏_1)>kf_T(t|Œ∏_0)\\\\\n",
        "t‚àâS &\\t{if }f_T(t|Œ∏_1)< kf_T(t|Œ∏_0)\n",
        "\\EC$ for some $k‚â•0$ where $Œ±=P_{Œ∏_0}(T‚ààS)$.\n",
        "(Casella 8.3.13)\n",
        "\n",
        "  - Instead of rejection region $\\v{X}‚ààR$ we use region $T‚ààS$ for sufficient statistic $T$ where $R‚àà\\m{X}=\\{\\v{x}:T(\\v{x})‚ààS\\}$. NP lemma uses $f_T(t|Œ∏)$ ratio instead of $f(\\v{x}|Œ∏)$ ratio.\n",
        "\n",
        "  - Proof: Factorization $f(\\v{x}|Œ∏)=g(T(\\v{x})|Œ∏)h(\\v{x})$ where $f_T(t|Œ∏)‚àùg(t|Œ∏)$ converts the rejection conditions back to the NP conditions.\n",
        "\n",
        "- Simple hypothesis is $H_0:Œ∏=Œ∏_0$. One-sided composite hypothesis is $H_0:Œ∏‚â§Œ∏_0$. Two-sided composite hypothesis is $H_1:Œ∏\\neqŒ∏_0$. Neyman-Pearson works with simple vs simple hypotheses. Karlin-Rubin works with composite hypotheses.\n",
        "\n",
        "**Monotone likelihood ratio**: A family of PDF/PMF $\\{g(t|Œ∏):Œ∏‚ààŒò\\}$ for $T$ has **MLR** if for every $Œ∏_1‚â•Œ∏_0$, the ratio $\\/{g(t|Œ∏_1)}{g(t|Œ∏_0)}$ is a nonincreasing or nondecreasing function of $t$ on $\\{t:g(t|Œ∏_0)>0\\t{ or }g(t|Œ∏_1)>0\\}$.\n",
        "(Casella 8.3.16)\n",
        "\n",
        "- If $r(t)=\\/{g(t|Œ∏_1)}{g(t|Œ∏_0)}$ is nondecreasing with $t$ for any $Œ∏_1>Œ∏_0$, then $Œ≤(Œ∏)=P_Œ∏(T>t_0)$ is nondecreasing.\n",
        "\n",
        "  - Proof: Setup NP test $H_0:Œ∏=Œ∏_0$ vs $H_1:Œ∏=Œ∏_1$. Then NP says amongst all tests that satisfy $P_{Œ∏_0}(T>t_0)=Œ±$, the UMP satisfies $S=\\{t:r(t)>k\\}=\\{t:t>t_0\\}$ for any $t_0$ by choosing $k$ small enough.\n",
        "  $r(t)>k$\n",
        "  $‚áíg(t|Œ∏_1)‚â•kg(t|Œ∏_0)$\n",
        "  $‚áí‚à´_{t>t_0}g(t|Œ∏_1)\\ dt‚â•k‚à´_{t>t_0}g(t|Œ∏_0)\\ dt$\n",
        "  $‚áíP_{Œ∏_1}(T>t_0)‚â•kP_{Œ∏_0}(T>t_0)$\n",
        "  $‚áíŒ≤(Œ∏_1)‚â•kŒ≤(Œ∏_0)$.\n",
        "\n",
        "  - Similarly if $r(t)=\\/{g(t|Œ∏_1)}{g(t|Œ∏_0)}$ is nonincreasing then $Œ≤(Œ∏)=P_Œ∏(T\\red{<}t_0)$ is nondecreasing.\n",
        "\n",
        "- **Karlin-Rubin Theorem**: Consider $H_0:Œ∏‚â§Œ∏_0$ vs $H_1:Œ∏>Œ∏_0$. Suppose $T$ is sufficient for $Œ∏$ and its family of PDF/PMFs $\\{g(t|Œ∏):Œ∏‚ààŒò\\}$ has MLR. Then the test that rejects $H_0$ iff $T>t_0$ for any $t_0$ is a UMP level-$Œ±$ test where $Œ±=P_{Œ∏_0}(T>t_0)$.\n",
        "(Casella 8.3.17)\n",
        "\n",
        "  - If sufficient $T$ has nondecreasing MLR, then $œï=I(T>t_0)$ is UMP for $H_0:Œ∏‚â§Œ∏_0$.\n",
        "\n",
        "  - Proof (nondecreasing MLR): Let $r(t)=\\/{g(t|Œ∏_1)}{g(t|Œ∏_0)}$ be nondecreasing with $t$ for any $Œ∏_1>Œ∏_0$, then $Œ≤(Œ∏)=P_Œ∏(T>t_0)$ is nondecreasing so that $H_0:Œ∏‚â§Œ∏_0$ is equivalent to $H_0':Œ∏=Œ∏_0$ because $\\sup_{Œ∏‚â§Œ∏_0}Œ≤(Œ∏)=Œ≤(Œ∏_0)=Œ±$. For any $t_0$ (selected based on $Œ±$), choose $k=\\inf_{t>t_0}r(t)$ then NP says the rejection region $S=\\{t:r(t)>k\\}=\\{t:t>t_0\\}$ is the UMP test for any $H_1':Œ∏=Œ∏_1$ and is the UMP test for $H_1:Œ∏>Œ∏_0$.\n",
        "  \n",
        "  - Proof ($\\arg\\sup_{Œ∏‚ààH_0}P_Œ∏(T>t)=Œ∏_0$ or $Œ±=P_{Œ∏_0}(T>t_0)$): The distribution of $T$ shifts to the right with higher $Œ∏$, and that makes $T>t$ more likely. Under $H_0$, the highest value that $Œ∏$ can take is $Œ∏_0$.\n",
        "  \n",
        "  - If sufficient $T$ has nondecreasing MLR, then $œï=I(T< t_0)$ is UMP for $H_0:Œ∏‚â•Œ∏_0$.\n",
        "\n",
        "  - If sufficient $T$ has nonincreasing MLR, then $œï=I(T< t_0)$ is UMP for $H_0:Œ∏‚â§Œ∏_0$.\n",
        "\n",
        "  - Proof (nonincreasing MLR): Let $r(t)=\\/{g(t|Œ∏_1)}{g(t|Œ∏_0)}$ be nonincreasing with $t$ for any $Œ∏_1>Œ∏_0$, then $Œ≤(Œ∏)=P_Œ∏(T< t_0)$ is nondecreasing so that $H_0:Œ∏‚â§Œ∏_0$ is equivalent to $H_0':Œ∏=Œ∏_0$. For any $t_0$, choose $k=\\inf_{t< t_0}r(t)$ and rejection region $S=\\{t:r(t)‚â•k\\}=\\{t:t< t_0\\}$ is the UMP test for $H_1':Œ∏=Œ∏_1$ and for $H_1:Œ∏>Œ∏_0$.\n",
        "  - If sufficient $T$ has nonincreasing MLR, then $œï=I(T>t_0)$ is UMP for $H_0:Œ∏‚â•Œ∏_0$."
      ],
      "metadata": {
        "id": "v84lLTes_Tbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**p-value** $0‚â§p(\\v{X})‚â§1$ is a statistic that is valid if $\\sup_{Œ∏‚ààŒò_0}P_Œ∏(p(\\v{X})‚â§Œ±)‚â§Œ±$ for $0‚â§Œ±‚â§1$. Small values of $p$ gives evidence that $H_1$ is true.\n",
        "(Casella 8.3.26)\n",
        "\n",
        "- The test that rejects $H_0$ iff $p(\\v{X})‚â§Œ±$ has false rejection rate not above $Œ±$.\n",
        "\n",
        "- Let $W(\\v{X})$ be a test statistic such that large values give evidence that $H_1$ is true. For each sample point $\\v{x}$ define $p(\\v{x})=\\sup_{Œ∏‚ààŒò_0}P_Œ∏(W(\\v{X})‚â•W(\\v{x}))$, then $p(\\v{X})$ is a valid p-value.\n",
        "(Casella 8.3.27)\n",
        "\n",
        "  - Proof: Let $F_Œ∏(w)$ be the CDF of $-W(\\v{X})$.\n",
        "  Then with $Œ∏$ fixed (without $\\sup$),\n",
        "  let\n",
        "  $p_Œ∏(\\v{x})=P_Œ∏(W(\\v{X})‚â•W(\\v{x}))$\n",
        "  $=P_Œ∏(-W(\\v{X})‚â§-W(\\v{x}))$\n",
        "  $=F_Œ∏(-W(\\v{x}))$\n",
        "  $‚áíp_Œ∏(\\v{X})=F_Œ∏(-W(\\v{X}))‚àº\\Unif(0,1)$ by uniform universality if $-W(\\v{X})$ is continuous.\n",
        "  $\\BC\n",
        "  P_Œ∏(p_Œ∏(\\v{X})‚â§Œ±)=Œ± &\\t{if $-W(\\v{X})$ is continuous}\\\\\n",
        "  P_Œ∏(p_Œ∏(\\v{X})‚â§Œ±)‚â§Œ± &\\t{if $-W(\\v{X})$ is discrete}\n",
        "  \\EC$.\n",
        "  Because $p(\\v{X})‚â•p_Œ∏(\\v{X})‚â•\\Unif(0,1)$,\n",
        "  $P_Œ∏(p(\\v{X})‚â§Œ±)‚â§P_Œ∏(p_Œ∏(\\v{X})‚â§Œ±)‚â§Œ±$,\n",
        "  and $p(\\v{X})$ is a valid p-value.\n",
        "\n",
        "- p-value in scientific literature is commonly setup as $H_0:Œ∏=Œ∏_0$ vs $H_1:Œ∏\\neqŒ∏_0$ where null hypothesis represents a normal distribution centered at $Œ∏_0$, and how far observed $\\bar{X}$ sits away from $Œ∏_0$ in the tail end is a representation of therapeutic/harm value. Small p-value means observed $\\bar{X}$ is very far into the tail of the null distribution. $p(\\v{x})=2P(T_{n-1}‚â•\\/{\\bar{x}-Œº_0}{s/\\sqrt{n}})$. (e8.3.28)"
      ],
      "metadata": {
        "id": "Fp07b0ZAPgxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 8.2.2 (normal): $X_1,...,X_n‚àº\\Normal(Œ∏,1)$. Testing $H_0:Œ∏=Œ∏_0$. MLE $\\hat{Œ∏}=\\bar{X}$.\n",
        "Therefore $Œõ=\\/{L(Œ∏_0|\\v{x})}{L(\\hat{Œ∏}|\\v{x})}$\n",
        "$=\\e{-\\sum_{i=1}^n(x_i-Œ∏_0)^2/2+\\sum_{i=1}^n(x_i-\\bar{x})^2/2}$\n",
        "$=\\e{-n(\\bar{x}-Œ∏_0)^2/2}$\n",
        "and $œï=I(\\sqrt{n}|\\bar{x}-Œ∏_0|‚â•\\sqrt{-2(\\ln c)})$\n",
        "is a test on $|\\bar{x}-Œ∏_0|$.\n",
        "\n",
        "  - 8.3.7 (size):\n",
        "  Test\n",
        "  $œï=I(\\sqrt{n}|\\bar{X}-Œ∏_0|‚â•\\sqrt{-2(\\ln c)})$.\n",
        "  Test size is determined under $H_0:Œ∏=Œ∏_0$: $\\sqrt{n}(\\bar{X}-Œ∏)=\\sqrt{n}(\\bar{X}-Œ∏_0)‚àº\\Normal(0,1)$.\n",
        "  To make it a size-$Œ±$ test we choose $c$ so that\n",
        "  $Œ±=\\sup_{Œ∏‚ààŒò_0}P_Œ∏(\\sqrt{n}|\\bar{X}-Œ∏_0|‚â•\\sqrt{-2(\\ln c)})$\n",
        "  $=P_{Œ∏_0}(\\sqrt{n}|\\bar{X}-Œ∏_0|‚â•\\sqrt{-2(\\ln c)})$\n",
        "  $=P(|Z|‚â•\\sqrt{-2(\\ln c)})$\n",
        "  $=P(Z‚â§-\\sqrt{-2(\\ln c)}, Z‚â•\\sqrt{-2(\\ln c)})$\n",
        "  $=2P(Z‚â•\\sqrt{-2(\\ln c)})$, then\n",
        "  $z_{Œ±/2}=\\sqrt{-2(\\ln c)}$\n",
        "  \n",
        "- 8.2.3 (exponential): $X_1,..,X_n‚àºf(x_i|Œ∏)=e^{-(x_i-Œ∏)}I_{x_i‚â•Œ∏}$ iid.\n",
        "$L(Œ∏|\\v{x})=e^{-\\sum_ix_i+nŒ∏}I_{x_{(1)}‚â•Œ∏}$ is an increasing function of $Œ∏$.\n",
        "Testing $H_0:Œ∏‚â§Œ∏_0$.\n",
        "If $Œ∏_0‚â§x_{(1)}$ then likelihood $e^{nŒ∏}$ peaks at $Œ∏_0$ otherwise likelihood peaks at $\\hat{Œ∏}=X_{(1)}$:\n",
        "$\\arg\\sup_{Œ∏‚â§Œ∏_0}L(Œ∏|\\v{x})=\\BC\n",
        "Œ∏_0&Œ∏_0‚â§x_{(1)}\\\\\n",
        "x_{(1)}&Œ∏_0>x_{(1)}\n",
        "\\EC$\n",
        "and\n",
        "$Œõ=\\BC\n",
        "\\/{L(Œ∏_0|\\v{x})}{L(x_{(1)}|\\v{x})}=e^{-n(x_{(1)}-Œ∏_0)}&Œ∏_0‚â§x_{(1)}\\\\\n",
        "\\/{L(x_{(1)}|\\v{x})}{L(x_{(1)}|\\v{x})}=1&\\t{otherwise}\n",
        "\\EC$.\n",
        "\n",
        "  - 8.2.5 (sufficiency): Find $T$ likelihood.\n",
        "  $1-F(t)=‚à´_t^‚àûe^{-(x-Œ∏)}\\ dx$\n",
        "  $=[-e^{-(x-Œ∏)}]_t^‚àû=e^{-(t-Œ∏)}I_{t‚â•Œ∏}$\n",
        "  $‚áíF_{X_{(1)}}(t|Œ∏)=1-e^{-n(t-Œ∏)}$\n",
        "  $‚áíL^*(Œ∏|t)=ne^{-n(t-Œ∏)}I_{t‚â•Œ∏}$.\n",
        "\n",
        "  - 8.3.7 (size):\n",
        "  Test $œï=I(Œ∏_0‚â§X_{(1)})$ and\n",
        "  $Œõ=e^{-n(X_{(1)}-Œ∏_0)}‚â§c$.\n",
        "  Under $H_0$ to make a size-$Œ±$ test,\n",
        "  $P_{Œ∏_0}(e^{-n(X_{(1)}-Œ∏_0)}‚â§c)$\n",
        "  $=P_{Œ∏_0}(X_{(1)}‚â•Œ∏_0-(\\ln c)/n)$\n",
        "  $=e^{-n(Œ∏_0-(\\ln c)/n-Œ∏_0)}$\n",
        "  $=c=Œ±$.\n",
        "  Therefore size-$Œ±$ test $œï=I(X_{(1)}‚â•Œ∏_0-\\/{\\ln Œ±}{n})$.\n",
        "\n",
        "- 8.3.2 (binomial power function): $X‚àº\\Binom(5,Œ∏)$. Test $H_0:Œ∏‚â§\\/{1}{2}$. The ideal test should have a power function like step function $I_{Œ∏>\\/{1}{2}}$. Test 1 rejects $H_0$ iff all successes are observed, then $Œ≤_1(Œ∏)=Œ∏^5$. Acceptance rate is high, false negative rate is low, and false positive rate is high. Test 2 rejects $H_0$ if $X=3,4,5$, then $Œ≤_2(Œ∏)=\\binom{5}{3}Œ∏^3(1-Œ∏)^2+\\binom{5}{4}Œ∏^4(1-Œ∏)+\\binom{5}{5}Œ∏^5$.\n",
        "\n",
        "- 8.3.3 (normal power function): $X_1,...,X_n‚àº\\Normal(Œ∏,œÉ^2)$ with $œÉ^2$ known. Testing $\\blue{H_0:Œ∏‚â§Œ∏_0}$.\n",
        "$f(\\v{x}|Œ∏)=\\/{1}{(\\sqrt{2œÄ}œÉ)^n}\\e{-\\/{\\sum_{i=1}^n(x_i-Œ∏)^2}{2œÉ^2}}$.\n",
        "$l'(Œ∏)=‚àÇ_Œ∏[\\sum_ix_i^2-\\sum_i2Œ∏x_i+nŒ∏^2]$\n",
        "$=2\\sum_ix_i-2nŒ∏=0$.\n",
        "$\\hat{Œ∏}=\\bar{x}$.\n",
        "LRT\n",
        "$Œõ(\\v{x})=\\/{\\sup_{Œ∏‚â§Œ∏_0}f(\\v{x}|Œ∏)}{\\sup_Œ∏ f(\\v{x}|Œ∏)}$\n",
        "$=\\BC\n",
        "\\/{L(Œ∏_0|\\v{x})}{L(\\bar{x}|\\v{x})}=\\e{-\\/{n}{2œÉ^2}(\\bar{x}-Œ∏_0)^2} &Œ∏_0‚â§\\bar{x}\\\\\n",
        "1 & Œ∏_0>\\bar{x}\n",
        "\\EC$\n",
        "$=\\e{-\\/{n}{2œÉ^2}\\max(\\bar{x}-Œ∏_0,0)^2}$.\n",
        "Power function\n",
        "$Œ≤(Œ∏)=P_Œ∏(Œõ(\\v{x})‚â§a)$\n",
        "$=P_Œ∏(\\/{\\bar{x}-Œ∏_0}{œÉ/\\sqrt{n}}‚â•c=\\sqrt{-2\\ln(a)})$\n",
        "$=P_Œ∏(\\/{\\bar{x}-Œ∏+Œ∏-Œ∏_0}{œÉ/\\sqrt{n}}‚â•c)$\n",
        "$=P_Œ∏(Z‚â•c+\\/{Œ∏_0-Œ∏}{œÉ/\\sqrt{n}})$.\n",
        "\n",
        "  - $\\blue{Œ≤(Œ∏)=P_Œ∏(\\/{\\bar{x}-Œ∏_0}{œÉ/\\sqrt{n}}‚â•c)=P_Œ∏(Z‚â•c+\\/{Œ∏_0-Œ∏}{œÉ/\\sqrt{n}})}$.\n",
        "\n",
        "  - $\\sqrt{(\\bar{x}-Œ∏_0)^2}=\\bar{x}-Œ∏_0$ without considering the $-(\\bar{x}-Œ∏_0)$ case because $P(Œõ=1‚â§a)=0$ in that case.\n",
        "\n",
        "  - 8.3.4: Choose $c$ and $n$ to achieve maximum Type 1 error probability 0.1, and maximum type 2 error probability 0.2 if $Œ∏‚â•Œ∏_0+œÉ$.\n",
        "  Devise test that rejects $H_0:Œ∏‚â§Œ∏_0$ if $\\/{\\bar{x}-Œ∏_0}{œÉ/\\sqrt{n}}‚â•c$ sample mean is significantly higher than $Œ∏_0$ and the data is unlikely to conclude $Œ∏‚â§Œ∏_0$.\n",
        "  Because $Œ≤(Œ∏)$ is an increasing function, the requirement $Œ≤(Œ∏_0)=0.1$ and $Œ≤(Œ∏_0+œÉ)=0.8$ are appropriate.\n",
        "  For example choosing $c=1.28$ results in $n=5$ as a pair that meets this requirement.\n",
        "\n",
        "- 8.3.15 (UMP normal): $X_1,...,X_n‚àº\\Normal(Œ∏,œÉ^2)$ with $œÉ^2$ known. $\\bar{X}‚àº\\Normal(Œ∏,\\/{œÉ^2}{n})$ is sufficient. Testing $H_0:Œ∏=Œ∏_0$ vs $H_1:Œ∏=Œ∏_1$ where $Œ∏_1< Œ∏_0$.\n",
        "Then $\\/{g(t|Œ∏_1)}{g(t|Œ∏_0)}$\n",
        "$=\\e{-\\/{(t-Œ∏_1)^2-(t-Œ∏_0)^2}{2œÉ^2/n}}$\n",
        "$=\\e{-\\/{(-2Œ∏_1t+Œ∏_1^2)-(-2Œ∏_0t+Œ∏_0^2)}{2œÉ^2/n}}$\n",
        "$=\\e{\\/{2t(Œ∏_1-Œ∏_0)-Œ∏_1^2+Œ∏_0^2}{2œÉ^2/n}}$\n",
        "$>k$\n",
        "$‚áít<\\/{2œÉ^2(\\ln k)/n-Œ∏_0^2+Œ∏_1^2}{2(Œ∏_1-Œ∏_0)}$ because $Œ∏_1-Œ∏_0< 0$.\n",
        "\n",
        "  - 8.3.18: $X_1,...,X_n‚àº\\Normal(Œ∏,œÉ^2)$ with $œÉ^2$ known. Testing $H_0:Œ∏‚â•Œ∏_0$ vs $H_1:Œ∏< Œ∏_0$ using rejection $\\bar{X}< Œ∏_0-\\/{œÉz_Œ±}{\\sqrt{n}}$. Show that this is a UMP level-$Œ±$ test.\n",
        "  For $Œ∏_1>Œ∏_0$,\n",
        "  $r(t)=\\/{g(t|Œ∏_1)}{g(t|Œ∏_0)}$\n",
        "  $=\\e{\\/{2t(Œ∏_1-Œ∏_0)-Œ∏_1^2+Œ∏_0^2}{2œÉ^2/n}}$.\n",
        "  $‚àÇ_t\\ln r(t)=\\/{2(Œ∏_1-Œ∏_0)}{2œÉ^2/n}>0$.\n",
        "  Therefore $\\bar{X}$ has a nondecreasing MLR, and Karlin-Rubin says $T< t_0$ is a UMP level-$Œ±$ test for $H_0:Œ∏‚â•Œ∏_0$.\n",
        "\n",
        "- 8.3.28/8.38 (two-sided normal p-value, student-t): $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$. Testing $\\blue{H_0:Œº=Œº_0}$ vs $\\blue{H_1:Œº\\neqŒº_0}$.\n",
        "$f(\\v{x}|Œº,œÉ^2)=\\/{1}{(\\sqrt{2œÄ}œÉ)^n}\\e{-\\/{\\sum_i(x_i-Œº)^2}{2œÉ^2}}$.\n",
        "$\\ln f=-\\/{n}{2}\\ln œÉ^2-\\/{\\sum_i(x_i-Œº)^2}{2œÉ^2}$.\n",
        "$‚àÇ_Œº\\ln f‚àù\\sum_i(x_i-Œº)=0$\n",
        "$‚áí\\hat{Œº}=\\bar{X}$.\n",
        "$‚àÇ_{œÉ^2}\\ln f‚àù-\\/{n}{2œÉ^2}+\\/{\\sum_i(x_i-Œº)^2}{4œÉ^4}=0$\n",
        "$‚áí\\hat{œÉ}^2=\\/{\\sum_i(x_i-\\bar{x})^2}{n}$\n",
        "$=\\/{nS^2}{n-1}$.\n",
        "$œÉ_0^2=\\/{\\sum_i(x_i-Œº_0)^2}{n}$\n",
        "$=\\/{\\sum_i(x_i-\\bar{x})^2+n(\\bar{x}-Œº_0)^2}{n}$\n",
        "$=\\hat{œÉ}^2+(\\bar{x}-Œº_0)^2$.\n",
        "Likelihood ratio test:\n",
        "$Œõ=\\/{f(\\v{x}|Œº_0,œÉ_0^2)}{f(\\v{x}|\\bar{x},\\hat{œÉ}^2)}$\n",
        "$=(\\/{\\hat{œÉ}^2}{œÉ_0^2})^{n/2}\\e{-\\/{\\sum_i(x_i-Œº_0)^2}{2œÉ_0^2}+\\/{\\sum_i(x_i-\\bar{x})^2}{2\\hat{œÉ}^2}}$\n",
        "$=(\\/{\\hat{œÉ}^2}{œÉ_0^2})^{n/2}\\e{-\\/{nœÉ_0^2}{2œÉ_0^2}+\\/{n\\hat{œÉ}^2}{2\\hat{œÉ}^2}}$\n",
        "$=(\\/{œÉ_0^2}{\\hat{œÉ}^2})^{-n/2}$\n",
        "$=(1+\\/{(\\bar{x}-Œº_0)^2}{\\hat{œÉ}^2})^{-n/2}$\n",
        "$=(1+\\/{n(\\bar{x}-Œº_0)^2}{(n-1)S^2})^{-n/2}$.\n",
        "Rejection region\n",
        "$Œõ< k$\n",
        "$‚áí\\/{n(\\bar{x}-Œº_0)^2}{(n-1)S^2}> k'$\n",
        "$‚áí\\green{(\\/{\\bar{x}-Œº_0}{S/\\sqrt{n}})^2>c}$.\n",
        "We reject $H_0$ on large values of $W=\\/{|\\bar{X}-Œº_0|}{S/\\sqrt{n}}=|T_{n-1}|$ where $T_{n-1}$ is student-t.\n",
        "Therefore\n",
        "$\\red{p(\\v{x})=P_{H_0}(|T_{n-1}|‚â•|\\/{\\bar{x}-Œº_0}{s/\\sqrt{n}}|)=2P_{H_0}(T_{n-1}‚â•|\\/{\\bar{x}-Œº_0}{s/\\sqrt{n}}|)}$\n",
        "\n",
        "  - $H_0:Œº=Œº_0$ is supported by having $\\bar{X}$ closely hugging $Œº_0$ - this is why two-sided statistic $W(\\v{x})=\\/{|\\bar{x}-Œº_0|}{\\hat{œÉ}/\\sqrt{n}}$ is chosen.\n",
        "\n",
        "  - $T_{n-1}=\\/{\\bar{X}-\\red{Œº}}{œÉ/\\sqrt{n}}$. Under $H_0:Œº=Œº_0$, $T_{n-1}=\\/{\\bar{X}-\\red{Œº_0}}{œÉ/\\sqrt{n}}$.\n",
        "\n",
        "  - This is the most common form of p-value use in scientific studies.\n",
        "\n",
        "- 8.3.29/8.37: (one-sided normal p-value): Testing $\\blue{H_0:Œº‚â§Œº_0}$ vs $\\blue{H_1:Œº>Œº_0}$.\n",
        "Under $H_0$,\n",
        "$\\arg\\sup_{Œº‚ààH_0}f(\\v{x}|Œº,œÉ^2)=\\tilde{Œº}=\\min(\\bar{x},Œº_0)$, and\n",
        "$\\arg\\sup_{œÉ^2‚ààH_0}f(\\v{x}|Œº,œÉ^2)=\\tilde{œÉ}^2$\n",
        "$=\\/{\\sum_i(x_i-\\tilde{Œº})^2}{n}$.\n",
        "Likelihood ratio test:\n",
        "$Œõ=\\/{f(\\v{x}|\\tilde{Œº},\\tilde{œÉ}^2)}{f(\\v{x}|\\hat{Œº},\\hat{œÉ}^2)}$\n",
        "$=(\\/{\\hat{œÉ}^2}{\\tilde{œÉ}^2})^{-n/2}$.\n",
        "From 8.2.28, $(\\/{\\bar{x}-Œº_0}{S/\\sqrt{n}})^2>c$ is the rejection region from the LRT.\n",
        "Because $H_0:Œº‚â§Œº_0$ is supported by smaller $\\bar{x}$, ideally $\\bar{x}< Œº_0$, we use one-sided statistic $W(\\v{X})=\\/{\\bar{X}-Œº_0}{S/\\sqrt{n}}$ where large values lead to smaller LR and rejection of $H_0$.\n",
        "In the one-sided case $T_{n-1}=\\/{\\bar{X}-Œº}{S/\\sqrt{n}}$ therefore $W(\\v{X})=T_{n-1}+\\/{Œº-Œº_0}{S/\\sqrt{n}}$.\n",
        "$p(\\v{x})=P_{H_0}(W(\\v{X})‚â•\\/{\\bar{x}-Œº_0}{s/\\sqrt{n}})$\n",
        "$=P_{H_0}(T_{n-1}‚â•\\/{\\bar{x}-Œº_0}{s/\\sqrt{n}}+\\/{Œº_0-Œº}{S/\\sqrt{n}})$\n",
        "$‚â§P(T_{n-1}‚â•\\/{\\bar{x}-Œº_0}{s/\\sqrt{n}})$\n",
        "\n",
        "  - One-sided p-value does not detect the extreme observations that occur on the other tail.\n",
        "\n",
        "- 8.6: $X_1,...,X_n‚àº\\Expo(Œ∏)$ iid and $Y_1,...,Y_m‚àº\\Expo(Œº)$ iid.\n",
        "  - Find LRT of $H_0:Œ∏=Œº$ vs $H_1:Œ∏\\neqŒº$.\n",
        "  $f(\\v{x},\\v{y}|Œ∏,Œº)=\\/{1}{Œ∏^nŒº^m}e^{-\\sum_ix_i/Œ∏-\\sum_jy_j/Œº}$.\n",
        "  $‚àÇ_Œ∏ \\ln f=-\\/{n}{Œ∏}+\\/{\\sum_ix_i}{Œ∏^2}$\n",
        "  $‚áí\\hat{Œ∏}=\\/{\\sum_ix_i}{n}$ and\n",
        "  $\\hat{Œº}=\\/{\\sum_jy_j}{m}$.\n",
        "  Under $H_0$:\n",
        "  $f(\\v{x},\\v{y}|Œ∏,Œ∏)=\\/{1}{Œ∏^{m+n}}e^{-\\/{1}{Œ∏}(\\sum_ix_i+\\sum_jy_j)}$.\n",
        "  $‚àÇ_Œ∏ \\ln f=-\\/{m+n}{Œ∏}+\\/{\\sum_ix_i+\\sum_jy_j}{Œ∏^2}$\n",
        "  $‚áí\\tilde{Œ∏}=\\tilde{Œº}=\\/{\\sum_ix_i+\\sum_jy_j}{m+n}$.\n",
        "  LRT\n",
        "  $Œõ=\\/{\\sup_{Œ∏,Œº‚ààH_0}f(\\v{x},\\v{y}|Œ∏,Œº)}{\\sup_{Œ∏,Œº}f(\\v{x},\\v{y}|Œ∏,Œº)}$\n",
        "  $=\\/{\\hat{Œ∏}^n\\hat{Œº}^m}{\\tilde{Œº}^{m+n}}\\e{-(m+n)+m+n}$\n",
        "  $=\\/{\\hat{Œ∏}^n\\hat{Œº}^m}{\\tilde{Œº}^{m+n}}$.\n",
        "  - Find the LRT statistic.\n",
        "  $Œõ=\\/{\\hat{Œ∏}^n\\hat{Œº}^m}{\\tilde{Œº}^{m+n}}$\n",
        "  $=\\/{(\\/{\\sum_ix_i}{n})^n(\\/{\\sum_jy_j}{m})^m}{(\\/{\\sum_ix_i+\\sum_jy_j}{m+n})^{m+n}}$\n",
        "  $=\\/{(m+n)^{m+n}}{n^nm^m}\\/{(\\sum_ix_i)^n(\\sum_jy_j)^m}{(\\sum_ix_i+\\sum_jy_j)^{m+n}}$\n",
        "  $‚àù(\\/{\\sum_ix_i}{\\sum_ix_i+\\sum_jy_j})^n(1-\\/{\\sum_ix_i}{\\sum_ix_i+\\sum_jy_j})^m$.\n",
        "  We have $\\sum_ix_i‚àº\\Gamma(n,Œ∏)$ and $\\sum_jy_j‚àº\\Gamma(m,Œº)$.\n",
        "  If $Œ∏=Œº$ then $W=\\/{\\sum_ix_i}{\\sum_ix_i+\\sum_jy_j}‚àº\\Beta(n,m)$.\n",
        "\n",
        "- 8.13: $X_1,X_2‚àº\\Unif(Œ∏,Œ∏+1)$ iid. Testing $H_0:Œ∏=0$ vs $H_1:Œ∏>0$. Two tests: $œï_1(X_1)=I(X_1>0.95)$ and $œï(X_1,X_2)=I(X_1+X_2>C)$. Find $C$ so that $œï_1$ and $œï_2$ have the same size.\n",
        "$œï_1$ has size\n",
        "$Œ±=\\sup_{H_0}P_Œ∏(œï_1(\\v{X})=1)$\n",
        "$=P_{Œ∏=0}(X_1>0.95)$\n",
        "$=0.05$.\n",
        "$œï_2$ has size\n",
        "$P_{Œ∏=0}(X_1+X_2>C)$.\n",
        "$f_{X_1+X_2}(x)=f_{X_1}(x)‚äóf_{X_2}(x)$\n",
        "$=‚à´_{-‚àû}^‚àûf_{X_1}(s)f_{X_2}(x-s)\\ ds$ subject to conditions\n",
        "$0< s< 1$,\n",
        "$0< x-s< 1$,\n",
        "$x>s>x-1$,\n",
        "$\\min(1,x)>s>\\max(x-1,0)$.\n",
        "$f_{X_1+X_2}(x)=\\BC\n",
        "‚à´_0^x\\ ds&=\\blue{x}& \\blue{0< x< 1}\\\\\n",
        "‚à´_{x-1}^1\\ ds&=\\green{2-x}&\\green{1< x< 2}\n",
        "\\EC$.\n",
        "If $0< C< 1$ then\n",
        "$P(X_1+X_2>C)=‚à´_C^1x\\ dx+‚à´_1^2(2-x)\\ dx$\n",
        "$=[\\/{x^2}{2}]_C^1+[2x-\\/{x^2}{2}]_1^2$\n",
        "$=\\/{1-C^2}{2}+\\/{1}{2}$\n",
        "$=1-\\/{C^2}{2}$\n",
        "$‚áíC=\\sqrt{1.9}\\not< 1$: invalid.\n",
        "If $1< C< 2$ then\n",
        "$P(X_1+X_2>C)=‚à´_C^2(2-x)\\ dx$\n",
        "$=[2x-\\/{x^2}{2}]_C^2$\n",
        "$=2-2C+\\/{C^2}{2}$\n",
        "$=\\/{(2-C)^2}{2}$\n",
        "$‚áíC=2-\\sqrt{0.1}=1.68$.\n",
        "\n",
        "- 8.18: $X_1,...,X_n‚àº\\Normal(Œ∏,œÉ^2)$ iid, $œÉ^2$ known. Test $H_0:Œ∏=Œ∏_0$ vs $H_1:Œ∏\\neqŒ∏_0$ LRT is $œï(\\v{X})=I(\\/{|\\bar{X}-Œ∏_0|}{œÉ/\\sqrt{n}}>c)$.\n",
        "  - Find $Œ≤(Œ∏)$ in terms of $Œ¶()$.\n",
        "  $Œ≤(Œ∏)=P_Œ∏(\\/{|\\bar{X}-Œ∏_0|}{œÉ/\\sqrt{n}}>c)$\n",
        "  $=1-P_Œ∏(\\/{|\\bar{X}-Œ∏_0|}{œÉ/\\sqrt{n}}‚â§c)$\n",
        "  $=1-P_Œ∏(-c‚â§\\/{\\bar{X}-Œ∏_0}{œÉ/\\sqrt{n}}‚â§c)$\n",
        "  $=1-P_Œ∏(-c-\\/{Œ∏-Œ∏_0}{œÉ/\\sqrt{n}}‚â§Z‚â§c-\\/{Œ∏-Œ∏_0}{œÉ/\\sqrt{n}})$\n",
        "  $=1-Œ¶(c+\\/{Œ∏_0-Œ∏}{œÉ/\\sqrt{n}})+Œ¶(-c+\\/{Œ∏_0-Œ∏}{œÉ/\\sqrt{n}})$\n",
        "  - Experimenter wants type-1 error 0.05 and maximum type-2 error 0.25 at $Œ∏=Œ∏_0+œÉ$. Find $n$ and $c$ that achieves this.\n",
        "  $0.05=Œ≤(Œ∏_0)$\n",
        "  $=1-Œ¶(c)+Œ¶(-c)$\n",
        "  $=2-2Œ¶(c)$\n",
        "  $‚áíc=1.96$.\n",
        "  $0.25=Œ≤(Œ∏_0+œÉ)$\n",
        "  $=1-Œ¶(c-\\sqrt{n})+Œ¶(-c-\\sqrt{n})$\n",
        "  $‚áín=7$.\n",
        "\n",
        "- 8.19: $f(x)=e^{-x}$, we observe $Y=X^Œ∏$. Test $H_0:Œ∏=1$ vs $H_1:Œ∏=2$. Find the UMP level $Œ±=0.1$ test.\n",
        "Change variable $x=y^{1/Œ∏}$.\n",
        "$|\\/{‚àÇx}{‚àÇy}|=\\/{1}{Œ∏}y^{\\/{1}{Œ∏}-1}$.\n",
        "$f(y|Œ∏)=\\/{1}{Œ∏}y^{\\/{1}{Œ∏}-1}e^{-y^{\\/{1}{Œ∏}}}$.\n",
        "$f(y|1)=e^{-y}$.\n",
        "$f(y|2)=\\/{1}{2\\sqrt{y}}e^{-\\sqrt{y}}$.\n",
        "NP $Œõ=\\/{f(y|2)}{f(y|1)}$\n",
        "$=\\/{1}{2\\sqrt{y}}e^{y-\\sqrt{y}}$.\n",
        "Therefore $œï(y)=I(\\/{1}{2\\sqrt{y}}e^{y-\\sqrt{y}}>k)$ is UMP.\n",
        "\n",
        "- 8.25/27: Show each of the following families has an MLR.\n",
        "  - $\\Normal(Œ∏,œÉ^2)$ family with $œÉ^2$ known.\n",
        "  Let $Œ∏_1>Œ∏_0$ then\n",
        "  $r(t)=\\/{f(t|Œ∏_1)}{f(t|Œ∏_0)}$\n",
        "  $=\\e{\\/{-(t-Œ∏_1)^2}{2œÉ^2}-\\/{-(t-Œ∏_0)^2}{2œÉ^2}}$\n",
        "  $=e^{-\\/{1}{2œÉ^2}(t^2-2Œ∏_1t+Œ∏_1^2-t^2+2Œ∏_0t-Œ∏_0^2)}$\n",
        "  $=e^{-\\/{1}{2œÉ^2}(2t(Œ∏_0-Œ∏_1)+Œ∏_1^2-Œ∏_0^2)}$.\n",
        "  $‚àÇ_t \\ln r(t)$\n",
        "  $=\\/{Œ∏_1-Œ∏_0}{œÉ^2}>0$ for all $t$.\n",
        "  Therefore $\\Normal(Œ∏,œÉ^2)$ family has nondescending MLR.\n",
        "  - $\\Pois(Œ∏)$ family.\n",
        "  $f(t|Œ∏)=\\/{e^{-Œ∏}Œ∏^t}{t!}$.\n",
        "  Let $Œ∏_1>Œ∏_0$ then\n",
        "  $r(t)=\\/{f(t|Œ∏_1)}{f(t|Œ∏_0)}$\n",
        "  $=e^{-(Œ∏_1-Œ∏_0)}(\\/{Œ∏_1}{Œ∏_0})^t$.\n",
        "  $‚àÇ_t \\ln r(t)$\n",
        "  $=\\ln(Œ∏_1)-\\ln(Œ∏_0)>0$ for all $t$.\n",
        "  Therefore $\\Pois(Œ∏)$ family has nondescending MLR.\n",
        "  - $\\Binom(n,Œ∏)$ family with $n$ known.\n",
        "  Let $Œ∏_1>Œ∏_0$ then\n",
        "  $r(t)=(\\/{Œ∏_1}{Œ∏_0})^t(\\/{1-Œ∏_1}{1-Œ∏_0})^{n-t}$.\n",
        "  $‚àÇ_t \\ln r(t)$\n",
        "  $=‚àÇ_t[t\\ln(\\/{Œ∏_1}{Œ∏_0})+(n-t)\\ln(\\/{1-Œ∏_1}{1-Œ∏_0})]$\n",
        "  $=\\ln(\\/{Œ∏_1}{Œ∏_0})-\\ln(\\/{1-Œ∏_1}{1-Œ∏_0})$\n",
        "  $=\\ln(\\/{Œ∏_1}{Œ∏_0})+\\ln(\\/{1-Œ∏_0}{1-Œ∏_1})>0$ for all $t$.\n",
        "  Therefore $\\Binom(Œ∏)$ family has nondescending MLR.\n",
        "  - $g(t|Œ∏)=h(t)c(Œ∏)e^{w(Œ∏)t}$ where $w(Œ∏)$ is increasing function of $Œ∏$.\n",
        "  Let $Œ∏_1>Œ∏_0$ then\n",
        "  $r(t)=e^{w(Œ∏_1)t-w(Œ∏_0)t}$.\n",
        "  $‚àÇ_t \\ln r(t)=w(Œ∏_1)-w(Œ∏_0)>0$ for all $t$.\n",
        "  Therefore exponential family has MLR if $w(Œ∏)$ has MLR.\n",
        "\n",
        "- 8.31: $X_1,...,X_n‚àº\\Pois(Œª)$. Find UMP test of $H_0:Œª‚â§Œª_0$ vs $H_1:Œª>Œª_0$.\n",
        "$f(x_i|Œª)=\\/{e^{-Œª}Œª^{x_i}}{x_i!}=\\/{e^{-Œª}}{x_i!}e^{x_i\\lnŒª}$ therefore $T=\\sum_iX_i‚àº\\Pois(nŒª)$ is sufficient for $Œª$. Since $T$ has nondecreasing MLR, $œï=I(T>t_0)$ is UMP.\n",
        "\n",
        "- 8.32 (solving for $t_0$): $X_1,...,X_n‚àº\\Normal(Œ∏,1)$ iid. Find size-$Œ±$ UMP test for $H_0:Œ∏‚â•Œ∏_0$ vs $H_1:Œ∏< Œ∏_0$.\n",
        "Because $\\bar{X}$ is sufficient for $Œ∏$ and has nondecreasing MLR, Karlin-Rubin says $œï=I(\\bar{X}< t_0)$ is UMP size-$Œ±$ test where\n",
        "$Œ±=P_{Œ∏_0}(\\bar{X}< t_0)$\n",
        "$=P_{Œ∏_0}(\\/{\\bar{X}-Œ∏_0}{1/\\sqrt{n}}< \\/{t_0-Œ∏_0}{1/\\sqrt{n}})$\n",
        "$=Œ¶(\\sqrt{n}(t_0-Œ∏_0))$\n",
        "$‚áíz_Œ±=\\sqrt{n}(t_0-Œ∏_0)$\n",
        "$‚áít_0=Œ∏_0+\\/{z_Œ±}{\\sqrt{n}}$.\n",
        "\n",
        "  - Prove $\\arg\\sup_{Œ∏‚ààH_0}P_Œ∏(\\bar{X}< t_0)=Œ∏_0$.\n",
        "  Distribution of $\\bar{X}$ shifts to the left with lower $Œ∏$, which makes $\\bar{X}< t_0$ more likely. Under $H_0$, the lowest value $Œ∏$ can take is $Œ∏_0$."
      ],
      "metadata": {
        "id": "Rgm8-TUWtQKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interval Estimation"
      ],
      "metadata": {
        "id": "3RL9B5Gq9mQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interval estimate** of $Œ∏$ is a pair of functions $L(\\v{x})‚â§Œ∏‚â§U(\\v{x})$ for all observations $\\v{x}‚àà\\m{X}$. The random interval $[L(\\v{X}),U(\\v{X})]$ is called an **interval estimator**.\n",
        "The **coverage probability** of the interval is $P_Œ∏(Œ∏‚àà[L,U])$.\n",
        "The **confidence coefficient** of the interval is $\\inf_Œ∏P_Œ∏(Œ∏‚àà[L,U])$.\n",
        "(Casella 9.1.1,9.1.4,9.1.5)\n",
        "\n",
        "- **Confidence interval**: Given sample data $\\v{X}$, we construct a random interval $[L,U]$ to cover the fixed but unknown $Œ∏$ such that $[L,U]$ covers $Œ∏$ with probability $P_Œ∏(Œ∏‚àà[L,U])‚â•1-Œ±$.\n",
        "\n",
        "  - Coverage probability reflects uncertainty in sampling. Frequentist 95% coverage means in a long sequence of identical trials, 95% of realized sets will cover the true parameter.\n",
        "\n",
        "**Bayesian intervals (credible sets)**: where frequentist interval $[L,U]$ covers $Œ∏$, the Bayesian setup allows us to say that $Œ∏$ is inside $[L,U]$ with probability assessed with respect to the posterior distribution of $Œ∏$.\n",
        "\n",
        "- For any set $A‚äÇŒò$, the **credible probability** of $A$ is $P(Œ∏‚ààA|\\v{x})=‚à´_AœÄ(Œ∏|\\v{x})\\ dŒ∏$, and $A$ is a **credible set** for $Œ∏$.\n",
        "\n",
        "  - Credible probability reflects experimenter's prior belief updated with data. Bayesian 95% coverage means the experimenter combines prior knowledge with data and is 95% sure.\n",
        "  \n",
        "  - The Bayesian credible probability of the frequentist 95% confidence interval approaches 0 as $n‚Üí‚àû$. The frequentist coverage probability of the Bayesian 95% credible interval approaches 0 as $n‚Üí‚àû$. (e9.2.17 gamma and e9.2.18 normal)\n"
      ],
      "metadata": {
        "id": "1rjydRYXomlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test inversion**: For each $Œ∏_0‚ààŒò$ let $A(Œ∏_0)$ be the acceptance region of a level-$Œ±$ test of $H_0:Œ∏=Œ∏_0$. For each $\\v{x}‚àà\\m{X}$ define random set $C(\\v{x})=\\{Œ∏_0:\\v{x}‚ààA(Œ∏_0)\\}‚äÜŒò$. Then $C(\\v{X})$ is a $1-Œ±$ confidence set. Conversely let $C(\\v{X})$ be a $1-Œ±$ confidence set then for any $Œ∏_0‚ààŒò$ define $A(Œ∏_0)=\\{\\v{x}:Œ∏‚ààC(\\v{x})\\}$. Then $A(Œ∏_0)$ is the acceptance region of a level-$Œ±$ test of $H_0:Œ∏=Œ∏_0$.\n",
        "(Casella 9.2.2)\n",
        "\n",
        "- Proof: Level-$Œ±$ test $P_{Œ∏_0}(\\v{X}‚àâA(Œ∏_0))‚â§a$ hence $P_{Œ∏_0}(\\v{X}‚ààA(Œ∏_0))>1-Œ±$.\n",
        "\n",
        "- Given sample $\\v{x}$, fix a level-$Œ±$ rejection region for $H_0:Œ∏=Œ∏_0$. Apply this same rejection region across all candidate values of $Œ∏_0$ and collect those $Œ∏_0$ for which $H_0$ is not rejected. The result set forms a $1-Œ±$ confidence set.\n",
        "\n",
        "- \"Inversion\": in hypothesis testing $A(Œ∏_0)=\\{\\v{x}:\\ ...\\}$, we fix $Œ∏_0$ and find what the regions of the sample space $\\v{x}‚àà\\m{X}$ lead to acceptance/rejection of $H_0:Œ∏=Œ∏_0$. In constructing confidence interval $C(\\v{x})=\\{Œ∏_0:\\ ...\\}$, we fix $\\v{x}$ and find what regions of the parameter space $Œ∏_0‚ààŒò$ lead to acceptance/rejection if we were to test $H_0:Œ∏=Œ∏_0$.\n",
        "\n",
        "**Pivotal quantity (pivot)** $Q(X_1,...,X_n,Œ∏)$ is a function of sample data $\\v{X}$ and parameter $Œ∏$ whose probability distribution does not depend on $Œ∏$. That is, if $\\v{X}‚àºf(\\v{x}|Œ∏)$ is known then $P_Œ∏(a‚â§Q(\\v{X},Œ∏)‚â§b)$ for all $Œ∏$ is known.\n",
        "(Casella 9.2.6)\n",
        "\n",
        "- $Q(\\v{X},Œ∏)$ is not a statistic because the function expression explicitly involves $Œ∏$. But if $Q$ is a pivot, its CDF and PDF/PMF do not contain $Œ∏$ in the expression and $Q$ behaves like a statistic.\n",
        "\n",
        "  - All test statistics are common pivots used in confidence intervals:\n",
        "  $\\/{\\bar{x}-Œº}{s/\\sqrt{n}}‚àºt_{n-1}$, $\\/{(n-1)s^2}{œÉ^2}‚àºœá_{n-1}^2$\n",
        "\n",
        "- Using the pivot, $P_Œ∏(a‚â§Q(\\v{X},Œ∏)‚â§b)‚â•1-Œ±$ gives $A(Œ∏_0)=\\{\\v{x}:a‚â§Q(\\v{x},Œ∏_0)‚â§b\\}$ to be the acceptance region for a level $Œ±$ test of $H_0:Œ∏=Œ∏_0$ and inverted form $C(\\v{x})=\\{Œ∏_0:a‚â§Q(\\v{x},Œ∏_0)‚â§b\\}$.\n",
        "\n",
        "  - If $Q(\\v{x},Œ∏)$ is a monotone function of $Œ∏$ then $C(\\v{x})$ is a confidence interval.\n",
        "  $C(\\v{x})=\\BC\n",
        "  L(\\v{x},a)‚â§Œ∏‚â§U(\\v{x},b) & \\t{if $Q(\\v{x},Œ∏)$ is increasing function of $Œ∏$}\\\\\n",
        "  L(\\v{x},b)‚â§Œ∏‚â§U(\\v{x},a) & \\t{if $Q(\\v{x},Œ∏)$ is decreasing function of $Œ∏$}\n",
        "  \\EC$.\n",
        "\n",
        "  - $a,b$ are solved using $P_Œ∏(a‚â§Q(\\v{X},Œ∏)‚â§b)=1-Œ±$, which is possible only if $Q(\\v{X},Œ∏)$ has a deterministic distribution independent of $Œ∏$.\n",
        "\n",
        "- $Q(T(\\v{X}),Œ∏)$ can be constructed from any statistic $T(\\v{X})$. PDF/PMF $f(t|Œ∏)$ often reveals the pivot. For pivot $Q(t,Œ∏)$ it appears in the form $f(t|Œ∏)=g(Q(t,Œ∏))|\\/{‚àÇ}{‚àÇt}Q(t,Œ∏)|$.\n",
        "\n",
        "  - Gamma\n",
        "  $f_{T=\\sum_iX_i}(t|Œª)=\\/{1}{Œª}\\/{e^{-t/Œª}(t/Œª)^{n-1}}{Œì(n)}$ shows\n",
        "  $t/Œª$\n",
        "  is a pivot.\n",
        "\n",
        "  - Normal\n",
        "  $f_{T=\\bar{X}}(t|Œº,œÉ)=\\/{1}{\\sqrt{2œÄœÉ^2/n}}\\e{-\\/{(t-Œº)^2}{2œÉ^2/n}}$ shows\n",
        "  $\\/{\\bar{x}-Œº}{œÉ}$\n",
        "  is a pivot.\n",
        "\n",
        "  - Length of confidence interval is proportional to the variance of $Q(T,Œ∏)$ used to construct it. If $T$ is sufficient for $Œ∏$, then Rao-Blackwell implies resulting confidence interval has minimum expected length as any other interval based on $\\v{X}$, since $T$ captures all information about $Œ∏$.\n",
        "\n",
        "**Pivoting a CDF**: By universality of uniform (probability integral transformation), $F_T(T|Œ∏)‚àº\\Unif(0,1)$ is a pivot. If $Œ±_1+Œ±_2=Œ±$, a level $Œ±$ acceptance region of hypothesis $H_0:Œ∏=Œ∏_0$ is\n",
        "$\\{t:Œ±_1‚â§F_T(t|Œ∏_0)‚â§1-Œ±_2\\}$.\n",
        "\n",
        "- **$T$ stochastically increasing**:\n",
        "increasing MLR $r(t)=\\/{f(t|Œ∏_1)}{f(t|Œ∏_0)}$ with $t$ where $Œ∏_1>Œ∏_0$.\n",
        "Larger $Œ∏$ ‚áí $f(t|Œ∏)$ shifts right ‚áí bigger $T$ ‚áí $P(T< t)$ smaller ‚áí $F_T(t|Œ∏)$ smaller.\n",
        "Location parameter $t-Œ∏$ shifts PDF to the right, scale parameter $\\/{t}{Œ∏}$ stretches PDF to the right, rate parameter $Œ∏t$ compresses PDF to the left.\n",
        "\n",
        "- **Pivoting continuous CDF**: Let $T$ be a statistic with continuous CDF $F_T(t|Œ∏)$ and $Œ±_1+Œ±_2=Œ±$. For $t‚àà\\m{T}$, $\\BC\n",
        "F_T(t|Œ∏_L(t))=Œ±_1 & F_T(t|Œ∏_U(t))=1-Œ±_2 & \\t{if $F_T(t|Œ∏)$ increases with $Œ∏$} \\\\\n",
        "F_T(t|Œ∏_L(t))=1-Œ±_2 & F_T(t|Œ∏_U(t))=Œ±_1  & \\t{if $F_T(t|Œ∏)$ decreases with $Œ∏$} \\\\\n",
        "\\EC$, then\n",
        "$[Œ∏_L(T),Œ∏_U(T)]$ is a $1-Œ±$ confidence interval for $Œ∏$.\n",
        "(Casella 9.2.12)\n",
        "\n",
        "  - $\\BC\n",
        "  ‚à´_{-‚àû}^tf_T(u|Œ∏_L(t))\\ du=Œ±_1 & ‚à´_t^{‚àû}f_T(u|Œ∏_U(t))\\ du=Œ±_2 & \\t{if $F_T(t|Œ∏)$ increases with $Œ∏$} \\\\\n",
        "  ‚à´_t^{‚àû}f_T(u|Œ∏_L(t))\\ du=Œ±_2 & ‚à´_{-‚àû}^tf_T(u|Œ∏_U(t))\\ du=Œ±_1 & \\t{if $F_T(t|Œ∏)$ decreases with $Œ∏$} \\\\\n",
        "  \\EC$.\n",
        "\n",
        "  - Proof: same behavior as $Q(\\v{x},Œ∏)$.\n",
        "\n",
        "- **Pivoting discrete CDF**: Let $T$ be a discrete statistic with CDF $F_T(t|Œ∏)=P_Œ∏(T‚â§t)$ and $Œ±_1+Œ±_2=Œ±$. For $t‚àà\\m{T}$, $\\BC\n",
        "P(T‚â§t|Œ∏_L(t))=Œ±_1 & P(T‚â•t|Œ∏_U(t))=Œ±_2 & \\t{if $F_T(t|Œ∏)$ increases with $Œ∏$} \\\\\n",
        "P(T‚â•t|Œ∏_L(t))=Œ±_2 & P(T‚â§t|Œ∏_U(t))=Œ±_1 & \\t{if $F_T(t|Œ∏)$ decreases with $Œ∏$} \\\\\n",
        "\\EC$, then\n",
        "$[Œ∏_L(T),Œ∏_U(T)]$ is a $1-Œ±$ confidence interval for $Œ∏$.\n",
        "(Casella 9.2.14)"
      ],
      "metadata": {
        "id": "b3C7m0cl9ztZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision theory**: After observing $\\v{X}=\\v{x}$, a hypothesis action $Œ¥(\\v{x})‚àà\\m{A}$ regarding $Œ∏‚ààŒò$ is made: a confidence interval is chosen $Œ¥(\\v{x})=C‚àà\\m{A}$ as an estimate of $Œ∏$.\n",
        "\n",
        "- The loss function $L(Œ∏,C)=b\\t{Length}(C)-I_C(Œ∏)$ for some weight $b$, and includes correctness $I_C(Œ∏)=I_{Œ∏‚ààC}$ and $\\t{Length}(C)$.\n",
        "\n",
        "**Highest density interval**: Let $f(x)$ be a unimodal PDF. If $[a,b]$ satisfies\n",
        "$‚à´_a^bf(x)\\ dx=1-Œ±$,\n",
        "$f(a)=f(b)>0$,\n",
        "and $a‚â§\\t{mode}(f)‚â§b$\n",
        "then\n",
        "$[a,b]$ is the shortest among all $1-Œ±$ intervals.\n",
        "(Casella 9.3.2)\n",
        "\n",
        "- Proof: The highest density interval across the peak where $f(a)=f(b)$ requires the shortest interval to hold the same probability.\n",
        "Let length $l=b(a)-a$\n",
        "$‚áí\\/{‚àÇl}{‚àÇa}=\\/{‚àÇb}{‚àÇa}-1=0$\n",
        "$‚áí\\/{‚àÇb}{‚àÇa}=1$.\n",
        "$‚à´_a^{b(a)}f_Y(y)\\ dy=1-Œ±$\n",
        "$‚áí\\/{‚àÇ}{‚àÇa}‚à´_a^{b(a)}f_Y(y)\\ dy=0$\n",
        "$=\\/{‚àÇb}{‚àÇa}f(b)-\\/{‚àÇa}{‚àÇa}f(a)+‚à´_a^{b(a)}\\/{‚àÇ}{‚àÇa}f_Y(y)\\ dy$\n",
        "$=\\/{‚àÇb}{‚àÇa}f(b)-\\/{‚àÇa}{‚àÇa}f(a)$\n",
        "$‚áíf(a)=f(b)$.\n",
        "\n",
        "- In general, say the interval $[\\/{1}{a},\\/{1}{b}]$, we can use the same strategy with $l=\\/{1}{b(a)}-\\/{1}{a}‚áí\\/{‚àÇl}{‚àÇa}=0$ and substituting into $\\/{‚àÇ}{‚àÇa}‚à´_a^{b(a)}f_Y(y)\\ dy=0$ then solve using Leibniz rule.\n",
        "\n",
        "- **Highest posterior density region**: If the posterior density $œÄ(Œ∏|\\v{x})$ is unimodal, then the shortest credible interval for $Œ∏$ for a given $Œ±$ is $\\{Œ∏:œÄ(Œ∏|\\v{x})‚â•k\\}$ where $‚à´_{\\{Œ∏:œÄ(Œ∏|\\v{x})‚â•k\\}}œÄ(Œ∏|\\v{x})\\ dŒ∏=1-Œ±$.\n",
        "(Casella 9.3.10)"
      ],
      "metadata": {
        "id": "8yWYZsot64G8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 9.2.1 (normal interval): $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$. Find confidence interval on $Œº$.\n",
        "Testing $H_0:Œº=Œº_0$ vs $H_1:Œº\\neqŒº_0$. Assuming $œÉ^2$ known.\n",
        "$Œõ=\\e{\\/{-1}{2œÉ^2}(\\sum_i(x_i-Œº_0)^2-\\sum_i(x_i-\\bar{x})^2)}$\n",
        "$=\\e{\\/{-1}{2œÉ^2}(\\sum_i(x_i-\\bar{x})^2+n(\\bar{x}-Œº_0)^2-\\sum_i(x_i-\\bar{x})^2)}$\n",
        "$=\\e{\\/{-n}{2œÉ^2}(\\bar{x}-Œº_0)^2}$.\n",
        "Therefore $œï=I(\\e{\\/{-n}{2œÉ^2}(\\bar{x}-Œº_0)^2}< c)$\n",
        "$=I(\\/{|\\bar{x}-Œº_0|}{œÉ/\\sqrt{n}}>\\sqrt{-2\\ln c})$.\n",
        "Test size is determined under $H_0$ to be\n",
        "$Œ±=P_{Œº_0}(|Z|>\\sqrt{-2\\ln c})$\n",
        "$‚áíz_{Œ±/2}=\\sqrt{-2\\ln c}$.\n",
        "Therefore\n",
        "$\\/{|\\bar{x}-Œº_0|}{œÉ/\\sqrt{n}}>z_{Œ±/2}$\n",
        "$‚áí-z_{Œ±/2}>\\/{\\bar{x}-Œº_0}{œÉ/\\sqrt{n}}>z_{Œ±/2}$\n",
        "$‚áí\\bar{x}-\\/{z_{Œ±/2}œÉ}{\\sqrt{n}}>Œº_0>\\bar{x}+\\/{z_{Œ±/2}œÉ}{\\sqrt{n}}$ is the rejection zone under $H_0$.\n",
        "The acceptance zone is the inverse:\n",
        "$\\bar{x}-\\/{z_{Œ±/2}œÉ}{\\sqrt{n}}‚â§\\red{Œº_0}‚â§\\bar{x}+\\/{z_{Œ±/2}œÉ}{\\sqrt{n}}$.\n",
        "In general\n",
        "$P_Œº(\\bar{x}-\\/{z_{Œ±/2}œÉ}{\\sqrt{n}}‚â§\\red{Œº}‚â§\\bar{x}+\\/{z_{Œ±/2}œÉ}{\\sqrt{n}})=1-Œ±$.\n",
        "Therefore $[\\bar{x}-\\/{z_{Œ±/2}œÉ}{\\sqrt{n}},\\bar{x}+\\/{z_{Œ±/2}œÉ}{\\sqrt{n}}]$ is a $1-Œ±$ confidence interval.\n",
        "\n",
        "  - 9.2.10 (normal pivot):\n",
        "  $\\/{\\bar{X}-Œº}{œÉ/\\sqrt{n}}‚àº\\Normal(0,1)$\n",
        "  and is therefore a pivot. If $œÉ^2$ is known then we can use\n",
        "  $P_Œº(-a‚â§\\red{\\/{\\bar{X}-Œº}{œÉ/\\sqrt{n}}}‚â§a)=1-Œ±$\n",
        "  where $a=z_{Œ±/2}$\n",
        "  to construct interval\n",
        "  $\\{Œº:\\bar{x}-z_{Œ±/2}\\/{œÉ}{\\sqrt{n}}‚â§Œº‚â§\\bar{x}+z_{Œ±/2}\\/{œÉ}{\\sqrt{n}}\\}$.\n",
        "  If $œÉ^2$ is unknown we can use\n",
        "  $P_Œº(-a‚â§\\red{\\/{\\bar{X}-Œº}{S/\\sqrt{n}}}‚â§a)=1-Œ±$, where\n",
        "  $a=t_{n-1,Œ±/2}$\n",
        "  to construct interval\n",
        "  $\\{Œº:\\bar{x}-t_{n-1,Œ±/2}\\/{s}{\\sqrt{n}}‚â§Œº‚â§\\bar{x}+t_{n-1,Œ±/2}\\/{s}{\\sqrt{n}}\\}$.\n",
        "  Since $\\/{(n-1)S^2}{œÉ^2}‚àºœá_{n-1}^2$ is a pivot, we can use\n",
        "  $P_{œÉ^2}(a‚â§\\red{\\/{(n-1)S^2}{œÉ^2}}‚â§b)=1-Œ±$\n",
        "  to construct interval\n",
        "  $\\{œÉ^2:\\/{(n-1)s^2}{b}‚â§œÉ^2‚â§\\/{(n-1)s^2}{a}\\}$.\n",
        "\n",
        "- 9.2.3 (inverting exponential LRT): $X_1,...,X_n‚àº\\Expo(Œª)$. Find confidence interval on $Œª$.\n",
        "Testing $H_0:Œª=Œª_0$ vs $H_1:Œª\\neqŒª_0$.\n",
        "$f(\\v{x}|Œª)=\\/{1}{Œª^n}e^{-\\/{1}{Œª}\\sum_ix_i}$.\n",
        "$‚àÇ_Œª\\ln f=\\/{-n}{Œª}+\\/{\\sum_ix_i}{Œª^2}$\n",
        "$‚áí\\hat{Œª}=\\bar{x}$.\n",
        "$Œõ=(\\/{\\bar{x}}{Œª_0})^ne^{-\\/{1}{Œª_0}\\sum_ix_i-n}$.\n",
        "The level-$Œ±$ test is\n",
        "$œï=I((\\/{\\sum_ix_i}{Œª_0})^ne^{-\\/{\\sum_ix_i}{Œª_0}}< k)$ with acceptance region $A(Œ∏_0)=\\{\\v{x}:(\\/{\\sum_ix_i}{Œª_0})^ne^{-\\/{\\sum_ix_i}{Œª_0}}‚â•k\\}$.\n",
        "Inverting the test gives the confidence interval\n",
        "$C(\\v{x})=\\{Œª:(\\/{S}{Œª})^ne^{-\\/{S}{Œª}}‚â•k\\}$\n",
        "$=\\{Œª:L‚â§Œª‚â§U\\}$ where $S=\\sum_ix_i$.\n",
        "Now we solve for bounds $L$ and $U$ by noting that $(\\/{S}{Œª})^ne^{-\\/{S}{Œª}}$ looks like a uni-modal exponential hump, whose bounds satisfy\n",
        "$(\\/{S}{L})^ne^{-\\/{S}{L}}=(\\/{S}{U})^ne^{-\\/{S}{U}}=k$.\n",
        "This is solved numerically given $\\v{x}$ and $Œ±$.\n",
        "\n",
        "  - 9.2.8 (gamma pivot): $X_1,...,X_n‚àº\\Expo(Œª)$, then $T=\\sum_iX_i$ is sufficient for $Œª$ and $T‚àº\\Gamma(n,Œª)$.\n",
        "  $f(t|Œª)=\\/{e^{-t/Œª}(t/Œª)^n}{tŒì(n)}$\n",
        "  $=\\/{1}{Œª}\\/{e^{-t/Œª}(t/Œª)^{n-1}}{Œì(n)}$, which is a scale family.\n",
        "  If $Q=2T/Œª$ then $t=Œªq/2$ and\n",
        "  $Q(T,Œª)‚àº\\Gamma(n,2)‚àºœá_{2n}^2$\n",
        "\n",
        "  - 9.2.9: $Q(T,Œª)=2T/Œª$.\n",
        "  Choose $a,b$ to satisfy $P_Œª(a‚â§œá_{2n}^2‚â§b)=1-Œ±$,\n",
        "  then the confidence interval is\n",
        "  $C(\\v{x})=\\{Œª: a‚â§\\/{2t}{Œª}‚â§b\\}$\n",
        "  $=\\{Œª: \\/{2t}{a}‚â•Œª‚â•\\/{2t}{b}\\}$.\n",
        "\n",
        "- 9.2.4 (normal one-sided bound): $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$. Find interval $C(\\v{x})=(-‚àû,U]$.\n",
        "Test $H_0:Œº=Œº_0$ vs $H_1:Œº< Œº_0$.\n",
        "$f(\\v{x}|Œº,œÉ^2)=\\/{1}{(\\sqrt{2œÄ}œÉ)^n}e^{-\\/{1}{2œÉ^2}\\sum_i(x_i-Œº)^2}$.\n",
        "$\\ln f=-\\/{n}{2}\\ln œÉ^2-\\/{1}{2œÉ^2}\\sum_i(x_i-Œº)^2$.\n",
        "$\\hat{Œº}=\\bar{X}$.\n",
        "$‚àÇ_{œÉ^2}\\ln f=-\\/{n/2}{œÉ^2}+\\/{\\sum_i(x_i-Œº)^2}{2œÉ^4}$\n",
        "$‚áí\\hat{œÉ}^2=\\/{\\sum_i(x_i-\\bar{x})^2}{n}$\n",
        "$=\\/{nS^2}{n-1}$.\n",
        "$œÉ_0^2=\\/{\\sum_i(x_i-Œº_0)^2}{n}$\n",
        "$=\\hat{œÉ}^2+(\\bar{x}-Œº_0)^2$\n",
        "LRT $Œõ=\\/{(œÉ_0^2)^{-n/2}}{(\\hat{œÉ}^2)^{-n/2}}\\e{\\/{-\\sum_i(x_i-Œº_0)^2}{2œÉ_0^2}-\\/{-\\sum_i(x_i-\\bar{x})^2}{2\\hat{œÉ}^2}}$\n",
        "$=(1+\\/{(\\bar{x}-Œº_0)^2}{\\hat{œÉ}^2})^{-n/2}$.\n",
        "The rejection region we want should have small $\\bar{x}$ relative to $Œº_0$ to support $H_1$:\n",
        "$(\\/{\\bar{x}-Œº_0}{S/\\sqrt{n}})^2>c$\n",
        "$‚áí\\/{\\bar{x}-Œº_0}{S/\\sqrt{n}}< -c$\n",
        "$‚áíT_{n-1}< -t_{n-1,Œ±}$.\n",
        "The acceptance region is\n",
        "$A(Œº_0)=\\{\\v{x}: \\bar{X}‚â•Œº_0-t_{n-1,Œ±}\\/{s}{\\sqrt{n}}\\}$.\n",
        "Inverted\n",
        "$C(\\v{x})=\\{Œº: Œº‚â§\\bar{X}+t_{n-1,Œ±}\\/{s}{\\sqrt{n}}\\}$.\n",
        "Therefore $(-‚àû,\\bar{X}+t_{n-1,Œ±}\\/{s}{\\sqrt{n}}]$ is the $1-Œ±$ one-sided confidence interval.\n",
        "\n",
        "- 9.2.7 (location-scale $\\bar{X}$-based pivots):\n",
        "  - $X_1,...,X_n‚àºf(x_i-Œº)$.\n",
        "  Let $Y_i=X_i-Œº$,\n",
        "  then by change of variables\n",
        "  $Y_i‚àºf(y_i)$\n",
        "  and $Q(\\v{X},Œº)=\\bar{X}-Œº=\\bar{Y}$\n",
        "  only depends on $f$ and $n$.\n",
        "  - $X_1,...,X_n‚àº\\/{1}{œÉ}f(\\/{x_i}{œÉ})$.\n",
        "  Let $Y_i‚àº\\/{x_i}{œÉ}$,\n",
        "  then by change of variables\n",
        "  $|\\/{‚àÇ_x}{‚àÇ_y}|=œÉ$ and\n",
        "  $Y_i‚àºf(y_i)$, and\n",
        "  $Q(\\v{x},œÉ)=\\/{\\bar{X}}{œÉ}=\\bar{Y}$\n",
        "  only depends on $f$ and $n$.\n",
        "  - $X_1,...,X_n‚àº\\/{1}{œÉ}f(\\/{x_i-Œº}{œÉ})$.\n",
        "  Let $Y_i‚àº\\/{x_i-Œº}{œÉ}$ then\n",
        "  $Q(\\v{x},Œº,œÉ)=\\/{\\bar{X}-Œº}{œÉ}=\\bar{Y}$\n",
        "  only depends on $f$ and $n$.\n",
        "  - $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$ then\n",
        "  $\\/{\\bar{X}-Œº}{S/\\sqrt{n}}$ does not depend on $Œº$ or $œÉ^2$.\n",
        "\n",
        "- 9.2.13 (location exponential CDF pivot): $X_1,...,X_n‚àºf(x_i|Œº)=e^{-(x_i-Œº)}I_{x_i‚â•Œº}$ has sufficient statistic $Y=X_{(1)}$ with Expo(1/n) PDF $f(y|Œº)=ne^{-n(y-Œº)}I_{y‚â•Œº}$, which is stochastically increasing.\n",
        "$Œ±_2=‚à´_y^‚àûf(u|Œº_L)\\ du$\n",
        "$=‚à´_y^‚àûne^{-n(u-Œº_L)}\\ du$\n",
        "$=[-e^{-n(u-Œº_L)}]_y^‚àû$\n",
        "$=e^{-n(y-Œº_L)}$\n",
        "$‚áíŒº_L=y+\\/{1}{n}\\ln(Œ±_2)$\n",
        "and\n",
        "$Œ±_1=‚à´_{-‚àû}^yf(u|Œº_U)\\ du$\n",
        "$=‚à´_{Œº_U}^yne^{-n(u-Œº_U)}\\ du$\n",
        "$=[-e^{-n(u-Œº_U)}]_{Œº_U}^y$\n",
        "$=1-e^{-n(y-Œº_U)}$\n",
        "$‚áíŒº_U=y+\\/{1}{n}\\ln(1-Œ±_1)$.\n",
        "Therefore with $Œ±_1=Œ±_2=\\/{Œ±}{2}$, the interval is\n",
        "$C(Y)=\\{Œº:Y+\\/{1}{n}\\ln(\\/{Œ±}{2})‚â§Œº‚â§Y+\\/{1}{n}\\ln(1-\\/{Œ±}{2})\\}$.\n",
        "\n",
        "- 9.2.15 (Poisson CDF pivot): $X_1,...,X_n‚àº\\Pois(Œª)$. Let $Y=\\sum_iX_i‚àº\\Pois(nŒª)$. If $Y=y_0$ is observed, find $1-Œ±$ confidence interval.\n",
        "Poisson has increasing MLR (e8.25) and therefore decreasing CDF with $Œª$.\n",
        "Lower bound\n",
        "$Œ±_2=P(Y‚â•y_0|Œª_L)$\n",
        "$=P(X‚â§2nŒª|Œª_L)$\n",
        "where $X‚àº\\Gamma(y_0+1,2)‚àºœá_{2(y_0+1)}^2$\n",
        "$‚áíŒ±_2=P(œá_{2(y_0+1)}^2‚â§2nŒª_L)$\n",
        "$‚áíŒª_L=\\/{1}{2n}œá_{2(y_0+1),Œ±_2}^2$.\n",
        "Upper bound\n",
        "$Œ±_1=P(Y‚â§y_0|Œª_U)$\n",
        "$=P(œá_{2(y_0+1)}^2‚â•2nŒª_U)$ and\n",
        "$Œª_U=\\/{1}{2n}œá_{2(y_0+1),1-Œ±_1}^2$.\n",
        "Therefore with $Œ±_1=Œ±_2=\\/{Œ±}{2}$, the interval is\n",
        "$C(Y)=\\{Œª:\\/{1}{2n}œá_{2(Y+1),Œ±_2}^2‚â§Œª‚â§\\/{1}{2n}œá_{2(Y+1),1-Œ±_1}^2\\}$.\n",
        "\n",
        "  - 9.2.16 (Poisson credible set):\n",
        "  $Œª‚àº\\Gamma(a,b)$ and\n",
        "  $f(y|Œª)=\\/{e^{-nŒª}(nŒª)^y}{y!}$.\n",
        "  Then\n",
        "  $œÄ(Œª|y)‚àù\\/{e^{-nŒª}(nŒª)^y}{y!}\\/{e^{-Œª/b}(Œª/b)^{a-1}}{bŒì(a)}$\n",
        "  $‚àùe^{-Œª(n+1/b)}(Œª(n+1/b))^{y+a-1}$.\n",
        "  Therefore\n",
        "  $Œª|y‚àº\\Gamma(a+y,\\/{1}{n+1/b})$\n",
        "  $‚áí2(n+1/b)Œª|\\sum_iX_i‚àº\\Gamma(a+\\sum_iX_i,2)$\n",
        "  $‚àºœá_{2(a+\\sum_iX_i)}^2$.\n",
        "  One way to form a credible set is to split $Œ±$\n",
        "  to get\n",
        "  $C(\\sum_iX_i)=\\{Œª:\\/{b}{2(nb+1)}œá_{2(a+\\sum_iX_i),Œ±/2}^2‚â§Œª‚â§\\/{b}{2(nb+1)}œá_{2(a+\\sum_iX_i),1-Œ±/2}^2\\}$\n",
        "\n",
        "- 9.3.3: for pivot $\\/{\\bar{X}-Œº}{S/\\sqrt{n}}$, the interval is $C(\\v{x})=[\\bar{x}-b\\/{s}{\\sqrt{n}},\\bar{x}-a\\/{s}{\\sqrt{n}}]$ where $a=-t_{n-1,Œ±/2}$ and $b=t_{n-1,Œ±/2}$.\n",
        "The length of interval is $(b-a)\\/{s}{\\sqrt{n}}$.\n",
        "\n",
        "- 9.4: $X_1,...,X_n‚àº\\Normal(0,œÉ_X^2)$ and $Y_1,...,Y_m‚àº\\Normal(0,œÉ_Y^2)$ independent. $Œª=\\/{œÉ_Y^2}{œÉ_X^2}$. Find LRT of $H_0:Œª=Œª_0$ vs $H_1:Œª\\neqŒª_0$.\n",
        "$f(\\v{x},\\v{y}|œÉ_X^2,œÉ_Y^2)$\n",
        "$=\\/{C}{œÉ_X^nœÉ_Y^m}\\e{\\/{-\\sum_iX_i^2}{2œÉ_X^2}+\\/{-\\sum_jY_j^2}{2œÉ_Y^2}}$.\n",
        "$\\hat{œÉ}_X^2=\\/{\\sum_iX_i^2}{n}$,\n",
        "$\\hat{œÉ}_Y^2=\\/{\\sum_iY_i^2}{m}$.\n",
        "Under $H_0$:\n",
        "$f_0(\\v{x},\\v{y}|œÉ_X^2)$\n",
        "$=\\/{C}{Œª_0œÉ_X^{n+m}}\\e{\\/{-\\sum_iX_i^2}{2œÉ_X^2}+\\/{-\\sum_jY_j^2}{2Œª_0œÉ_X^2}}$.\n",
        "$‚àÇ_{œÉ_X^2}=\\/{-(n+m)/2}{œÉ_X^2}+\\/{\\sum_iX_i^2}{2œÉ_X^4}+\\/{\\sum_jY_j^2}{2Œª_0œÉ_X^4}$.\n",
        "$\\hat{œÉ}_{X,0}^2=\\/{\\sum_iX_i^2+\\sum_jY_j^2/Œª_0}{n+m}$.\n",
        "LRT\n",
        "$Œõ=\\/{f_0(\\v{x},\\v{y}|\\hat{œÉ}_{X,0}^2)}{f(\\v{x},\\v{y}|\\hat{œÉ}_X^2,\\hat{œÉ}_Y^2)}$\n",
        "$=\\/{(\\hat{œÉ}_X^2)^{n/2}(\\hat{œÉ}_Y^2)^{m/2}}{Œª_0^{m/2}(\\hat{œÉ}_{X,0}^2)^{(m+n)/2}}$.\n",
        "\n",
        "- 9.6 (10.34): $X_1,...,X_n‚àº\\t{Bernoulli}(p)$ iid. $H_0:p=p_0$ vs $H_1:p\\neq p_0$.\n",
        "Let $Y=\\sum_iX_i$ then\n",
        "$f(\\v{x}|p)=p^{y}(1-p)^{n-y}$.\n",
        "$‚àÇ_p \\ln f=\\/{y}{p}-\\/{n-y}{1-p}$\n",
        "$‚áí\\hat{p}=\\/{y}{n}$.\n",
        "LRT\n",
        "$Œõ=(\\/{p_0}{\\bar{x}})^{y}(\\/{1-p_0}{1-\\bar{x}})^{n-y}$.\n",
        "Then acceptance region is\n",
        "$A(p_0)=\\{\\v{x}:(\\/{p_0}{\\bar{x}})^{y}(\\/{1-p_0}{1-\\bar{x}})^{n-y}‚â•k\\}$\n",
        "and the interval is\n",
        "$C(y)=\\{p:(\\/{p}{\\bar{x}})^{y}(\\/{1-p}{1-\\bar{x}})^{n-y}‚â•k\\}$."
      ],
      "metadata": {
        "id": "DClt9sGRFTEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asymptotics"
      ],
      "metadata": {
        "id": "jbbaQ6_8c8WA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estimators consistency**: A sequence of estimators $W_n=W_n(X_1,...,X_n)$ is a **consistent sequence** of estimators for parameter $Œ∏$ if for every $œµ>0$ and every $Œ∏‚ààŒò$, $\\liml_{n‚Üí‚àû}P_Œ∏(|W_n-Œ∏|< œµ)=1$ (i.e., $W_n\\arr{p}Œ∏$)\n",
        "(Casella 10.1.1)\n",
        "\n",
        "- If $W_n$ is a sequence of estimators of $Œ∏$ satisfying $\\liml_{n‚Üí‚àû}\\E[W_n]=Œ∏$ and $\\liml_{n‚Üí‚àû}\\Var(W_n)=0$ for every $Œ∏‚ààŒò$,\n",
        "then $W_n$ is a consistent sequence.\n",
        "(Casella 10.1.3)\n",
        "\n",
        "  - Proof: By Chebychev $P_Œ∏(|W_n-Œ∏|‚â•œµ)‚â§\\/{\\Var_Œ∏(W_n-Œ∏)}{œµ^2}$\n",
        "  $=\\/{\\Var_Œ∏(W_n)}{œµ^2}$\n",
        "  $=\\/{\\E_Œ∏[(W_n-Œ∏)^2]}{œµ^2}$\n",
        "  $=\\/{\\Var_Œ∏(W_n)+(\\Bias_Œ∏(W_n))^2}{œµ^2}$\n",
        "\n",
        "- Let $W_n$ be consistent sequence of estimators for $Œ∏$. If $\\liml_{n‚Üí‚àû}a_n=1$ and $\\liml_{n‚Üí‚àû}b_n=0$, then sequence $U_n=a_nW_n+b_n$ is consistent sequence of estimators for $Œ∏$.\n",
        "(Casella 10.1.5)\n",
        "\n",
        "- **MLE consistency**: Let $X_1,...,X_n‚àºf(x_i|Œ∏)$ iid and let $œÑ(Œ∏)$ be a continuous function of $Œ∏$. For every $œµ>0$ and every $Œ∏‚ààŒò$, $\\liml_{n‚Üí‚àû}P_Œ∏(|œÑ(\\hat{Œ∏})-œÑ(Œ∏)|‚â•œµ)=0$ (i.e., $œÑ(\\hat{Œ∏})\\arr{p}œÑ(Œ∏)$).\n",
        "(Casella 10.1.6)\n",
        "\n",
        "  - Proof: from sample $\\v{x}$ (where $x_i$ is a single observation) we construct $l(Œ∏|\\v{x})$ and find $\\hat{Œ∏}$. By LLN, the average likelihood\n",
        "  $\\/{1}{n}l(Œ∏|\\v{x})=\\/{\\sum_il(Œ∏|x_i)}{n}$\n",
        "  $\\arr{p}\\E_Œ∏[\\ln f(x_i|Œ∏)]$\n",
        "  and therefore\n",
        "  $\\hat{Œ∏}\\arr{p}Œ∏$.\n",
        "\n",
        "  - $l(Œ∏|\\v{x})=\\suml_{i=1}^nl(Œ∏|x_i)$ is a sum of sample mountains that peaks at MLE $\\hat{Œ∏}$. The average of all possible sample mountains $\\/{\\sum_il(Œ∏|x_i)}{n}$ converges to the \"true mountain\" $\\E[\\ln f(x_i|Œ∏)]$, and under $H_0:Œ∏=Œ∏_0$ peaks at $Œ∏_0$.\n",
        "\n",
        "**Estimators efficiency**: For estimator $T_n$ and constants $\\{k_n\\}$, if $\\liml_{n‚Üí‚àû}k_n\\Var(T_n)=r^2< ‚àû$, then $r^2$ is the **limit of variance** of $T_n$.\n",
        "(Casella 10.1.7)\n",
        "Suppose $k_n(T_n-œÑ(Œ∏))\\arr{d}\\Normal(0,œÉ^2)$, then $œÉ^2$ is the **asymptotic variance** or variance of the limit distribution of $T_n$.\n",
        "(Casella 10.1.9)\n",
        "Suppose $\\green{\\sqrt{n}(T_n-œÑ(Œ∏))\\arr{d}\\Normal(0,v(Œ∏))}$, then $T_n$ is **asymptotically efficient** if its asymptotic variance attains Cram√©r-Rao lower bound\n",
        "$\\red{v(Œ∏)=\\/{[\\/{d}{dŒ∏}œÑ(Œ∏)]^2}{\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(x_i|Œ∏))^2]}}$.\n",
        "(Casella 10.1.11)\n",
        "\n",
        "- The limit of the variance is the exact finite-$n$ variance of the estimator taken to $n‚Üí‚àû$: $\\liml_{n‚Üí‚àû}\\Var(T_n)$. The asymptotic variance is of the normal distribution that $T_n$ converges to when $n‚Üí‚àû$: $\\Var(g(T_n))=[g'(Œ∏)]^2\\Var(T_n)$.\n",
        "\n",
        "- **Expected information number** or Fisher information (1) denominates the variance of all asymptotically efficient estimators (2) is variance of score function\n",
        "$I_n(Œ∏)=\\Var_Œ∏(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))$\n",
        "$=\\E_Œ∏[(\\/{‚àÇ}{‚àÇŒ∏}\\ln f(\\v{x}|Œ∏))^2]$\n",
        "$=-\\E_Œ∏[\\/{‚àÇ^2}{‚àÇŒ∏^2}\\ln f(\\v{x}|Œ∏)]$ (7.3.11)\n",
        "$=-n\\E_Œ∏[\\/{‚àÇ^2}{‚àÇŒ∏^2}\\ln f(x_i|Œ∏)]$ (7.3.10)\n",
        "or with likelihood\n",
        "$I_n(Œ∏)=\\E_Œ∏[(l'(Œ∏|\\v{x}))^2]$\n",
        "$=-\\E_Œ∏[l''(Œ∏|\\v{x})]$\n",
        "$=-n\\E_Œ∏[l''(Œ∏|x_i)]$\n",
        "$=n\\E_Œ∏[(l'(Œ∏|x_i))^2]$\n",
        "$=nI_1(Œ∏)$.\n",
        "\n",
        "  - The score function is centered at 0:\n",
        "  $\\E_Œ∏[l'(Œ∏|\\v{x})]=0$. (7.3.9)\n",
        "  Intuitively the slope goes up, plateaus at $l'(\\hat{Œ∏}|\\v{x})=0$, then goes down. The weighted average slope is 0.\n",
        "\n",
        "- **MLE asymptotic efficiency**: Let $X_1,...,X_n‚àºf(x_i|Œ∏)$ iid where $f$ is $C^3$ and respect Cram√©r-Rao regularity conditions. Let $œÑ(Œ∏)$ be continuous, then $\\blue{\\sqrt{n}[œÑ(\\hat{Œ∏})-œÑ(Œ∏)]\\arr{d}\\Normal(0,v(Œ∏))}$ where $v(Œ∏)$ is CRLB.\n",
        "(Casella 10.1.12)\n",
        "\n",
        "  - Proof: Let $l'(Œ∏)=\\/{‚àÇ}{‚àÇŒ∏}\\ln L(Œ∏|\\v{x})$. MLE $\\hat{Œ∏}$ is a consistent estimator of $Œ∏$, and therefore is in the neighborhood of $Œ∏$. Then by Taylor expansion\n",
        "  $l'(\\hat{Œ∏}|\\v{x})‚âàl'(Œ∏|\\v{x})+(\\hat{Œ∏}-Œ∏)l''(Œ∏|\\v{x})=0$\n",
        "  $‚áí\\sqrt{n}(\\hat{Œ∏}-Œ∏)=-\\sqrt{n}\\/{l'(Œ∏|\\v{x})}{l''(Œ∏|\\v{x})}$\n",
        "  $=\\/{-\\/{1}{\\sqrt{n}}l'(Œ∏|\\v{x})}{\\/{1}{n}l''(Œ∏|\\v{x})}$.\n",
        "  Denominator uses LLN\n",
        "  $\\/{1}{n}l''(Œ∏|\\v{x})$\n",
        "  $=\\/{\\sum_il''(Œ∏|x_i)}{n}$\n",
        "  $\\arr{p}\\E_Œ∏[l''(Œ∏|x_i)]$\n",
        "  $=-I_1(Œ∏)$.\n",
        "  Numerator uses CLT\n",
        "  $\\/{-1}{\\sqrt{n}}l'(Œ∏|\\v{x})$\n",
        "  $=-\\sqrt{n}\\/{l'(Œ∏|\\v{x})}{n}$\n",
        "  $=-\\sqrt{n}(\\/{\\sum_{i=1}^nl'(Œ∏|x_i)}{n}-0)$\n",
        "  $\\arr{d}-\\Normal(0,I_1(Œ∏))$.\n",
        "  By Slutsky's theorem\n",
        "  $\\sqrt{n}(\\hat{Œ∏}-Œ∏)\\arr{d}\\/{-\\Normal(0,I_1(Œ∏))}{-I_1(Œ∏)}$\n",
        "  $=\\Normal(0,\\/{1}{I_1(Œ∏)})$,\n",
        "  and by Delta Method\n",
        "  $\\sqrt{n}(œÑ(\\hat{Œ∏})-œÑ(Œ∏))\\arr{d}\\Normal(0,\\/{[œÑ'(Œ∏)]^2}{I_1(Œ∏)})$\n",
        "  $=\\Normal(0,v(Œ∏))$.\n",
        "\n",
        "  - Delta Method $\\green{\\sqrt{n}(œÑ(\\hat{Œ∏})-œÑ(Œ∏))\\arr{d}\\Normal(0,\\/{[œÑ'(Œ∏)]^2}{I_1(Œ∏)})}$\n",
        "  vs Cram√©r-Rao\n",
        "  $\\green{œÑ(\\hat{Œ∏})-œÑ(Œ∏)\\arr{d}\\Normal(0,\\/{[œÑ'(Œ∏)]^2}{I_n(Œ∏)})}$\n",
        "\n",
        "- **Observed information number**: the asymptotic variance\n",
        "$\\Var(h(\\hat{Œ∏}))=\\/{[h'(Œ∏)]^2}{I_n(Œ∏)}$\n",
        "uses unknown $Œ∏$ in\n",
        "$I_n(Œ∏)=-\\E_Œ∏[l''(Œ∏|\\v{x})]$.\n",
        "Instead we use\n",
        "$\\red{\\widehat{\\Var}_Œ∏(h(\\hat{Œ∏}))‚â°\\/{[h'(\\hat{Œ∏})]^2}{\\hat{I}_n(\\hat{Œ∏})}}$ where\n",
        "$\\red{\\hat{I}_n(\\hat{Œ∏})=-l''(Œ∏|\\v{x})|_{Œ∏=\\hat{Œ∏}}\\arr{p}I_n(Œ∏)}$ is a consistent estimator of $I_n(Œ∏)$.\n",
        "\n",
        "- **Asymptotic relative efficiency**: $\\sqrt{n}[W_n-Œº]\\arr{d}\\Normal(0,œÉ_W^2)$ and $\\sqrt{n}[V_n-Œº]\\arr{d}\\Normal(0,œÉ_V^2)$ ‚áí $\\t{ARE}(V_n,W_n)=\\/{œÉ_W^2}{œÉ_V^2}$."
      ],
      "metadata": {
        "id": "bQDsbeY3dE9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Asymptotic hypothesis testing and confidence intervals**: Asymptotic test properties are accurate in large samples.\n",
        "\n",
        "- **Wilks' Theorem**: Testing $H_0:Œ∏=Œ∏_0$ vs $H_1:Œ∏\\neqŒ∏_0$ with $P_{Œ∏_0}(Œõ(\\v{X})‚â§c_Œ±)‚â§Œ±$ where $Œõ(\\v{x})=\\/{L(Œ∏_0|\\v{x})}{L(\\hat{Œ∏}|\\v{x})}$, then under $H_0$ as $n‚Üí‚àû$, $\\red{-2\\ln Œõ(\\v{X})\\arr{d}œá_1^2}$.\n",
        "(Casella 10.3.1)\n",
        "\n",
        "  - Wilks' Theorem says\n",
        "  $\\blue{P_{Œ∏_0}(Œõ‚â§c_Œ±)=\\liml_{n‚Üí‚àû}P_{Œ∏_0}(-2\\lnŒõ>œá_{1,Œ±}^2)=Œ±}$\n",
        "  $‚áí\\red{c_Œ±=e^{-\\/{1}{2}œá_{1,Œ±}^2}}$ without having to know the distribution of $Œõ$ to solve for $c_Œ±$. For size-0.05 test $c_Œ±‚âà0.14$.\n",
        "\n",
        "  - Proof: $-2\\lnŒõ=-2[l(Œ∏_0|\\v{x})-l(\\hat{Œ∏}|\\v{x})]$. If $H_0$ is true then $Œ∏_0$ and $\\hat{Œ∏}$ converge. We expand $l(Œ∏_0|\\v{x})$ in the neighborhood of $\\hat{Œ∏}$. Because $l'(\\hat{Œ∏}|\\v{x})=0$, second-order Taylor expansion\n",
        "  $l(Œ∏_0)‚âàl(\\hat{Œ∏})+\\/{l''(\\hat{Œ∏}|\\v{x})}{2}(Œ∏_0-\\hat{Œ∏})^2$\n",
        "  $‚áí-2[l(Œ∏_0)-l(\\hat{Œ∏})]=-l''(\\hat{Œ∏}|\\v{x})(Œ∏_0-\\hat{Œ∏})^2$\n",
        "  $=-\\/{l''(\\hat{Œ∏}|\\v{x})}{n}[\\sqrt{n}(\\hat{Œ∏}-Œ∏_0)]^2$\n",
        "  $=-\\/{\\sum_il''(\\hat{Œ∏}|x_i)}{n}[\\sqrt{n}(\\hat{Œ∏}-Œ∏_0)]^2$\n",
        "  $\\arr{p}-\\E_{Œ∏_0}[l''(\\hat{Œ∏}|x_i)][\\Normal(0,\\/{1}{I_1(Œ∏_0)})]^2$\n",
        "  $\\arr{p}[\\sqrt{I_1(Œ∏_0)}\\Normal(0,\\/{1}{I_1(Œ∏_0)})]^2$\n",
        "  $=Z^2=œá_1^2$.\n",
        "\n",
        "  - **General Wilks' Theorem**: $H_0:Œ∏‚ààŒò_0$ is rejected if $\\blue{-2\\ln Œõ(\\v{X})‚â•œá_{v,Œ±}^2}$, where $v$ is the difference between the number of free parameters specified by $Œ∏‚ààŒò_0$ and free parameters specified by $Œ∏‚ààŒò$.\n",
        "  (Casella 10.3.3)\n",
        "\n",
        "    - E.g., $Œò:Œº$ vs $H_0:Œº=5$ then $v=1-0=1$, $Œò:Œº,œÉ^2$ vs $H_1:Œº=0$ then $v=2-1=1$, $Œò:Œº,œÉ^2$ vs $H_1:Œº=0,œÉ^2=1$ then $v=2-0=2$.\n",
        "\n",
        "- **Wald Test (Z Test)**: To test $H_0:Œ∏=Œ∏_0$, start with an asymptotically normal estimator $W_n(\\v{X})\\arr{p}Œ∏$. The **Z Test** under $H_0$ is\n",
        "$Z_n=\\/{W_n-Œ∏_0}{œÉ_n}\\arr{d}\\Normal(0,1)$.\n",
        "Use Slutsky's Theorem to get the **Wald Test**:\n",
        "$\\blue{Z_n=\\/{W_n-Œ∏_0}{S_n}\\arr{d}\\Normal(0,1)}$ and $œï(\\v{x})=I(|Z_n|>z_{Œ±/2})$.\n",
        "The estimator is not required to be efficient (e.g., MoM estimators).\n",
        "\n",
        "  - **Standard error $S_n$**:\n",
        "  If using MLE $\\sqrt{n}(\\hat{Œ∏}-Œ∏)\\arr{d}\\Normal(0,\\/{1}{I_1(Œ∏)})$ then use expected information\n",
        "  $S_n=\\left.\\sqrt{\\/{1}{nI_1(Œ∏)}}\\right|_{Œ∏=\\hat{Œ∏}}$\n",
        "  where\n",
        "  $I_1(Œ∏)=-E_Œ∏[l''(Œ∏|x_i)]$,\n",
        "  or use observed information\n",
        "  $S_n=\\left.\\sqrt{\\/{1}{\\hat{I}_n(Œ∏)}}\\right|_{Œ∏=\\hat{Œ∏}}$ where\n",
        "  $\\hat{I}_n(Œ∏)=-l''(Œ∏|\\v{x})$.\n",
        "\n",
        "  - If $H_0$ is true, $\\hat{Œ∏}‚ÜíŒ∏_0$, $S_n‚Üí0$, and $Z_n\\arr{d}\\Normal(0,1)$. Rejection region $œï(\\v{x})=I(|Z_n|>z_{Œ±/2})$.\n",
        "  If $H_0$ is false, $\\hat{Œ∏}‚ÜíŒ∏_1\\neqŒ∏_0$, $S_n‚Üí0$, $Z_n‚Üí¬±‚àû$, and $Œ≤(Œ∏_1)=P_{Œ∏_1}(|Z_n|>z_{Œ±/2})‚Üí1$.\n",
        "\n",
        "  - For one-sided hypotheses $H_0:Œ∏‚â§Œ∏_0$, the Wald test or Z test are constructed on the $H_0$ boundary $Œ∏_0$.\n",
        "\n",
        "- **Score Test (Lagrange Multiplier Test)**: The score function is\n",
        "$S(Œ∏)=l'(Œ∏|\\v{x})$\n",
        "such that $S(\\hat{Œ∏})=0$.\n",
        "If $H_0:Œ∏=Œ∏_0$ is true then\n",
        "$S(Œ∏_0)\\arr{p}0$\n",
        "and the **Score Test** is\n",
        "$\\blue{Z_S=\\/{S(Œ∏_0)}{\\sqrt{I_n(Œ∏_0)}}\\arr{d}\\Normal(0,1)}$ and $œï(Œ∏)=I(|Z_S|>z_{Œ±/2})$.\n",
        "\n",
        "  - $Z_S=\\/{S(Œ∏_0)-\\E_{Œ∏_0}[S(Œ∏)]}{\\sqrt{\\Var_Œ∏(S(Œ∏))}}$\n",
        "  $=\\/{S(Œ∏_0)}{\\sqrt{I_n(Œ∏_0)}}$ where $I_n=-\\E_Œ∏[l''(Œ∏|\\v{x})]$.\n",
        "  \n",
        "  - The constrained optimization problem:\n",
        "  Maximize $l(Œ∏|\\v{x})$ constrained by $g(Œ∏)=Œ∏-Œ∏_0=0$. The Lagrangian is $\\m{L}(Œ∏,Œª)=l(Œ∏|\\v{x})-Œªg(Œ∏)$\n",
        "  $‚áí\\m{L}'(Œ∏,Œª)=l'(Œ∏|\\v{x})-Œªg'(Œ∏)=0$\n",
        "  $‚áíl'(\\hat{Œ∏}_0|\\v{x})=Œªg'(\\hat{Œ∏}_0)$.\n",
        "  If $H_0$ is true, then\n",
        "  $\\hat{Œ∏}_0=\\hat{Œ∏}$\n",
        "  $‚áíl'(\\hat{Œ∏}_0|\\v{x})=0$\n",
        "  $‚áíŒª=0$.\n",
        "  If $H_0$ is false, then\n",
        "  $l'(\\hat{Œ∏}_0|\\v{x})\\neq0$\n",
        "  $‚áíŒª\\neq0$.\n",
        "\n",
        "- **Wald-type interval**: by MLE efficiency\n",
        "$\\/{h(\\hat{Œ∏})-h(Œ∏)}{\\sqrt{\\widehat{\\Var}_Œ∏(h(\\hat{Œ∏}))}}\\arr{d}\\Normal(0,1)$\n",
        "with\n",
        "$\\widehat{\\Var}_Œ∏(h(\\hat{Œ∏}))‚âà\\/{[h'(\\hat{Œ∏})]^2}{-l''(Œ∏|\\v{x})|_{Œ∏=\\hat{Œ∏}}}$ and $Œ∏$ being the true value.\n",
        "Then the $1-Œ±$ confidence interval is\n",
        "$C(\\v{x})=\\{h(Œ∏):\\blue{h(Œ∏)‚ààh(\\hat{Œ∏})¬±z_{Œ±/2}\\sqrt{\\widehat{\\Var}_Œ∏(h(\\hat{Œ∏}))}}\\}$.\n",
        "\n",
        "- **Score interval**: Let $Q(\\v{X}|Œ∏)=\\/{S(Œ∏)}{\\sqrt{I_n(Œ∏)}}\\arr{d}\\Normal(0,1)$. Then the $1-Œ±$ confidence interval is\n",
        "$C(\\v{x})=\\{Œ∏:\\blue{|Q(\\v{X}|Œ∏)|‚â§z_{Œ±/2}}\\}$.\n",
        "These have short length, but difficult to isolate $h(Œ∏)$ from the inequality.\n",
        "\n",
        "- **Wilks' LRT inversion**:\n",
        "$A(Œ∏_0)=\\{\\v{x}:-2\\ln(\\/{L(Œ∏_0|\\v{x})}{L(\\hat{Œ∏}|\\v{x})})‚â§œá_{1,Œ±}^2\\}$ is inverted to get\n",
        "$\\{Œ∏:-2\\ln(\\/{L(Œ∏|\\v{x})}{L(\\hat{Œ∏}|\\v{x})})‚â§œá_{1,Œ±}^2\\}$\n",
        "or $C(Œ∏)=\\{Œ∏:\\blue{\\/{L(Œ∏|\\v{x})}{L(\\hat{Œ∏}|\\v{x})}‚â•e^{-\\/{1}{2}œá_{1,Œ±}^2}}\\}$.\n",
        "For 95% confidence interval the right side is ~0.14."
      ],
      "metadata": {
        "id": "oU4Yq_sYRrWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 10.1.2 ($\\bar{X}_n$ consistency): $X_1,...,X_n‚àº\\Normal(Œ∏,1)$ iid.\n",
        "$P(|\\bar{X}_n-Œ∏|< œµ)=\\/{1}{\\sqrt{2œÄ/n}}‚à´_{Œ∏-œµ}^{Œ∏+œµ}e^{-\\/{n(x-Œ∏)^2}{2}}\\ dx$.\n",
        "Let $y=\\sqrt{n}(x-Œ∏)$ and $dy=\\sqrt{n}\\ dx$ then\n",
        "$P(|\\bar{X}_n-Œ∏|< œµ)=\\/{1}{\\sqrt{2œÄ}}‚à´_{-\\sqrt{n}œµ}^{\\sqrt{n}œµ}e^{-\\/{y^2}{2}}\\ dy$\n",
        "$=Œ¶(\\sqrt{n}œµ)-Œ¶(-\\sqrt{n}œµ)$\n",
        "$\\arr{p}1$.\n",
        "\n",
        "  - 10.1.4: $\\E[\\bar{X}_n]=Œ∏$ unbiased. $\\Var(\\bar{X}_n)=\\/{1}{n}‚Üí0$. Therefore consistent.\n",
        "\n",
        "- 10.1.8/10.5: If $T_n=\\bar{X}_n$, then $n\\Var(T_n)‚ÜíœÉ^2$ is the limiting variance of $T_n$.\n",
        "For $T_n=\\/{1}{\\bar{X}_n}$, the exact variance is\n",
        "$\\Var(\\/{1}{\\bar{X}_n})$\n",
        "$=\\E[\\/{1}{\\bar{X}_n^2}]-\\E[\\/{1}{\\bar{X}_n}]^2$ where\n",
        "$\\E[\\/{1}{\\bar{X}_n}]=‚àû$ because $P(\\bar{X}_n=0)>0$ so the limit of variance $\\Var(T_n)=‚àû$.\n",
        "Using Delta Method,\n",
        "$\\Var_Œ∏(T_n)=\\Var_Œ∏(g(T_n))$\n",
        "$=[g'(Œ∏)]^2\\Var_Œ∏(T_n)$\n",
        "$=\\/{1}{Œ∏^4}\\Var(\\bar{X}_n)$\n",
        "$=\\/{œÉ^2}{nŒ∏^4}$, which is the asymptotic variance of $T_n$.\n",
        "\n",
        "- 10.1.10/10.6: $(Y_n|W_n=w_n)‚àº\\Normal(0,w_n+(1-w_n)œÉ_n^2)$ where $W_n‚àº\\t{Bernoulli}(p_n)$. Let $p_n‚Üí1$ and $œÉ_n‚Üí‚àû$.\n",
        "\n",
        "  - Find $\\E[Y_n]$ and $\\Var(Y_n)$.\n",
        "  By Adam, $\\E[Y_n]=\\E[\\E[Y_n|W_n]]=0$.\n",
        "  By Eve, $\\Var(Y_n)=\\Var(\\E[Y_n|W_n])+\\E[\\Var(Y_n|W_n)]$\n",
        "  $=p_n+(1-p_n)œÉ^2$.\n",
        "\n",
        "  - Show $Y_n‚ÜíZ$ as $p_n‚Üí1$ and $œÉ_n‚Üí‚àû$ and find variance.\n",
        "  $P(Y_n< a)=P(Y_n< a|W_n=1)p_n+P(Y_n< a|W_n=0)(1-p_n)$\n",
        "  $=P(Z< a)(1)+P(Y_n< a|W_n=0)(0)$\n",
        "  $=P(Z< a)$.\n",
        "  Therefore $\\Var(Y_n)=1$ asymptotically.\n",
        "\n",
        "- 10.1.14 (binomial variance): $X_1,...,X_n‚àº\\t{Bernoulli}(p)$ then $\\hat{p}=\\/{\\sum_iX_i}{n}$.\n",
        "By direct calculation\n",
        "$\\Var_p(X_1)=p(1-p)$\n",
        "$‚áí\\Var_p(\\hat{p})=\\/{p(1-p)}{n}$.\n",
        "The observed information number is\n",
        "$\\hat{I}_n(\\hat{p})=-l''(p|\\v{x})|_{p=\\hat{p}}$\n",
        "$=-‚àÇ_p[\\/{\\sum_ix_i}{p}-\\/{n-\\sum_ix_i}{1-p}]$\n",
        "$=\\/{\\sum_ix_i}{p^2}+\\/{n-\\sum_ix_i}{(1-p)^2}$\n",
        "$=\\/{n}{\\hat{p}}+\\/{n}{1-\\hat{p}}$\n",
        "$=\\/{n}{\\hat{p}(1-\\hat{p})}$\n",
        "$‚áí\\widehat{\\Var}_p(\\hat{p})=\\/{\\hat{p}(1-\\hat{p})}{n}$.\n",
        "Therefore\n",
        "$\\sqrt{n}(\\hat{p}-p)\\arr{d}\\Normal(0,p(1-p))$, and by\n",
        "Slutsky's theorem\n",
        "$\\/{\\sqrt{n}(\\hat{p}-p)}{\\sqrt{\\hat{p}(1-\\hat{p})}}\\arr{d}\\Normal(0,1)$.\n",
        "\n",
        "  - 5.5.22 (odds): Let $œÑ(p)=\\/{p}{1-p}$.\n",
        "  $œÑ'(p)=\\/{1}{1-p}+\\/{p}{(1-p)^2}$\n",
        "  $=\\/{1}{(1-p)^2}$.\n",
        "  Then\n",
        "  $\\widehat{\\Var}_p(œÑ(\\hat{p}))$\n",
        "  $=\\/{[œÑ'(\\hat{p})]^2}{\\hat{I}_n(\\hat{p})}$\n",
        "  $=(\\/{1}{(1-p)^2})^2(\\/{\\hat{p}(1-\\hat{p})}{n})$\n",
        "  $=\\/{\\hat{p}}{n(1-\\hat{p})^3}$.\n",
        "\n",
        "  - 10.1.15/10.10 (Bernoulli variance): Let $œÑ(p)=p(1-p)$.\n",
        "  $œÑ'(p)=(1-p)-p=1-2p$.\n",
        "  $\\widehat{\\Var}_p(œÑ(\\hat{p}))=\\/{\\hat{p}(1-\\hat{p})(1-2\\hat{p})^2}{n}$.\n",
        "  However if $\\hat{p}=\\/{1}{2}$ the variance is 0, because $œÑ'(\\hat{p})=0$.\n",
        "  Now we use second-order Delta Method:\n",
        "  $n(œÑ(\\hat{p})-œÑ(p))\\arr{d}\\/{œÑ''(p)\\Var(\\sqrt{n}\\hat{p})}{2}œá_1^2$\n",
        "  $=\\/{-2\\hat{p}(1-\\hat{p})}{2}œá_1^2$\n",
        "  $‚áí\\widehat{\\Var}_p(œÑ(\\hat{p}))=\\/{\\Var(œá_1^2)}{16n^2}$\n",
        "  $=\\/{1}{8n^2}$.\n",
        "\n",
        "  - 10.4.1 (confidence interval): Let $œÑ(p)=\\/{p}{1-p}$, then by invariance $œÑ(\\hat{p})=\\/{\\hat{p}}{1-\\hat{p}}$ and asymptotic variance\n",
        "  $\\widehat{\\Var}_p(\\/{\\hat{p}}{1-\\hat{p}})$\n",
        "  $=\\/{\\hat{p}}{n(1-\\hat{p})^3}$.\n",
        "  Therefore the $1-Œ±$ confidence interval is\n",
        "  $C(\\v{x})=\\{p:œÑ(\\hat{p})-z_{Œ±/2}\\sqrt{\\widehat{\\Var}_p(\\/{\\hat{p}}{1-\\hat{p}})}‚â§œÑ(p)‚â§œÑ(\\hat{p})+z_{Œ±/2}\\sqrt{\\widehat{\\Var}_p(\\/{\\hat{p}}{1-\\hat{p}})}\\}$.\n",
        "\n",
        "  - 10.4.2 (score interval): Let $Y=\\sum_iX_i$ then\n",
        "  $Q(Y|p)=\\/{l'(p|Y)}{\\sqrt{-\\E[l''(p|Y)]}}$\n",
        "  $=\\/{\\/{Y}{p}-\\/{n-Y}{1-p}}{\\sqrt{\\/{\\E[Y]}{p^2}+\\/{n-\\E[Y]}{(1-p)^2}}}$\n",
        "  $=\\/{\\/{Y-np}{p(1-p)}}{\\sqrt{\\/{n}{p(1-p)}}}$\n",
        "  $=\\/{\\hat{p}-p}{p(1-p)/n}$.\n",
        "  Then $C(Y)=\\{p:\\left|\\/{\\hat{p}-p}{p(1-p)/n}\\right|‚â§z_{Œ±/2}\\}$.\n",
        "\n",
        "- 10.2.3/10.22 (median asymptotics): $X_1,...,X_n‚àºf$ iid, and let $Œº$ and $M_n$ be the population and sample median so that $P(X_i‚â§Œº)=1/2$. We want to extract the asymptotics of $M_n$ from\n",
        "$\\liml_{n‚Üí‚àû}P(\\sqrt{n}(M_n-Œº)‚â§a)$\n",
        "by applying **binomial conversion** with\n",
        "$Y_i=I(\\sqrt{n}(X_i-Œº)‚â§a)$ where\n",
        "$Y_i‚àº\\t{Bernoulli}(p_n)$ and\n",
        "$p_n=F(Œº+\\/{a}{\\sqrt{n}})$.\n",
        "Then $\\sum_iY_i‚àº\\Binom(n,p_n)$ and\n",
        "$P(\\sqrt{n}(M_n-Œº)‚â§a)=P(\\sum_iY_i‚â•\\/{n+1}{2})$\n",
        "$=P(\\/{\\sum_iY_i-np_n}{\\sqrt{np_n(1-p_n)}}‚â•\\/{(n+1)/2-np_n}{\\sqrt{np_n(1-p_n)}})$\n",
        "$\\arr{d}P(Z‚â•\\liml_{n‚Üí‚àû}\\/{(n+1)/2-np_n}{\\sqrt{np_n(1-p_n)}})$.\n",
        "Because the limit of $\\/{n+1}{2}-np_n$ is indeterminate, we use first-order Taylor expansion\n",
        "$p_n=F(Œº+\\/{a}{\\sqrt{n}})$\n",
        "$‚âàF(Œº)+\\/{a}{\\sqrt{n}}F'(Œº)$\n",
        "$=\\/{1}{2}+\\/{a}{\\sqrt{n}}f(Œº)$.\n",
        "Then $\\liml_{n‚Üí‚àû}\\/{(n+1)/2-np_n}{\\sqrt{np_n(1-p_n)}}$\n",
        "$=\\liml_{n‚Üí‚àû}\\/{\\/{n}{2}+\\/{1}{2}-\\/{n}{2}-\\/{naf(Œº)}{\\sqrt{n}}}{\\sqrt{np_n(1-p_n)}}$\n",
        "$=\\liml_{n‚Üí‚àû}\\/{\\/{1}{2}-\\sqrt{n}af(Œº)}{\\sqrt{n}/2}$\n",
        "$=-2af(Œº)$.\n",
        "Therefore\n",
        "$\\liml_{n‚Üí‚àû}P(\\sqrt{n}(M_n-Œº)‚â§a)$\n",
        "$=P(Z‚â•-2af(Œº))$\n",
        "$=P(Z‚â§2af(Œº))$\n",
        "$=P(\\/{Z}{2f(Œº)}‚â§a)$ and\n",
        "$\\sqrt{n}(M_n-Œº)\\arr{d}\\Normal(0,\\/{1}{[2f(Œº)]^2})$.\n",
        "\n",
        "- 10.3.2: $H_0:Œª=Œª_0$ vs $H_1:Œª\\neqŒª_0$ based on $X_1,...,X_n‚àº\\Pois(Œª)$ iid.\n",
        "$-2\\lnŒõ=-2\\ln(\\/{e^{-nŒª_0}Œª_0^{\\sum_ix_i}}{e^{-n\\hat{Œª}}\\hat{Œª}^{\\sum_ix_i}})]$\n",
        "$=-2[-n(Œª_0-\\hat{Œª})+(\\ln(Œª_0)-\\ln(\\hat{Œª}))\\sum_ix_i]$\n",
        "$=2n[(Œª_0-\\hat{Œª})-\\hat{Œª}\\ln(Œª_0/\\hat{Œª})]$.\n",
        "We can now use $-2\\lnŒõ>œá_{1,Œ±}^2$ instead of $\\/{e^{-nŒª_0}Œª_0^{\\sum_ix_i}}{e^{-n\\hat{Œª}}\\hat{Œª}^{\\sum_ix_i}}< c$\n",
        "\n",
        "- 10.3.5 (binomial Z test): $X_1,...,X_n‚àº\\t{Bernoulli}(p)$. Testing $H_0:p=p_0$ vs $H_1:p\\neq p_0$. MLE $\\hat{p}_n=\\/{\\sum_iX_i}{n}$.\n",
        "Then by CLT\n",
        "$Z_n=\\/{\\hat{p}-p_0}{S_n}$.\n",
        "Information $I_1(Œ∏)=-\\E[l''(Œ∏|x_i)]$\n",
        "$=-\\E[‚àÇ_p^2[\\ln p^{x_i}(1-p)^{1-x_i}]]$\n",
        "$=-\\E[‚àÇ_p[\\/{x_i}{p}-\\/{1-x_i}{1-p}]]$\n",
        "$=-\\E[\\/{-x_i}{p^2}-\\/{1-x_i}{(1-p)^2}]$\n",
        "$=\\/{p}{p^2}+\\/{1-p}{(1-p)^2}$\n",
        "$=\\/{1}{p(1-p)}$.\n",
        "Therefore $S_n=\\/{1}{\\sqrt{nI_1(\\hat{p})}}$ and\n",
        "$Z_n=\\/{\\hat{p}-p_0}{\\sqrt{\\/{\\hat{p}(1-\\hat{p})}{n}}}\\arr{d}\\Normal(0,1)$.\n",
        "\n",
        "  - Alternatively use $S_n=\\/{1}{\\sqrt{nI_1(p_0)}}$\n",
        "  $‚áíZ^*=\\/{\\hat{p}-p_0}{\\sqrt{\\/{p_0(1-p_0)}{n}}}$\n",
        "  which makes sense under $H_0:p=p_0$.\n",
        "  $S_n=\\/{1}{\\sqrt{nI_1(p_0)}}$ tends to outperform for hypothesis testing, and\n",
        "  $S_n=\\/{1}{\\sqrt{nI_1(\\hat{p})}}$ tends to work better for confidence intervals.\n",
        "\n",
        "  - 10.3.6: Use score test.\n",
        "  $Z_S=\\/{S(p_0)}{\\sqrt{I_n(p_0)}}$\n",
        "  $=\\/{\\/{\\sum_ix_i}{p_0}-\\/{n-\\sum_ix_i}{1-p_0}}{\\sqrt{\\/{n}{p_0(1-p_0)}}}$\n",
        "  $=\\/{\\/{n\\hat{p}}{p_0}-\\/{n-n\\hat{p}}{1-p_0}}{\\sqrt{\\/{n}{p_0(1-p_0)}}}$\n",
        "  $=\\/{\\/{n(\\hat{p}-p_0)}{p_0(1-p_0)}}{\\sqrt{\\/{n}{p_0(1-p_0)}}}$\n",
        "  $=\\/{\\hat{p}-p_0}{\\sqrt{\\/{p_0(1-p_0)}{n}}}$\n",
        "\n",
        "- 10.1: $X_1,...,X_n‚àºf(x_i|Œ∏)=\\/{1}{2}(1+Œ∏x_i)I_{-1< x_i< 1}$ iid where $Œ∏‚àà(-1,1)$. Find the consistent estimator.\n",
        "$f(\\v{x}|Œ∏)=\\/{1}{2^n}\\prod_i(1+Œ∏x_i)$\n",
        "$l(Œ∏|\\v{x})=\\sum_i\\ln(1+Œ∏x_i)$.\n",
        "$l'(Œ∏|\\v{x})=\\sum_i\\/{x_i}{1+Œ∏x_i}=0$\n",
        "does not have a closed form solution for $\\hat{Œ∏}$.\n",
        "$\\E[X_1]=‚à´_{-1}^1(\\/{x}{2}+\\/{Œ∏}{2}x^2)\\ dx$\n",
        "$=[x^2+\\/{Œ∏}{6}x^3]_{-1}^1$\n",
        "$=\\/{Œ∏}{3}$.\n",
        "$\\E[X_1^2]=‚à´_{-1}^1(\\/{x^2}{2}+\\/{Œ∏}{2}x^3)\\ dx$\n",
        "$=[\\/{x^3}{6}+\\/{Œ∏}{8}x^4]_{-1}^1$\n",
        "$=\\/{1}{3}$.\n",
        "$\\Var(X_1)=\\/{1}{3}(1-\\/{Œ∏^2}{3})$.\n",
        "Let $W_n=3\\bar{X}_n$.\n",
        "Then $\\E[W_n]=Œ∏$ and $\\Var(W_n)=\\/{1}{3n}(1-\\/{Œ∏^2}{3})‚Üí0$.\n",
        "Therefore $W_n$ is a consistent estimator for $Œ∏$.\n",
        "\n",
        "- 10.3: $X_1,...,X_n‚àº\\Normal(Œ∏,Œ∏)$. Find MLE and variance.\n",
        "$f(x_i|Œ∏)=\\/{1}{\\sqrt{2œÄŒ∏}}e^{-\\/{(x_i-Œ∏)^2}{2Œ∏}}$.\n",
        "$l(Œ∏|x_i)=-\\/{\\lnŒ∏}{2}-\\/{x_i^2}{2Œ∏}+x_i-\\/{Œ∏}{2}$.\n",
        "$l'(Œ∏|x_i)=\\/{-1}{2Œ∏}+\\/{x_i^2}{2Œ∏^2}-\\/{1}{2}$.\n",
        "$l'(Œ∏|\\v{x})=\\/{-nŒ∏^2-nŒ∏+\\sum_ix_i^2}{2Œ∏^2}=0$\n",
        "$=Œ∏^2+Œ∏-\\sum_ix_i^2/n$.\n",
        "$l''(Œ∏|x_i)=\\/{1}{2Œ∏^2}-\\/{x_i^2}{Œ∏^3}$.\n",
        "$I_n(Œ∏)=-n\\E_Œ∏[\\/{1}{2Œ∏^2}-\\/{x_i^2}{Œ∏^3}]$\n",
        "$=\\/{-nŒ∏+2n\\E_Œ∏[X_i^2]}{2Œ∏^3}$\n",
        "$=\\/{-nŒ∏+2nŒ∏+2nŒ∏^2}{2Œ∏^3}$\n",
        "$=\\/{2nŒ∏+n}{2Œ∏^3}$.\n",
        "Therefore $\\Var_Œ∏(\\hat{Œ∏})‚Üí\\/{2Œ∏^3}{2nŒ∏+n}$.\n",
        "\n",
        "- 10.4 (7.19): $X_1,...,X_n‚àº\\Normal(Œº,œÑ^2)$ and $Y_i=Œ≤X_i+œµ_i$ where $œµ_i‚àº\\Normal(0,œÉ^2)$. Find asymptotic means and variances.\n",
        "\n",
        "  - MLE $\\hat{Œ≤}=\\/{\\sum_iX_iY_i}{\\sum_iX_i^2}$\n",
        "  $=\\/{\\sum_iX_i(Œ≤X_i+œµ_i)}{\\sum_iX_i^2}$\n",
        "  $=Œ≤+\\/{\\sum_iX_iœµ_i}{\\sum_iX_i^2}$.\n",
        "  $\\E[\\hat{Œ≤}|\\v{X}]=Œ≤+\\/{\\sum_iX_i\\E[œµ_i]}{\\sum_iX_i^2}$\n",
        "  $=Œ≤$\n",
        "  $‚áí\\E[\\hat{Œ≤}]=Œ≤$.\n",
        "  $\\Var(\\hat{Œ≤}|\\v{X})=\\/{\\Var(\\sum_iX_iœµ_i)}{(\\sum_iX_i^2)^2}$\n",
        "  $=\\/{\\sum_iX_i^2\\Var(œµ_i)}{(\\sum_iX_i^2)^2}$\n",
        "  $=\\/{œÉ^2}{\\sum_iX_i^2}$.\n",
        "  $\\Var(\\hat{Œ≤})=\\E[\\Var(\\hat{Œ≤}|\\v{X})]$\n",
        "  $=œÉ^2\\E[\\/{1}{\\sum_iX_i^2}]$\n",
        "  $‚Üí\\/{œÉ^2}{\\E[\\sum_iX_i^2]}$\n",
        "  $=\\/{œÉ^2}{\\sum_i\\E[X_i^2]}$\n",
        "  $=\\/{œÉ^2}{n(œÑ^2+Œº^2)}$.\n",
        "\n",
        "  - $W=\\/{\\sum_iY_i}{\\sum_iX_i}$\n",
        "  $=\\/{\\sum_i(Œ≤X_i+œµ_i)}{\\sum_iX_i}$\n",
        "  $=Œ≤+\\/{\\sum_iœµ_i}{\\sum_iX_i}$.\n",
        "  $\\E[W|\\v{X}]=Œ≤$\n",
        "  $‚áí\\E[W]=Œ≤$.\n",
        "  $\\Var(W|\\v{X})=\\/{\\sum_i\\Var(œµ_i)}{(\\sum_iX_i)^2}$\n",
        "  $=\\/{nœÉ^2}{(\\sum_iX_i)^2}$.\n",
        "  $\\Var(W)=\\E[\\Var(W|\\v{X})]$\n",
        "  $=\\E[\\/{nœÉ^2}{(\\sum_iX_i)^2}]$\n",
        "  $‚Üí\\/{nœÉ^2}{(\\E[\\sum_iX_i])^2}$\n",
        "  $=\\/{œÉ^2}{nŒº^2}$.\n",
        "\n",
        "- 10.34 (9.6): $X_1,...,X_n‚àº\\t{Bernoulli}(p)$. Testing $H_0:p=p_0$ vs $H_1:p\\neq p_0$.\n",
        "Let $Y=\\sum_iX_i$ then\n",
        "$L(p|y)=p^y(1-p)^{n-y}$.\n",
        "$\\hat{p}=\\/{y}{n}$.\n",
        "$Œõ=(\\/{p_0}{\\hat{p}})^y(\\/{1-p_0}{1-\\hat{p}})^{n-y}$.\n",
        "Then by Wilks, $-2\\lnŒõ\\arr{d}œá_1^2$, then\n",
        "$P_p(Œõ‚â§k_Œ±)$ where $k_Œ±=e^{-\\/{1}{2}œá_{1,Œ±}^2}$.\n",
        "\n",
        "- 10.35: $X_1,...,X_n‚àº\\Normal(Œº,œÉ^2)$.\n",
        "\n",
        "  - If $Œº$ is unknown and $œÉ^2$ is known, find Wald statistic for $H_0:Œº=Œº_0$.\n",
        "  Wald test $\\/{\\bar{X}-Œº_0}{œÉ/\\sqrt{n}}\\arr{d}\\Normal(0,1)$.\n",
        "\n",
        "  - If $œÉ^2$ is unknown and $Œº$ is known, find Wald statistic for $H_0:œÉ=œÉ_0$.\n",
        "  $f(x_i|Œº,œÉ^2)=\\/{1}{\\sqrt{2œÄœÉ^2}}\\e{-\\/{(x_i-Œº)^2}{2œÉ^2}}$.\n",
        "  $l(Œº,œÉ^2|x_i)=-\\/{1}{2}\\ln(œÉ^2)-\\/{(x_i-Œº)^2}{2œÉ^2}$.\n",
        "  $l'(œÉ^2|x_i)=\\/{-1}{2œÉ^2}+\\/{(x_i-Œº)^2}{2œÉ^4}$.\n",
        "  $l''(œÉ^2|x_i)=\\/{1}{2œÉ^4}-\\/{(x_i-Œº)^2}{œÉ^6}$.\n",
        "  $I_1(œÉ^2)=-\\E[l''(œÉ^2|x_i)]$\n",
        "  $=-\\/{1}{2œÉ^4}+\\/{\\E[(x_i-Œº)^2]}{œÉ^6}$\n",
        "  $=-\\/{1}{2œÉ^4}+\\/{œÉ^2}{œÉ^6}$\n",
        "  $=\\/{1}{2œÉ^4}$.\n",
        "  For MLE,\n",
        "  $\\Var(\\hat{œÉ}^2)=\\/{1}{I_n(œÉ^2)}$\n",
        "  $=\\/{2œÉ^4}{n}$.\n",
        "  $\\Var(\\sqrt{\\hat{œÉ}^2})‚Üí[\\/{1}{2\\sqrt{œÉ^2}}]^2\\Var(\\hat{œÉ}^2)$\n",
        "  $=\\/{1}{4œÉ^2}\\/{2œÉ^4}{n}$\n",
        "  $=\\/{œÉ^2}{2n}$.\n",
        "  Wald test\n",
        "  $\\/{\\hat{œÉ}-œÉ_0}{\\sqrt{œÉ^2/2n}}\\arr{d}\\Normal(0,1)$.\n",
        "\n",
        "  - 10.37a: $Œº$ unknown and $œÉ^2$ known. Find score test for $H_0:Œº=Œº_0$.\n",
        "  $l'(Œº|x_i)=\\/{x_i-Œº}{œÉ^2}$.\n",
        "  $S(Œº_0)=\\/{n(\\bar{X}-Œº_0)}{œÉ^2}$.\n",
        "  $l''(Œº|x_i)=\\/{-1}{œÉ^2}$.\n",
        "  $I_n(Œº)=-\\E[l''(Œº|x_i)]=\\/{n}{œÉ^2}$.\n",
        "  Score test $\\/{S(Œº_0)}{\\sqrt{I_n(Œº_0)}}$\n",
        "  $=\\/{\\sqrt{n}(\\bar{X}-Œº_0)}{œÉ}\\arr{d}\\Normal(0,1)$.\n",
        "\n",
        "  - 10.37b: $œÉ^2$ unknown and $Œº$ known. Find score test for $H_0:œÉ=œÉ_0$.\n",
        "  $l(œÉ|x_i)=-\\ln(œÉ)-\\/{(x_i-Œº)^2}{2œÉ^2}$.\n",
        "  $l'(œÉ|x_i)=-\\/{1}{œÉ}+\\/{(x_i-Œº)^2}{œÉ^3}$.\n",
        "  $S(œÉ_0)=-\\/{n}{œÉ_0}+\\/{\\sum_i(x_i-Œº)^2}{œÉ_0^3}$\n",
        "  $=-\\/{n}{œÉ_0}+\\/{n\\hat{œÉ}^2}{œÉ_0^3}$\n",
        "  $=\\/{n(\\hat{œÉ}^2-œÉ_0^2)}{œÉ_0^3}$.\n",
        "  $l''(œÉ|x_i)=\\/{1}{œÉ^2}-\\/{3(x_i-Œº)^2}{œÉ^4}$.\n",
        "  $I_1(œÉ)=-E[l''(œÉ|x_i)]$\n",
        "  $=\\/{3\\E[(x_i-Œº)^2]}{œÉ^4}-\\/{1}{œÉ^2}$\n",
        "  $=\\/{2}{œÉ^2}$.\n",
        "  Score test\n",
        "  $\\/{S(œÉ_0)}{\\sqrt{nI_1(œÉ_0)}}$\n",
        "  $=\\/{\\sqrt{n}(\\hat{œÉ}^2-œÉ_0^2)}{\\sqrt{2}œÉ_0^2}\\arr{d}\\Normal(0,1)$.\n",
        "\n",
        "- 10.36: $X_1,...,X_n‚àº\\Gamma(a,b)$ assuming $a$ known and $b$ unknown. Testing $H_0:b=b_0$.\n",
        "\n",
        "  - MLE $\\hat{b}$.\n",
        "  $f(x_i|a,b)=\\/{e^{-x_i/b}(x_i/b)^{a}}{x_iŒì(a)}$.\n",
        "  $l(b|x_i)=-\\/{x_i}{b}+a\\ln(\\/{x_i}{b})$\n",
        "  $=-\\/{x_i}{b}-a\\ln b$.\n",
        "  $l'(b|x_i)=\\/{x_i}{b^2}-\\/{a}{b}$.\n",
        "  MLE $\\hat{b}=\\/{x_i}{a}$.\n",
        "\n",
        "  - Find Wald statistic for $H_0$.\n",
        "  $l''(b|x_i)=\\/{-2x_i}{b^3}+\\/{a}{b^2}$.\n",
        "  $I_1(b)=-\\E[l''(b|x_i)]$\n",
        "  $=\\/{2\\E[X_i]}{b^3}-\\/{a}{b^2}$\n",
        "  $=\\/{a}{b^2}$.\n",
        "  $\\Var(\\hat{b})=\\/{1}{I_n(b)}$\n",
        "  $=\\/{b^2}{na}$.\n",
        "  Wald test\n",
        "  $\\/{\\hat{b}-b_0}{b/\\sqrt{na}}\\arr{d}\\Normal(0,1)$.\n",
        "\n",
        "  - 10.38: Find score test for $H_0$.\n",
        "  $S(b)=\\/{\\sum_ix_i}{b^2}-\\/{na}{b}$\n",
        "  $=\\/{n(\\bar{X}-ab)}{b^2}$.\n",
        "  Score test\n",
        "  $\\/{S(b_0)}{\\sqrt{nI_1(b_0)}}$\n",
        "  $=\\/{\\sqrt{n}(\\bar{X}-ab_0)}{b_0/\\sqrt{a}}$\n",
        "  $\\arr{d}\\Normal(0,1)$."
      ],
      "metadata": {
        "id": "BMmUwVGNJWmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: Brandon Foltz Youtube, Organic Chemistry Tutor Youtube\n",
        "\n",
        "**Sample distribution**: We repeat identical trials on $X_1,...,X_n‚àº(Œº,œÉ^2)$ to collect $m$ equal-$n$-size samples $\\bar{X}_1,...,\\bar{X}_m‚àºf_{\\bar{X}}(x|Œº)$, then by CLT $f_{\\bar{X}}(x|Œº)‚àº\\Normal(Œº,\\/{œÉ^2}{n})$ is the sample distribution of $T$.\n",
        "\n",
        "- $œÉ_{\\bar{X}}=\\/{œÉ}{\\sqrt{n}}$ is the **standard error** of $T$, where $œÉ$ is the standard deviation of the population. It represents how close a sample mean $\\bar{X}$ is relative to population mean $Œº$.\n",
        "\n",
        "**Confidence interval**: In the sample distribution, 95% of samples $\\bar{X}_i$ will fall within $Œº¬±z_{0.95}œÉ_{\\bar{X}}$. From a sample, we build 95% confidence interval $\\bar{X}¬±z_{0.95}œÉ_{\\bar{X}}$. There is a 5% probability that this interval does not contain $Œº$.\n",
        "\n",
        "- When $œÉ$ is unknown, we use t-distribution $t_{n-1}=\\/{\\bar{X}-Œº}{S/\\sqrt{n}}$. Every sample will have its own $\\bar{x}$ and standard error $s_{\\bar{X}}=\\/{s}{\\sqrt{n}}$ where $s$ is the sample standard deviation $s=\\/{\\sum_i(x_i-\\bar{x})^2}{n-1}$. Then $Œº¬±t_{n-1,0.95}\\/{s}{\\sqrt{n}}$ is the 95% confidence interval (by pivotal inversion).\n",
        "\n",
        "**Hypothesis testing**: Two-sided test $H_0:Œº=Œº_0$ vs $H_1:Œº\\neqŒº_0$, the Z test is $Z=\\/{\\bar{x}-Œº_0}{œÉ/\\sqrt{n}}‚àº\\Normal(0,1)$ and reject $H_0$ if $|Z|>z_{Œ±/2}$. p-value is sum of both tails $p=2Œ¶(-|Z|)$.\n",
        "\n",
        "- One-sided $H_0:Œº‚â§Œº_0$ vs $H_1:Œº>Œº_0$, again $Z=\\/{\\bar{x}-Œº_0}{œÉ/\\sqrt{n}}‚àº\\Normal(0,1)$, but reject $H_0$ if $Z>z_{Œ±}$. $p=Œ¶(-|Z|)$.\n",
        "\n",
        "- If $œÉ$ is unknown then $\\/{(n-1)S^2}{œÉ^2}‚àºœá_{n-1}^2$, two-sided T test is $T=\\/{\\bar{x}-Œº_0}{s/\\sqrt{n}}‚àºt_{n-1}$ and reject $H_0$ if $|T|>t_{n-1,Œ±/2}$.\n",
        "\n",
        "- If $œÉ$ is unknown then $\\/{(n-1)S^2}{œÉ^2}‚àºœá_{n-1}^2$,  one-sided T test is $T=\\/{\\bar{x}-Œº_0}{s/\\sqrt{n}}‚àºt_{n-1}$ and reject $H_0$ if $T>t_{n-1,Œ±}$.\n",
        "\n",
        "**Sampling variance**: 95% confidence interval $\\/{(n-1)s^2}{œá_{n-1,Œ±/2}^2}‚â§œÉ^2‚â§\\/{(n-1)s^2}{œá_{n-1,1-Œ±/2}^2}$\n",
        "\n",
        "- Testing two-sided $H_0:œÉ^2=œÉ_0^2$ vs $H_1:œÉ^2\\neqœÉ_0^2$, the Chi-square test is $œá^2=\\/{(n-1)s^2}{œÉ_0^2}‚àºœá_{n-1}^2$ and reject $H_0$ if $œá^2>œá_{n-1,Œ±/2}^2$ or $œá^2< œá_{n-1,1-Œ±/2}^2$. Testing one-sided $H_0:œÉ^2‚â§œÉ_0^2$ sv $H_1:œÉ^2>œÉ_0^2$ then the Chi-square test is $œá^2=\\/{(n-1)s^2}{œÉ_0^2}‚àºœá_{n-1}^2$ and reject $H_0$ if $œá^2‚â•œá_{n-1,Œ±}$.\n",
        "\n",
        "**Two samples**: $H_0:Œº_1-Œº_2=D_0$ vs $H_1:Œº_1-Œº_2\\neq D_0$, $\\bar{X}_1-\\bar{X}_2‚àº\\Normal(Œº_1-Œº_2,\\/{œÉ_1^2}{n_1}+\\/{œÉ_2^2}{n_2})$.\n",
        "The Z statistic is\n",
        "$Z=\\/{(\\bar{x}_1-\\bar{x}_2)-D_0}{\\sqrt{\\/{œÉ_1^2}{n_1}+\\/{œÉ_2^2}{n_2}}}‚àº\\Normal(0,1)$.\n",
        "The T statistic is\n",
        "$T=\\/{(\\bar{x}_1-\\bar{x}_2)-D_0}{\\sqrt{\\/{s_1^2}{n_1}+\\/{s_2^2}{n_2}}}‚àºt_{df}$ where $df=\\t{floor}(\\/{s_1^2/n_1+s_2^2/n_2}{\\/{s_1^2/n_1}{n_1-1}+\\/{s_2^2/n_2}{n_2-1}})$."
      ],
      "metadata": {
        "id": "bgyFdsWWeGgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANOVA"
      ],
      "metadata": {
        "id": "-z6CpbDWMWwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-way analysis of variance**: Effects of one factor are observed and categorized into $k$ groups. $Y_{ij}$ represent the $j$-th observation ($j=1,...,n_i$) in the $i$-th group ($i=1,...,k$). Then ANOVA assumes $Y_{ij}=Œ∏_i+œµ_{ij}$ where $œµ_{ij}‚àº\\Normal(0,œÉ^2< ‚àû)$ iid.\n",
        "\n",
        "- Let $t_{1,...,k}$ be variables and $\\v{a}=a_{1,...,k}$ be constants. Then $\\suml_{i=1}^ka_it_k$ is a **contrast** if $\\suml_{i=1}^ka_i=0$.\n",
        "(Casella 11.2.4)\n",
        "\n",
        "- Let $\\m{A}=\\{\\v{a}:\\suml_{i=1}^ka_i=0\\}$. Then $H_0:Œ∏_1=...=Œ∏_k‚ü∫H_0:\\suml_{i=1}^ka_iŒ∏_i=0$ for all $\\v{a}‚àà\\m{A}$.\n",
        "(Casella 11.2.5)\n",
        "\n",
        "  - Proof: $\\red{‚áí}$ Let all $Œ∏_i=Œ∏$, then $\\suml_{i=1}^ka_iŒ∏_i$\n",
        "  $=Œ∏(\\suml_{i=1}^ka_i)$\n",
        "  $=0$.\n",
        "  $\\red{‚áê}$ $\\v{a}_1=(1,-1,0,0)$ implies $Œ∏_1=Œ∏_2$.\n",
        "  $\\v{a}_2=(0,1,-1,0)$ implies $Œ∏_2=Œ∏_3$.\n",
        "  $\\v{a}_3=(0,0,1,-1)$ implies $Œ∏_3=Œ∏_4$.\n",
        "  Then $Œ∏_1=...=Œ∏_4$.\n",
        "\n",
        "- The sample mean of a group\n",
        "$\\bar{Y}_{i:}=\\/{1}{n_i}\\suml_{j=1}^{n_i}Y_{ij}‚àº\\Normal(Œ∏_i,\\/{œÉ^2}{n_i})$,\n",
        "then\n",
        "$\\suml_{i=1}^ka_i\\bar{Y}_{i:}‚àº\\Normal(\\suml_{i=1}^ka_iŒ∏_i,œÉ^2\\suml_{i=1}^k\\/{a_i^2}{n_i})$.\n",
        "Then with $œÉ^2$ known, Z test\n",
        "$\\blue{Z_\\v{a}=\\/{\\sum_{i=1}^ka_i\\bar{Y}_{i:}-\\sum_{i=1}^ka_iŒ∏_i}{\\sqrt{\\green{œÉ^2}\\sum_{i=1}^ka_i^2/n_i}}‚àº\\Normal(0,1)}$.\n",
        "With $œÉ^2$ unknown, then\n",
        "$S_i^2=\\/{1}{n_i-1}\\suml_{j=1}^{n_i}(Y_{ij}-\\bar{Y}_{i:})^2$\n",
        "follows\n",
        "$\\/{(n_i-1)S_i^2}{œÉ^2}‚àºœá_{n_i-1}^2$.\n",
        "The *pooled estimator* is the weighted average of the $S_i^2$s, and provides a better estimator:\n",
        "$S_p^2=\\suml_{i=1}^k\\/{n_i-1}{N-k}S_i^2$\n",
        "$=\\/{1}{N-k}\\suml_{i=1}^k\\suml_{j=1}^{n_i}(Y_{ij}-\\bar{Y}_{i:})^2$\n",
        "follows\n",
        "$\\green{\\/{(N-k)S_p^2}{œÉ^2}‚àºœá_{N-k}^2}$.\n",
        "T test\n",
        "$\\blue{T_\\v{a}=\\/{\\sum_{i=1}^ka_i\\bar{Y}_{i:}-\\sum_{i=1}^ka_iŒ∏_i}{\\sqrt{\\green{S_p^2}\\sum_{i=1}^ka_i^2/n_i}}‚àºt_{N-k}}$.\n",
        "\n",
        "  - Let $Œò_\\v{a}=\\{(Œ∏_1,...,Œ∏_k):\\suml_{i=1}^ka_iŒ∏_i=0\\}$. Then reject $H_{0\\v{a}}:(Œ∏_1,...,Œ∏_k)‚ààŒò_\\v{a}$ if $|T_\\v{a}|>t_{N-k,Œ±/2}$.\n",
        "\n",
        "  - $1-Œ±$ confidence interval\n",
        "  $(\\suml_{i=1}^ka_i\\bar{Y}_{i:})¬±t_{N-k,Œ±/2}\\sqrt{S_p^2\\sum_ia_i^2/n_i}$ by pivot inversion.\n"
      ],
      "metadata": {
        "id": "owiPAe8UQYA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANOVA hypothesis** Let $Œò_\\v{a}=\\{(Œ∏_1,...,Œ∏_k):\\suml_{i=1}^ka_iŒ∏_i=0\\}$, then hypothesis testing\n",
        "$H_0:Œ∏_1=...=Œ∏_k$\n",
        "is equivalent to\n",
        "$H_0:(Œ∏_1,...,Œ∏_k)‚àà\\bigcap\\limits_{\\v{a}‚àà\\m{A}}Œò_\\v{a}$ vs $H_1:(Œ∏_1,...,Œ∏_k)‚àà\\bigcup\\limits_{\\v{a}‚àà\\m{A}}Œò_\\v{a}^c$,\n",
        "and only needs to test against the largest T statistic $\\sup_\\v{a}T_\\v{a}>k$ to reject $H_{0\\v{a}}:(Œ∏_1,...,Œ∏_k)‚ààŒò_\\v{a}$.\n",
        "\n",
        "- Let $y_{1,...,k}$ be constants and $n_{1,...,k}$ be positive, then\n",
        "$\\max\\limits_{\\v{a}‚àà\\m{A}}\\/{(\\sum_{i=1}^ka_iy_i)^2}{\\sum_{i=1}^ka_i^2/n_i}=\\suml_{i=1}^kn_i(y_i-\\bar{y}_n)^2$\n",
        "where $\\bar{y}_n=\\/{\\sum_{i=1}^kn_iy_i}{\\sum_{i=1}^kn_i}$. Maximum is attained with $\\v{a}$ in the form $a_i=C(n_iy_i-\\bar{y}_n)$ for some constant $C$.\n",
        "(Casella 11.2.7)\n",
        "\n",
        "  - Proof: Let $K‚â°\\sqrt{\\sum_{i=1}^ka_i^2/n_i}$ and\n",
        "  $b_i=\\/{a_i}{K}$,\n",
        "  then\n",
        "  $\\max\\/{(\\sum_{i=1}^ka_iy_i)^2}{\\sum_{i=1}^ka_i^2/n_i}$\n",
        "  $=\\max\\left(\\suml_{i=1}^kb_iy_i\\right)^2$\n",
        "  where\n",
        "  $\\suml_{i=1}^kb_i=0$.\n",
        "  Define dependent discrete $B$ and $Y$ such that $P(B=\\/{b_i}{n_i})=P(Y=y_i)=P(B=\\/{b_i}{n_i},Y=y_i)=\\/{n_i}{N}$.\n",
        "  Then\n",
        "  $\\BC\n",
        "  \\E[B]=\\sum_{i=1}^k\\/{b_i}{n_i}\\/{n_i}{N}=\\/{1}{N}\\sum_{i=1}^kb_i=0 \\qquad\n",
        "  \\Var(B)=\\E[B^2]=\\sum_{i=1}^k\\/{b_i^2}{n_i^2}\\/{n_i}{N}=\\/{1}{NK^2}\\sum_{i=1}^k\\/{a_i^2}{n_i}=\\/{1}{N} \\\\\n",
        "  \\E[Y]=\\bar{y}_n \\qquad\n",
        "  \\Var(Y)=\\E[(Y-\\E[Y])^2]=\\sum_{i=1}^k(y_i-\\bar{y}_n)^2\\/{n_i}{N}=\\/{1}{N}\\sum_{i=1}^kn_i(y_i-\\bar{y}_n)\n",
        "  \\EC$.\n",
        "  $\\Cov(B,Y)=\\E[BY]-\\E[B]\\E[Y]$\n",
        "  $=\\E[BY]$\n",
        "  $=\\suml_{i=1}^k\\suml_{j=1}^k\\/{b_i}{n_i}y_jP(B=\\/{b_i}{n_i},Y=y_j)$\n",
        "  $=\\suml_{i=1}^k\\/{b_i}{n_i}y_iP(B=\\/{b_i}{n_i},Y=y_i)$\n",
        "  $=\\suml_{i=1}^k\\/{b_i}{n_i}y_i\\/{n_i}{N}$\n",
        "  $=\\/{1}{N}\\suml_{i=1}^kb_iy_i$.\n",
        "  Then by Cauchy-Schwarz,\n",
        "  $\\left(\\suml_{i=1}^kb_iy_i\\right)^2$\n",
        "  $=(N\\Cov(B,Y))^2‚â§N^2\\Var(B)\\Var(Y)$\n",
        "  $=\\suml_{i=1}^kn_i(y_i-\\bar{y}_n)^2$.\n",
        "  Finally substituting $a_i=Cn_i(y_i-\\bar{y}_n)$ into\n",
        "  $\\/{(\\sum_{i=1}^ka_iy_i)^2}{\\sum_{i=1}^ka_i^2/n_i}$\n",
        "  $=\\/{(\\sum_{i=1}^ka_iy_i-\\sum_{i=1}^ka_i\\bar{y}_n)^2}{\\sum_{i=1}^ka_i^2/n_i}$\n",
        "  $=\\/{[\\sum_{i=1}^kCn_i(y_i-\\bar{y}_n)(y_i-\\bar{y}_n)]^2}{\\sum_{i=1}^kCn_i^2(y_i-\\bar{y}_n)^2/n_i}$\n",
        "  $=\\/{C^2[\\sum_{i=1}^kn_i(y_i-\\bar{y}_n)^2]^2}{C^2\\sum_{i=1}^kn_i(y_i-\\bar{y}_n)^2}$\n",
        "  $=\\suml_{i=1}^kn_i(y_i-\\bar{y}_n)^2$.\n",
        "\n",
        "  - $\\sup\\limits_{\\v{a}‚àà\\m{A}}T_\\v{a}^2$\n",
        "  $=\\max\\limits_{\\v{a}‚àà\\m{A}}\\left(\\/{\\sum_{i=1}^ka_i\\bar{Y}_{i:}-\\sum_{i=1}^ka_iŒ∏_i}{\\sqrt{S_p^2\\sum_{i=1}^ka_i^2/n_i}}\\right)^2$\n",
        "  $=\\max\\limits_{\\v{a}‚àà\\m{A}}\\/{(\\sum_{i=1}^ka_i(\\bar{Y}_{i:}-Œ∏_i))^2}{S_p^2\\sum_{i=1}^ka_i^2/n_i}$\n",
        "  $=\\/{\\sum_{i=1}^kn_i((\\bar{Y}_{i:}-Œ∏_i)-(\\bar{Y}-\\bar{Œ∏}))^2}{S_p^2}$\n",
        "\n",
        "- **ANOVA F-test**: $\\blue{\\sup\\limits_{\\v{a}‚àà\\m{A}}T_\\v{a}^2=\\/{\\sum_{i=1}^kn_i((\\bar{Y}_{i:}-\\bar{Y})-(Œ∏_i-\\bar{Œ∏}))^2}{S_p^2}‚àº(k-1)F_{k-1,N-k}}$\n",
        "where $\\bar{Y}=\\/{\\sum_{i=1}^kn_i\\bar{Y}_{i:}}{N}$ and\n",
        "$\\bar{Œ∏}=\\/{\\sum_{i=1}^kn_iŒ∏_i}{N}$.\n",
        "(Casella 11.2.8)\n",
        "\n",
        "  - Proof: Denominator is\n",
        "  $S_p^2=\\/{1}{N-k}\\suml_{i=1}^k\\suml_{j=1}^{n_i}(Y_{ij}-\\bar{Y}_{i:})^2$.\n",
        "  Numerator under $H_0$ is\n",
        "  $\\suml_{i=1}^kn_i(\\bar{Y}_{i:}-\\bar{Y})^2$.\n",
        "  By Core ANOVA identity below, these two sums of squares are independent and have $œá_{N-k}^2$ and $œá_{k-1}^2$ distributions.\n",
        "\n",
        "  - Reject $H_0$ if\n",
        "  $\\blue{\\/{\\sum_{i=1}^kn_i(\\bar{Y}_{i:}-\\bar{Y}))^2}{(k-1)S_p^2}>F_{k-1,N-k,Œ±}}$.\n",
        "\n",
        "**Core ANOVA identity**: $\\suml_{i=1}^k\\suml_{j=1}^{n_i}(y_{ij}-\\bar{y})^2=\\suml_{i=1}^k\\suml_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i:})^2+\\suml_{i=1}^kn_i(\\bar{y}_{i:}-\\bar{y})^2$\n",
        "where $\\bar{y}_{i:}=\\/{\\sum_{j=1}^{n_i}y_{ij}}{n_i}$ and $\\bar{y}=\\/{\\sum_{i=1}^kn_i\\bar{y}_{i:}}{\\sum_{i=1}^kn_i}$\n",
        "is a partition of sum of squares by Cochran's Theorem.\n",
        "(Casella 11.2.11)\n",
        "\n",
        "- $\\BC\n",
        "SS_{\\t{total}}=\\sum_{i=1}^k\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y})^2 & \\/{\\sum_{i=1}^k\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y})^2}{œÉ^2}‚àºœá_{N-1}^2 & \\t{total variation} \\\\\n",
        "SS_{\\t{error}}=\\sum_{i=1}^k\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i:})^2 & \\/{\\sum_{i=1}^k\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i:})^2}{œÉ^2}‚àºœá_{N-k}^2 & \\t{within-group variation} \\\\\n",
        "SS_{\\t{regression}}=\\sum_{i=1}^kn_i(\\bar{y}_{i:}-\\bar{y})^2 & \\/{\\sum_{i=1}^kn_i(\\bar{y}_{i:}-\\bar{y})^2}{œÉ^2}‚àºœá_{k-1}^2 & \\t{between-groups variation}\n",
        "\\EC$"
      ],
      "metadata": {
        "id": "fM7Yy-gjG30i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "HBJcCN56Rak2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple linear regression model**: Given data $(x_1,Y_1),...,(x_n,Y_n)$, we model $\\red{Y_i=Œ±+Œ≤x_i+œµ_i}$ with assumptions $\\E[œµ_i]=0$, $\\E[œµ_i]=0$, $\\Var(œµ_i)=œÉ^2$, and $\\Cov(œµ_i,œµ_j)=0$. No assumption of Normal distribution for $œµ_i$.\n",
        "\n",
        "- The regression line $\\E[Y_i|x_i]=Œ±+Œ≤x_i$ represents inferencing of conditional mean. $Y_i$ and $x_i$ are called the response and predictor variables (instead of dependent vs independent). $x_i$ is treated as fixed, non-random constants. The only source of randomness comes from $œµ_i$.\n",
        "\n",
        "- \"Linear\" refers to linearity in parameters $Œ±$ and $Œ≤$. \"Regression\" comes from \"regression toward the mean\".\n",
        "\n",
        "- The first step is data fitting. Finding the sample means $\\bar{x}$ and $\\bar{y}$, and the sums of squares $S_{xx}=\\sum_i(x_i-\\bar{x})^2$, $S_{yy}=\\sum_i(y_i-\\bar{y})$, and $S_{xy}=\\sum_i(x_i-\\bar{x})(y_i-\\bar{y})$.\n",
        "\n",
        "**Residual sum of squares** for a line $y=a+bx$ measures vertical distances: $\\t{RSS}(a,b)=\\suml_{i=1}^n(y_i-a-bx_i)^2$. The estimators for $Œ±$ and $Œ≤$ that algebraically minimizes the RSS are called the **ordinary least squares estimators**.\n",
        "\n",
        "- $a_{LS}=\\arg\\min\\limits_{a}\\suml_{i=1}^n(y_i-a-bx_i)^2$\n",
        "$=\\arg\\min\\limits_{a}\\suml_{i=1}^n((\\green{y_i-bx_i})-a)^2$\n",
        "$‚áí\\blue{a_{\\t{LS}}=\\bar{y}-b\\bar{x}}$ (by 5.2.4) is the LSE for $Œ±$, which ensures the fit will go through $(\\bar{x},\\bar{y})$.\n",
        "- $\\t{RSS}(b)=\\suml_{i=1}^n(y_i-(\\bar{y}-b\\bar{x}+bx_i))^2$\n",
        "$=\\suml_{i=1}^n((y_i-\\bar{y})-b(x_i-\\bar{x}))^2$\n",
        "$=S_{yy}-2bS_{xy}+b^2S_{xx}$.\n",
        "$\\/{d}{db}\\t{RSS}(b)=0=-2S_{xy}+2bS_{xx}$\n",
        "$‚áí\\blue{b_{\\t{LS}}=\\/{S_{xy}}{S_{xx}}}$\n",
        "is the LSE for $Œ≤$.\n",
        "\n",
        "**Best linear unbiased estimator**: Linear estimators have the form $\\suml_{i=1}^nd_iY_i$. BLUE estimators are statistically optimized.\n",
        "\n",
        "- \"Linear\": $b=\\sum_id_iY_i$.\n",
        "\"Unbiased\": $Œ≤=\\E[b]=\\sum_id_i\\E[Y_i]$\n",
        "$=\\sum_id_i(Œ±+Œ≤x_i)$\n",
        "gives 2 constraints $\\green{\\sum_id_i=0}$ and $\\green{\\sum_id_ix_i=1}$.\n",
        "\"Best\": $\\Var(b)=œÉ^2\\sum_id_i^2$.\n",
        "Solve for $b$ by minimizing $\\green{\\sum_id_i^2}$ subject to the 2 constraints.\n",
        "\n",
        "  - Using ANOVA 11.2.7, let $\\m{D}=\\{\\v{d}:\\sum_id_i=0\\}$ then\n",
        "  $\\max\\limits_{\\v{d}‚àà\\m{D}}\\/{1}{\\sum_id_i^2}$\n",
        "  $=\\max\\limits_{\\v{d}‚àà\\m{D}}\\/{(\\sum_id_ix_i)^2}{\\sum_id_i^2}$\n",
        "  $=\\sum_i(x_i-\\bar{x})^2$\n",
        "  $=S_{xx}$\n",
        "  $‚áí\\min\\limits_{\\v{d}‚àà\\m{D}}\\sum_id_i^2=\\/{1}{S_{xx}}$\n",
        "  is achieved with $d_i=K(x_i-\\bar{x})$ for some $K$.\n",
        "  Substitute back into constraint\n",
        "  $1=\\sum_id_ix_i$\n",
        "  $=\\sum_id_ix_i-\\bar{x}\\sum_id_i$\n",
        "  $=K\\sum_i(x_i-\\bar{x})^2$\n",
        "  $=KS_{xx}$\n",
        "  $‚áíK=\\/{1}{S_{xx}}$.\n",
        "  Therefore minimizing\n",
        "  $\\sum_id_i^2$\n",
        "  subject to constraints $\\sum_id_i=0$ and $\\sum_id_ix_i=1$\n",
        "  gives $\\green{d_i=\\/{x_i-\\bar{x}}{S_{xx}}}$.\n",
        "  Therefore $b=\\sum_i\\/{x_i-\\bar{x}}{S_{xx}}Y_i$\n",
        "  $‚áí\\blue{b_{BLUE}=\\/{S_{xy}}{S_{xx}}}$.\n",
        "\n",
        "- \"Linear\": $a=\\sum_id_iY_i$.\n",
        "\"Unbiased\": $Œ±=\\E[a]=\\sum_id_i(Œ±+Œ≤x_i)$\n",
        "gives 2 constraints $\\green{\\sum_id_i=1}$ and $\\green{\\sum_id_ix_i=0}$.\n",
        "\"Best\": $\\Var(a)=œÉ^2\\sum_id_i^2$.\n",
        "Solve for $a$ by minimizing $\\green{\\sum_id_i^2}$ subject to the 2 constraints.\n",
        "\n",
        "  - Lagrangian\n",
        "  $\\m{L}(\\v{d},Œª_1,Œª_2)=f(\\v{d})-Œª_1g_1(\\v{d})-Œª_2g_2(\\v{d})$\n",
        "  $=\\sum_id_i^2-Œª_1(\\sum_id_i-1)-Œª_2(\\sum_id_ix_i)$.\n",
        "  Differentiate\n",
        "  $\\/{‚àÇ\\m{L}}{‚àÇd_i}=2d_i-Œª_1-Œª_2x_i=0$\n",
        "  $‚áíd_i=\\/{Œª_1}{2}+\\/{Œª_2}{2}x_i$.\n",
        "  Substitute back into constraints\n",
        "  $1=\\sum_i(\\/{Œª_1}{2}+\\/{Œª_2}{2}x_i)$\n",
        "  $=\\/{nŒª_1}{2}+\\/{n\\bar{x}Œª_2}{2}$ and\n",
        "  $0=\\sum_i(\\/{Œª_1}{2}+\\/{Œª_2}{2}x_i)x_i$\n",
        "  $=\\/{n\\bar{x}Œª_1}{2}+\\/{Œª_2}{2}\\sum_ix_i^2$.\n",
        "  $Œª_2=-\\/{2\\bar{x}}{S_{xx}}$,\n",
        "  $Œª_1=\\/{2}{n}+\\/{2\\bar{x}^2}{S_{xx}}$, and\n",
        "  $\\green{d_i=\\/{1}{n}-\\/{\\bar{x}(x_i-\\bar{x})}{S_{xx}}}$.\n",
        "  Therefore $b=\\sum_i(\\/{1}{n}-\\/{\\bar{x}(x_i-\\bar{x})}{S_{xx}})Y_i$\n",
        "  $‚áí\\blue{a_{\\t{BLUE}}=\\bar{Y}-b\\bar{x}}$.\n",
        "\n",
        "- Geometrically, $\\sum_{i=1}^nd_i=0$ and $\\sum_{i=1}^nd_ix_i=1$ (or vice versa) define 2 planes in $‚Ñù^n$ whose intersecting line represent all unbiased estimators. The point $\\v{d}$ on the line with the shortest distance $\\sum_{i=1}^nd_i^2$ to origin $\\v{0}$ is the BLUE.\n",
        "\n",
        "- **Gauss-Markov Theorem**: Under the simple linear regression model and assumptions, the least square (LS) estimators are the best linear unbiased (BLUE) estimators for the coefficients $Œ±$ and $Œ≤$."
      ],
      "metadata": {
        "id": "VxlZd00JRdLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditional normal model** is built on the simple linear regression model but assumes $œµ_i‚àº\\Normal(0,œÉ^2)$ iid.\n",
        "\n",
        "- $Y_i‚àº\\Normal(Œ±+Œ≤x_i,œÉ^2)$ with joint PDF $f(\\v{y}|Œ±,Œ≤,œÉ^2)=\\/{1}{(2œÄœÉ^2)^{n/2}}\\e{-\\/{1}{2œÉ^2}\\suml_{i=1}^n(y_i-Œ±-Œ≤x_i)^2}$.\n",
        "\n",
        "- **Bivariate normal model** is built on the conditional normal model, but $(X_i,Y_i)$ is now a random tuple: $X_i‚àº\\Normal(Œº_X,œÉ_X^2)$, $Y_i‚àº\\Normal(Œº_Y,œÉ_Y^2)$, and $œÅ=\\t{Corr}(X_i,Y_i)$.\n",
        "\n",
        "  - $\\E[Y|x]=Œº_Y+œÅ\\/{œÉ_Y}{œÉ_X}(x-Œº_X)$\n",
        "  $=œÅ\\/{œÉ_Y}{œÉ_X}x+(Œº_Y-œÅ\\/{œÉ_Y}{œÉ_X}Œº_X)$\n",
        "  $‚áí\\blue{Œ≤=œÅ\\/{œÉ_Y}{œÉ_X}}$ and\n",
        "  $\\blue{Œ±=Œº_Y-Œ≤Œº_X}$.\n",
        "\n",
        "  - Proof:\n",
        "  $f(x,y)=\\/{1}{2œÄœÉ_XœÉ_Y\\sqrt{1-œÅ^2}}\\e{-\\/{1}{2(1-œÅ^2)}[(\\/{x-Œº_X}{œÉ_X})^2+(\\/{x-Œº_Y}{œÉ_Y})^2-2œÅ\\/{x-Œº_X}{œÉ_X}\\/{x-Œº_Y}{œÉ_Y}]}$\n",
        "  $=\\/{1}{2œÄœÉ_XœÉ_Y\\sqrt{1-œÅ^2}}\\e{E}$ where\n",
        "  $E=-\\/{Z_X^2-2œÅZ_XZ_Y+Z_Y^2}{2(1-œÅ^2)}$\n",
        "  $=-\\/{(Z_Y^2-2œÅZ_XZ_Y+œÅ^2Z_X^2)-œÅ^2Z_X^2+Z_X^2}{2(1-œÅ^2)}$\n",
        "  $=-\\/{1}{2}(\\/{Z_Y-œÅZ_X}{\\sqrt{1-œÅ^2}})^2-\\/{Z_X^2}{2}$.\n",
        "  Now factor\n",
        "  $f(x,y)=\\/{1}{\\sqrt{2œÄ}œÉ_X}\\e{-\\/{Z_X^2}{2}}‚ãÖ\\/{1}{\\sqrt{2œÄ(1-œÅ^2)}œÉ_Y}\\e{-\\/{1}{2}(\\/{Z_Y-œÅZ_X}{\\sqrt{1-œÅ^2}})^2}$\n",
        "  $=f(x)f(y|x)$.\n",
        "  Therefore\n",
        "  $f(y|x)=\\/{1}{\\sqrt{2œÄ\\green{(1-œÅ^2)œÉ_Y^2}}}\\e{-\\/{\\green{(Z_Y-œÅZ_X)^2}}{2(1-œÅ^2)}}$\n",
        "  $‚áí\\green{\\Var(Y|x)=œÉ_Y^2(1-œÅ^2)}$ and\n",
        "  $\\green{\\E[Y|x]=Œº_Y+œÅ\\/{œÉ_Y}{œÉ_X}(x-Œº_X)}$.\n",
        "\n",
        "  - With straight regression line $\\E[Y|X=x]$ and constant variance $\\Var(Y|X=x)=œÉ_Y^2(1-œÅ^2)$, the distribution of $X$ is not used and the bivariate model is the same as the conditional normal model.\n",
        "\n",
        "- **MLE**: $l(Œ±,Œ≤,œÉ^2|\\v{x},\\v{y})=-\\/{n}{2}\\lnœÉ^2-\\/{\\sum_i(y_i-Œ±-Œ≤x_i)^2}{2œÉ^2}$\n",
        "$‚áí\\green{l(Œ±,Œ≤|\\v{x},\\v{y})=-\\sum_i(y_i-Œ±-Œ≤x_i)^2=-\\t{RSS}(Œ±,Œ≤)}$.\n",
        "Therefore\n",
        "$\\arg\\max\\limits_{Œ±,Œ≤}[-\\t{RSS}(Œ±,Œ≤)]$\n",
        "$=\\arg\\min\\limits_{Œ±,Œ≤}\\t{RSS}(Œ±,Œ≤)$.\n",
        "Then\n",
        "$\\blue{\\hat{Œ±}=\\bar{y}-\\hat{Œ≤}\\bar{x}}$ and\n",
        "$\\blue{\\hat{Œ≤}=\\/{S_{xy}}{S_{xx}}}$\n",
        "are BLUE estimators.\n",
        "$l(\\hat{Œ±},\\hat{Œ≤},œÉ^2|\\v{x},\\v{y})=-\\/{n}{2}\\lnœÉ^2-\\/{\\sum_i(y_i-\\hat{Œ±}-\\hat{Œ≤}x_i)^2}{2œÉ^2}$.\n",
        "$\\/{‚àÇl}{‚àÇœÉ^2}=-\\/{n}{2œÉ^2}+\\/{\\sum_i(y_i-\\hat{Œ±}-\\hat{Œ≤}x_i)^2}{2œÉ^4}$\n",
        "$‚áí\\blue{\\hat{œÉ}^2=\\/{\\sum_i(y_i-\\hat{y}_i)^2}{n}}$.\n",
        "\n",
        "- $\\hat{œÉ}^2$ **biased estimator**: $\\E[\\hat{œÉ}^2]$\n",
        "$=\\E[\\/{\\sum_i(y_i-\\hat{Œ±}-\\hat{Œ≤}x_i)^2}{n}]$\n",
        "$=\\/{1}{n}\\sum_i\\E[\\hat{œµ}_i^2]$\n",
        "$=\\/{n-2}{n}œÉ^2$.\n",
        "Of the $n$ total degrees of freedom, $k=2$ are used for $\\hat{\\v{Œ≤}}$, and the residual space $\\red{\\hat{\\v{œµ}}=\\v{Y}-\\hat{\\v{Y}}\\t{ has $n-2$ degrees of freedom}}$.\n",
        "\n",
        "  - Proof: $\\hat{\\v{œµ}}=\\v{Y}-\\hat{\\v{Y}}$\n",
        "  $=(\\v{I}-\\v{H})\\v{Y}$.\n",
        "  $\\Var(\\hat{\\v{œµ}})=\\Var((\\v{I}-\\v{H})\\v{Y})$\n",
        "  $=(\\v{I}-\\v{H})œÉ^2$ because idempotent.\n",
        "  $\\sum_i\\E[\\hat{œµ}_i^2]=\\t{Tr}((\\v{I}-\\v{H})œÉ^2)$\n",
        "  $=œÉ^2(\\t{Tr}(\\v{I})-\\t{Tr}(\\v{H}))$\n",
        "  $=œÉ^2(\\t{Tr}(\\v{I}_n)-\\t{Tr}(\\v{I}_k))$\n",
        "  $=œÉ^2(n-2)$.\n",
        "\n",
        "  - We use estimator\n",
        "  $S^2=\\/{n}{n-2}\\hat{œÉ}^2=\\/{\\sum_i\\hat{œµ}_i^2}{n-2}$ to approximate $\\Var(œµ)$\n",
        "  $‚áí\\red{S^2=\\/{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{n-2}=\\/{\\sum_{i=1}^n(y_i-\\hat{Œ±}-\\hat{Œ≤}x_i)^2}{n-2}}$.\n"
      ],
      "metadata": {
        "id": "AMcvGOPFG5Dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix model**: $Y_i=Œ±+Œ≤x_i+œµ_i‚áí\\v{Y}=\\v{X}\\v{Œ≤}+\\v{œµ}$\n",
        "$‚áí\\BM y_1 \\\\ \\vdots \\\\ y_n \\EM\n",
        "=\\BM 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\EM\n",
        "\\BM Œ± \\\\ Œ≤ \\EM\n",
        "+\\BM œµ_1 \\\\ \\vdots \\\\ œµ_n \\EM$ where\n",
        "$\\v{œµ}‚àº\\Normal(0,œÉ^2\\v{I})$.\n",
        "Let $\\hat{Y}_i=\\hat{Œ±}+\\hat{Œ≤}x_i$ be the fitted line, then\n",
        "$\\hat{\\v{Y}}=\\v{X}\\hat{\\v{Œ≤}}$\n",
        "$‚áí\\BM \\hat{y}_1 \\\\ \\vdots \\\\ \\hat{y}_n \\EM\n",
        "=\\BM 1 & x_1 \\\\ \\vdots & \\vdots\\\\ 1 & x_n \\EM\n",
        "\\BM \\hat{Œ±} \\\\ \\hat{Œ≤} \\EM$.\n",
        "$\\v{Y}$ is a $n√ó1$ point in $‚Ñù^n$. $\\v{X}$ is a $n√ók$ matrix whose column space $C(\\v{X})$ defines a $k$-dimensional predictor plane spanned by linear combinations of its columns. $\\hat{\\v{Y}}$ is a linear combination of the columns of $\\v{X}$, which is a point that lie in $C(\\v{X})$. The difference $n-k$ is the dimension of the residual space where $\\hat{\\v{œµ}}=\\v{Y}-\\hat{\\v{Y}}$ is in.\n",
        "\n",
        "  - If $n=3$ and $k=2$, point $\\v{Y}$ is a balloon floating in the room. $C(\\v{X})$ is the floor, and $\\hat{\\v{Y}}$ is the shadow of the balloon on the floor. $n-k=1$: the height of the balloon is $\\norm{\\hat{\\v{œµ}}}$.\n",
        "\n",
        "- **MLE coefficients** $\\hat{\\v{Œ≤}}$ is derived from least squares calculations:\n",
        "$\\t{RSS}=(\\v{Y}-\\v{X}\\hat{\\v{Œ≤}})^‚ä§(\\v{Y}-\\v{X}\\hat{\\v{Œ≤}})$\n",
        "$=(\\v{Y}^‚ä§-\\hat{\\v{Œ≤}}^‚ä§\\v{X}^‚ä§)(\\v{Y}-\\v{X}\\hat{\\v{Œ≤}})$\n",
        "$=(\\v{Y}^‚ä§\\v{Y})_{\\red{1√ó1}}-(\\v{Y}^‚ä§\\v{X}\\hat{\\v{Œ≤}})_{\\red{1√ó1}}$\n",
        "$-(\\hat{\\v{Œ≤}}^‚ä§\\v{X}^‚ä§\\v{Y})_{\\red{1√ó1}}$\n",
        "$+(\\hat{\\v{Œ≤}}^‚ä§\\v{X}^‚ä§\\v{X}\\hat{\\v{Œ≤}})_{\\red{1√ó1}}$\n",
        "$=\\v{Y}^‚ä§\\v{Y}$\n",
        "$-2\\hat{\\v{Œ≤}}^‚ä§\\v{X}^‚ä§\\v{Y}$\n",
        "$+\\hat{\\v{Œ≤}}^‚ä§\\v{X}^‚ä§\\v{X}\\hat{\\v{Œ≤}}$.\n",
        "Solve by matrix calculus.\n",
        "$\\/{‚àÇ}{‚àÇ\\hat{\\v{Œ≤}}}\\t{RSS}=\\/{‚àÇ}{‚àÇ\\hat{\\v{Œ≤}}}(\\v{Y}^‚ä§\\v{Y})$\n",
        "$-2\\/{‚àÇ}{‚àÇ\\hat{\\v{Œ≤}}}(\\hat{\\v{Œ≤}}^‚ä§\\v{X}^‚ä§\\v{Y})$\n",
        "$+\\ob{\\/{‚àÇ}{‚àÇ\\hat{\\v{Œ≤}}}(\\hat{\\v{Œ≤}}^‚ä§\\v{X}^‚ä§\\v{X}\\hat{\\v{Œ≤}})}{\\/{‚àÇ}{‚àÇ\\v{x}}(\\v{x}^‚ä§\\v{A}\\v{x})=2\\v{A}\\v{x}}$\n",
        "$=\\v{0}-(2\\v{X}^‚ä§\\v{Y})_{\\red{2√ó1}}+(2\\v{X}^‚ä§\\v{X}\\hat{\\v{Œ≤}})_{\\red{2√ó1}}=\\v{0}$\n",
        "$‚áí2\\v{X}^‚ä§\\v{Y}=2\\v{X}^‚ä§\\v{X}\\hat{\\v{Œ≤}}$\n",
        "$‚áí\\blue{\\hat{\\v{Œ≤}}=(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§\\v{Y}}$.\n",
        "\n",
        "- **Hat matrix** $\\hat{\\v{Y}}=\\v{H}\\v{Y}$ projects point $\\v{Y}$ onto the plane $C(\\v{X})$ to get point $\\hat{\\v{Y}}$. Matrix $(\\v{I}-\\v{H})$ projects $\\v{Y}$ onto the residual space to get $\\hat{\\v{œµ}}$. The two are orthogonal: $\\v{H}\\hat{\\v{œµ}}=0$ and $(\\v{I}-\\v{H})\\v{X}=\\v{0}$. Start with $\\hat{\\v{Y}}=\\v{X}\\hat{\\v{Œ≤}}$\n",
        "$=\\v{X}(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§\\v{Y}$.\n",
        "Therefore\n",
        "$\\blue{\\v{H}=\\v{X}(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§}$.\n",
        "\n",
        "  - $\\v{H}$ and $\\v{I}-\\v{H}$ are projection matrices, and therefore are symmetrical and idempotent. I.e., $\\v{H}^2=\\v{H}$. $\\t{Rank}(\\v{P})=\\t{Tr}(\\v{P})$ for any idempotent symmetric matrix $\\v{P}$.\n",
        "\n",
        "  - **Trace** of a matrix is $\\t{Tr}(\\v{A})=\\sum_iA_{ii}$. Trace has cyclic property: $\\t{Tr}(\\v{ABC})=\\t{Tr}(\\v{BCA})=\\t{Tr}(\\v{CAB})$.\n",
        "  If $\\v{X}$ is $n√ók$, then the trace $\\t{Tr}(\\v{H})=\\t{Tr}(\\v{X}(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§)$\n",
        "  $=\\t{Tr}((\\v{X}^‚ä§\\v{X})(\\v{X}^‚ä§\\v{X})^{-1})$\n",
        "  $=\\t{Tr}(\\v{I}_k)$\n",
        "  $=k$ (size of $\\hat{\\v{Œ≤}}$).\n",
        "\n",
        "  - $(\\v{I}-\\v{H})\\v{X}=\\v{0}$.\n",
        "  Proof: $(\\v{I}-\\v{H})\\v{X}=\\v{X}-\\v{H}\\v{X}$\n",
        "  $=\\v{X}-\\v{X}(\\v{X}^‚ä§\\v{X})^{-1}(\\v{X}^‚ä§\\v{X})$\n",
        "  $=\\v{X}-\\v{X}\\v{I}=\\v{0}$.\n",
        "  Similarly\n",
        "  $\\v{H}\\hat{\\v{œµ}}=0$.\n",
        "  Proof: $\\v{H}\\hat{\\v{œµ}}=\\v{H}(\\v{I}-\\v{H})\\v{Y}$\n",
        "  $=(\\v{H}-\\v{H}\\v{H})\\v{Y}$\n",
        "  $=(\\v{H}-\\v{H})\\v{Y}$\n",
        "  $=\\v{0}$.\n",
        "\n",
        "- **Residual** $\\hat{\\v{œµ}}=(\\v{I}-\\v{H})\\v{Y}$. Residual sum of squares $\\t{RSS}=\\norm{\\hat{\\v{œµ}}}^2$\n",
        "$=[(\\v{I}-\\v{H})\\v{Y}]^‚ä§[(\\v{I}-\\v{H})\\v{Y}]$\n",
        "$=\\v{Y}^‚ä§(\\v{I}-\\v{H})^2\\v{Y}$\n",
        "$=\\v{Y}^‚ä§(\\v{I}-\\v{H})\\v{Y}$.\n",
        "$S^2=\\/{\\sum_{i=1}^n(y_i-\\hat{Œ±}-\\hat{Œ≤}x_i)^2}{n-2}$\n",
        "$=\\/{\\norm{\\hat{\\v{œµ}}}^2}{n-2}$\n",
        "$=\\blue{\\/{\\v{Y}^‚ä§(\\v{I}-\\v{H})\\v{Y}}{n-2}}$.\n",
        "\n",
        "- **Variance covariance matrix** for random vector $\\v{V}$ is $\\Var(\\v{V})=\\E[(\\v{V}-\\E[\\v{V}])(\\v{V}-\\E[\\v{V}])^‚ä§]$\n",
        "$=\\E[\\BM V_1-\\E[V_1] \\\\ \\vdots \\\\ V_n-\\E[V_n] \\EM\n",
        "\\BM V_1-\\E[V_1] & ... & V_n-\\E[V_n]\\EM]$\n",
        "$=\\BM \\E[(V_1-\\E[V_1])^2] & ... & \\E[(V_1-\\E[V_1])(V_n-\\E[V_n])] \\\\ \\vdots & \\ddots & \\vdots \\\\\n",
        "\\E[(V_1-\\E[V_1])(V_n-\\E[V_n])] & ... & \\E[(V_n-\\E[V_n])^2] \\EM=\\BM \\Var(V_1) & ... & \\Cov(V_1,V_n) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\ \\Cov(V_n,V_1) & ... & \\Var(V_n)\\EM$.\n",
        "\n",
        "  - $\\Var(\\v{Y})=œÉ^2\\v{I}$.\n",
        "\n",
        "  - $\\Var(\\hat{\\v{œµ}})=(\\v{I}-\\v{H})\\Var(\\v{Y})(\\v{I}-\\v{H})^‚ä§$\n",
        "  $=(\\v{I}-\\v{H})œÉ^2\\v{I}(\\v{I}-\\v{H})^‚ä§$\n",
        "  $=œÉ^2(\\v{I}-\\v{H})(\\v{I}-\\v{H})^‚ä§$\n",
        "  $=œÉ^2(\\v{I}-\\v{H})$.\n"
      ],
      "metadata": {
        "id": "kw3OvFT0wgmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression Statistics**:\n",
        "\n",
        "- $\\red{\\BC\n",
        "\\hat{Œ±}=\\bar{y}-\\hat{Œ≤}\\bar{x}‚àº\\Normal(Œ±,\\/{œÉ^2}{nS_{xx}}\\sum_{i=1}^nx_i^2) \\\\\n",
        "\\hat{Œ≤}=\\/{S_{xy}}{S_{xx}}‚àº\\Normal(Œ≤,\\/{œÉ^2}{S_{xx}}) \\\\\n",
        "\\Cov(\\hat{Œ±},\\hat{Œ≤})=-\\/{œÉ^2\\bar{x}}{S_{xx}} \\\\\n",
        "\\/{(n-2)S^2}{œÉ^2}‚àºœá_{n-2}^2 \\\\\n",
        "(\\hat{Œ±},\\hat{Œ≤})\\t{ and }S^2\\t{ are independent}\n",
        "\\EC}$\n",
        "(Casella 11.3.3)\n",
        "\n",
        "  - Proof (normality): as BLUE estimators,\n",
        "  $\\hat{Œ±}=\\sum_id_iY_i$ and\n",
        "  $\\hat{Œ≤}=\\sum_id_iY_i$ and\n",
        "  $Y_i‚àº\\Normal(Œ±+Œ≤x_i,œÉ^2)$.\n",
        "\n",
        "  - Proof ($\\hat{Œ≤}$): $\\Var(\\hat{Œ≤})=\\Var(\\sum_id_iY_i)$\n",
        "  $=œÉ^2\\sum_id_i^2$\n",
        "  $=\\/{œÉ^2}{S_{xx}}$.\n",
        "\n",
        "  - Proof ($\\hat{Œ±}$): $\\Var(\\hat{Œ±})=\\Var(\\bar{y}-\\hat{Œ≤}\\bar{x})$\n",
        "  $=\\Var(\\bar{y})+\\bar{x}^2\\Var(\\hat{Œ≤})-2\\bar{x}\\Cov(\\bar{y},\\hat{Œ≤})$.\n",
        "  $\\Cov(\\bar{y},\\hat{Œ≤})=\\sum_i\\sum_j\\Cov(\\/{1}{n}Y_i,d_{bj}Y_j)$\n",
        "  $=\\sum_i\\Cov(\\/{1}{n}Y_i,d_{bi}Y_i)$\n",
        "  $=\\sum_i\\/{œÉ^2}{n}d_{bi}$\n",
        "  $=0$.\n",
        "  $\\Var(\\hat{Œ±})=\\Var(\\bar{y})+\\bar{x}^2\\Var(\\hat{Œ≤})$\n",
        "  $=\\/{œÉ^2}{n}+\\/{\\bar{x}^2œÉ^2}{S_{xx}}$\n",
        "  $=\\/{œÉ^2}{nS_{xx}}[S_{xx}+n\\bar{x}^2]$\n",
        "  $=\\/{œÉ^2}{nS_{xx}}[\\sum_i(x_i-\\bar{x})^2+n\\bar{x}^2]$\n",
        "  $=\\/{œÉ^2}{nS_{xx}}\\sum_ix_i^2$.\n",
        "\n",
        "  - Proof (Cov): $\\Cov(\\hat{Œ±},\\hat{Œ≤})$\n",
        "  $=\\sum_i\\sum_j\\Cov(d_{ai}Y_i,d_{bj}Y_j)$\n",
        "  $=\\sum_i\\Cov(d_{ai}Y_i,d_{bi}Y_i)$\n",
        "  $=œÉ^2\\sum_id_{ai}d_{bi}$\n",
        "  $=œÉ^2\\sum_i\\/{x_i-\\bar{x}}{S_{xx}}(\\/{1}{n}-\\/{\\bar{x}(x_i-\\bar{x})}{S_{xx}})$\n",
        "  $=\\/{œÉ^2}{S_{xx}}\\sum_i\\/{S_{xx}(x_i-\\bar{x})-n\\bar{x}(x_i-\\bar{x})^2}{nS_{xx}}$\n",
        "  $=\\/{œÉ^2}{S_{xx}}\\/{-n\\bar{x}S_{xx}}{nS_{xx}}$\n",
        "  $=-\\/{œÉ^2\\bar{x}}{S_{xx}}$.\n",
        "\n",
        "  - Proof ($S^2$): $S^2=\\/{\\t{SSE}(\\hat{\\v{Œ≤}})}{n-2}=\\/{\\sum_i\\hat{e}_i^2}{n-2}=\\/{\\norm{\\hat{\\v{œµ}}}^2}{n-2}$\n",
        "  where $\\hat{\\v{œµ}}=\\v{Y}-\\hat{\\v{Y}}=(\\v{I}-\\v{H})\\v{Y}$.\n",
        "  Then by $(\\v{I}-\\v{H})\\v{X}=\\v{0}$ orthogonality,\n",
        "  $\\norm{\\hat{\\v{œµ}}}^2=\\v{Y}^‚ä§(\\v{I}-\\v{H})\\v{Y}$\n",
        "  $=(\\v{X}\\v{Œ≤}+\\v{œµ})^‚ä§(\\v{I}-\\v{H})(\\v{X}\\v{Œ≤}+\\v{œµ})$\n",
        "  $=\\v{œµ}^‚ä§(\\v{I}-\\v{H})\\v{œµ}$\n",
        "  $‚áí\\/{\\norm{\\hat{\\v{œµ}}}^2}{œÉ^2}=(\\/{\\v{œµ}}{œÉ^2})^‚ä§(\\v{I}-\\v{H})(\\/{\\v{œµ}}{œÉ^2})$\n",
        "  $=\\v{Z}^‚ä§(\\v{I}-\\v{H})\\v{Z}$.\n",
        "  Because idempotent, $\\t{Rank}(\\v{I}-\\v{H})=\\t{Tr}(\\v{I}-\\v{H})$\n",
        "  $=\\t{Tr}(\\v{I})-\\t{Tr}(\\v{H})=n-2$.\n",
        "  Then by Cochran's corollary, $\\/{(n-2)S^2}{œÉ^2}=\\v{Z}^‚ä§(\\v{I}-\\v{H})\\v{Z}‚àºœá_{n-2}^2$.\n",
        "\n",
        "  - Proof ($\\hat{\\v{Œ≤}}$ vs $S^2$): $\\hat{\\v{Œ≤}}=(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§\\v{Y}=\\v{L}\\v{Y}$ and $(n-2)S^2=\\v{Y}^‚ä§(\\v{I}-\\v{H})\\v{Y}=\\v{Y}^‚ä§\\v{A}\\v{Y}$ are independent iff $\\v{L}\\v{A}=\\v{0}$ (Craig's Theorem).\n",
        "  $((\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§)(\\v{I}-\\v{H})$\n",
        "  $=(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§-(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§(\\v{X}(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§)$\n",
        "  $=(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§-(\\v{X}^‚ä§\\v{X})^{-1}(\\v{X}^‚ä§\\v{X})(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§$\n",
        "  $=(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§-(\\v{X}^‚ä§\\v{X})^{-1}\\v{X}^‚ä§=\\v{0}$.\n",
        "\n",
        "- $\\red{t_{n-2}^2=F_{1,n-2}}$. $\\blue{T=\\/{\\hat{Œ≤}-Œ≤_{H_0}}{S/\\sqrt{S_{xx}}}‚àºt_{n-2}‚áîF=(\\/{\\hat{Œ≤}-Œ≤_{H_0}}{S/\\sqrt{S_{xx}}})^2‚àºF_{1,n-2}}$.\n",
        "\n",
        "  - $1-Œ±$ confidence interval: $\\green{\\hat{Œ≤}-t_{n-2,Œ±/2}\\/{S}{\\sqrt{S_{xx}}}< Œ≤< \\hat{Œ≤}+t_{n-2,Œ±/2}\\/{S}{\\sqrt{S_{xx}}}}$ by pivoting $T$\n",
        "\n",
        "  - Testing $H_0:Œ≤=Œ≤_0$: $\\green{\\left|\\/{\\hat{Œ≤}-Œ≤_0}{S/\\sqrt{S_{xx}}}\\right|>t_{n-2,Œ±/2}}$\n",
        "\n",
        "**Regression vs ANOVA**: ANOVA is considered a special case of regression by the following setup. Let $x=\\{0,1\\}$, $Œ±=Œ∏_1$, $Œ≤=Œ∏_2-Œ∏_1$, $Y=\\BC Œ∏_1 &\\t{if }x=0\\\\ Œ∏_2 &\\t{if }x=1\\EC$. Then ANOVA $H_0:Œ∏_1=Œ∏_2$ is equivalent to regression $H_0:Œ≤=0$ with $n=k=2$ groups.\n",
        "\n",
        "- Regression $F=\\/{\\hat{Œ≤}^2}{S^2/S_{xx}}=\\/{\\blue{S_{xy}^2/S_{xx}}}{S^2}=\\/{\\blue{\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}}{S^2}‚àºF_{1,n-2}$ vs ANOVA $F=\\/{\\sum_{i=1}^kn_i(\\bar{Y}_{i‚ãÖ}-\\bar{Y})^2}{S_p^2}‚àº(k-1)F_{k-1,N-k}$\n",
        "\n",
        "  - Proof:\n",
        "  $\\sum_i(\\hat{y}_i-\\bar{y})^2=\\sum_i(\\hat{Œ±}+\\hat{Œ≤}x_i-\\bar{y})^2$\n",
        "  $=\\sum_i(\\bar{y}-\\hat{Œ≤}\\bar{x}+\\hat{Œ≤}x_i-\\bar{y})^2$\n",
        "  $=\\sum_i(\\hat{Œ≤}(x_i-\\bar{x}))^2$\n",
        "  $=\\/{S_{xy}^2}{S_{xx}}$.\n",
        "\n",
        "- Cochran's Sum of Squares (SS) partition: $\\suml_{i=1}^n(y_i-\\bar{y})^2=\\suml_{i=1}^n(\\hat{y}_i-\\bar{y})^2+\\suml_{i=1}^n(y_i-\\hat{y}_i)^2$\n",
        "\n",
        "  - $\\BC\n",
        "  SS_{\\t{total}}=\\sum_{i=1}^n(y_i-\\bar{y})^2 & \\/{\\sum_{i=1}^n(y_i-\\bar{y})^2}{œÉ^2}‚àºœá_{n-1}^2 & \\t{total variation}\\\\\n",
        "  SS_{\\t{residual}}=\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 & \\/{\\sum_{i=1}^n\\hat{œµ}_i^2}{œÉ^2}‚àºœá_{n-2}^2 & \\t{residual variation}\\\\\n",
        "  SS_{\\t{regression}}=\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2 & \\/{S_{xy}^2/S_{xx}}{œÉ^2}‚àºœá_1^2 & \\t{regression variation}\n",
        "  \\EC$\n",
        "\n",
        "- **Coefficient of determination** $\\red{r^2=\\/{SS_{\\t{regression}}}{SS_{\\t{total}}}=\\/{\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}=\\/{S_{xy}^2}{S_{xx}S_{yy}}}$\n",
        "\n",
        "  - Correlation $œÅ_{X,Y}=\\/{\\Cov(X,Y)}{\\sqrt{\\Var(X)\\Var(Y)}}$\n",
        "  $=\\/{\\E[(X-Œº_X)(Y-Œº_Y)]}{\\sqrt{\\E[(X-Œº_X)^2]\\E[(Y-Œº_Y)^2]}}$.\n",
        "  $r_{\\v{X},\\v{Y}}=\\/{\\/{\\sum_i(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}}{\\sqrt{\\/{\\sum_i(x_i-\\bar{x})^2}{n-1}\\/{\\sum_i(y_i-\\bar{y})^2}{n-1}}}$\n",
        "  $=\\/{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}$"
      ],
      "metadata": {
        "id": "5fKyHsbc2zta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression Prediction:** A $1-Œ±$ **prediction interval** for unobserved random variable $Y_0$ based on observed data $\\v{X}$ is random interval $[L(\\v{X}),U(\\v{X})]$ that satisfies property $P_Œ∏(L(\\v{X})‚â§Y_0‚â§U(\\v{X}))‚â•1-Œ±$ for all values of $Œ∏$.\n",
        "(Casella 11.3.5)\n",
        "\n",
        "- Prediction interval is confidence interval that covers a random variable $Y‚àà[L,U]$ instead of a parameter $Œ∏‚àà[L,U]$.\n",
        "\n",
        "- Let $(Y|x=x_0)‚àº\\Normal(Œ±+Œ≤x_0,œÉ^2)$ represent population of observations at $x=x_0$. Then the estimator for the population mean $\\E[Y|x=x_0]=Œ±+Œ≤x_0$ is\n",
        "$\\red{\\hat{Y}_0=\\hat{Œ±}+\\hat{Œ≤}x_0‚àº\\Normal(Œ±+Œ≤x_0,œÉ^2[\\/{1}{n}+\\/{(x_0-\\bar{x})^2}{S_{xx}}])}$.\n",
        "\n",
        "  - Proof: $\\Var(\\hat{Œ±}+\\hat{Œ≤}x_0)$\n",
        "  $=\\Var(\\hat{Œ±})+x_0^2\\Var(\\hat{Œ≤})+2x_0\\Cov(\\hat{Œ±},\\hat{Œ≤})$\n",
        "  $=\\/{œÉ^2}{nS_{xx}}\\sum_ix_i^2+\\/{œÉ^2x_0^2}{S_{xx}}-\\/{2x_0œÉ^2\\bar{x}}{S_{xx}}$\n",
        "  $=\\/{œÉ^2}{S_{xx}}[\\/{\\sum_ix_i^2}{n}+x_0^2-2x_0\\bar{x}]$\n",
        "  $=\\/{œÉ^2}{S_{xx}}[\\/{1}{n}(\\sum_ix_i^2-n\\bar{x}^2)+\\bar{x}^2+x_0^2-2x_0\\bar{x}]$\n",
        "  $=\\/{œÉ^2}{S_{xx}}[\\/{1}{n}S_{xx}+(x_0-\\bar{x})^2]$\n",
        "  $=œÉ^2[\\/{1}{n}+\\/{(x_0-\\bar{x})^2}{S_{xx}}]$.\n",
        "\n",
        "  - $\\blue{T=\\/{\\hat{Œ±}+\\hat{Œ≤}x_0-Œ±-Œ≤x_0}{S\\sqrt{\\/{1}{n}+\\/{(x_0-\\bar{x})^2}{S_{xx}}}}‚àºt_{n-2}}$\n",
        "  gives *confidence interval*\n",
        "  $Œ±+Œ≤x_0‚àà[\\green{\\hat{Œ±}+\\hat{Œ≤}x_0\\red{¬±}t_{n-2,Œ±/2}S\\sqrt{\\/{1}{n}+\\/{(x_0-\\bar{x})^2}{S_{xx}}}}]$.\n",
        "  The length of the interval is shortest if $x_0$ is near $\\bar{x}$.\n",
        "\n",
        "- Let $Y_0=Œ±+Œ≤x_0+œµ‚àº\\Normal(Œ±+Œ≤x_0,œÉ^2)$ be one observation to be taken at $x=x_0$, then $Y_0-\\hat{Y}_0$ is the **prediction error** and\n",
        "$\\red{Y_0-\\hat{Y}_0‚àº\\Normal(0,œÉ^2[1+\\/{1}{n}+\\/{(x_0-\\bar{x})^2}{S_{xx}}])}$.\n",
        "\n",
        "  - Proof: $\\Var(Y_0-\\hat{Y}_0)=\\Var(Y_0)+\\Var(\\hat{Œ±}+\\hat{Œ≤}x_0)$\n",
        "  $=œÉ^2[1+\\/{1}{n}+\\/{(x_0-\\bar{x})^2}{S_{xx}}]$. Independence $\\Cov(Y_0,\\hat{Y}_0)=0$ is because $(\\hat{Œ±},\\hat{Œ≤})$ are calculated from older data and $Y_0$ is yet to be observed.\n",
        "\n",
        "  - $\\blue{T=\\/{Y_0-\\hat{Œ±}+\\hat{Œ≤}x_0}{S\\sqrt{1+\\/{1}{n}+\\/{(x_0-\\bar{x})^2}{S_{xx}}}}‚àºt_{n-2}}$\n",
        "  gives *prediction interval*\n",
        "  $Y_0‚àà[\\green{\\hat{Œ±}+\\hat{Œ≤}x_0\\red{¬±}t_{n-2,Œ±/2}S\\sqrt{1+\\/{1}{n}+\\/{(x_0-\\bar{x})^2}{S_{xx}}}}]$.\n",
        "  The interval for one observation is wider than interval for population mean.\n",
        "\n",
        "**Simultaneously multiple estimates**: if we simultaneously take $1-Œ±$ confidence intervals for multiple population means $\\E[Y|x=x_{0i}]$ for $i=1..m$, then overall confidence is lower $P(\\bigcap\\limits_{i=1}^mC_i)‚â•1-\\suml_{i=1}^m(1-Œ±)$ by Bonferroni Inequality.\n",
        "\n",
        "- **Bonferroni correction** is to use $t_{n-2,Œ±/(2m)}$ for critical value instead of $t_{n-2,Œ±/2}$. This results in wider intervals:\n",
        "$Œ±+Œ≤x_{0i}‚àà[\\blue{\\hat{Œ±}+\\hat{Œ≤}x_{0i}\\red{¬±}t_{n-2,Œ±/(2m)}S\\sqrt{\\/{1}{n}+\\/{(x_{0i}-\\bar{x})^2}{S_{xx}}}}]$\n",
        "\n",
        "  - all-purpose and usually used for small number of points.\n",
        "\n",
        "- **Scheff√© Band**: Under conditional normal regression model, the probability is at least $1-Œ±$ that simultaneously for all $x$,\n",
        "$Œ±+Œ≤x‚àà[\\blue{\\hat{Œ±}+\\hat{Œ≤}x\\red{¬±}M_Œ±S\\sqrt{\\/{1}{n}+\\/{(x-\\bar{x})^2}{S_{xx}}}}]$\n",
        "where\n",
        "$\\blue{M_Œ±=\\sqrt{2F_{2,n-2,Œ±}}}$.\n",
        "(Casella 11.3.6)\n",
        "\n",
        "  - Proof:\n",
        "  $P(\\/{((\\hat{Œ±}+\\hat{Œ≤}x)-(Œ±+Œ≤x))^2}{S^2[\\/{1}{n}+\\/{(x-\\bar{x})^2}{S_{xx}}]}‚â§M_Œ±^2\\ ‚àÄx)‚â§1-Œ±$\n",
        "  $‚áîP(\\max\\limits_{\\green{x}}\\/{((\\hat{Œ±}+\\hat{Œ≤}\\green{x})-(Œ±+Œ≤\\green{x}))^2}{S^2[\\/{1}{n}+\\/{(\\green{x}-\\bar{x})^2}{S_{xx}}]}‚â§M_Œ±^2)‚â§1-Œ±$.\n",
        "  We want to use the $F_{p,q}$-pivot to solve for critical value $M_Œ±$, but we can't find $p$ because $\\hat{Œ±}$ and $\\hat{Œ≤}$ are dependent so the numerator quadratic forms cannot be summed.\n",
        "  The fitted line $\\hat{Œ±}+\\hat{Œ≤}x$ goes through point $(\\bar{x},\\bar{Y})$ i.e.,\n",
        "  $\\bar{Y}=\\hat{Œ±}+\\hat{Œ≤}\\bar{x}$\n",
        "  $‚áí\\green{\\hat{Œ±}+\\hat{Œ≤}x=\\bar{Y}+\\hat{Œ≤}(x-\\bar{x})}$, which are independent.\n",
        "  Then\n",
        "  $\\/{(\\hat{Œ±}+\\hat{Œ≤}x-Œ±-Œ≤x)^2}{S^2[\\/{1}{n}+\\/{(x-\\bar{x})^2}{S_{xx}}]}$\n",
        "  $=\\/{(\\bar{Y}+\\hat{Œ≤}(x-\\bar{x})-Œ±-Œ≤\\bar{x}-Œ≤(x-\\bar{x}))^2}{S^2[\\/{1}{n}+\\/{(x-\\bar{x})^2}{S_{xx}}]}$\n",
        "  $=\\/{((\\bar{Y}-Œ±-Œ≤\\bar{x})+(\\hat{Œ≤}-Œ≤)(x-\\bar{x}))^2}{S^2[\\/{1}{n}+\\/{(x-\\bar{x})^2}{S_{xx}}]}$\n",
        "  $‚áí\\max\\limits_{\\green{t}}\\/{((\\bar{Y}-Œ±-Œ≤\\bar{x})+(\\hat{Œ≤}-Œ≤)\\green{t})^2}{S^2[\\/{1}{n}+\\/{\\green{t}^2}{S_{xx}}]}$ is the new pivot.\n",
        "  Let $a=\\bar{Y}-Œ±-Œ≤\\bar{x}$,\n",
        "  $b=\\hat{Œ≤}-Œ≤$,\n",
        "  $c=\\/{1}{n}$, and\n",
        "  $d=\\/{1}{S_{xx}}$.\n",
        "  Then\n",
        "  $g(t)=2\\ln(a+bt)-\\ln(c+dt^2)$\n",
        "  $‚áí\\/{‚àÇg}{‚àÇt}=\\/{2b}{a+bt}-\\/{2dt}{c+dt^2}=0$\n",
        "  $‚áíbc+bdt^2=adt+bdt^2$.\n",
        "  $\\/{‚àÇ^2g}{‚àÇt^2}=-\\/{2b^2}{(a+bt)^2}-\\/{2d}{c+dt^2}+\\/{(2dt)^2}{(c+dt^2)^2}< 0$\n",
        "  $‚áí\\green{t_\\max=\\/{bc}{ad}}$.\n",
        "  Therefore\n",
        "  $\\max\\limits_{t}\\/{(a+bt)^2}{S^2[c+dt^2]}$\n",
        "  $=\\/{a^2(1+\\/{b}{a}t)^2}{S^2[c+dt^2]}$\n",
        "  $=\\/{a^2(1+\\/{d}{c}t^2)^2}{S^2[c+dt^2]}$\n",
        "  $=\\/{\\/{a^2}{c^2}(c+dt^2)^2}{S^2[c+dt^2]}$\n",
        "  $=\\/{a^2(c+dt^2)}{c^2S^2}$\n",
        "  $=\\/{a^2}{cS^2}+\\/{b^2}{dS^2}$\n",
        "  $=\\/{n(\\bar{Y}-Œ±-Œ≤\\bar{x})^2+S_{xx}(\\hat{Œ≤}-Œ≤)^2}{S^2}$\n",
        "  $=\\/{\\/{(\\bar{Y}-Œ±-Œ≤\\bar{x})^2}{œÉ^2/n}+\\/{(\\hat{Œ≤}-Œ≤)^2}{œÉ^2/S_{xx}}}{S^2/œÉ^2}‚àº\\/{œá_1^2+œá_1^2}{œá_{n-2}^2/(n-2)}$\n",
        "  $=2F_{2,n-2}$.\n",
        "\n",
        "  - Bonferroni interval is wider than Scheff√© band when $t_{n-2,Œ±/(2m)}>\\sqrt{2F_{2,n-2,Œ±}}$."
      ],
      "metadata": {
        "id": "NKv9EMfUQM2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression with Errors in Variables**: we observe $(X_i,Y_i)$ pairs according to $\\BC\n",
        "Y_i=Œ±+Œ≤Œæ_i+œµ_i &œµ_i‚àº\\Normal(0,œÉ_œµ^2) \\\\\n",
        "X_i=Œæ_i+Œ¥_i &Œ¥_i‚àº\\Normal(0,œÉ_Œ¥^2)\\EC$ whose means $\\E[Y_i]=Œ∑_i=Œ±+Œ≤Œæ_i$ and $\\E[X_i]=Œæ_i$ are unobserved **latent variables**. $Œæ_i$ is the true unobserved predictor, and $Œ¥_i$ is the measurement error in predictor $X_i$.\n",
        "\n",
        "- **Total Least Squares**: ordinary LS minimizing vertical distance assumes predictor variables $x$ have no error, and all errors are confined to response variables $y$. Total LS treats $x$ and $y$ equitably.\n",
        "\n",
        "  - Let $(\\hat{x}_i,\\hat{y}_i)$ be orthogonal projection of observed $(x_i,y_i)$ onto line $y=a+bx$ with direction $(1,b)$. The orthogonal line with direction $(-b,1)$ through $(x_i,y_i)$ is\n",
        "  $y=y_i+\\/{1}{b}x_i-\\/{1}{b}x$.\n",
        "  The two lines intersect at\n",
        "  $y_i+\\/{1}{b}x_i-\\/{1}{b}x=a+bx$\n",
        "  $‚áí(\\hat{x}_i,\\hat{y}_i)=(\\/{by_i+x_i-ab}{b^2+1},a+b\\/{by_i+x_i-ab}{b^2+1})$.\n",
        "  Then the **total least squares** error is\n",
        "  $\\t{TLS}=\\sum_{i=1}^n((x_i-\\hat{x}_i)^2+(y_i-\\hat{y}_i)^2)$\n",
        "  $=\\sum_{i=1}^n((\\/{b^2x_i-by_i+ab}{b^2+1})^2+(\\/{y_i-a-bx_i}{b^2+1})^2)$\n",
        "  $=\\sum_{i=1}^n(\\/{b^2}{(b^2+1)^2}+\\/{1}{(b^2+1)^2})(y_i-bx_i-a)^2$\n",
        "  $=\\green{\\/{1}{b^2+1}\\sum_{i=1}^n(y_i-(a+bx_i))^2}$,\n",
        "  which is minimized when $(a+bx_i)=\\bar{y}$\n",
        "  $‚áí\\blue{a_\\t{TLS}=\\bar{y}-b\\bar{x}}$.\n",
        "  Therefore\n",
        "  $\\t{TLS}=\\/{1}{b^2+1}\\sum_{i=1}^n(y_i-(\\bar{y}-b\\bar{x}+bx_i))^2$\n",
        "  $=\\/{1}{b^2+1}\\sum_{i=1}^n((y_i-\\bar{y})-b(x_i-\\bar{x}))^2$\n",
        "  $=\\/{1}{b^2+1}[S_{yy}-2bS_{xy}+b^2S_{xx}]$.\n",
        "  $\\/{‚àÇ\\t{TLS}}{‚àÇb}=\\/{-2}{(b^2+1)^2}[S_{yy}-2bS_{xy}+b^2S_{xx}]+\\/{1}{b^2+1}[-2S_{xy}+2bS_{xx}]=0$\n",
        "  $‚áí\\blue{b_\\t{TLS}=\\/{-(S_{xx}-S_{yy})+\\sqrt{(S_{xx}-S_{yy})^2+4S_{xy}^2}}{2S_{xy}}}$.\n",
        "\n",
        "  - The TLS regression lines lies in between ordinary LS regression lines of $y$ on $x$ and $x$ on $y$.\n",
        "\n",
        "- **EIV MLE**: $L(Œ±,Œ≤,Œæ_{1,...,n},œÉ_Œ¥^2,œÉ_œµ^2|(\\v{x},\\v{y})=\\/{1}{(2œÄ)^n(œÉ_Œ¥^2œÉ_œµ^2)^{n/2}}\\e{-\\suml_{i=1}^n\\/{(x_i-Œæ_i)^2}{2œÉ_Œ¥^2}}\\e{-\\suml_{i=1}^n\\/{(y_i-(Œ±+Œ≤Œæ_i))^2}{2œÉ_œµ^2}}$\n",
        "does not have a finite maximum. To get around this problem we bind the errors $œÉ_Œ¥^2=ŒªœÉ_œµ^2$ where $Œª>0$ is fixed and known.\n",
        "Then we have\n",
        "$L(Œ±,Œ≤,Œæ_{1,...,n},œÉ_Œ¥^2|(\\v{x},\\v{y}))=\\/{Œª^{n/2}}{(2œÄœÉ_Œ¥^2)^n}\\e{-\\suml_{i=1}^n\\/{(x_i-Œæ_i)^2+Œª(y_i-(Œ±+Œ≤Œæ_i))^2}{sœÉ_Œ¥^2}}$.\n",
        "Find $Œæ_i^*=\\arg\\max\\limits_{Œæ_i}L=\\arg\\min\\limits_{Œæ_i}[(x_i-Œæ_i)^2+Œª(y_i-(Œ±+Œ≤Œæ_i))^2]$\n",
        "$=\\/{x_i+ŒªŒ≤(y_i-Œ±)}{1+ŒªŒ≤^2}$ by derivative and substitute back to get likelihood\n",
        "$L(Œ±,Œ≤,œÉ_Œ¥^2|\\v{x},\\v{y})=\\/{Œª^{n/2}}{(2œÄœÉ_Œ¥^2)^n}\\e{-\\/{1}{2œÉ_Œ¥^2}\\/{Œª}{1+ŒªŒ≤^2}\\suml_{i=1}^n(y_i-(Œ±-Œ≤x_i))^2}$\n",
        "$=\\/{Œª^{n/2}}{(2œÄœÉ_Œ¥^2)^n}\\e{-\\/{1}{2œÉ_Œ¥^2}[\\green{\\/{1}{1+Œ≤^2}\\suml_{i=1}^n(y_i^*-(Œ±^*-Œ≤^*x_i))^2}]}$\n",
        "where $Œ±^*=\\sqrt{Œª}Œ±$, $Œ≤^*=\\sqrt{Œª}Œ≤$, and $y_i^*=\\sqrt{Œª}y_i$.\n",
        "Therefore\n",
        "$\\blue{\\hat{Œ±}=\\bar{y}-\\hat{Œ≤}\\bar{x}}$ and\n",
        "$\\blue{\\hat{Œ≤}=\\/{-(S_{xx}-ŒªS_{yy})+\\sqrt{(S_{xx}-ŒªS_{yy})^2+4ŒªS_{xy}^2}}{2ŒªS_{xy}}}$"
      ],
      "metadata": {
        "id": "yfkkHRisSKr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**: Given data $(x_1,Y_1),...,(x_n,Y_n)$ where $Y_i‚àº\\t{Bernoulli}(œÄ_i)$ with unobservable latent variable $\\E[Y_i|x_i]=œÄ_i$. Where conditional normal regression models the conditional mean of a continuous outcome, logistic regression models the conditional probability of a binary outcome\n",
        "$œÄ_i=P(Y_i=1|x_i)=\\/{e^{Œ±+Œ≤x_i}}{1+e^{Œ±+Œ≤x_i}}$\n",
        "\n",
        "- The **logit** link function $\\red{\\ln(\\/{œÄ_i}{1-œÄ_i})=Œ±+Œ≤x_i}$ transforms $œÄ_i‚àà[0,1]$ to log-odds $\\ln(\\/{œÄ_i}{1-œÄ_i})‚àà‚Ñù$\n",
        "\n",
        "  - Logistic regression is linear regression of the logit link against $x$: $\\E[Y_i|x_i]=Œ±+Œ≤x_i$ from simple linear regression becomes $\\ln(\\/{œÄ_i}{1-œÄ_i})=Œ±+Œ≤x_i$. $Œ±$ is the log-odds at $x=0$, and $Œ≤$ is the slope of log-odds with respect to $x$. The regression line crosses the $x$-axis at $œÄ_i=0.5$.\n",
        "\n",
        "  - The logit link function is called a canonical link because it is the natural parameter of the Bernoulli exponential family\n",
        "  $f(y_i|œÄ_i)=œÄ_i^{y_i}(1-œÄ_i)^{1-y_i}=(1-œÄ_i)\\e{y_i\\ln(\\/{œÄ_i}{1-œÄ_i})}$.\n",
        "\n",
        "- **Logistic distribution** has PDF $\\blue{f(x)=\\/{e^{-x}}{(1+e^{-x})^2}}$ and CDF $\\blue{F(x)=\\/{e^{x}}{1+e^{x}}=\\/{1}{1+e^{-x}}}$.\n",
        "The inverse of logit is the sigmoidal\n",
        "$\\red{œÄ(x)=\\/{e^{Œ±+Œ≤x}}{1+e^{Œ±+Œ≤x}}}=F(Œ±+Œ≤x)$, which is a monotone function of $x$ with slope $\\/{dœÄ(x)}{dx}=Œ≤œÄ(x)(1-œÄ(x))$.\n",
        "\n",
        "  - The logit link is modeled to be a linear function of $x$. The latent probability is modeled to be a sigmoidal function of $x$, using the logit CDF mostly only for its sigmoidal shape.\n",
        "\n",
        "  - Unobserved error term logistic vs probit\n",
        "\n",
        "- **MLE**: Let $œÄ(x_i)=\\green{F_i=F(Œ±+Œ≤x_i)}$, then\n",
        "$l(Œ±,Œ≤|y_i)=\\ln(1-œÄ)+y_i\\ln(\\/{œÄ}{1-œÄ})$\n",
        "$=\\ln(1-F_i)+y_i\\ln(\\/{F_i}{1-F_i})$.\n",
        "Then\n",
        "$\\purple{\\/{‚àÇl}{‚àÇŒ±}}=\\/{‚àÇl}{‚àÇF_i}\\/{‚àÇF_i}{‚àÇŒ±}$\n",
        "$=\\/{-f_i}{1-F_i}+y_i\\/{f_i/(1-F_i)+f_iF_i/(1-F_i)^2}{F_i/(1-F_i)}$\n",
        "$=\\/{-f_iF_i}{F_i(1-F_i)}+y_i\\/{f_i(1-F_i)+f_iF_i}{F_i(1-F_i)}$\n",
        "$=\\/{f_i(y_i-F_i)}{F_i(1-F_i)}$\n",
        "$=y_i-F_i$ and\n",
        "$\\purple{\\/{‚àÇl}{‚àÇŒ≤}}=\\/{‚àÇl}{‚àÇF_i}\\/{‚àÇF_i}{‚àÇŒ≤}$\n",
        "$=\\/{f_ix_i(y_i-F_i)}{F_i(1-F_i)}$\n",
        "$=x_i(y_i-F_i)$.\n",
        "\n",
        "  - Logit is canonical link in Bernoulli/binomial:\n",
        "  $F_i(1-F_i)=\\/{1}{1+e^{-x}}\\/{e^{-x}}{1+e^{-x}}$\n",
        "  $=\\/{e^{-x}}{(1+e^{-x})^2}=f_i$\n",
        "  $‚áí\\green{\\/{f_i}{F_i(1-F_i)}=1}$ is the reason why logistic CDF is commonly used instead of normal CDF (probit regression).\n",
        "\n",
        "  - Score equations:\n",
        "  $\\green{\\/{‚àÇl(Œ±,Œ≤|\\v{y})}{‚àÇŒ±}=\\sum_i(y_i-F_i)=0}$ and $\\green{\\/{‚àÇl(Œ±,Œ≤|\\v{y})}{‚àÇŒ≤}=\\sum_ix_i(y_i-F_i)=0}$.\n",
        "\n",
        "- **Grouped data MLE**: At each $x_{j=1..J}$, we observe $Y_j^*‚àº\\Binom(n_j,œÄ(x_j))$. Let $œÄ(x_j)=\\green{F_j=F(Œ±+Œ≤x_j)}$ then\n",
        "$L(Œ±,Œ≤|Y_j^*)=\\binom{n_j}{Y_j^*}(1-œÄ(x_j))^{n_j}\\e{Y_j^*\\ln(\\/{œÄ(x_j)}{1-œÄ(x_j)})}$\n",
        "$‚áíl(Œ±,Œ≤|Y_j^*)=n_j\\ln(1-F_j)+Y_j^*\\ln(\\/{F_j}{1-F_j})$.\n",
        "$\\purple{\\/{‚àÇl}{‚àÇŒ±}}=y_j-n_jF_j$.\n",
        "$\\purple{\\/{‚àÇl}{‚àÇŒ≤}}=x_j(y_j-n_jF_j)$.\n",
        "$\\purple{\\/{‚àÇ^2l}{‚àÇŒ±^2}}=-n_jf_j=-n_jF_j(1-F_j)$.\n",
        "$\\purple{\\/{‚àÇ^2l}{‚àÇŒ≤^2}}=-x_j^2n_jF_j(1-F_j)$.\n",
        "$\\purple{\\/{‚àÇ^2l}{‚àÇŒ±‚àÇŒ≤}}=-x_jn_jF_j(1-F_j)$.\n",
        "\n",
        "  - Score equations: $\\green{\\/{‚àÇl(Œ±,Œ≤|\\v{y})}{‚àÇŒ±}=\\suml_{j=1}^J(Y_j^*-n_jF_j)=0}$ and\n",
        "  $\\green{\\/{‚àÇl(Œ±,Œ≤|\\v{y})}{‚àÇŒ≤}=\\suml_{j=1}^Jx_j(Y_j^*-n_jF_j)=0}$.\n",
        "\n",
        "  - Fisher information (expected):\n",
        "  $I_n(Œ±,Œ≤)=\\BM\n",
        "  -\\/{‚àÇ^2l}{‚àÇŒ±^2} & -\\/{‚àÇ^2l}{‚àÇŒ±‚àÇŒ≤} \\\\\n",
        "  -\\/{‚àÇ^2l}{‚àÇŒ±‚àÇŒ≤} & -\\/{‚àÇ^2l}{‚àÇŒ≤^2} \\EM$\n",
        "  $=\\BM\n",
        "  \\suml_{j=1}^Jn_jF_j(1-F_j) & \\suml_{j=1}^Jx_jn_jF_j(1-F_j) \\\\\n",
        "  \\suml_{j=1}^Jx_jn_jF_j(1-F_j) & \\suml_{j=1}^Jx_j^2n_jF_j(1-F_j)\n",
        "  \\EM$\n",
        "\n",
        "  - MLEs are asymptotically efficient: $\\Var(\\hat{\\v{Œ≤}})‚âàI_n(Œ±,Œ≤)^{-1}$."
      ],
      "metadata": {
        "id": "9X3JBapGLC3O"
      }
    }
  ]
}